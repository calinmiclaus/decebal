diff -Naur gcc-3.4.4/gcc/calls.c gcc-3.4.4-ssp/gcc/calls.c
--- gcc-3.4.4/gcc/calls.c	2005-04-07 00:01:44.000000000 +0300
+++ gcc-3.4.4-ssp/gcc/calls.c	2005-05-25 14:03:21.000000000 +0300
@@ -2321,8 +2321,12 @@
 	  {
 	    /* For variable-sized objects, we must be called with a target
 	       specified.  If we were to allocate space on the stack here,
-	       we would have no way of knowing when to free it.  */
-	    rtx d = assign_temp (TREE_TYPE (exp), 1, 1, 1);
+	       we would have no way of knowing when to free it.
+
+	       This is the structure of a function return object and it isn't
+	       a character array for the stack protection, so it is
+	       marked using the assignment of the KEEP argument to 5.  */
+	    rtx d = assign_temp (TREE_TYPE (exp), 5, 1, 1);
 
 	    mark_temp_addr_taken (d);
 	    structure_value_addr = XEXP (d, 0);
diff -Naur gcc-3.4.4/gcc/c-cppbuiltin.c gcc-3.4.4-ssp/gcc/c-cppbuiltin.c
--- gcc-3.4.4/gcc/c-cppbuiltin.c	2004-03-04 12:24:54.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/c-cppbuiltin.c	2005-05-25 14:03:21.000000000 +0300
@@ -408,6 +408,12 @@
   if (c_dialect_objc () && flag_next_runtime)
     cpp_define (pfile, "__NEXT_RUNTIME__");
 
+  /* Make the choice of the stack protector runtime visible to source code.  */
+  if (flag_propolice_protection)
+    cpp_define (pfile, "__SSP__=1");
+  if (flag_stack_protection)
+    cpp_define (pfile, "__SSP_ALL__=2");
+
   /* A straightforward target hook doesn't work, because of problems
      linking that hook's body when part of non-C front ends.  */
 # define preprocessing_asm_p() (cpp_get_options (pfile)->lang == CLK_ASM)
diff -Naur gcc-3.4.4/gcc/combine.c gcc-3.4.4-ssp/gcc/combine.c
--- gcc-3.4.4/gcc/combine.c	2005-03-17 03:36:08.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/combine.c	2005-05-25 14:03:21.000000000 +0300
@@ -1402,6 +1402,10 @@
 	      && ! fixed_regs[REGNO (dest)]
 	      && CLASS_LIKELY_SPILLED_P (REGNO_REG_CLASS (REGNO (dest))))))
     return 1;
+  /* Never combine loads and stores protecting argument that use set insn
+     with used flag on.  */
+  if (SET_VOLATILE_P (set))
+    return 1;
 
   return 0;
 }
@@ -3782,7 +3786,20 @@
 	  rtx inner_op0 = XEXP (XEXP (x, 0), 1);
 	  rtx inner_op1 = XEXP (x, 1);
 	  rtx inner;
-
+	  
+#ifndef FRAME_GROWS_DOWNWARD
+	  /* For the case where the frame grows upward,
+	     the stack protector keeps the offset of the frame pointer
+	     positive integer.  */
+	  if (flag_propolice_protection
+	      && code == PLUS
+	      && other == frame_pointer_rtx
+	      && GET_CODE (inner_op0) == CONST_INT
+	      && GET_CODE (inner_op1) == CONST_INT
+	      && INTVAL (inner_op0) > 0
+	      && INTVAL (inner_op0) + INTVAL (inner_op1) <= 0)
+	    return x;
+#endif
 	  /* Make sure we pass the constant operand if any as the second
 	     one if this is a commutative operation.  */
 	  if (CONSTANT_P (inner_op0) && GET_RTX_CLASS (code) == 'c')
@@ -4147,6 +4164,13 @@
 	 they are now checked elsewhere.  */
       if (GET_CODE (XEXP (x, 0)) == PLUS
 	  && CONSTANT_ADDRESS_P (XEXP (XEXP (x, 0), 1)))
+#ifndef FRAME_GROWS_DOWNWARD
+	/* The stack protector keeps the addressing style of a local variable
+	   to be able to change its stack position.  */
+	if (! (flag_propolice_protection
+	       && XEXP (XEXP (x, 0), 0) == frame_pointer_rtx
+	       && GET_CODE (XEXP (XEXP (x, 0), 1)) == CONST_INT))
+#endif
 	return gen_binary (PLUS, mode,
 			   gen_binary (PLUS, mode, XEXP (XEXP (x, 0), 0),
 				       XEXP (x, 1)),
@@ -4274,8 +4298,14 @@
 	}
 
       /* Canonicalize (minus A (plus B C)) to (minus (minus A B) C) for
-	 integers.  */
-      if (GET_CODE (XEXP (x, 1)) == PLUS && INTEGRAL_MODE_P (mode))
+	 integers.
+	 
+	 The stack protector keeps the addressing style of
+	 a local variable.  */
+      if (GET_CODE (XEXP (x, 1)) == PLUS && INTEGRAL_MODE_P (mode)
+	  && (! (flag_propolice_protection
+		 && XEXP (XEXP (x, 1), 0) == frame_pointer_rtx
+		 && GET_CODE (XEXP (XEXP (x, 1), 1)) == CONST_INT)))
 	return gen_binary (MINUS, mode,
 			   gen_binary (MINUS, mode, XEXP (x, 0),
 				       XEXP (XEXP (x, 1), 0)),
diff -Naur gcc-3.4.4/gcc/combine.c~ gcc-3.4.4-ssp/gcc/combine.c~
--- gcc-3.4.4/gcc/combine.c~	1970-01-01 02:00:00.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/combine.c~	2005-03-17 03:36:08.000000000 +0200
@@ -0,0 +1,13134 @@
+/* Optimize by combining instructions for GNU compiler.
+   Copyright (C) 1987, 1988, 1992, 1993, 1994, 1995, 1996, 1997, 1998,
+   1999, 2000, 2001, 2002, 2003, 2004 Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 2, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+02111-1307, USA.  */
+
+/* This module is essentially the "combiner" phase of the U. of Arizona
+   Portable Optimizer, but redone to work on our list-structured
+   representation for RTL instead of their string representation.
+
+   The LOG_LINKS of each insn identify the most recent assignment
+   to each REG used in the insn.  It is a list of previous insns,
+   each of which contains a SET for a REG that is used in this insn
+   and not used or set in between.  LOG_LINKs never cross basic blocks.
+   They were set up by the preceding pass (lifetime analysis).
+
+   We try to combine each pair of insns joined by a logical link.
+   We also try to combine triples of insns A, B and C when
+   C has a link back to B and B has a link back to A.
+
+   LOG_LINKS does not have links for use of the CC0.  They don't
+   need to, because the insn that sets the CC0 is always immediately
+   before the insn that tests it.  So we always regard a branch
+   insn as having a logical link to the preceding insn.  The same is true
+   for an insn explicitly using CC0.
+
+   We check (with use_crosses_set_p) to avoid combining in such a way
+   as to move a computation to a place where its value would be different.
+
+   Combination is done by mathematically substituting the previous
+   insn(s) values for the regs they set into the expressions in
+   the later insns that refer to these regs.  If the result is a valid insn
+   for our target machine, according to the machine description,
+   we install it, delete the earlier insns, and update the data flow
+   information (LOG_LINKS and REG_NOTES) for what we did.
+
+   There are a few exceptions where the dataflow information created by
+   flow.c aren't completely updated:
+
+   - reg_live_length is not updated
+   - a LOG_LINKS entry that refers to an insn with multiple SETs may be
+     removed because there is no way to know which register it was
+     linking
+
+   To simplify substitution, we combine only when the earlier insn(s)
+   consist of only a single assignment.  To simplify updating afterward,
+   we never combine when a subroutine call appears in the middle.
+
+   Since we do not represent assignments to CC0 explicitly except when that
+   is all an insn does, there is no LOG_LINKS entry in an insn that uses
+   the condition code for the insn that set the condition code.
+   Fortunately, these two insns must be consecutive.
+   Therefore, every JUMP_INSN is taken to have an implicit logical link
+   to the preceding insn.  This is not quite right, since non-jumps can
+   also use the condition code; but in practice such insns would not
+   combine anyway.  */
+
+#include "config.h"
+#include "system.h"
+#include "coretypes.h"
+#include "tm.h"
+#include "rtl.h"
+#include "tree.h"
+#include "tm_p.h"
+#include "flags.h"
+#include "regs.h"
+#include "hard-reg-set.h"
+#include "basic-block.h"
+#include "insn-config.h"
+#include "function.h"
+/* Include expr.h after insn-config.h so we get HAVE_conditional_move.  */
+#include "expr.h"
+#include "insn-attr.h"
+#include "recog.h"
+#include "real.h"
+#include "toplev.h"
+#include "target.h"
+#include "params.h"
+
+#ifndef SHIFT_COUNT_TRUNCATED
+#define SHIFT_COUNT_TRUNCATED 0
+#endif
+
+/* It is not safe to use ordinary gen_lowpart in combine.
+   Use gen_lowpart_for_combine instead.  See comments there.  */
+#define gen_lowpart dont_use_gen_lowpart_you_dummy
+
+/* Number of attempts to combine instructions in this function.  */
+
+static int combine_attempts;
+
+/* Number of attempts that got as far as substitution in this function.  */
+
+static int combine_merges;
+
+/* Number of instructions combined with added SETs in this function.  */
+
+static int combine_extras;
+
+/* Number of instructions combined in this function.  */
+
+static int combine_successes;
+
+/* Totals over entire compilation.  */
+
+static int total_attempts, total_merges, total_extras, total_successes;
+
+
+/* Vector mapping INSN_UIDs to cuids.
+   The cuids are like uids but increase monotonically always.
+   Combine always uses cuids so that it can compare them.
+   But actually renumbering the uids, which we used to do,
+   proves to be a bad idea because it makes it hard to compare
+   the dumps produced by earlier passes with those from later passes.  */
+
+static int *uid_cuid;
+static int max_uid_cuid;
+
+/* Get the cuid of an insn.  */
+
+#define INSN_CUID(INSN) \
+(INSN_UID (INSN) > max_uid_cuid ? insn_cuid (INSN) : uid_cuid[INSN_UID (INSN)])
+
+/* In case BITS_PER_WORD == HOST_BITS_PER_WIDE_INT, shifting by
+   BITS_PER_WORD would invoke undefined behavior.  Work around it.  */
+
+#define UWIDE_SHIFT_LEFT_BY_BITS_PER_WORD(val) \
+  (((unsigned HOST_WIDE_INT) (val) << (BITS_PER_WORD - 1)) << 1)
+
+#define nonzero_bits(X, M) \
+  cached_nonzero_bits (X, M, NULL_RTX, VOIDmode, 0)
+
+#define num_sign_bit_copies(X, M) \
+  cached_num_sign_bit_copies (X, M, NULL_RTX, VOIDmode, 0)
+
+/* Maximum register number, which is the size of the tables below.  */
+
+static unsigned int combine_max_regno;
+
+/* Record last point of death of (hard or pseudo) register n.  */
+
+static rtx *reg_last_death;
+
+/* Record last point of modification of (hard or pseudo) register n.  */
+
+static rtx *reg_last_set;
+
+/* Record the cuid of the last insn that invalidated memory
+   (anything that writes memory, and subroutine calls, but not pushes).  */
+
+static int mem_last_set;
+
+/* Record the cuid of the last CALL_INSN
+   so we can tell whether a potential combination crosses any calls.  */
+
+static int last_call_cuid;
+
+/* When `subst' is called, this is the insn that is being modified
+   (by combining in a previous insn).  The PATTERN of this insn
+   is still the old pattern partially modified and it should not be
+   looked at, but this may be used to examine the successors of the insn
+   to judge whether a simplification is valid.  */
+
+static rtx subst_insn;
+
+/* This is the lowest CUID that `subst' is currently dealing with.
+   get_last_value will not return a value if the register was set at or
+   after this CUID.  If not for this mechanism, we could get confused if
+   I2 or I1 in try_combine were an insn that used the old value of a register
+   to obtain a new value.  In that case, we might erroneously get the
+   new value of the register when we wanted the old one.  */
+
+static int subst_low_cuid;
+
+/* This contains any hard registers that are used in newpat; reg_dead_at_p
+   must consider all these registers to be always live.  */
+
+static HARD_REG_SET newpat_used_regs;
+
+/* This is an insn to which a LOG_LINKS entry has been added.  If this
+   insn is the earlier than I2 or I3, combine should rescan starting at
+   that location.  */
+
+static rtx added_links_insn;
+
+/* Basic block in which we are performing combines.  */
+static basic_block this_basic_block;
+
+/* A bitmap indicating which blocks had registers go dead at entry.
+   After combine, we'll need to re-do global life analysis with
+   those blocks as starting points.  */
+static sbitmap refresh_blocks;
+
+/* The next group of arrays allows the recording of the last value assigned
+   to (hard or pseudo) register n.  We use this information to see if an
+   operation being processed is redundant given a prior operation performed
+   on the register.  For example, an `and' with a constant is redundant if
+   all the zero bits are already known to be turned off.
+
+   We use an approach similar to that used by cse, but change it in the
+   following ways:
+
+   (1) We do not want to reinitialize at each label.
+   (2) It is useful, but not critical, to know the actual value assigned
+       to a register.  Often just its form is helpful.
+
+   Therefore, we maintain the following arrays:
+
+   reg_last_set_value		the last value assigned
+   reg_last_set_label		records the value of label_tick when the
+				register was assigned
+   reg_last_set_table_tick	records the value of label_tick when a
+				value using the register is assigned
+   reg_last_set_invalid		set to nonzero when it is not valid
+				to use the value of this register in some
+				register's value
+
+   To understand the usage of these tables, it is important to understand
+   the distinction between the value in reg_last_set_value being valid
+   and the register being validly contained in some other expression in the
+   table.
+
+   Entry I in reg_last_set_value is valid if it is nonzero, and either
+   reg_n_sets[i] is 1 or reg_last_set_label[i] == label_tick.
+
+   Register I may validly appear in any expression returned for the value
+   of another register if reg_n_sets[i] is 1.  It may also appear in the
+   value for register J if reg_last_set_label[i] < reg_last_set_label[j] or
+   reg_last_set_invalid[j] is zero.
+
+   If an expression is found in the table containing a register which may
+   not validly appear in an expression, the register is replaced by
+   something that won't match, (clobber (const_int 0)).
+
+   reg_last_set_invalid[i] is set nonzero when register I is being assigned
+   to and reg_last_set_table_tick[i] == label_tick.  */
+
+/* Record last value assigned to (hard or pseudo) register n.  */
+
+static rtx *reg_last_set_value;
+
+/* Record the value of label_tick when the value for register n is placed in
+   reg_last_set_value[n].  */
+
+static int *reg_last_set_label;
+
+/* Record the value of label_tick when an expression involving register n
+   is placed in reg_last_set_value.  */
+
+static int *reg_last_set_table_tick;
+
+/* Set nonzero if references to register n in expressions should not be
+   used.  */
+
+static char *reg_last_set_invalid;
+
+/* Incremented for each label.  */
+
+static int label_tick;
+
+/* Some registers that are set more than once and used in more than one
+   basic block are nevertheless always set in similar ways.  For example,
+   a QImode register may be loaded from memory in two places on a machine
+   where byte loads zero extend.
+
+   We record in the following array what we know about the nonzero
+   bits of a register, specifically which bits are known to be zero.
+
+   If an entry is zero, it means that we don't know anything special.  */
+
+static unsigned HOST_WIDE_INT *reg_nonzero_bits;
+
+/* Mode used to compute significance in reg_nonzero_bits.  It is the largest
+   integer mode that can fit in HOST_BITS_PER_WIDE_INT.  */
+
+static enum machine_mode nonzero_bits_mode;
+
+/* Nonzero if we know that a register has some leading bits that are always
+   equal to the sign bit.  */
+
+static unsigned char *reg_sign_bit_copies;
+
+/* Nonzero when reg_nonzero_bits and reg_sign_bit_copies can be safely used.
+   It is zero while computing them and after combine has completed.  This
+   former test prevents propagating values based on previously set values,
+   which can be incorrect if a variable is modified in a loop.  */
+
+static int nonzero_sign_valid;
+
+/* These arrays are maintained in parallel with reg_last_set_value
+   and are used to store the mode in which the register was last set,
+   the bits that were known to be zero when it was last set, and the
+   number of sign bits copies it was known to have when it was last set.  */
+
+static enum machine_mode *reg_last_set_mode;
+static unsigned HOST_WIDE_INT *reg_last_set_nonzero_bits;
+static char *reg_last_set_sign_bit_copies;
+
+/* Record one modification to rtl structure
+   to be undone by storing old_contents into *where.
+   is_int is 1 if the contents are an int.  */
+
+struct undo
+{
+  struct undo *next;
+  int is_int;
+  union {rtx r; int i;} old_contents;
+  union {rtx *r; int *i;} where;
+};
+
+/* Record a bunch of changes to be undone, up to MAX_UNDO of them.
+   num_undo says how many are currently recorded.
+
+   other_insn is nonzero if we have modified some other insn in the process
+   of working on subst_insn.  It must be verified too.  */
+
+struct undobuf
+{
+  struct undo *undos;
+  struct undo *frees;
+  rtx other_insn;
+};
+
+static struct undobuf undobuf;
+
+/* Number of times the pseudo being substituted for
+   was found and replaced.  */
+
+static int n_occurrences;
+
+static void do_SUBST (rtx *, rtx);
+static void do_SUBST_INT (int *, int);
+static void init_reg_last_arrays (void);
+static void setup_incoming_promotions (void);
+static void set_nonzero_bits_and_sign_copies (rtx, rtx, void *);
+static int cant_combine_insn_p (rtx);
+static int can_combine_p (rtx, rtx, rtx, rtx, rtx *, rtx *);
+static int combinable_i3pat (rtx, rtx *, rtx, rtx, int, rtx *);
+static int contains_muldiv (rtx);
+static rtx try_combine (rtx, rtx, rtx, int *);
+static void undo_all (void);
+static void undo_commit (void);
+static rtx *find_split_point (rtx *, rtx);
+static rtx subst (rtx, rtx, rtx, int, int);
+static rtx combine_simplify_rtx (rtx, enum machine_mode, int, int);
+static rtx simplify_if_then_else (rtx);
+static rtx simplify_set (rtx);
+static rtx simplify_logical (rtx, int);
+static rtx expand_compound_operation (rtx);
+static rtx expand_field_assignment (rtx);
+static rtx make_extraction (enum machine_mode, rtx, HOST_WIDE_INT,
+			    rtx, unsigned HOST_WIDE_INT, int, int, int);
+static rtx extract_left_shift (rtx, int);
+static rtx make_compound_operation (rtx, enum rtx_code);
+static int get_pos_from_mask (unsigned HOST_WIDE_INT,
+			      unsigned HOST_WIDE_INT *);
+static rtx force_to_mode (rtx, enum machine_mode,
+			  unsigned HOST_WIDE_INT, rtx, int);
+static rtx if_then_else_cond (rtx, rtx *, rtx *);
+static rtx known_cond (rtx, enum rtx_code, rtx, rtx);
+static int rtx_equal_for_field_assignment_p (rtx, rtx);
+static rtx make_field_assignment (rtx);
+static rtx apply_distributive_law (rtx);
+static rtx simplify_and_const_int (rtx, enum machine_mode, rtx,
+				   unsigned HOST_WIDE_INT);
+static unsigned HOST_WIDE_INT cached_nonzero_bits (rtx, enum machine_mode,
+						   rtx, enum machine_mode,
+						   unsigned HOST_WIDE_INT);
+static unsigned HOST_WIDE_INT nonzero_bits1 (rtx, enum machine_mode, rtx,
+					     enum machine_mode,
+					     unsigned HOST_WIDE_INT);
+static unsigned int cached_num_sign_bit_copies (rtx, enum machine_mode, rtx,
+						enum machine_mode,
+						unsigned int);
+static unsigned int num_sign_bit_copies1 (rtx, enum machine_mode, rtx,
+					  enum machine_mode, unsigned int);
+static int merge_outer_ops (enum rtx_code *, HOST_WIDE_INT *, enum rtx_code,
+			    HOST_WIDE_INT, enum machine_mode, int *);
+static rtx simplify_shift_const	(rtx, enum rtx_code, enum machine_mode, rtx,
+				 int);
+static int recog_for_combine (rtx *, rtx, rtx *);
+static rtx gen_lowpart_for_combine (enum machine_mode, rtx);
+static rtx gen_binary (enum rtx_code, enum machine_mode, rtx, rtx);
+static enum rtx_code simplify_comparison (enum rtx_code, rtx *, rtx *);
+static void update_table_tick (rtx);
+static void record_value_for_reg (rtx, rtx, rtx);
+static void check_promoted_subreg (rtx, rtx);
+static void record_dead_and_set_regs_1 (rtx, rtx, void *);
+static void record_dead_and_set_regs (rtx);
+static int get_last_value_validate (rtx *, rtx, int, int);
+static rtx get_last_value (rtx);
+static int use_crosses_set_p (rtx, int);
+static void reg_dead_at_p_1 (rtx, rtx, void *);
+static int reg_dead_at_p (rtx, rtx);
+static void move_deaths (rtx, rtx, int, rtx, rtx *);
+static int reg_bitfield_target_p (rtx, rtx);
+static void distribute_notes (rtx, rtx, rtx, rtx);
+static void distribute_links (rtx);
+static void mark_used_regs_combine (rtx);
+static int insn_cuid (rtx);
+static void record_promoted_value (rtx, rtx);
+static rtx reversed_comparison (rtx, enum machine_mode, rtx, rtx);
+static enum rtx_code combine_reversed_comparison_code (rtx);
+
+/* Substitute NEWVAL, an rtx expression, into INTO, a place in some
+   insn.  The substitution can be undone by undo_all.  If INTO is already
+   set to NEWVAL, do not record this change.  Because computing NEWVAL might
+   also call SUBST, we have to compute it before we put anything into
+   the undo table.  */
+
+static void
+do_SUBST (rtx *into, rtx newval)
+{
+  struct undo *buf;
+  rtx oldval = *into;
+
+  if (oldval == newval)
+    return;
+
+  /* We'd like to catch as many invalid transformations here as
+     possible.  Unfortunately, there are way too many mode changes
+     that are perfectly valid, so we'd waste too much effort for
+     little gain doing the checks here.  Focus on catching invalid
+     transformations involving integer constants.  */
+  if (GET_MODE_CLASS (GET_MODE (oldval)) == MODE_INT
+      && GET_CODE (newval) == CONST_INT)
+    {
+      /* Sanity check that we're replacing oldval with a CONST_INT
+	 that is a valid sign-extension for the original mode.  */
+      if (INTVAL (newval) != trunc_int_for_mode (INTVAL (newval),
+						 GET_MODE (oldval)))
+	abort ();
+
+      /* Replacing the operand of a SUBREG or a ZERO_EXTEND with a
+	 CONST_INT is not valid, because after the replacement, the
+	 original mode would be gone.  Unfortunately, we can't tell
+	 when do_SUBST is called to replace the operand thereof, so we
+	 perform this test on oldval instead, checking whether an
+	 invalid replacement took place before we got here.  */
+      if ((GET_CODE (oldval) == SUBREG
+	   && GET_CODE (SUBREG_REG (oldval)) == CONST_INT)
+	  || (GET_CODE (oldval) == ZERO_EXTEND
+	      && GET_CODE (XEXP (oldval, 0)) == CONST_INT))
+	abort ();
+    }
+
+  if (undobuf.frees)
+    buf = undobuf.frees, undobuf.frees = buf->next;
+  else
+    buf = xmalloc (sizeof (struct undo));
+
+  buf->is_int = 0;
+  buf->where.r = into;
+  buf->old_contents.r = oldval;
+  *into = newval;
+
+  buf->next = undobuf.undos, undobuf.undos = buf;
+}
+
+#define SUBST(INTO, NEWVAL)	do_SUBST(&(INTO), (NEWVAL))
+
+/* Similar to SUBST, but NEWVAL is an int expression.  Note that substitution
+   for the value of a HOST_WIDE_INT value (including CONST_INT) is
+   not safe.  */
+
+static void
+do_SUBST_INT (int *into, int newval)
+{
+  struct undo *buf;
+  int oldval = *into;
+
+  if (oldval == newval)
+    return;
+
+  if (undobuf.frees)
+    buf = undobuf.frees, undobuf.frees = buf->next;
+  else
+    buf = xmalloc (sizeof (struct undo));
+
+  buf->is_int = 1;
+  buf->where.i = into;
+  buf->old_contents.i = oldval;
+  *into = newval;
+
+  buf->next = undobuf.undos, undobuf.undos = buf;
+}
+
+#define SUBST_INT(INTO, NEWVAL)  do_SUBST_INT(&(INTO), (NEWVAL))
+
+/* Main entry point for combiner.  F is the first insn of the function.
+   NREGS is the first unused pseudo-reg number.
+
+   Return nonzero if the combiner has turned an indirect jump
+   instruction into a direct jump.  */
+int
+combine_instructions (rtx f, unsigned int nregs)
+{
+  rtx insn, next;
+#ifdef HAVE_cc0
+  rtx prev;
+#endif
+  int i;
+  rtx links, nextlinks;
+
+  int new_direct_jump_p = 0;
+
+  combine_attempts = 0;
+  combine_merges = 0;
+  combine_extras = 0;
+  combine_successes = 0;
+
+  combine_max_regno = nregs;
+
+  reg_nonzero_bits = xcalloc (nregs, sizeof (unsigned HOST_WIDE_INT));
+  reg_sign_bit_copies = xcalloc (nregs, sizeof (unsigned char));
+
+  reg_last_death = xmalloc (nregs * sizeof (rtx));
+  reg_last_set = xmalloc (nregs * sizeof (rtx));
+  reg_last_set_value = xmalloc (nregs * sizeof (rtx));
+  reg_last_set_table_tick = xmalloc (nregs * sizeof (int));
+  reg_last_set_label = xmalloc (nregs * sizeof (int));
+  reg_last_set_invalid = xmalloc (nregs * sizeof (char));
+  reg_last_set_mode = xmalloc (nregs * sizeof (enum machine_mode));
+  reg_last_set_nonzero_bits = xmalloc (nregs * sizeof (HOST_WIDE_INT));
+  reg_last_set_sign_bit_copies = xmalloc (nregs * sizeof (char));
+
+  init_reg_last_arrays ();
+
+  init_recog_no_volatile ();
+
+  /* Compute maximum uid value so uid_cuid can be allocated.  */
+
+  for (insn = f, i = 0; insn; insn = NEXT_INSN (insn))
+    if (INSN_UID (insn) > i)
+      i = INSN_UID (insn);
+
+  uid_cuid = xmalloc ((i + 1) * sizeof (int));
+  max_uid_cuid = i;
+
+  nonzero_bits_mode = mode_for_size (HOST_BITS_PER_WIDE_INT, MODE_INT, 0);
+
+  /* Don't use reg_nonzero_bits when computing it.  This can cause problems
+     when, for example, we have j <<= 1 in a loop.  */
+
+  nonzero_sign_valid = 0;
+
+  /* Compute the mapping from uids to cuids.
+     Cuids are numbers assigned to insns, like uids,
+     except that cuids increase monotonically through the code.
+
+     Scan all SETs and see if we can deduce anything about what
+     bits are known to be zero for some registers and how many copies
+     of the sign bit are known to exist for those registers.
+
+     Also set any known values so that we can use it while searching
+     for what bits are known to be set.  */
+
+  label_tick = 1;
+
+  setup_incoming_promotions ();
+
+  refresh_blocks = sbitmap_alloc (last_basic_block);
+  sbitmap_zero (refresh_blocks);
+
+  for (insn = f, i = 0; insn; insn = NEXT_INSN (insn))
+    {
+      uid_cuid[INSN_UID (insn)] = ++i;
+      subst_low_cuid = i;
+      subst_insn = insn;
+
+      if (INSN_P (insn))
+	{
+	  note_stores (PATTERN (insn), set_nonzero_bits_and_sign_copies,
+		       NULL);
+	  record_dead_and_set_regs (insn);
+
+#ifdef AUTO_INC_DEC
+	  for (links = REG_NOTES (insn); links; links = XEXP (links, 1))
+	    if (REG_NOTE_KIND (links) == REG_INC)
+	      set_nonzero_bits_and_sign_copies (XEXP (links, 0), NULL_RTX,
+						NULL);
+#endif
+	}
+
+      if (GET_CODE (insn) == CODE_LABEL)
+	label_tick++;
+    }
+
+  nonzero_sign_valid = 1;
+
+  /* Now scan all the insns in forward order.  */
+
+  label_tick = 1;
+  last_call_cuid = 0;
+  mem_last_set = 0;
+  init_reg_last_arrays ();
+  setup_incoming_promotions ();
+
+  FOR_EACH_BB (this_basic_block)
+    {
+      for (insn = BB_HEAD (this_basic_block);
+           insn != NEXT_INSN (BB_END (this_basic_block));
+	   insn = next ? next : NEXT_INSN (insn))
+	{
+	  next = 0;
+
+	  if (GET_CODE (insn) == CODE_LABEL)
+	    label_tick++;
+
+	  else if (INSN_P (insn))
+	    {
+	      /* See if we know about function return values before this
+		 insn based upon SUBREG flags.  */
+	      check_promoted_subreg (insn, PATTERN (insn));
+
+	      /* Try this insn with each insn it links back to.  */
+
+	      for (links = LOG_LINKS (insn); links; links = XEXP (links, 1))
+		if ((next = try_combine (insn, XEXP (links, 0),
+					 NULL_RTX, &new_direct_jump_p)) != 0)
+		  goto retry;
+
+	      /* Try each sequence of three linked insns ending with this one.  */
+
+	      for (links = LOG_LINKS (insn); links; links = XEXP (links, 1))
+		{
+		  rtx link = XEXP (links, 0);
+
+		  /* If the linked insn has been replaced by a note, then there
+		     is no point in pursuing this chain any further.  */
+		  if (GET_CODE (link) == NOTE)
+		    continue;
+
+		  for (nextlinks = LOG_LINKS (link);
+		       nextlinks;
+		       nextlinks = XEXP (nextlinks, 1))
+		    if ((next = try_combine (insn, link,
+					     XEXP (nextlinks, 0),
+					     &new_direct_jump_p)) != 0)
+		      goto retry;
+		}
+
+#ifdef HAVE_cc0
+	      /* Try to combine a jump insn that uses CC0
+		 with a preceding insn that sets CC0, and maybe with its
+		 logical predecessor as well.
+		 This is how we make decrement-and-branch insns.
+		 We need this special code because data flow connections
+		 via CC0 do not get entered in LOG_LINKS.  */
+
+	      if (GET_CODE (insn) == JUMP_INSN
+		  && (prev = prev_nonnote_insn (insn)) != 0
+		  && GET_CODE (prev) == INSN
+		  && sets_cc0_p (PATTERN (prev)))
+		{
+		  if ((next = try_combine (insn, prev,
+					   NULL_RTX, &new_direct_jump_p)) != 0)
+		    goto retry;
+
+		  for (nextlinks = LOG_LINKS (prev); nextlinks;
+		       nextlinks = XEXP (nextlinks, 1))
+		    if ((next = try_combine (insn, prev,
+					     XEXP (nextlinks, 0),
+					     &new_direct_jump_p)) != 0)
+		      goto retry;
+		}
+
+	      /* Do the same for an insn that explicitly references CC0.  */
+	      if (GET_CODE (insn) == INSN
+		  && (prev = prev_nonnote_insn (insn)) != 0
+		  && GET_CODE (prev) == INSN
+		  && sets_cc0_p (PATTERN (prev))
+		  && GET_CODE (PATTERN (insn)) == SET
+		  && reg_mentioned_p (cc0_rtx, SET_SRC (PATTERN (insn))))
+		{
+		  if ((next = try_combine (insn, prev,
+					   NULL_RTX, &new_direct_jump_p)) != 0)
+		    goto retry;
+
+		  for (nextlinks = LOG_LINKS (prev); nextlinks;
+		       nextlinks = XEXP (nextlinks, 1))
+		    if ((next = try_combine (insn, prev,
+					     XEXP (nextlinks, 0),
+					     &new_direct_jump_p)) != 0)
+		      goto retry;
+		}
+
+	      /* Finally, see if any of the insns that this insn links to
+		 explicitly references CC0.  If so, try this insn, that insn,
+		 and its predecessor if it sets CC0.  */
+	      for (links = LOG_LINKS (insn); links; links = XEXP (links, 1))
+		if (GET_CODE (XEXP (links, 0)) == INSN
+		    && GET_CODE (PATTERN (XEXP (links, 0))) == SET
+		    && reg_mentioned_p (cc0_rtx, SET_SRC (PATTERN (XEXP (links, 0))))
+		    && (prev = prev_nonnote_insn (XEXP (links, 0))) != 0
+		    && GET_CODE (prev) == INSN
+		    && sets_cc0_p (PATTERN (prev))
+		    && (next = try_combine (insn, XEXP (links, 0),
+					    prev, &new_direct_jump_p)) != 0)
+		  goto retry;
+#endif
+
+	      /* Try combining an insn with two different insns whose results it
+		 uses.  */
+	      for (links = LOG_LINKS (insn); links; links = XEXP (links, 1))
+		for (nextlinks = XEXP (links, 1); nextlinks;
+		     nextlinks = XEXP (nextlinks, 1))
+		  if ((next = try_combine (insn, XEXP (links, 0),
+					   XEXP (nextlinks, 0),
+					   &new_direct_jump_p)) != 0)
+		    goto retry;
+
+	      if (GET_CODE (insn) != NOTE)
+		record_dead_and_set_regs (insn);
+
+	    retry:
+	      ;
+	    }
+	}
+    }
+  clear_bb_flags ();
+
+  EXECUTE_IF_SET_IN_SBITMAP (refresh_blocks, 0, i,
+			     BASIC_BLOCK (i)->flags |= BB_DIRTY);
+  new_direct_jump_p |= purge_all_dead_edges (0);
+  delete_noop_moves (f);
+
+  update_life_info_in_dirty_blocks (UPDATE_LIFE_GLOBAL_RM_NOTES,
+				    PROP_DEATH_NOTES | PROP_SCAN_DEAD_CODE
+				    | PROP_KILL_DEAD_CODE);
+
+  /* Clean up.  */
+  sbitmap_free (refresh_blocks);
+  free (reg_nonzero_bits);
+  free (reg_sign_bit_copies);
+  free (reg_last_death);
+  free (reg_last_set);
+  free (reg_last_set_value);
+  free (reg_last_set_table_tick);
+  free (reg_last_set_label);
+  free (reg_last_set_invalid);
+  free (reg_last_set_mode);
+  free (reg_last_set_nonzero_bits);
+  free (reg_last_set_sign_bit_copies);
+  free (uid_cuid);
+
+  {
+    struct undo *undo, *next;
+    for (undo = undobuf.frees; undo; undo = next)
+      {
+	next = undo->next;
+	free (undo);
+      }
+    undobuf.frees = 0;
+  }
+
+  total_attempts += combine_attempts;
+  total_merges += combine_merges;
+  total_extras += combine_extras;
+  total_successes += combine_successes;
+
+  nonzero_sign_valid = 0;
+
+  /* Make recognizer allow volatile MEMs again.  */
+  init_recog ();
+
+  return new_direct_jump_p;
+}
+
+/* Wipe the reg_last_xxx arrays in preparation for another pass.  */
+
+static void
+init_reg_last_arrays (void)
+{
+  unsigned int nregs = combine_max_regno;
+
+  memset (reg_last_death, 0, nregs * sizeof (rtx));
+  memset (reg_last_set, 0, nregs * sizeof (rtx));
+  memset (reg_last_set_value, 0, nregs * sizeof (rtx));
+  memset (reg_last_set_table_tick, 0, nregs * sizeof (int));
+  memset (reg_last_set_label, 0, nregs * sizeof (int));
+  memset (reg_last_set_invalid, 0, nregs * sizeof (char));
+  memset (reg_last_set_mode, 0, nregs * sizeof (enum machine_mode));
+  memset (reg_last_set_nonzero_bits, 0, nregs * sizeof (HOST_WIDE_INT));
+  memset (reg_last_set_sign_bit_copies, 0, nregs * sizeof (char));
+}
+
+/* Set up any promoted values for incoming argument registers.  */
+
+static void
+setup_incoming_promotions (void)
+{
+  unsigned int regno;
+  rtx reg;
+  enum machine_mode mode;
+  int unsignedp;
+  rtx first = get_insns ();
+
+  if (targetm.calls.promote_function_args (TREE_TYPE (cfun->decl)))
+    {
+#ifndef OUTGOING_REGNO
+#define OUTGOING_REGNO(N) N
+#endif
+      for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)
+	/* Check whether this register can hold an incoming pointer
+	   argument.  FUNCTION_ARG_REGNO_P tests outgoing register
+	   numbers, so translate if necessary due to register windows.  */
+	if (FUNCTION_ARG_REGNO_P (OUTGOING_REGNO (regno))
+	    && (reg = promoted_input_arg (regno, &mode, &unsignedp)) != 0)
+	  {
+	    record_value_for_reg
+	      (reg, first, gen_rtx_fmt_e ((unsignedp ? ZERO_EXTEND
+					   : SIGN_EXTEND),
+					  GET_MODE (reg),
+					  gen_rtx_CLOBBER (mode, const0_rtx)));
+	  }
+    }
+}
+
+/* Called via note_stores.  If X is a pseudo that is narrower than
+   HOST_BITS_PER_WIDE_INT and is being set, record what bits are known zero.
+
+   If we are setting only a portion of X and we can't figure out what
+   portion, assume all bits will be used since we don't know what will
+   be happening.
+
+   Similarly, set how many bits of X are known to be copies of the sign bit
+   at all locations in the function.  This is the smallest number implied
+   by any set of X.  */
+
+static void
+set_nonzero_bits_and_sign_copies (rtx x, rtx set,
+				  void *data ATTRIBUTE_UNUSED)
+{
+  unsigned int num;
+
+  if (GET_CODE (x) == REG
+      && REGNO (x) >= FIRST_PSEUDO_REGISTER
+      /* If this register is undefined at the start of the file, we can't
+	 say what its contents were.  */
+      && ! REGNO_REG_SET_P (ENTRY_BLOCK_PTR->next_bb->global_live_at_start, REGNO (x))
+      && GET_MODE_BITSIZE (GET_MODE (x)) <= HOST_BITS_PER_WIDE_INT)
+    {
+      if (set == 0 || GET_CODE (set) == CLOBBER)
+	{
+	  reg_nonzero_bits[REGNO (x)] = GET_MODE_MASK (GET_MODE (x));
+	  reg_sign_bit_copies[REGNO (x)] = 1;
+	  return;
+	}
+
+      /* If this is a complex assignment, see if we can convert it into a
+	 simple assignment.  */
+      set = expand_field_assignment (set);
+
+      /* If this is a simple assignment, or we have a paradoxical SUBREG,
+	 set what we know about X.  */
+
+      if (SET_DEST (set) == x
+	  || (GET_CODE (SET_DEST (set)) == SUBREG
+	      && (GET_MODE_SIZE (GET_MODE (SET_DEST (set)))
+		  > GET_MODE_SIZE (GET_MODE (SUBREG_REG (SET_DEST (set)))))
+	      && SUBREG_REG (SET_DEST (set)) == x))
+	{
+	  rtx src = SET_SRC (set);
+
+#ifdef SHORT_IMMEDIATES_SIGN_EXTEND
+	  /* If X is narrower than a word and SRC is a non-negative
+	     constant that would appear negative in the mode of X,
+	     sign-extend it for use in reg_nonzero_bits because some
+	     machines (maybe most) will actually do the sign-extension
+	     and this is the conservative approach.
+
+	     ??? For 2.5, try to tighten up the MD files in this regard
+	     instead of this kludge.  */
+
+	  if (GET_MODE_BITSIZE (GET_MODE (x)) < BITS_PER_WORD
+	      && GET_CODE (src) == CONST_INT
+	      && INTVAL (src) > 0
+	      && 0 != (INTVAL (src)
+		       & ((HOST_WIDE_INT) 1
+			  << (GET_MODE_BITSIZE (GET_MODE (x)) - 1))))
+	    src = GEN_INT (INTVAL (src)
+			   | ((HOST_WIDE_INT) (-1)
+			      << GET_MODE_BITSIZE (GET_MODE (x))));
+#endif
+
+	  /* Don't call nonzero_bits if it cannot change anything.  */
+	  if (reg_nonzero_bits[REGNO (x)] != ~(unsigned HOST_WIDE_INT) 0)
+	    reg_nonzero_bits[REGNO (x)]
+	      |= nonzero_bits (src, nonzero_bits_mode);
+	  num = num_sign_bit_copies (SET_SRC (set), GET_MODE (x));
+	  if (reg_sign_bit_copies[REGNO (x)] == 0
+	      || reg_sign_bit_copies[REGNO (x)] > num)
+	    reg_sign_bit_copies[REGNO (x)] = num;
+	}
+      else
+	{
+	  reg_nonzero_bits[REGNO (x)] = GET_MODE_MASK (GET_MODE (x));
+	  reg_sign_bit_copies[REGNO (x)] = 1;
+	}
+    }
+}
+
+/* See if INSN can be combined into I3.  PRED and SUCC are optionally
+   insns that were previously combined into I3 or that will be combined
+   into the merger of INSN and I3.
+
+   Return 0 if the combination is not allowed for any reason.
+
+   If the combination is allowed, *PDEST will be set to the single
+   destination of INSN and *PSRC to the single source, and this function
+   will return 1.  */
+
+static int
+can_combine_p (rtx insn, rtx i3, rtx pred ATTRIBUTE_UNUSED, rtx succ,
+	       rtx *pdest, rtx *psrc)
+{
+  int i;
+  rtx set = 0, src, dest;
+  rtx p;
+#ifdef AUTO_INC_DEC
+  rtx link;
+#endif
+  int all_adjacent = (succ ? (next_active_insn (insn) == succ
+			      && next_active_insn (succ) == i3)
+		      : next_active_insn (insn) == i3);
+
+  /* Can combine only if previous insn is a SET of a REG, a SUBREG or CC0.
+     or a PARALLEL consisting of such a SET and CLOBBERs.
+
+     If INSN has CLOBBER parallel parts, ignore them for our processing.
+     By definition, these happen during the execution of the insn.  When it
+     is merged with another insn, all bets are off.  If they are, in fact,
+     needed and aren't also supplied in I3, they may be added by
+     recog_for_combine.  Otherwise, it won't match.
+
+     We can also ignore a SET whose SET_DEST is mentioned in a REG_UNUSED
+     note.
+
+     Get the source and destination of INSN.  If more than one, can't
+     combine.  */
+
+  if (GET_CODE (PATTERN (insn)) == SET)
+    set = PATTERN (insn);
+  else if (GET_CODE (PATTERN (insn)) == PARALLEL
+	   && GET_CODE (XVECEXP (PATTERN (insn), 0, 0)) == SET)
+    {
+      for (i = 0; i < XVECLEN (PATTERN (insn), 0); i++)
+	{
+	  rtx elt = XVECEXP (PATTERN (insn), 0, i);
+	  rtx note;
+
+	  switch (GET_CODE (elt))
+	    {
+	    /* This is important to combine floating point insns
+	       for the SH4 port.  */
+	    case USE:
+	      /* Combining an isolated USE doesn't make sense.
+		 We depend here on combinable_i3pat to reject them.  */
+	      /* The code below this loop only verifies that the inputs of
+		 the SET in INSN do not change.  We call reg_set_between_p
+		 to verify that the REG in the USE does not change between
+		 I3 and INSN.
+		 If the USE in INSN was for a pseudo register, the matching
+		 insn pattern will likely match any register; combining this
+		 with any other USE would only be safe if we knew that the
+		 used registers have identical values, or if there was
+		 something to tell them apart, e.g. different modes.  For
+		 now, we forgo such complicated tests and simply disallow
+		 combining of USES of pseudo registers with any other USE.  */
+	      if (GET_CODE (XEXP (elt, 0)) == REG
+		  && GET_CODE (PATTERN (i3)) == PARALLEL)
+		{
+		  rtx i3pat = PATTERN (i3);
+		  int i = XVECLEN (i3pat, 0) - 1;
+		  unsigned int regno = REGNO (XEXP (elt, 0));
+
+		  do
+		    {
+		      rtx i3elt = XVECEXP (i3pat, 0, i);
+
+		      if (GET_CODE (i3elt) == USE
+			  && GET_CODE (XEXP (i3elt, 0)) == REG
+			  && (REGNO (XEXP (i3elt, 0)) == regno
+			      ? reg_set_between_p (XEXP (elt, 0),
+						   PREV_INSN (insn), i3)
+			      : regno >= FIRST_PSEUDO_REGISTER))
+			return 0;
+		    }
+		  while (--i >= 0);
+		}
+	      break;
+
+	      /* We can ignore CLOBBERs.  */
+	    case CLOBBER:
+	      break;
+
+	    case SET:
+	      /* Ignore SETs whose result isn't used but not those that
+		 have side-effects.  */
+	      if (find_reg_note (insn, REG_UNUSED, SET_DEST (elt))
+		  && (!(note = find_reg_note (insn, REG_EH_REGION, NULL_RTX))
+		      || INTVAL (XEXP (note, 0)) <= 0)
+		  && ! side_effects_p (elt))
+		break;
+
+	      /* If we have already found a SET, this is a second one and
+		 so we cannot combine with this insn.  */
+	      if (set)
+		return 0;
+
+	      set = elt;
+	      break;
+
+	    default:
+	      /* Anything else means we can't combine.  */
+	      return 0;
+	    }
+	}
+
+      if (set == 0
+	  /* If SET_SRC is an ASM_OPERANDS we can't throw away these CLOBBERs,
+	     so don't do anything with it.  */
+	  || GET_CODE (SET_SRC (set)) == ASM_OPERANDS)
+	return 0;
+    }
+  else
+    return 0;
+
+  if (set == 0)
+    return 0;
+
+  set = expand_field_assignment (set);
+  src = SET_SRC (set), dest = SET_DEST (set);
+
+  /* Don't eliminate a store in the stack pointer.  */
+  if (dest == stack_pointer_rtx
+      /* Don't combine with an insn that sets a register to itself if it has
+	 a REG_EQUAL note.  This may be part of a REG_NO_CONFLICT sequence.  */
+      || (rtx_equal_p (src, dest) && find_reg_note (insn, REG_EQUAL, NULL_RTX))
+      /* Can't merge an ASM_OPERANDS.  */
+      || GET_CODE (src) == ASM_OPERANDS
+      /* Can't merge a function call.  */
+      || GET_CODE (src) == CALL
+      /* Don't eliminate a function call argument.  */
+      || (GET_CODE (i3) == CALL_INSN
+	  && (find_reg_fusage (i3, USE, dest)
+	      || (GET_CODE (dest) == REG
+		  && REGNO (dest) < FIRST_PSEUDO_REGISTER
+		  && global_regs[REGNO (dest)])))
+      /* Don't substitute into an incremented register.  */
+      || FIND_REG_INC_NOTE (i3, dest)
+      || (succ && FIND_REG_INC_NOTE (succ, dest))
+#if 0
+      /* Don't combine the end of a libcall into anything.  */
+      /* ??? This gives worse code, and appears to be unnecessary, since no
+	 pass after flow uses REG_LIBCALL/REG_RETVAL notes.  Local-alloc does
+	 use REG_RETVAL notes for noconflict blocks, but other code here
+	 makes sure that those insns don't disappear.  */
+      || find_reg_note (insn, REG_RETVAL, NULL_RTX)
+#endif
+      /* Make sure that DEST is not used after SUCC but before I3.  */
+      || (succ && ! all_adjacent
+	  && reg_used_between_p (dest, succ, i3))
+      /* Make sure that the value that is to be substituted for the register
+	 does not use any registers whose values alter in between.  However,
+	 If the insns are adjacent, a use can't cross a set even though we
+	 think it might (this can happen for a sequence of insns each setting
+	 the same destination; reg_last_set of that register might point to
+	 a NOTE).  If INSN has a REG_EQUIV note, the register is always
+	 equivalent to the memory so the substitution is valid even if there
+	 are intervening stores.  Also, don't move a volatile asm or
+	 UNSPEC_VOLATILE across any other insns.  */
+      || (! all_adjacent
+	  && (((GET_CODE (src) != MEM
+		|| ! find_reg_note (insn, REG_EQUIV, src))
+	       && use_crosses_set_p (src, INSN_CUID (insn)))
+	      || (GET_CODE (src) == ASM_OPERANDS && MEM_VOLATILE_P (src))
+	      || GET_CODE (src) == UNSPEC_VOLATILE))
+      /* If there is a REG_NO_CONFLICT note for DEST in I3 or SUCC, we get
+	 better register allocation by not doing the combine.  */
+      || find_reg_note (i3, REG_NO_CONFLICT, dest)
+      || (succ && find_reg_note (succ, REG_NO_CONFLICT, dest))
+      /* Don't combine across a CALL_INSN, because that would possibly
+	 change whether the life span of some REGs crosses calls or not,
+	 and it is a pain to update that information.
+	 Exception: if source is a constant, moving it later can't hurt.
+	 Accept that special case, because it helps -fforce-addr a lot.  */
+      || (INSN_CUID (insn) < last_call_cuid && ! CONSTANT_P (src)))
+    return 0;
+
+  /* DEST must either be a REG or CC0.  */
+  if (GET_CODE (dest) == REG)
+    {
+      /* If register alignment is being enforced for multi-word items in all
+	 cases except for parameters, it is possible to have a register copy
+	 insn referencing a hard register that is not allowed to contain the
+	 mode being copied and which would not be valid as an operand of most
+	 insns.  Eliminate this problem by not combining with such an insn.
+
+	 Also, on some machines we don't want to extend the life of a hard
+	 register.  */
+
+      if (GET_CODE (src) == REG
+	  && ((REGNO (dest) < FIRST_PSEUDO_REGISTER
+	       && ! HARD_REGNO_MODE_OK (REGNO (dest), GET_MODE (dest)))
+	      /* Don't extend the life of a hard register unless it is
+		 user variable (if we have few registers) or it can't
+		 fit into the desired register (meaning something special
+		 is going on).
+		 Also avoid substituting a return register into I3, because
+		 reload can't handle a conflict with constraints of other
+		 inputs.  */
+	      || (REGNO (src) < FIRST_PSEUDO_REGISTER
+		  && ! HARD_REGNO_MODE_OK (REGNO (src), GET_MODE (src)))))
+	return 0;
+    }
+  else if (GET_CODE (dest) != CC0)
+    return 0;
+
+  /* Don't substitute for a register intended as a clobberable operand.
+     Similarly, don't substitute an expression containing a register that
+     will be clobbered in I3.  */
+  if (GET_CODE (PATTERN (i3)) == PARALLEL)
+    for (i = XVECLEN (PATTERN (i3), 0) - 1; i >= 0; i--)
+      if (GET_CODE (XVECEXP (PATTERN (i3), 0, i)) == CLOBBER
+	  && (reg_overlap_mentioned_p (XEXP (XVECEXP (PATTERN (i3), 0, i), 0),
+				       src)
+	      || rtx_equal_p (XEXP (XVECEXP (PATTERN (i3), 0, i), 0), dest)))
+	return 0;
+
+  /* If INSN contains anything volatile, or is an `asm' (whether volatile
+     or not), reject, unless nothing volatile comes between it and I3 */
+
+  if (GET_CODE (src) == ASM_OPERANDS || volatile_refs_p (src))
+    {
+      /* Make sure succ doesn't contain a volatile reference.  */
+      if (succ != 0 && volatile_refs_p (PATTERN (succ)))
+        return 0;
+
+      for (p = NEXT_INSN (insn); p != i3; p = NEXT_INSN (p))
+        if (INSN_P (p) && p != succ && volatile_refs_p (PATTERN (p)))
+	  return 0;
+    }
+
+  /* If INSN is an asm, and DEST is a hard register, reject, since it has
+     to be an explicit register variable, and was chosen for a reason.  */
+
+  if (GET_CODE (src) == ASM_OPERANDS
+      && GET_CODE (dest) == REG && REGNO (dest) < FIRST_PSEUDO_REGISTER)
+    return 0;
+
+  /* If there are any volatile insns between INSN and I3, reject, because
+     they might affect machine state.  */
+
+  for (p = NEXT_INSN (insn); p != i3; p = NEXT_INSN (p))
+    if (INSN_P (p) && p != succ && volatile_insn_p (PATTERN (p)))
+      return 0;
+
+  /* If INSN or I2 contains an autoincrement or autodecrement,
+     make sure that register is not used between there and I3,
+     and not already used in I3 either.
+     Also insist that I3 not be a jump; if it were one
+     and the incremented register were spilled, we would lose.  */
+
+#ifdef AUTO_INC_DEC
+  for (link = REG_NOTES (insn); link; link = XEXP (link, 1))
+    if (REG_NOTE_KIND (link) == REG_INC
+	&& (GET_CODE (i3) == JUMP_INSN
+	    || reg_used_between_p (XEXP (link, 0), insn, i3)
+	    || reg_overlap_mentioned_p (XEXP (link, 0), PATTERN (i3))))
+      return 0;
+#endif
+
+#ifdef HAVE_cc0
+  /* Don't combine an insn that follows a CC0-setting insn.
+     An insn that uses CC0 must not be separated from the one that sets it.
+     We do, however, allow I2 to follow a CC0-setting insn if that insn
+     is passed as I1; in that case it will be deleted also.
+     We also allow combining in this case if all the insns are adjacent
+     because that would leave the two CC0 insns adjacent as well.
+     It would be more logical to test whether CC0 occurs inside I1 or I2,
+     but that would be much slower, and this ought to be equivalent.  */
+
+  p = prev_nonnote_insn (insn);
+  if (p && p != pred && GET_CODE (p) == INSN && sets_cc0_p (PATTERN (p))
+      && ! all_adjacent)
+    return 0;
+#endif
+
+  /* If we get here, we have passed all the tests and the combination is
+     to be allowed.  */
+
+  *pdest = dest;
+  *psrc = src;
+
+  return 1;
+}
+
+/* LOC is the location within I3 that contains its pattern or the component
+   of a PARALLEL of the pattern.  We validate that it is valid for combining.
+
+   One problem is if I3 modifies its output, as opposed to replacing it
+   entirely, we can't allow the output to contain I2DEST or I1DEST as doing
+   so would produce an insn that is not equivalent to the original insns.
+
+   Consider:
+
+         (set (reg:DI 101) (reg:DI 100))
+	 (set (subreg:SI (reg:DI 101) 0) <foo>)
+
+   This is NOT equivalent to:
+
+         (parallel [(set (subreg:SI (reg:DI 100) 0) <foo>)
+		    (set (reg:DI 101) (reg:DI 100))])
+
+   Not only does this modify 100 (in which case it might still be valid
+   if 100 were dead in I2), it sets 101 to the ORIGINAL value of 100.
+
+   We can also run into a problem if I2 sets a register that I1
+   uses and I1 gets directly substituted into I3 (not via I2).  In that
+   case, we would be getting the wrong value of I2DEST into I3, so we
+   must reject the combination.  This case occurs when I2 and I1 both
+   feed into I3, rather than when I1 feeds into I2, which feeds into I3.
+   If I1_NOT_IN_SRC is nonzero, it means that finding I1 in the source
+   of a SET must prevent combination from occurring.
+
+   Before doing the above check, we first try to expand a field assignment
+   into a set of logical operations.
+
+   If PI3_DEST_KILLED is nonzero, it is a pointer to a location in which
+   we place a register that is both set and used within I3.  If more than one
+   such register is detected, we fail.
+
+   Return 1 if the combination is valid, zero otherwise.  */
+
+static int
+combinable_i3pat (rtx i3, rtx *loc, rtx i2dest, rtx i1dest,
+		  int i1_not_in_src, rtx *pi3dest_killed)
+{
+  rtx x = *loc;
+
+  if (GET_CODE (x) == SET)
+    {
+      rtx set = x ;
+      rtx dest = SET_DEST (set);
+      rtx src = SET_SRC (set);
+      rtx inner_dest = dest;
+
+      while (GET_CODE (inner_dest) == STRICT_LOW_PART
+	     || GET_CODE (inner_dest) == SUBREG
+	     || GET_CODE (inner_dest) == ZERO_EXTRACT)
+	inner_dest = XEXP (inner_dest, 0);
+
+      /* Check for the case where I3 modifies its output, as discussed
+	 above.  We don't want to prevent pseudos from being combined
+	 into the address of a MEM, so only prevent the combination if
+	 i1 or i2 set the same MEM.  */
+      if ((inner_dest != dest &&
+	   (GET_CODE (inner_dest) != MEM
+	    || rtx_equal_p (i2dest, inner_dest)
+	    || (i1dest && rtx_equal_p (i1dest, inner_dest)))
+	   && (reg_overlap_mentioned_p (i2dest, inner_dest)
+	       || (i1dest && reg_overlap_mentioned_p (i1dest, inner_dest))))
+
+	  /* This is the same test done in can_combine_p except we can't test
+	     all_adjacent; we don't have to, since this instruction will stay
+	     in place, thus we are not considering increasing the lifetime of
+	     INNER_DEST.
+
+	     Also, if this insn sets a function argument, combining it with
+	     something that might need a spill could clobber a previous
+	     function argument; the all_adjacent test in can_combine_p also
+	     checks this; here, we do a more specific test for this case.  */
+
+	  || (GET_CODE (inner_dest) == REG
+	      && REGNO (inner_dest) < FIRST_PSEUDO_REGISTER
+	      && (! HARD_REGNO_MODE_OK (REGNO (inner_dest),
+					GET_MODE (inner_dest))))
+	  || (i1_not_in_src && reg_overlap_mentioned_p (i1dest, src)))
+	return 0;
+
+      /* If DEST is used in I3, it is being killed in this insn,
+	 so record that for later.
+	 Never add REG_DEAD notes for the FRAME_POINTER_REGNUM or the
+	 STACK_POINTER_REGNUM, since these are always considered to be
+	 live.  Similarly for ARG_POINTER_REGNUM if it is fixed.  */
+      if (pi3dest_killed && GET_CODE (dest) == REG
+	  && reg_referenced_p (dest, PATTERN (i3))
+	  && REGNO (dest) != FRAME_POINTER_REGNUM
+#if HARD_FRAME_POINTER_REGNUM != FRAME_POINTER_REGNUM
+	  && REGNO (dest) != HARD_FRAME_POINTER_REGNUM
+#endif
+#if ARG_POINTER_REGNUM != FRAME_POINTER_REGNUM
+	  && (REGNO (dest) != ARG_POINTER_REGNUM
+	      || ! fixed_regs [REGNO (dest)])
+#endif
+	  && REGNO (dest) != STACK_POINTER_REGNUM)
+	{
+	  if (*pi3dest_killed)
+	    return 0;
+
+	  *pi3dest_killed = dest;
+	}
+    }
+
+  else if (GET_CODE (x) == PARALLEL)
+    {
+      int i;
+
+      for (i = 0; i < XVECLEN (x, 0); i++)
+	if (! combinable_i3pat (i3, &XVECEXP (x, 0, i), i2dest, i1dest,
+				i1_not_in_src, pi3dest_killed))
+	  return 0;
+    }
+
+  return 1;
+}
+
+/* Return 1 if X is an arithmetic expression that contains a multiplication
+   and division.  We don't count multiplications by powers of two here.  */
+
+static int
+contains_muldiv (rtx x)
+{
+  switch (GET_CODE (x))
+    {
+    case MOD:  case DIV:  case UMOD:  case UDIV:
+      return 1;
+
+    case MULT:
+      return ! (GET_CODE (XEXP (x, 1)) == CONST_INT
+		&& exact_log2 (INTVAL (XEXP (x, 1))) >= 0);
+    default:
+      switch (GET_RTX_CLASS (GET_CODE (x)))
+	{
+	case 'c':  case '<':  case '2':
+	  return contains_muldiv (XEXP (x, 0))
+	    || contains_muldiv (XEXP (x, 1));
+
+	case '1':
+	  return contains_muldiv (XEXP (x, 0));
+
+	default:
+	  return 0;
+	}
+    }
+}
+
+/* Determine whether INSN can be used in a combination.  Return nonzero if
+   not.  This is used in try_combine to detect early some cases where we
+   can't perform combinations.  */
+
+static int
+cant_combine_insn_p (rtx insn)
+{
+  rtx set;
+  rtx src, dest;
+
+  /* If this isn't really an insn, we can't do anything.
+     This can occur when flow deletes an insn that it has merged into an
+     auto-increment address.  */
+  if (! INSN_P (insn))
+    return 1;
+
+  /* Never combine loads and stores involving hard regs that are likely
+     to be spilled.  The register allocator can usually handle such
+     reg-reg moves by tying.  If we allow the combiner to make
+     substitutions of likely-spilled regs, we may abort in reload.
+     As an exception, we allow combinations involving fixed regs; these are
+     not available to the register allocator so there's no risk involved.  */
+
+  set = single_set (insn);
+  if (! set)
+    return 0;
+  src = SET_SRC (set);
+  dest = SET_DEST (set);
+  if (GET_CODE (src) == SUBREG)
+    src = SUBREG_REG (src);
+  if (GET_CODE (dest) == SUBREG)
+    dest = SUBREG_REG (dest);
+  if (REG_P (src) && REG_P (dest)
+      && ((REGNO (src) < FIRST_PSEUDO_REGISTER
+	   && ! fixed_regs[REGNO (src)]
+	   && CLASS_LIKELY_SPILLED_P (REGNO_REG_CLASS (REGNO (src))))
+	  || (REGNO (dest) < FIRST_PSEUDO_REGISTER
+	      && ! fixed_regs[REGNO (dest)]
+	      && CLASS_LIKELY_SPILLED_P (REGNO_REG_CLASS (REGNO (dest))))))
+    return 1;
+
+  return 0;
+}
+
+/* Adjust INSN after we made a change to its destination.
+
+   Changing the destination can invalidate notes that say something about
+   the results of the insn and a LOG_LINK pointing to the insn.  */
+
+static void
+adjust_for_new_dest (rtx insn)
+{
+  rtx *loc;
+
+  /* For notes, be conservative and simply remove them.  */
+  loc = &REG_NOTES (insn);
+  while (*loc)
+    {
+      enum reg_note kind = REG_NOTE_KIND (*loc);
+      if (kind == REG_EQUAL || kind == REG_EQUIV)
+	*loc = XEXP (*loc, 1);
+      else
+	loc = &XEXP (*loc, 1);
+    }
+
+  /* The new insn will have a destination that was previously the destination
+     of an insn just above it.  Call distribute_links to make a LOG_LINK from
+     the next use of that destination.  */
+  distribute_links (gen_rtx_INSN_LIST (VOIDmode, insn, NULL_RTX));
+}
+
+/* Try to combine the insns I1 and I2 into I3.
+   Here I1 and I2 appear earlier than I3.
+   I1 can be zero; then we combine just I2 into I3.
+
+   If we are combining three insns and the resulting insn is not recognized,
+   try splitting it into two insns.  If that happens, I2 and I3 are retained
+   and I1 is pseudo-deleted by turning it into a NOTE.  Otherwise, I1 and I2
+   are pseudo-deleted.
+
+   Return 0 if the combination does not work.  Then nothing is changed.
+   If we did the combination, return the insn at which combine should
+   resume scanning.
+
+   Set NEW_DIRECT_JUMP_P to a nonzero value if try_combine creates a
+   new direct jump instruction.  */
+
+static rtx
+try_combine (rtx i3, rtx i2, rtx i1, int *new_direct_jump_p)
+{
+  /* New patterns for I3 and I2, respectively.  */
+  rtx newpat, newi2pat = 0;
+  int substed_i2 = 0, substed_i1 = 0;
+  /* Indicates need to preserve SET in I1 or I2 in I3 if it is not dead.  */
+  int added_sets_1, added_sets_2;
+  /* Total number of SETs to put into I3.  */
+  int total_sets;
+  /* Nonzero is I2's body now appears in I3.  */
+  int i2_is_used;
+  /* INSN_CODEs for new I3, new I2, and user of condition code.  */
+  int insn_code_number, i2_code_number = 0, other_code_number = 0;
+  /* Contains I3 if the destination of I3 is used in its source, which means
+     that the old life of I3 is being killed.  If that usage is placed into
+     I2 and not in I3, a REG_DEAD note must be made.  */
+  rtx i3dest_killed = 0;
+  /* SET_DEST and SET_SRC of I2 and I1.  */
+  rtx i2dest, i2src, i1dest = 0, i1src = 0;
+  /* PATTERN (I2), or a copy of it in certain cases.  */
+  rtx i2pat;
+  /* Indicates if I2DEST or I1DEST is in I2SRC or I1_SRC.  */
+  int i2dest_in_i2src = 0, i1dest_in_i1src = 0, i2dest_in_i1src = 0;
+  int i1_feeds_i3 = 0;
+  /* Notes that must be added to REG_NOTES in I3 and I2.  */
+  rtx new_i3_notes, new_i2_notes;
+  /* Notes that we substituted I3 into I2 instead of the normal case.  */
+  int i3_subst_into_i2 = 0;
+  /* Notes that I1, I2 or I3 is a MULT operation.  */
+  int have_mult = 0;
+
+  int maxreg;
+  rtx temp;
+  rtx link;
+  int i;
+
+  /* Exit early if one of the insns involved can't be used for
+     combinations.  */
+  if (cant_combine_insn_p (i3)
+      || cant_combine_insn_p (i2)
+      || (i1 && cant_combine_insn_p (i1))
+      /* We also can't do anything if I3 has a
+	 REG_LIBCALL note since we don't want to disrupt the contiguity of a
+	 libcall.  */
+#if 0
+      /* ??? This gives worse code, and appears to be unnecessary, since no
+	 pass after flow uses REG_LIBCALL/REG_RETVAL notes.  */
+      || find_reg_note (i3, REG_LIBCALL, NULL_RTX)
+#endif
+      )
+    return 0;
+
+  combine_attempts++;
+  undobuf.other_insn = 0;
+
+  /* Reset the hard register usage information.  */
+  CLEAR_HARD_REG_SET (newpat_used_regs);
+
+  /* If I1 and I2 both feed I3, they can be in any order.  To simplify the
+     code below, set I1 to be the earlier of the two insns.  */
+  if (i1 && INSN_CUID (i1) > INSN_CUID (i2))
+    temp = i1, i1 = i2, i2 = temp;
+
+  added_links_insn = 0;
+
+  /* First check for one important special-case that the code below will
+     not handle.  Namely, the case where I1 is zero, I2 is a PARALLEL
+     and I3 is a SET whose SET_SRC is a SET_DEST in I2.  In that case,
+     we may be able to replace that destination with the destination of I3.
+     This occurs in the common code where we compute both a quotient and
+     remainder into a structure, in which case we want to do the computation
+     directly into the structure to avoid register-register copies.
+
+     Note that this case handles both multiple sets in I2 and also
+     cases where I2 has a number of CLOBBER or PARALLELs.
+
+     We make very conservative checks below and only try to handle the
+     most common cases of this.  For example, we only handle the case
+     where I2 and I3 are adjacent to avoid making difficult register
+     usage tests.  */
+
+  if (i1 == 0 && GET_CODE (i3) == INSN && GET_CODE (PATTERN (i3)) == SET
+      && GET_CODE (SET_SRC (PATTERN (i3))) == REG
+      && REGNO (SET_SRC (PATTERN (i3))) >= FIRST_PSEUDO_REGISTER
+      && find_reg_note (i3, REG_DEAD, SET_SRC (PATTERN (i3)))
+      && GET_CODE (PATTERN (i2)) == PARALLEL
+      && ! side_effects_p (SET_DEST (PATTERN (i3)))
+      /* If the dest of I3 is a ZERO_EXTRACT or STRICT_LOW_PART, the code
+	 below would need to check what is inside (and reg_overlap_mentioned_p
+	 doesn't support those codes anyway).  Don't allow those destinations;
+	 the resulting insn isn't likely to be recognized anyway.  */
+      && GET_CODE (SET_DEST (PATTERN (i3))) != ZERO_EXTRACT
+      && GET_CODE (SET_DEST (PATTERN (i3))) != STRICT_LOW_PART
+      && ! reg_overlap_mentioned_p (SET_SRC (PATTERN (i3)),
+				    SET_DEST (PATTERN (i3)))
+      && next_real_insn (i2) == i3)
+    {
+      rtx p2 = PATTERN (i2);
+
+      /* Make sure that the destination of I3,
+	 which we are going to substitute into one output of I2,
+	 is not used within another output of I2.  We must avoid making this:
+	 (parallel [(set (mem (reg 69)) ...)
+		    (set (reg 69) ...)])
+	 which is not well-defined as to order of actions.
+	 (Besides, reload can't handle output reloads for this.)
+
+	 The problem can also happen if the dest of I3 is a memory ref,
+	 if another dest in I2 is an indirect memory ref.  */
+      for (i = 0; i < XVECLEN (p2, 0); i++)
+	if ((GET_CODE (XVECEXP (p2, 0, i)) == SET
+	     || GET_CODE (XVECEXP (p2, 0, i)) == CLOBBER)
+	    && reg_overlap_mentioned_p (SET_DEST (PATTERN (i3)),
+					SET_DEST (XVECEXP (p2, 0, i))))
+	  break;
+
+      if (i == XVECLEN (p2, 0))
+	for (i = 0; i < XVECLEN (p2, 0); i++)
+	  if ((GET_CODE (XVECEXP (p2, 0, i)) == SET
+	       || GET_CODE (XVECEXP (p2, 0, i)) == CLOBBER)
+	      && SET_DEST (XVECEXP (p2, 0, i)) == SET_SRC (PATTERN (i3)))
+	    {
+	      combine_merges++;
+
+	      subst_insn = i3;
+	      subst_low_cuid = INSN_CUID (i2);
+
+	      added_sets_2 = added_sets_1 = 0;
+	      i2dest = SET_SRC (PATTERN (i3));
+
+	      /* Replace the dest in I2 with our dest and make the resulting
+		 insn the new pattern for I3.  Then skip to where we
+		 validate the pattern.  Everything was set up above.  */
+	      SUBST (SET_DEST (XVECEXP (p2, 0, i)),
+		     SET_DEST (PATTERN (i3)));
+
+	      newpat = p2;
+	      i3_subst_into_i2 = 1;
+	      goto validate_replacement;
+	    }
+    }
+
+  /* If I2 is setting a double-word pseudo to a constant and I3 is setting
+     one of those words to another constant, merge them by making a new
+     constant.  */
+  if (i1 == 0
+      && (temp = single_set (i2)) != 0
+      && (GET_CODE (SET_SRC (temp)) == CONST_INT
+	  || GET_CODE (SET_SRC (temp)) == CONST_DOUBLE)
+      && GET_CODE (SET_DEST (temp)) == REG
+      && GET_MODE_CLASS (GET_MODE (SET_DEST (temp))) == MODE_INT
+      && GET_MODE_SIZE (GET_MODE (SET_DEST (temp))) == 2 * UNITS_PER_WORD
+      && GET_CODE (PATTERN (i3)) == SET
+      && GET_CODE (SET_DEST (PATTERN (i3))) == SUBREG
+      && SUBREG_REG (SET_DEST (PATTERN (i3))) == SET_DEST (temp)
+      && GET_MODE_CLASS (GET_MODE (SET_DEST (PATTERN (i3)))) == MODE_INT
+      && GET_MODE_SIZE (GET_MODE (SET_DEST (PATTERN (i3)))) == UNITS_PER_WORD
+      && GET_CODE (SET_SRC (PATTERN (i3))) == CONST_INT)
+    {
+      HOST_WIDE_INT lo, hi;
+
+      if (GET_CODE (SET_SRC (temp)) == CONST_INT)
+	lo = INTVAL (SET_SRC (temp)), hi = lo < 0 ? -1 : 0;
+      else
+	{
+	  lo = CONST_DOUBLE_LOW (SET_SRC (temp));
+	  hi = CONST_DOUBLE_HIGH (SET_SRC (temp));
+	}
+
+      if (subreg_lowpart_p (SET_DEST (PATTERN (i3))))
+	{
+	  /* We don't handle the case of the target word being wider
+	     than a host wide int.  */
+	  if (HOST_BITS_PER_WIDE_INT < BITS_PER_WORD)
+	    abort ();
+
+	  lo &= ~(UWIDE_SHIFT_LEFT_BY_BITS_PER_WORD (1) - 1);
+	  lo |= (INTVAL (SET_SRC (PATTERN (i3)))
+		 & (UWIDE_SHIFT_LEFT_BY_BITS_PER_WORD (1) - 1));
+	}
+      else if (HOST_BITS_PER_WIDE_INT == BITS_PER_WORD)
+	hi = INTVAL (SET_SRC (PATTERN (i3)));
+      else if (HOST_BITS_PER_WIDE_INT >= 2 * BITS_PER_WORD)
+	{
+	  int sign = -(int) ((unsigned HOST_WIDE_INT) lo
+			     >> (HOST_BITS_PER_WIDE_INT - 1));
+
+	  lo &= ~ (UWIDE_SHIFT_LEFT_BY_BITS_PER_WORD
+		   (UWIDE_SHIFT_LEFT_BY_BITS_PER_WORD (1) - 1));
+	  lo |= (UWIDE_SHIFT_LEFT_BY_BITS_PER_WORD
+		 (INTVAL (SET_SRC (PATTERN (i3)))));
+	  if (hi == sign)
+	    hi = lo < 0 ? -1 : 0;
+	}
+      else
+	/* We don't handle the case of the higher word not fitting
+	   entirely in either hi or lo.  */
+	abort ();
+
+      combine_merges++;
+      subst_insn = i3;
+      subst_low_cuid = INSN_CUID (i2);
+      added_sets_2 = added_sets_1 = 0;
+      i2dest = SET_DEST (temp);
+
+      SUBST (SET_SRC (temp),
+	     immed_double_const (lo, hi, GET_MODE (SET_DEST (temp))));
+
+      newpat = PATTERN (i2);
+      goto validate_replacement;
+    }
+
+#ifndef HAVE_cc0
+  /* If we have no I1 and I2 looks like:
+	(parallel [(set (reg:CC X) (compare:CC OP (const_int 0)))
+		   (set Y OP)])
+     make up a dummy I1 that is
+	(set Y OP)
+     and change I2 to be
+        (set (reg:CC X) (compare:CC Y (const_int 0)))
+
+     (We can ignore any trailing CLOBBERs.)
+
+     This undoes a previous combination and allows us to match a branch-and-
+     decrement insn.  */
+
+  if (i1 == 0 && GET_CODE (PATTERN (i2)) == PARALLEL
+      && XVECLEN (PATTERN (i2), 0) >= 2
+      && GET_CODE (XVECEXP (PATTERN (i2), 0, 0)) == SET
+      && (GET_MODE_CLASS (GET_MODE (SET_DEST (XVECEXP (PATTERN (i2), 0, 0))))
+	  == MODE_CC)
+      && GET_CODE (SET_SRC (XVECEXP (PATTERN (i2), 0, 0))) == COMPARE
+      && XEXP (SET_SRC (XVECEXP (PATTERN (i2), 0, 0)), 1) == const0_rtx
+      && GET_CODE (XVECEXP (PATTERN (i2), 0, 1)) == SET
+      && GET_CODE (SET_DEST (XVECEXP (PATTERN (i2), 0, 1))) == REG
+      && rtx_equal_p (XEXP (SET_SRC (XVECEXP (PATTERN (i2), 0, 0)), 0),
+		      SET_SRC (XVECEXP (PATTERN (i2), 0, 1))))
+    {
+      for (i = XVECLEN (PATTERN (i2), 0) - 1; i >= 2; i--)
+	if (GET_CODE (XVECEXP (PATTERN (i2), 0, i)) != CLOBBER)
+	  break;
+
+      if (i == 1)
+	{
+	  /* We make I1 with the same INSN_UID as I2.  This gives it
+	     the same INSN_CUID for value tracking.  Our fake I1 will
+	     never appear in the insn stream so giving it the same INSN_UID
+	     as I2 will not cause a problem.  */
+
+	  i1 = gen_rtx_INSN (VOIDmode, INSN_UID (i2), NULL_RTX, i2,
+			     BLOCK_FOR_INSN (i2), INSN_LOCATOR (i2),
+			     XVECEXP (PATTERN (i2), 0, 1), -1, NULL_RTX,
+			     NULL_RTX);
+
+	  SUBST (PATTERN (i2), XVECEXP (PATTERN (i2), 0, 0));
+	  SUBST (XEXP (SET_SRC (PATTERN (i2)), 0),
+		 SET_DEST (PATTERN (i1)));
+	}
+    }
+#endif
+
+  /* Verify that I2 and I1 are valid for combining.  */
+  if (! can_combine_p (i2, i3, i1, NULL_RTX, &i2dest, &i2src)
+      || (i1 && ! can_combine_p (i1, i3, NULL_RTX, i2, &i1dest, &i1src)))
+    {
+      undo_all ();
+      return 0;
+    }
+
+  /* Record whether I2DEST is used in I2SRC and similarly for the other
+     cases.  Knowing this will help in register status updating below.  */
+  i2dest_in_i2src = reg_overlap_mentioned_p (i2dest, i2src);
+  i1dest_in_i1src = i1 && reg_overlap_mentioned_p (i1dest, i1src);
+  i2dest_in_i1src = i1 && reg_overlap_mentioned_p (i2dest, i1src);
+
+  /* See if I1 directly feeds into I3.  It does if I1DEST is not used
+     in I2SRC.  */
+  i1_feeds_i3 = i1 && ! reg_overlap_mentioned_p (i1dest, i2src);
+
+  /* Ensure that I3's pattern can be the destination of combines.  */
+  if (! combinable_i3pat (i3, &PATTERN (i3), i2dest, i1dest,
+			  i1 && i2dest_in_i1src && i1_feeds_i3,
+			  &i3dest_killed))
+    {
+      undo_all ();
+      return 0;
+    }
+
+  /* See if any of the insns is a MULT operation.  Unless one is, we will
+     reject a combination that is, since it must be slower.  Be conservative
+     here.  */
+  if (GET_CODE (i2src) == MULT
+      || (i1 != 0 && GET_CODE (i1src) == MULT)
+      || (GET_CODE (PATTERN (i3)) == SET
+	  && GET_CODE (SET_SRC (PATTERN (i3))) == MULT))
+    have_mult = 1;
+
+  /* If I3 has an inc, then give up if I1 or I2 uses the reg that is inc'd.
+     We used to do this EXCEPT in one case: I3 has a post-inc in an
+     output operand.  However, that exception can give rise to insns like
+	mov r3,(r3)+
+     which is a famous insn on the PDP-11 where the value of r3 used as the
+     source was model-dependent.  Avoid this sort of thing.  */
+
+#if 0
+  if (!(GET_CODE (PATTERN (i3)) == SET
+	&& GET_CODE (SET_SRC (PATTERN (i3))) == REG
+	&& GET_CODE (SET_DEST (PATTERN (i3))) == MEM
+	&& (GET_CODE (XEXP (SET_DEST (PATTERN (i3)), 0)) == POST_INC
+	    || GET_CODE (XEXP (SET_DEST (PATTERN (i3)), 0)) == POST_DEC)))
+    /* It's not the exception.  */
+#endif
+#ifdef AUTO_INC_DEC
+    for (link = REG_NOTES (i3); link; link = XEXP (link, 1))
+      if (REG_NOTE_KIND (link) == REG_INC
+	  && (reg_overlap_mentioned_p (XEXP (link, 0), PATTERN (i2))
+	      || (i1 != 0
+		  && reg_overlap_mentioned_p (XEXP (link, 0), PATTERN (i1)))))
+	{
+	  undo_all ();
+	  return 0;
+	}
+#endif
+
+  /* See if the SETs in I1 or I2 need to be kept around in the merged
+     instruction: whenever the value set there is still needed past I3.
+     For the SETs in I2, this is easy: we see if I2DEST dies or is set in I3.
+
+     For the SET in I1, we have two cases:  If I1 and I2 independently
+     feed into I3, the set in I1 needs to be kept around if I1DEST dies
+     or is set in I3.  Otherwise (if I1 feeds I2 which feeds I3), the set
+     in I1 needs to be kept around unless I1DEST dies or is set in either
+     I2 or I3.  We can distinguish these cases by seeing if I2SRC mentions
+     I1DEST.  If so, we know I1 feeds into I2.  */
+
+  added_sets_2 = ! dead_or_set_p (i3, i2dest);
+
+  added_sets_1
+    = i1 && ! (i1_feeds_i3 ? dead_or_set_p (i3, i1dest)
+	       : (dead_or_set_p (i3, i1dest) || dead_or_set_p (i2, i1dest)));
+
+  /* If the set in I2 needs to be kept around, we must make a copy of
+     PATTERN (I2), so that when we substitute I1SRC for I1DEST in
+     PATTERN (I2), we are only substituting for the original I1DEST, not into
+     an already-substituted copy.  This also prevents making self-referential
+     rtx.  If I2 is a PARALLEL, we just need the piece that assigns I2SRC to
+     I2DEST.  */
+
+  i2pat = (GET_CODE (PATTERN (i2)) == PARALLEL
+	   ? gen_rtx_SET (VOIDmode, i2dest, i2src)
+	   : PATTERN (i2));
+
+  if (added_sets_2)
+    i2pat = copy_rtx (i2pat);
+
+  combine_merges++;
+
+  /* Substitute in the latest insn for the regs set by the earlier ones.  */
+
+  maxreg = max_reg_num ();
+
+  subst_insn = i3;
+
+  /* It is possible that the source of I2 or I1 may be performing an
+     unneeded operation, such as a ZERO_EXTEND of something that is known
+     to have the high part zero.  Handle that case by letting subst look at
+     the innermost one of them.
+
+     Another way to do this would be to have a function that tries to
+     simplify a single insn instead of merging two or more insns.  We don't
+     do this because of the potential of infinite loops and because
+     of the potential extra memory required.  However, doing it the way
+     we are is a bit of a kludge and doesn't catch all cases.
+
+     But only do this if -fexpensive-optimizations since it slows things down
+     and doesn't usually win.  */
+
+  if (flag_expensive_optimizations)
+    {
+      /* Pass pc_rtx so no substitutions are done, just simplifications.
+	 The cases that we are interested in here do not involve the few
+	 cases were is_replaced is checked.  */
+      if (i1)
+	{
+	  subst_low_cuid = INSN_CUID (i1);
+	  i1src = subst (i1src, pc_rtx, pc_rtx, 0, 0);
+	}
+      else
+	{
+	  subst_low_cuid = INSN_CUID (i2);
+	  i2src = subst (i2src, pc_rtx, pc_rtx, 0, 0);
+	}
+    }
+
+#ifndef HAVE_cc0
+  /* Many machines that don't use CC0 have insns that can both perform an
+     arithmetic operation and set the condition code.  These operations will
+     be represented as a PARALLEL with the first element of the vector
+     being a COMPARE of an arithmetic operation with the constant zero.
+     The second element of the vector will set some pseudo to the result
+     of the same arithmetic operation.  If we simplify the COMPARE, we won't
+     match such a pattern and so will generate an extra insn.   Here we test
+     for this case, where both the comparison and the operation result are
+     needed, and make the PARALLEL by just replacing I2DEST in I3SRC with
+     I2SRC.  Later we will make the PARALLEL that contains I2.  */
+
+  if (i1 == 0 && added_sets_2 && GET_CODE (PATTERN (i3)) == SET
+      && GET_CODE (SET_SRC (PATTERN (i3))) == COMPARE
+      && XEXP (SET_SRC (PATTERN (i3)), 1) == const0_rtx
+      && rtx_equal_p (XEXP (SET_SRC (PATTERN (i3)), 0), i2dest))
+    {
+#ifdef SELECT_CC_MODE
+      rtx *cc_use;
+      enum machine_mode compare_mode;
+#endif
+
+      newpat = PATTERN (i3);
+      SUBST (XEXP (SET_SRC (newpat), 0), i2src);
+
+      i2_is_used = 1;
+
+#ifdef SELECT_CC_MODE
+      /* See if a COMPARE with the operand we substituted in should be done
+	 with the mode that is currently being used.  If not, do the same
+	 processing we do in `subst' for a SET; namely, if the destination
+	 is used only once, try to replace it with a register of the proper
+	 mode and also replace the COMPARE.  */
+      if (undobuf.other_insn == 0
+	  && (cc_use = find_single_use (SET_DEST (newpat), i3,
+					&undobuf.other_insn))
+	  && ((compare_mode = SELECT_CC_MODE (GET_CODE (*cc_use),
+					      i2src, const0_rtx))
+	      != GET_MODE (SET_DEST (newpat))))
+	{
+	  unsigned int regno = REGNO (SET_DEST (newpat));
+	  rtx new_dest = gen_rtx_REG (compare_mode, regno);
+
+	  if (regno < FIRST_PSEUDO_REGISTER
+	      || (REG_N_SETS (regno) == 1 && ! added_sets_2
+		  && ! REG_USERVAR_P (SET_DEST (newpat))))
+	    {
+	      if (regno >= FIRST_PSEUDO_REGISTER)
+		SUBST (regno_reg_rtx[regno], new_dest);
+
+	      SUBST (SET_DEST (newpat), new_dest);
+	      SUBST (XEXP (*cc_use, 0), new_dest);
+	      SUBST (SET_SRC (newpat),
+		     gen_rtx_COMPARE (compare_mode, i2src, const0_rtx));
+	    }
+	  else
+	    undobuf.other_insn = 0;
+	}
+#endif
+    }
+  else
+#endif
+    {
+      n_occurrences = 0;		/* `subst' counts here */
+
+      /* If I1 feeds into I2 (not into I3) and I1DEST is in I1SRC, we
+	 need to make a unique copy of I2SRC each time we substitute it
+	 to avoid self-referential rtl.  */
+
+      subst_low_cuid = INSN_CUID (i2);
+      newpat = subst (PATTERN (i3), i2dest, i2src, 0,
+		      ! i1_feeds_i3 && i1dest_in_i1src);
+      substed_i2 = 1;
+
+      /* Record whether i2's body now appears within i3's body.  */
+      i2_is_used = n_occurrences;
+    }
+
+  /* If we already got a failure, don't try to do more.  Otherwise,
+     try to substitute in I1 if we have it.  */
+
+  if (i1 && GET_CODE (newpat) != CLOBBER)
+    {
+      /* Before we can do this substitution, we must redo the test done
+	 above (see detailed comments there) that ensures  that I1DEST
+	 isn't mentioned in any SETs in NEWPAT that are field assignments.  */
+
+      if (! combinable_i3pat (NULL_RTX, &newpat, i1dest, NULL_RTX,
+			      0, (rtx*) 0))
+	{
+	  undo_all ();
+	  return 0;
+	}
+
+      n_occurrences = 0;
+      subst_low_cuid = INSN_CUID (i1);
+      newpat = subst (newpat, i1dest, i1src, 0, 0);
+      substed_i1 = 1;
+    }
+
+  /* Fail if an autoincrement side-effect has been duplicated.  Be careful
+     to count all the ways that I2SRC and I1SRC can be used.  */
+  if ((FIND_REG_INC_NOTE (i2, NULL_RTX) != 0
+       && i2_is_used + added_sets_2 > 1)
+      || (i1 != 0 && FIND_REG_INC_NOTE (i1, NULL_RTX) != 0
+	  && (n_occurrences + added_sets_1 + (added_sets_2 && ! i1_feeds_i3)
+	      > 1))
+      /* Fail if we tried to make a new register (we used to abort, but there's
+	 really no reason to).  */
+      || max_reg_num () != maxreg
+      /* Fail if we couldn't do something and have a CLOBBER.  */
+      || GET_CODE (newpat) == CLOBBER
+      /* Fail if this new pattern is a MULT and we didn't have one before
+	 at the outer level.  */
+      || (GET_CODE (newpat) == SET && GET_CODE (SET_SRC (newpat)) == MULT
+	  && ! have_mult))
+    {
+      undo_all ();
+      return 0;
+    }
+
+  /* If the actions of the earlier insns must be kept
+     in addition to substituting them into the latest one,
+     we must make a new PARALLEL for the latest insn
+     to hold additional the SETs.  */
+
+  if (added_sets_1 || added_sets_2)
+    {
+      combine_extras++;
+
+      if (GET_CODE (newpat) == PARALLEL)
+	{
+	  rtvec old = XVEC (newpat, 0);
+	  total_sets = XVECLEN (newpat, 0) + added_sets_1 + added_sets_2;
+	  newpat = gen_rtx_PARALLEL (VOIDmode, rtvec_alloc (total_sets));
+	  memcpy (XVEC (newpat, 0)->elem, &old->elem[0],
+		  sizeof (old->elem[0]) * old->num_elem);
+	}
+      else
+	{
+	  rtx old = newpat;
+	  total_sets = 1 + added_sets_1 + added_sets_2;
+	  newpat = gen_rtx_PARALLEL (VOIDmode, rtvec_alloc (total_sets));
+	  XVECEXP (newpat, 0, 0) = old;
+	}
+
+      if (added_sets_1)
+	XVECEXP (newpat, 0, --total_sets)
+	  = (GET_CODE (PATTERN (i1)) == PARALLEL
+	     ? gen_rtx_SET (VOIDmode, i1dest, i1src) : PATTERN (i1));
+
+      if (added_sets_2)
+	{
+	  /* If there is no I1, use I2's body as is.  We used to also not do
+	     the subst call below if I2 was substituted into I3,
+	     but that could lose a simplification.  */
+	  if (i1 == 0)
+	    XVECEXP (newpat, 0, --total_sets) = i2pat;
+	  else
+	    /* See comment where i2pat is assigned.  */
+	    XVECEXP (newpat, 0, --total_sets)
+	      = subst (i2pat, i1dest, i1src, 0, 0);
+	}
+    }
+
+  /* We come here when we are replacing a destination in I2 with the
+     destination of I3.  */
+ validate_replacement:
+
+  /* Note which hard regs this insn has as inputs.  */
+  mark_used_regs_combine (newpat);
+
+  /* Is the result of combination a valid instruction?  */
+  insn_code_number = recog_for_combine (&newpat, i3, &new_i3_notes);
+
+  /* If the result isn't valid, see if it is a PARALLEL of two SETs where
+     the second SET's destination is a register that is unused and isn't
+     marked as an instruction that might trap in an EH region.  In that case,
+     we just need the first SET.   This can occur when simplifying a divmod
+     insn.  We *must* test for this case here because the code below that
+     splits two independent SETs doesn't handle this case correctly when it
+     updates the register status.  Also check the case where the first
+     SET's destination is unused.  That would not cause incorrect code, but
+     does cause an unneeded insn to remain.  */
+
+  if (insn_code_number < 0 && GET_CODE (newpat) == PARALLEL
+      && XVECLEN (newpat, 0) == 2
+      && GET_CODE (XVECEXP (newpat, 0, 0)) == SET
+      && GET_CODE (XVECEXP (newpat, 0, 1)) == SET
+      && asm_noperands (newpat) < 0)
+    {
+      rtx set0 = XVECEXP (newpat, 0, 0);
+      rtx set1 = XVECEXP (newpat, 0, 1);
+      rtx note;
+
+      if (((GET_CODE (SET_DEST (set1)) == REG
+	    && find_reg_note (i3, REG_UNUSED, SET_DEST (set1)))
+	   || (GET_CODE (SET_DEST (set1)) == SUBREG
+	       && find_reg_note (i3, REG_UNUSED, SUBREG_REG (SET_DEST (set1)))))
+	  && (!(note = find_reg_note (i3, REG_EH_REGION, NULL_RTX))
+	      || INTVAL (XEXP (note, 0)) <= 0)
+	  && ! side_effects_p (SET_SRC (set1)))
+	{
+	  newpat = set0;
+	  insn_code_number = recog_for_combine (&newpat, i3, &new_i3_notes);
+	}
+
+      else if (((GET_CODE (SET_DEST (set0)) == REG
+		 && find_reg_note (i3, REG_UNUSED, SET_DEST (set0)))
+		|| (GET_CODE (SET_DEST (set0)) == SUBREG
+		    && find_reg_note (i3, REG_UNUSED,
+				      SUBREG_REG (SET_DEST (set0)))))
+	       && (!(note = find_reg_note (i3, REG_EH_REGION, NULL_RTX))
+		   || INTVAL (XEXP (note, 0)) <= 0)
+	       && ! side_effects_p (SET_SRC (set0)))
+	{
+	  newpat = set1;
+	  insn_code_number = recog_for_combine (&newpat, i3, &new_i3_notes);
+
+	  if (insn_code_number >= 0)
+	    {
+	      /* If we will be able to accept this, we have made a
+		 change to the destination of I3.  This requires us to
+		 do a few adjustments.  */
+
+	      PATTERN (i3) = newpat;
+	      adjust_for_new_dest (i3);
+	    }
+	}
+    }
+
+  /* If we were combining three insns and the result is a simple SET
+     with no ASM_OPERANDS that wasn't recognized, try to split it into two
+     insns.  There are two ways to do this.  It can be split using a
+     machine-specific method (like when you have an addition of a large
+     constant) or by combine in the function find_split_point.  */
+
+  if (i1 && insn_code_number < 0 && GET_CODE (newpat) == SET
+      && asm_noperands (newpat) < 0)
+    {
+      rtx m_split, *split;
+      rtx ni2dest = i2dest;
+
+      /* See if the MD file can split NEWPAT.  If it can't, see if letting it
+	 use I2DEST as a scratch register will help.  In the latter case,
+	 convert I2DEST to the mode of the source of NEWPAT if we can.  */
+
+      m_split = split_insns (newpat, i3);
+
+      /* We can only use I2DEST as a scratch reg if it doesn't overlap any
+	 inputs of NEWPAT.  */
+
+      /* ??? If I2DEST is not safe, and I1DEST exists, then it would be
+	 possible to try that as a scratch reg.  This would require adding
+	 more code to make it work though.  */
+
+      if (m_split == 0 && ! reg_overlap_mentioned_p (ni2dest, newpat))
+	{
+	  /* If I2DEST is a hard register or the only use of a pseudo,
+	     we can change its mode.  */
+	  if (GET_MODE (SET_DEST (newpat)) != GET_MODE (i2dest)
+	      && GET_MODE (SET_DEST (newpat)) != VOIDmode
+	      && GET_CODE (i2dest) == REG
+	      && (REGNO (i2dest) < FIRST_PSEUDO_REGISTER
+		  || (REG_N_SETS (REGNO (i2dest)) == 1 && ! added_sets_2
+		      && ! REG_USERVAR_P (i2dest))))
+	    ni2dest = gen_rtx_REG (GET_MODE (SET_DEST (newpat)),
+				   REGNO (i2dest));
+
+	  m_split = split_insns (gen_rtx_PARALLEL
+				 (VOIDmode,
+				  gen_rtvec (2, newpat,
+					     gen_rtx_CLOBBER (VOIDmode,
+							      ni2dest))),
+				 i3);
+	  /* If the split with the mode-changed register didn't work, try
+	     the original register.  */
+	  if (! m_split && ni2dest != i2dest)
+	    {
+	      ni2dest = i2dest;
+	      m_split = split_insns (gen_rtx_PARALLEL
+				     (VOIDmode,
+				      gen_rtvec (2, newpat,
+						 gen_rtx_CLOBBER (VOIDmode,
+								  i2dest))),
+				     i3);
+	    }
+	}
+
+      if (m_split && NEXT_INSN (m_split) == NULL_RTX)
+	{
+	  m_split = PATTERN (m_split);
+	  insn_code_number = recog_for_combine (&m_split, i3, &new_i3_notes);
+	  if (insn_code_number >= 0)
+	    newpat = m_split;
+	}
+      else if (m_split && NEXT_INSN (NEXT_INSN (m_split)) == NULL_RTX
+	       && (next_real_insn (i2) == i3
+		   || ! use_crosses_set_p (PATTERN (m_split), INSN_CUID (i2))))
+	{
+	  rtx i2set, i3set;
+	  rtx newi3pat = PATTERN (NEXT_INSN (m_split));
+	  newi2pat = PATTERN (m_split);
+
+	  i3set = single_set (NEXT_INSN (m_split));
+	  i2set = single_set (m_split);
+
+	  /* In case we changed the mode of I2DEST, replace it in the
+	     pseudo-register table here.  We can't do it above in case this
+	     code doesn't get executed and we do a split the other way.  */
+
+	  if (REGNO (i2dest) >= FIRST_PSEUDO_REGISTER)
+	    SUBST (regno_reg_rtx[REGNO (i2dest)], ni2dest);
+
+	  i2_code_number = recog_for_combine (&newi2pat, i2, &new_i2_notes);
+
+	  /* If I2 or I3 has multiple SETs, we won't know how to track
+	     register status, so don't use these insns.  If I2's destination
+	     is used between I2 and I3, we also can't use these insns.  */
+
+	  if (i2_code_number >= 0 && i2set && i3set
+	      && (next_real_insn (i2) == i3
+		  || ! reg_used_between_p (SET_DEST (i2set), i2, i3)))
+	    insn_code_number = recog_for_combine (&newi3pat, i3,
+						  &new_i3_notes);
+	  if (insn_code_number >= 0)
+	    newpat = newi3pat;
+
+	  /* It is possible that both insns now set the destination of I3.
+	     If so, we must show an extra use of it.  */
+
+	  if (insn_code_number >= 0)
+	    {
+	      rtx new_i3_dest = SET_DEST (i3set);
+	      rtx new_i2_dest = SET_DEST (i2set);
+
+	      while (GET_CODE (new_i3_dest) == ZERO_EXTRACT
+		     || GET_CODE (new_i3_dest) == STRICT_LOW_PART
+		     || GET_CODE (new_i3_dest) == SUBREG)
+		new_i3_dest = XEXP (new_i3_dest, 0);
+
+	      while (GET_CODE (new_i2_dest) == ZERO_EXTRACT
+		     || GET_CODE (new_i2_dest) == STRICT_LOW_PART
+		     || GET_CODE (new_i2_dest) == SUBREG)
+		new_i2_dest = XEXP (new_i2_dest, 0);
+
+	      if (GET_CODE (new_i3_dest) == REG
+		  && GET_CODE (new_i2_dest) == REG
+		  && REGNO (new_i3_dest) == REGNO (new_i2_dest))
+		REG_N_SETS (REGNO (new_i2_dest))++;
+	    }
+	}
+
+      /* If we can split it and use I2DEST, go ahead and see if that
+	 helps things be recognized.  Verify that none of the registers
+	 are set between I2 and I3.  */
+      if (insn_code_number < 0 && (split = find_split_point (&newpat, i3)) != 0
+#ifdef HAVE_cc0
+	  && GET_CODE (i2dest) == REG
+#endif
+	  /* We need I2DEST in the proper mode.  If it is a hard register
+	     or the only use of a pseudo, we can change its mode.  */
+	  && (GET_MODE (*split) == GET_MODE (i2dest)
+	      || GET_MODE (*split) == VOIDmode
+	      || REGNO (i2dest) < FIRST_PSEUDO_REGISTER
+	      || (REG_N_SETS (REGNO (i2dest)) == 1 && ! added_sets_2
+		  && ! REG_USERVAR_P (i2dest)))
+	  && (next_real_insn (i2) == i3
+	      || ! use_crosses_set_p (*split, INSN_CUID (i2)))
+	  /* We can't overwrite I2DEST if its value is still used by
+	     NEWPAT.  */
+	  && ! reg_referenced_p (i2dest, newpat))
+	{
+	  rtx newdest = i2dest;
+	  enum rtx_code split_code = GET_CODE (*split);
+	  enum machine_mode split_mode = GET_MODE (*split);
+
+	  /* Get NEWDEST as a register in the proper mode.  We have already
+	     validated that we can do this.  */
+	  if (GET_MODE (i2dest) != split_mode && split_mode != VOIDmode)
+	    {
+	      newdest = gen_rtx_REG (split_mode, REGNO (i2dest));
+
+	      if (REGNO (i2dest) >= FIRST_PSEUDO_REGISTER)
+		SUBST (regno_reg_rtx[REGNO (i2dest)], newdest);
+	    }
+
+	  /* If *SPLIT is a (mult FOO (const_int pow2)), convert it to
+	     an ASHIFT.  This can occur if it was inside a PLUS and hence
+	     appeared to be a memory address.  This is a kludge.  */
+	  if (split_code == MULT
+	      && GET_CODE (XEXP (*split, 1)) == CONST_INT
+	      && INTVAL (XEXP (*split, 1)) > 0
+	      && (i = exact_log2 (INTVAL (XEXP (*split, 1)))) >= 0)
+	    {
+	      SUBST (*split, gen_rtx_ASHIFT (split_mode,
+					     XEXP (*split, 0), GEN_INT (i)));
+	      /* Update split_code because we may not have a multiply
+		 anymore.  */
+	      split_code = GET_CODE (*split);
+	    }
+
+#ifdef INSN_SCHEDULING
+	  /* If *SPLIT is a paradoxical SUBREG, when we split it, it should
+	     be written as a ZERO_EXTEND.  */
+	  if (split_code == SUBREG && GET_CODE (SUBREG_REG (*split)) == MEM)
+	    {
+#ifdef LOAD_EXTEND_OP
+	      /* Or as a SIGN_EXTEND if LOAD_EXTEND_OP says that that's
+		 what it really is.  */
+	      if (LOAD_EXTEND_OP (GET_MODE (SUBREG_REG (*split)))
+		  == SIGN_EXTEND)
+		SUBST (*split, gen_rtx_SIGN_EXTEND (split_mode,
+						    SUBREG_REG (*split)));
+	      else
+#endif
+		SUBST (*split, gen_rtx_ZERO_EXTEND (split_mode,
+						    SUBREG_REG (*split)));
+	    }
+#endif
+
+	  newi2pat = gen_rtx_SET (VOIDmode, newdest, *split);
+	  SUBST (*split, newdest);
+	  i2_code_number = recog_for_combine (&newi2pat, i2, &new_i2_notes);
+
+	  /* If the split point was a MULT and we didn't have one before,
+	     don't use one now.  */
+	  if (i2_code_number >= 0 && ! (split_code == MULT && ! have_mult))
+	    insn_code_number = recog_for_combine (&newpat, i3, &new_i3_notes);
+	}
+    }
+
+  /* Check for a case where we loaded from memory in a narrow mode and
+     then sign extended it, but we need both registers.  In that case,
+     we have a PARALLEL with both loads from the same memory location.
+     We can split this into a load from memory followed by a register-register
+     copy.  This saves at least one insn, more if register allocation can
+     eliminate the copy.
+
+     We cannot do this if the destination of the first assignment is a
+     condition code register or cc0.  We eliminate this case by making sure
+     the SET_DEST and SET_SRC have the same mode.
+
+     We cannot do this if the destination of the second assignment is
+     a register that we have already assumed is zero-extended.  Similarly
+     for a SUBREG of such a register.  */
+
+  else if (i1 && insn_code_number < 0 && asm_noperands (newpat) < 0
+	   && GET_CODE (newpat) == PARALLEL
+	   && XVECLEN (newpat, 0) == 2
+	   && GET_CODE (XVECEXP (newpat, 0, 0)) == SET
+	   && GET_CODE (SET_SRC (XVECEXP (newpat, 0, 0))) == SIGN_EXTEND
+	   && (GET_MODE (SET_DEST (XVECEXP (newpat, 0, 0)))
+	       == GET_MODE (SET_SRC (XVECEXP (newpat, 0, 0))))
+	   && GET_CODE (XVECEXP (newpat, 0, 1)) == SET
+	   && rtx_equal_p (SET_SRC (XVECEXP (newpat, 0, 1)),
+			   XEXP (SET_SRC (XVECEXP (newpat, 0, 0)), 0))
+	   && ! use_crosses_set_p (SET_SRC (XVECEXP (newpat, 0, 1)),
+				   INSN_CUID (i2))
+	   && GET_CODE (SET_DEST (XVECEXP (newpat, 0, 1))) != ZERO_EXTRACT
+	   && GET_CODE (SET_DEST (XVECEXP (newpat, 0, 1))) != STRICT_LOW_PART
+	   && ! (temp = SET_DEST (XVECEXP (newpat, 0, 1)),
+		 (GET_CODE (temp) == REG
+		  && reg_nonzero_bits[REGNO (temp)] != 0
+		  && GET_MODE_BITSIZE (GET_MODE (temp)) < BITS_PER_WORD
+		  && GET_MODE_BITSIZE (GET_MODE (temp)) < HOST_BITS_PER_INT
+		  && (reg_nonzero_bits[REGNO (temp)]
+		      != GET_MODE_MASK (word_mode))))
+	   && ! (GET_CODE (SET_DEST (XVECEXP (newpat, 0, 1))) == SUBREG
+		 && (temp = SUBREG_REG (SET_DEST (XVECEXP (newpat, 0, 1))),
+		     (GET_CODE (temp) == REG
+		      && reg_nonzero_bits[REGNO (temp)] != 0
+		      && GET_MODE_BITSIZE (GET_MODE (temp)) < BITS_PER_WORD
+		      && GET_MODE_BITSIZE (GET_MODE (temp)) < HOST_BITS_PER_INT
+		      && (reg_nonzero_bits[REGNO (temp)]
+			  != GET_MODE_MASK (word_mode)))))
+	   && ! reg_overlap_mentioned_p (SET_DEST (XVECEXP (newpat, 0, 1)),
+					 SET_SRC (XVECEXP (newpat, 0, 1)))
+	   && ! find_reg_note (i3, REG_UNUSED,
+			       SET_DEST (XVECEXP (newpat, 0, 0))))
+    {
+      rtx ni2dest;
+
+      newi2pat = XVECEXP (newpat, 0, 0);
+      ni2dest = SET_DEST (XVECEXP (newpat, 0, 0));
+      newpat = XVECEXP (newpat, 0, 1);
+      SUBST (SET_SRC (newpat),
+	     gen_lowpart_for_combine (GET_MODE (SET_SRC (newpat)), ni2dest));
+      i2_code_number = recog_for_combine (&newi2pat, i2, &new_i2_notes);
+
+      if (i2_code_number >= 0)
+	insn_code_number = recog_for_combine (&newpat, i3, &new_i3_notes);
+
+      if (insn_code_number >= 0)
+	{
+	  rtx insn;
+	  rtx link;
+
+	  /* If we will be able to accept this, we have made a change to the
+	     destination of I3.  This requires us to do a few adjustments.  */
+	  PATTERN (i3) = newpat;
+	  adjust_for_new_dest (i3);
+
+	  /* I3 now uses what used to be its destination and which is
+	     now I2's destination.  That means we need a LOG_LINK from
+	     I3 to I2.  But we used to have one, so we still will.
+
+	     However, some later insn might be using I2's dest and have
+	     a LOG_LINK pointing at I3.  We must remove this link.
+	     The simplest way to remove the link is to point it at I1,
+	     which we know will be a NOTE.  */
+
+	  for (insn = NEXT_INSN (i3);
+	       insn && (this_basic_block->next_bb == EXIT_BLOCK_PTR
+			|| insn != BB_HEAD (this_basic_block->next_bb));
+	       insn = NEXT_INSN (insn))
+	    {
+	      if (INSN_P (insn) && reg_referenced_p (ni2dest, PATTERN (insn)))
+		{
+		  for (link = LOG_LINKS (insn); link;
+		       link = XEXP (link, 1))
+		    if (XEXP (link, 0) == i3)
+		      XEXP (link, 0) = i1;
+
+		  break;
+		}
+	    }
+	}
+    }
+
+  /* Similarly, check for a case where we have a PARALLEL of two independent
+     SETs but we started with three insns.  In this case, we can do the sets
+     as two separate insns.  This case occurs when some SET allows two
+     other insns to combine, but the destination of that SET is still live.  */
+
+  else if (i1 && insn_code_number < 0 && asm_noperands (newpat) < 0
+	   && GET_CODE (newpat) == PARALLEL
+	   && XVECLEN (newpat, 0) == 2
+	   && GET_CODE (XVECEXP (newpat, 0, 0)) == SET
+	   && GET_CODE (SET_DEST (XVECEXP (newpat, 0, 0))) != ZERO_EXTRACT
+	   && GET_CODE (SET_DEST (XVECEXP (newpat, 0, 0))) != STRICT_LOW_PART
+	   && GET_CODE (XVECEXP (newpat, 0, 1)) == SET
+	   && GET_CODE (SET_DEST (XVECEXP (newpat, 0, 1))) != ZERO_EXTRACT
+	   && GET_CODE (SET_DEST (XVECEXP (newpat, 0, 1))) != STRICT_LOW_PART
+	   && ! use_crosses_set_p (SET_SRC (XVECEXP (newpat, 0, 1)),
+				   INSN_CUID (i2))
+	   /* Don't pass sets with (USE (MEM ...)) dests to the following.  */
+	   && GET_CODE (SET_DEST (XVECEXP (newpat, 0, 1))) != USE
+	   && GET_CODE (SET_DEST (XVECEXP (newpat, 0, 0))) != USE
+	   && ! reg_referenced_p (SET_DEST (XVECEXP (newpat, 0, 1)),
+				  XVECEXP (newpat, 0, 0))
+	   && ! reg_referenced_p (SET_DEST (XVECEXP (newpat, 0, 0)),
+				  XVECEXP (newpat, 0, 1))
+	   && ! (contains_muldiv (SET_SRC (XVECEXP (newpat, 0, 0)))
+		 && contains_muldiv (SET_SRC (XVECEXP (newpat, 0, 1)))))
+    {
+      /* Normally, it doesn't matter which of the two is done first,
+	 but it does if one references cc0.  In that case, it has to
+	 be first.  */
+#ifdef HAVE_cc0
+      if (reg_referenced_p (cc0_rtx, XVECEXP (newpat, 0, 0)))
+	{
+	  newi2pat = XVECEXP (newpat, 0, 0);
+	  newpat = XVECEXP (newpat, 0, 1);
+	}
+      else
+#endif
+	{
+	  newi2pat = XVECEXP (newpat, 0, 1);
+	  newpat = XVECEXP (newpat, 0, 0);
+	}
+
+      i2_code_number = recog_for_combine (&newi2pat, i2, &new_i2_notes);
+
+      if (i2_code_number >= 0)
+	insn_code_number = recog_for_combine (&newpat, i3, &new_i3_notes);
+    }
+
+  /* If it still isn't recognized, fail and change things back the way they
+     were.  */
+  if ((insn_code_number < 0
+       /* Is the result a reasonable ASM_OPERANDS?  */
+       && (! check_asm_operands (newpat) || added_sets_1 || added_sets_2)))
+    {
+      undo_all ();
+      return 0;
+    }
+
+  /* If we had to change another insn, make sure it is valid also.  */
+  if (undobuf.other_insn)
+    {
+      rtx other_pat = PATTERN (undobuf.other_insn);
+      rtx new_other_notes;
+      rtx note, next;
+
+      CLEAR_HARD_REG_SET (newpat_used_regs);
+
+      other_code_number = recog_for_combine (&other_pat, undobuf.other_insn,
+					     &new_other_notes);
+
+      if (other_code_number < 0 && ! check_asm_operands (other_pat))
+	{
+	  undo_all ();
+	  return 0;
+	}
+
+      PATTERN (undobuf.other_insn) = other_pat;
+
+      /* If any of the notes in OTHER_INSN were REG_UNUSED, ensure that they
+	 are still valid.  Then add any non-duplicate notes added by
+	 recog_for_combine.  */
+      for (note = REG_NOTES (undobuf.other_insn); note; note = next)
+	{
+	  next = XEXP (note, 1);
+
+	  if (REG_NOTE_KIND (note) == REG_UNUSED
+	      && ! reg_set_p (XEXP (note, 0), PATTERN (undobuf.other_insn)))
+	    {
+	      if (GET_CODE (XEXP (note, 0)) == REG)
+		REG_N_DEATHS (REGNO (XEXP (note, 0)))--;
+
+	      remove_note (undobuf.other_insn, note);
+	    }
+	}
+
+      for (note = new_other_notes; note; note = XEXP (note, 1))
+	if (GET_CODE (XEXP (note, 0)) == REG)
+	  REG_N_DEATHS (REGNO (XEXP (note, 0)))++;
+
+      distribute_notes (new_other_notes, undobuf.other_insn,
+			undobuf.other_insn, NULL_RTX);
+    }
+#ifdef HAVE_cc0
+  /* If I2 is the setter CC0 and I3 is the user CC0 then check whether
+     they are adjacent to each other or not.  */
+  {
+    rtx p = prev_nonnote_insn (i3);
+    if (p && p != i2 && GET_CODE (p) == INSN && newi2pat
+	&& sets_cc0_p (newi2pat))
+      {
+	undo_all ();
+	return 0;
+      }
+  }
+#endif
+
+  /* We now know that we can do this combination.  Merge the insns and
+     update the status of registers and LOG_LINKS.  */
+
+  {
+    rtx i3notes, i2notes, i1notes = 0;
+    rtx i3links, i2links, i1links = 0;
+    rtx midnotes = 0;
+    unsigned int regno;
+
+    /* Get the old REG_NOTES and LOG_LINKS from all our insns and
+       clear them.  */
+    i3notes = REG_NOTES (i3), i3links = LOG_LINKS (i3);
+    i2notes = REG_NOTES (i2), i2links = LOG_LINKS (i2);
+    if (i1)
+      i1notes = REG_NOTES (i1), i1links = LOG_LINKS (i1);
+
+    /* Ensure that we do not have something that should not be shared but
+       occurs multiple times in the new insns.  Check this by first
+       resetting all the `used' flags and then copying anything is shared.  */
+
+    reset_used_flags (i3notes);
+    reset_used_flags (i2notes);
+    reset_used_flags (i1notes);
+    reset_used_flags (newpat);
+    reset_used_flags (newi2pat);
+    if (undobuf.other_insn)
+      reset_used_flags (PATTERN (undobuf.other_insn));
+
+    i3notes = copy_rtx_if_shared (i3notes);
+    i2notes = copy_rtx_if_shared (i2notes);
+    i1notes = copy_rtx_if_shared (i1notes);
+    newpat = copy_rtx_if_shared (newpat);
+    newi2pat = copy_rtx_if_shared (newi2pat);
+    if (undobuf.other_insn)
+      reset_used_flags (PATTERN (undobuf.other_insn));
+
+    INSN_CODE (i3) = insn_code_number;
+    PATTERN (i3) = newpat;
+
+    if (GET_CODE (i3) == CALL_INSN && CALL_INSN_FUNCTION_USAGE (i3))
+      {
+	rtx call_usage = CALL_INSN_FUNCTION_USAGE (i3);
+
+	reset_used_flags (call_usage);
+	call_usage = copy_rtx (call_usage);
+
+	if (substed_i2)
+	  replace_rtx (call_usage, i2dest, i2src);
+
+	if (substed_i1)
+	  replace_rtx (call_usage, i1dest, i1src);
+
+	CALL_INSN_FUNCTION_USAGE (i3) = call_usage;
+      }
+
+    if (undobuf.other_insn)
+      INSN_CODE (undobuf.other_insn) = other_code_number;
+
+    /* We had one special case above where I2 had more than one set and
+       we replaced a destination of one of those sets with the destination
+       of I3.  In that case, we have to update LOG_LINKS of insns later
+       in this basic block.  Note that this (expensive) case is rare.
+
+       Also, in this case, we must pretend that all REG_NOTEs for I2
+       actually came from I3, so that REG_UNUSED notes from I2 will be
+       properly handled.  */
+
+    if (i3_subst_into_i2)
+      {
+	for (i = 0; i < XVECLEN (PATTERN (i2), 0); i++)
+	  if (GET_CODE (XVECEXP (PATTERN (i2), 0, i)) != USE
+	      && GET_CODE (SET_DEST (XVECEXP (PATTERN (i2), 0, i))) == REG
+	      && SET_DEST (XVECEXP (PATTERN (i2), 0, i)) != i2dest
+	      && ! find_reg_note (i2, REG_UNUSED,
+				  SET_DEST (XVECEXP (PATTERN (i2), 0, i))))
+	    for (temp = NEXT_INSN (i2);
+		 temp && (this_basic_block->next_bb == EXIT_BLOCK_PTR
+			  || BB_HEAD (this_basic_block) != temp);
+		 temp = NEXT_INSN (temp))
+	      if (temp != i3 && INSN_P (temp))
+		for (link = LOG_LINKS (temp); link; link = XEXP (link, 1))
+		  if (XEXP (link, 0) == i2)
+		    XEXP (link, 0) = i3;
+
+	if (i3notes)
+	  {
+	    rtx link = i3notes;
+	    while (XEXP (link, 1))
+	      link = XEXP (link, 1);
+	    XEXP (link, 1) = i2notes;
+	  }
+	else
+	  i3notes = i2notes;
+	i2notes = 0;
+      }
+
+    LOG_LINKS (i3) = 0;
+    REG_NOTES (i3) = 0;
+    LOG_LINKS (i2) = 0;
+    REG_NOTES (i2) = 0;
+
+    if (newi2pat)
+      {
+	INSN_CODE (i2) = i2_code_number;
+	PATTERN (i2) = newi2pat;
+      }
+    else
+      {
+	PUT_CODE (i2, NOTE);
+	NOTE_LINE_NUMBER (i2) = NOTE_INSN_DELETED;
+	NOTE_SOURCE_FILE (i2) = 0;
+      }
+
+    if (i1)
+      {
+	LOG_LINKS (i1) = 0;
+	REG_NOTES (i1) = 0;
+	PUT_CODE (i1, NOTE);
+	NOTE_LINE_NUMBER (i1) = NOTE_INSN_DELETED;
+	NOTE_SOURCE_FILE (i1) = 0;
+      }
+
+    /* Get death notes for everything that is now used in either I3 or
+       I2 and used to die in a previous insn.  If we built two new
+       patterns, move from I1 to I2 then I2 to I3 so that we get the
+       proper movement on registers that I2 modifies.  */
+
+    if (newi2pat)
+      {
+	move_deaths (newi2pat, NULL_RTX, INSN_CUID (i1), i2, &midnotes);
+	move_deaths (newpat, newi2pat, INSN_CUID (i1), i3, &midnotes);
+      }
+    else
+      move_deaths (newpat, NULL_RTX, i1 ? INSN_CUID (i1) : INSN_CUID (i2),
+		   i3, &midnotes);
+
+    /* Distribute all the LOG_LINKS and REG_NOTES from I1, I2, and I3.  */
+    if (i3notes)
+      distribute_notes (i3notes, i3, i3, newi2pat ? i2 : NULL_RTX);
+    if (i2notes)
+      distribute_notes (i2notes, i2, i3, newi2pat ? i2 : NULL_RTX);
+    if (i1notes)
+      distribute_notes (i1notes, i1, i3, newi2pat ? i2 : NULL_RTX);
+    if (midnotes)
+      distribute_notes (midnotes, NULL_RTX, i3, newi2pat ? i2 : NULL_RTX);
+
+    /* Distribute any notes added to I2 or I3 by recog_for_combine.  We
+       know these are REG_UNUSED and want them to go to the desired insn,
+       so we always pass it as i3.  We have not counted the notes in
+       reg_n_deaths yet, so we need to do so now.  */
+
+    if (newi2pat && new_i2_notes)
+      {
+	for (temp = new_i2_notes; temp; temp = XEXP (temp, 1))
+	  if (GET_CODE (XEXP (temp, 0)) == REG)
+	    REG_N_DEATHS (REGNO (XEXP (temp, 0)))++;
+
+	distribute_notes (new_i2_notes, i2, i2, NULL_RTX);
+      }
+
+    if (new_i3_notes)
+      {
+	for (temp = new_i3_notes; temp; temp = XEXP (temp, 1))
+	  if (GET_CODE (XEXP (temp, 0)) == REG)
+	    REG_N_DEATHS (REGNO (XEXP (temp, 0)))++;
+
+	distribute_notes (new_i3_notes, i3, i3, NULL_RTX);
+      }
+
+    /* If I3DEST was used in I3SRC, it really died in I3.  We may need to
+       put a REG_DEAD note for it somewhere.  If NEWI2PAT exists and sets
+       I3DEST, the death must be somewhere before I2, not I3.  If we passed I3
+       in that case, it might delete I2.  Similarly for I2 and I1.
+       Show an additional death due to the REG_DEAD note we make here.  If
+       we discard it in distribute_notes, we will decrement it again.  */
+
+    if (i3dest_killed)
+      {
+	if (GET_CODE (i3dest_killed) == REG)
+	  REG_N_DEATHS (REGNO (i3dest_killed))++;
+
+	if (newi2pat && reg_set_p (i3dest_killed, newi2pat))
+	  distribute_notes (gen_rtx_EXPR_LIST (REG_DEAD, i3dest_killed,
+					       NULL_RTX),
+			    NULL_RTX, i2, NULL_RTX);
+	else
+	  distribute_notes (gen_rtx_EXPR_LIST (REG_DEAD, i3dest_killed,
+					       NULL_RTX),
+			    NULL_RTX, i3, newi2pat ? i2 : NULL_RTX);
+      }
+
+    if (i2dest_in_i2src)
+      {
+	if (GET_CODE (i2dest) == REG)
+	  REG_N_DEATHS (REGNO (i2dest))++;
+
+	if (newi2pat && reg_set_p (i2dest, newi2pat))
+	  distribute_notes (gen_rtx_EXPR_LIST (REG_DEAD, i2dest, NULL_RTX),
+			    NULL_RTX, i2, NULL_RTX);
+	else
+	  distribute_notes (gen_rtx_EXPR_LIST (REG_DEAD, i2dest, NULL_RTX),
+			    NULL_RTX, i3, newi2pat ? i2 : NULL_RTX);
+      }
+
+    if (i1dest_in_i1src)
+      {
+	if (GET_CODE (i1dest) == REG)
+	  REG_N_DEATHS (REGNO (i1dest))++;
+
+	if (newi2pat && reg_set_p (i1dest, newi2pat))
+	  distribute_notes (gen_rtx_EXPR_LIST (REG_DEAD, i1dest, NULL_RTX),
+			    NULL_RTX, i2, NULL_RTX);
+	else
+	  distribute_notes (gen_rtx_EXPR_LIST (REG_DEAD, i1dest, NULL_RTX),
+			    NULL_RTX, i3, newi2pat ? i2 : NULL_RTX);
+      }
+
+    distribute_links (i3links);
+    distribute_links (i2links);
+    distribute_links (i1links);
+
+    if (GET_CODE (i2dest) == REG)
+      {
+	rtx link;
+	rtx i2_insn = 0, i2_val = 0, set;
+
+	/* The insn that used to set this register doesn't exist, and
+	   this life of the register may not exist either.  See if one of
+	   I3's links points to an insn that sets I2DEST.  If it does,
+	   that is now the last known value for I2DEST. If we don't update
+	   this and I2 set the register to a value that depended on its old
+	   contents, we will get confused.  If this insn is used, thing
+	   will be set correctly in combine_instructions.  */
+
+	for (link = LOG_LINKS (i3); link; link = XEXP (link, 1))
+	  if ((set = single_set (XEXP (link, 0))) != 0
+	      && rtx_equal_p (i2dest, SET_DEST (set)))
+	    i2_insn = XEXP (link, 0), i2_val = SET_SRC (set);
+
+	record_value_for_reg (i2dest, i2_insn, i2_val);
+
+	/* If the reg formerly set in I2 died only once and that was in I3,
+	   zero its use count so it won't make `reload' do any work.  */
+	if (! added_sets_2
+	    && (newi2pat == 0 || ! reg_mentioned_p (i2dest, newi2pat))
+	    && ! i2dest_in_i2src)
+	  {
+	    regno = REGNO (i2dest);
+	    REG_N_SETS (regno)--;
+	  }
+      }
+
+    if (i1 && GET_CODE (i1dest) == REG)
+      {
+	rtx link;
+	rtx i1_insn = 0, i1_val = 0, set;
+
+	for (link = LOG_LINKS (i3); link; link = XEXP (link, 1))
+	  if ((set = single_set (XEXP (link, 0))) != 0
+	      && rtx_equal_p (i1dest, SET_DEST (set)))
+	    i1_insn = XEXP (link, 0), i1_val = SET_SRC (set);
+
+	record_value_for_reg (i1dest, i1_insn, i1_val);
+
+	regno = REGNO (i1dest);
+	if (! added_sets_1 && ! i1dest_in_i1src)
+	  REG_N_SETS (regno)--;
+      }
+
+    /* Update reg_nonzero_bits et al for any changes that may have been made
+       to this insn.  The order of set_nonzero_bits_and_sign_copies() is
+       important.  Because newi2pat can affect nonzero_bits of newpat */
+    if (newi2pat)
+      note_stores (newi2pat, set_nonzero_bits_and_sign_copies, NULL);
+    note_stores (newpat, set_nonzero_bits_and_sign_copies, NULL);
+
+    /* Set new_direct_jump_p if a new return or simple jump instruction
+       has been created.
+
+       If I3 is now an unconditional jump, ensure that it has a
+       BARRIER following it since it may have initially been a
+       conditional jump.  It may also be the last nonnote insn.  */
+
+    if (returnjump_p (i3) || any_uncondjump_p (i3))
+      {
+	*new_direct_jump_p = 1;
+	mark_jump_label (PATTERN (i3), i3, 0);
+
+	if ((temp = next_nonnote_insn (i3)) == NULL_RTX
+	    || GET_CODE (temp) != BARRIER)
+	  emit_barrier_after (i3);
+      }
+
+    if (undobuf.other_insn != NULL_RTX
+	&& (returnjump_p (undobuf.other_insn)
+	    || any_uncondjump_p (undobuf.other_insn)))
+      {
+	*new_direct_jump_p = 1;
+
+	if ((temp = next_nonnote_insn (undobuf.other_insn)) == NULL_RTX
+	    || GET_CODE (temp) != BARRIER)
+	  emit_barrier_after (undobuf.other_insn);
+      }
+
+    /* An NOOP jump does not need barrier, but it does need cleaning up
+       of CFG.  */
+    if (GET_CODE (newpat) == SET
+	&& SET_SRC (newpat) == pc_rtx
+	&& SET_DEST (newpat) == pc_rtx)
+      *new_direct_jump_p = 1;
+  }
+
+  combine_successes++;
+  undo_commit ();
+
+  if (added_links_insn
+      && (newi2pat == 0 || INSN_CUID (added_links_insn) < INSN_CUID (i2))
+      && INSN_CUID (added_links_insn) < INSN_CUID (i3))
+    return added_links_insn;
+  else
+    return newi2pat ? i2 : i3;
+}
+
+/* Undo all the modifications recorded in undobuf.  */
+
+static void
+undo_all (void)
+{
+  struct undo *undo, *next;
+
+  for (undo = undobuf.undos; undo; undo = next)
+    {
+      next = undo->next;
+      if (undo->is_int)
+	*undo->where.i = undo->old_contents.i;
+      else
+	*undo->where.r = undo->old_contents.r;
+
+      undo->next = undobuf.frees;
+      undobuf.frees = undo;
+    }
+
+  undobuf.undos = 0;
+}
+
+/* We've committed to accepting the changes we made.  Move all
+   of the undos to the free list.  */
+
+static void
+undo_commit (void)
+{
+  struct undo *undo, *next;
+
+  for (undo = undobuf.undos; undo; undo = next)
+    {
+      next = undo->next;
+      undo->next = undobuf.frees;
+      undobuf.frees = undo;
+    }
+  undobuf.undos = 0;
+}
+
+
+/* Find the innermost point within the rtx at LOC, possibly LOC itself,
+   where we have an arithmetic expression and return that point.  LOC will
+   be inside INSN.
+
+   try_combine will call this function to see if an insn can be split into
+   two insns.  */
+
+static rtx *
+find_split_point (rtx *loc, rtx insn)
+{
+  rtx x = *loc;
+  enum rtx_code code = GET_CODE (x);
+  rtx *split;
+  unsigned HOST_WIDE_INT len = 0;
+  HOST_WIDE_INT pos = 0;
+  int unsignedp = 0;
+  rtx inner = NULL_RTX;
+
+  /* First special-case some codes.  */
+  switch (code)
+    {
+    case SUBREG:
+#ifdef INSN_SCHEDULING
+      /* If we are making a paradoxical SUBREG invalid, it becomes a split
+	 point.  */
+      if (GET_CODE (SUBREG_REG (x)) == MEM)
+	return loc;
+#endif
+      return find_split_point (&SUBREG_REG (x), insn);
+
+    case MEM:
+#ifdef HAVE_lo_sum
+      /* If we have (mem (const ..)) or (mem (symbol_ref ...)), split it
+	 using LO_SUM and HIGH.  */
+      if (GET_CODE (XEXP (x, 0)) == CONST
+	  || GET_CODE (XEXP (x, 0)) == SYMBOL_REF)
+	{
+	  SUBST (XEXP (x, 0),
+		 gen_rtx_LO_SUM (Pmode,
+				 gen_rtx_HIGH (Pmode, XEXP (x, 0)),
+				 XEXP (x, 0)));
+	  return &XEXP (XEXP (x, 0), 0);
+	}
+#endif
+
+      /* If we have a PLUS whose second operand is a constant and the
+	 address is not valid, perhaps will can split it up using
+	 the machine-specific way to split large constants.  We use
+	 the first pseudo-reg (one of the virtual regs) as a placeholder;
+	 it will not remain in the result.  */
+      if (GET_CODE (XEXP (x, 0)) == PLUS
+	  && GET_CODE (XEXP (XEXP (x, 0), 1)) == CONST_INT
+	  && ! memory_address_p (GET_MODE (x), XEXP (x, 0)))
+	{
+	  rtx reg = regno_reg_rtx[FIRST_PSEUDO_REGISTER];
+	  rtx seq = split_insns (gen_rtx_SET (VOIDmode, reg, XEXP (x, 0)),
+				 subst_insn);
+
+	  /* This should have produced two insns, each of which sets our
+	     placeholder.  If the source of the second is a valid address,
+	     we can make put both sources together and make a split point
+	     in the middle.  */
+
+	  if (seq
+	      && NEXT_INSN (seq) != NULL_RTX
+	      && NEXT_INSN (NEXT_INSN (seq)) == NULL_RTX
+	      && GET_CODE (seq) == INSN
+	      && GET_CODE (PATTERN (seq)) == SET
+	      && SET_DEST (PATTERN (seq)) == reg
+	      && ! reg_mentioned_p (reg,
+				    SET_SRC (PATTERN (seq)))
+	      && GET_CODE (NEXT_INSN (seq)) == INSN
+	      && GET_CODE (PATTERN (NEXT_INSN (seq))) == SET
+	      && SET_DEST (PATTERN (NEXT_INSN (seq))) == reg
+	      && memory_address_p (GET_MODE (x),
+				   SET_SRC (PATTERN (NEXT_INSN (seq)))))
+	    {
+	      rtx src1 = SET_SRC (PATTERN (seq));
+	      rtx src2 = SET_SRC (PATTERN (NEXT_INSN (seq)));
+
+	      /* Replace the placeholder in SRC2 with SRC1.  If we can
+		 find where in SRC2 it was placed, that can become our
+		 split point and we can replace this address with SRC2.
+		 Just try two obvious places.  */
+
+	      src2 = replace_rtx (src2, reg, src1);
+	      split = 0;
+	      if (XEXP (src2, 0) == src1)
+		split = &XEXP (src2, 0);
+	      else if (GET_RTX_FORMAT (GET_CODE (XEXP (src2, 0)))[0] == 'e'
+		       && XEXP (XEXP (src2, 0), 0) == src1)
+		split = &XEXP (XEXP (src2, 0), 0);
+
+	      if (split)
+		{
+		  SUBST (XEXP (x, 0), src2);
+		  return split;
+		}
+	    }
+
+	  /* If that didn't work, perhaps the first operand is complex and
+	     needs to be computed separately, so make a split point there.
+	     This will occur on machines that just support REG + CONST
+	     and have a constant moved through some previous computation.  */
+
+	  else if (GET_RTX_CLASS (GET_CODE (XEXP (XEXP (x, 0), 0))) != 'o'
+		   && ! (GET_CODE (XEXP (XEXP (x, 0), 0)) == SUBREG
+			 && (GET_RTX_CLASS (GET_CODE (SUBREG_REG (XEXP (XEXP (x, 0), 0))))
+			     == 'o')))
+	    return &XEXP (XEXP (x, 0), 0);
+	}
+      break;
+
+    case SET:
+#ifdef HAVE_cc0
+      /* If SET_DEST is CC0 and SET_SRC is not an operand, a COMPARE, or a
+	 ZERO_EXTRACT, the most likely reason why this doesn't match is that
+	 we need to put the operand into a register.  So split at that
+	 point.  */
+
+      if (SET_DEST (x) == cc0_rtx
+	  && GET_CODE (SET_SRC (x)) != COMPARE
+	  && GET_CODE (SET_SRC (x)) != ZERO_EXTRACT
+	  && GET_RTX_CLASS (GET_CODE (SET_SRC (x))) != 'o'
+	  && ! (GET_CODE (SET_SRC (x)) == SUBREG
+		&& GET_RTX_CLASS (GET_CODE (SUBREG_REG (SET_SRC (x)))) == 'o'))
+	return &SET_SRC (x);
+#endif
+
+      /* See if we can split SET_SRC as it stands.  */
+      split = find_split_point (&SET_SRC (x), insn);
+      if (split && split != &SET_SRC (x))
+	return split;
+
+      /* See if we can split SET_DEST as it stands.  */
+      split = find_split_point (&SET_DEST (x), insn);
+      if (split && split != &SET_DEST (x))
+	return split;
+
+      /* See if this is a bitfield assignment with everything constant.  If
+	 so, this is an IOR of an AND, so split it into that.  */
+      if (GET_CODE (SET_DEST (x)) == ZERO_EXTRACT
+	  && (GET_MODE_BITSIZE (GET_MODE (XEXP (SET_DEST (x), 0)))
+	      <= HOST_BITS_PER_WIDE_INT)
+	  && GET_CODE (XEXP (SET_DEST (x), 1)) == CONST_INT
+	  && GET_CODE (XEXP (SET_DEST (x), 2)) == CONST_INT
+	  && GET_CODE (SET_SRC (x)) == CONST_INT
+	  && ((INTVAL (XEXP (SET_DEST (x), 1))
+	       + INTVAL (XEXP (SET_DEST (x), 2)))
+	      <= GET_MODE_BITSIZE (GET_MODE (XEXP (SET_DEST (x), 0))))
+	  && ! side_effects_p (XEXP (SET_DEST (x), 0)))
+	{
+	  HOST_WIDE_INT pos = INTVAL (XEXP (SET_DEST (x), 2));
+	  unsigned HOST_WIDE_INT len = INTVAL (XEXP (SET_DEST (x), 1));
+	  unsigned HOST_WIDE_INT src = INTVAL (SET_SRC (x));
+	  rtx dest = XEXP (SET_DEST (x), 0);
+	  enum machine_mode mode = GET_MODE (dest);
+	  unsigned HOST_WIDE_INT mask = ((HOST_WIDE_INT) 1 << len) - 1;
+
+	  if (BITS_BIG_ENDIAN)
+	    pos = GET_MODE_BITSIZE (mode) - len - pos;
+
+	  if (src == mask)
+	    SUBST (SET_SRC (x),
+		   gen_binary (IOR, mode, dest, GEN_INT (src << pos)));
+	  else
+	    SUBST (SET_SRC (x),
+		   gen_binary (IOR, mode,
+			       gen_binary (AND, mode, dest,
+					   gen_int_mode (~(mask << pos),
+							 mode)),
+			       GEN_INT (src << pos)));
+
+	  SUBST (SET_DEST (x), dest);
+
+	  split = find_split_point (&SET_SRC (x), insn);
+	  if (split && split != &SET_SRC (x))
+	    return split;
+	}
+
+      /* Otherwise, see if this is an operation that we can split into two.
+	 If so, try to split that.  */
+      code = GET_CODE (SET_SRC (x));
+
+      switch (code)
+	{
+	case AND:
+	  /* If we are AND'ing with a large constant that is only a single
+	     bit and the result is only being used in a context where we
+	     need to know if it is zero or nonzero, replace it with a bit
+	     extraction.  This will avoid the large constant, which might
+	     have taken more than one insn to make.  If the constant were
+	     not a valid argument to the AND but took only one insn to make,
+	     this is no worse, but if it took more than one insn, it will
+	     be better.  */
+
+	  if (GET_CODE (XEXP (SET_SRC (x), 1)) == CONST_INT
+	      && GET_CODE (XEXP (SET_SRC (x), 0)) == REG
+	      && (pos = exact_log2 (INTVAL (XEXP (SET_SRC (x), 1)))) >= 7
+	      && GET_CODE (SET_DEST (x)) == REG
+	      && (split = find_single_use (SET_DEST (x), insn, (rtx*) 0)) != 0
+	      && (GET_CODE (*split) == EQ || GET_CODE (*split) == NE)
+	      && XEXP (*split, 0) == SET_DEST (x)
+	      && XEXP (*split, 1) == const0_rtx)
+	    {
+	      rtx extraction = make_extraction (GET_MODE (SET_DEST (x)),
+						XEXP (SET_SRC (x), 0),
+						pos, NULL_RTX, 1, 1, 0, 0);
+	      if (extraction != 0)
+		{
+		  SUBST (SET_SRC (x), extraction);
+		  return find_split_point (loc, insn);
+		}
+	    }
+	  break;
+
+	case NE:
+	  /* If STORE_FLAG_VALUE is -1, this is (NE X 0) and only one bit of X
+	     is known to be on, this can be converted into a NEG of a shift.  */
+	  if (STORE_FLAG_VALUE == -1 && XEXP (SET_SRC (x), 1) == const0_rtx
+	      && GET_MODE (SET_SRC (x)) == GET_MODE (XEXP (SET_SRC (x), 0))
+	      && 1 <= (pos = exact_log2
+		       (nonzero_bits (XEXP (SET_SRC (x), 0),
+				      GET_MODE (XEXP (SET_SRC (x), 0))))))
+	    {
+	      enum machine_mode mode = GET_MODE (XEXP (SET_SRC (x), 0));
+
+	      SUBST (SET_SRC (x),
+		     gen_rtx_NEG (mode,
+				  gen_rtx_LSHIFTRT (mode,
+						    XEXP (SET_SRC (x), 0),
+						    GEN_INT (pos))));
+
+	      split = find_split_point (&SET_SRC (x), insn);
+	      if (split && split != &SET_SRC (x))
+		return split;
+	    }
+	  break;
+
+	case SIGN_EXTEND:
+	  inner = XEXP (SET_SRC (x), 0);
+
+	  /* We can't optimize if either mode is a partial integer
+	     mode as we don't know how many bits are significant
+	     in those modes.  */
+	  if (GET_MODE_CLASS (GET_MODE (inner)) == MODE_PARTIAL_INT
+	      || GET_MODE_CLASS (GET_MODE (SET_SRC (x))) == MODE_PARTIAL_INT)
+	    break;
+
+	  pos = 0;
+	  len = GET_MODE_BITSIZE (GET_MODE (inner));
+	  unsignedp = 0;
+	  break;
+
+	case SIGN_EXTRACT:
+	case ZERO_EXTRACT:
+	  if (GET_CODE (XEXP (SET_SRC (x), 1)) == CONST_INT
+	      && GET_CODE (XEXP (SET_SRC (x), 2)) == CONST_INT)
+	    {
+	      inner = XEXP (SET_SRC (x), 0);
+	      len = INTVAL (XEXP (SET_SRC (x), 1));
+	      pos = INTVAL (XEXP (SET_SRC (x), 2));
+
+	      if (BITS_BIG_ENDIAN)
+		pos = GET_MODE_BITSIZE (GET_MODE (inner)) - len - pos;
+	      unsignedp = (code == ZERO_EXTRACT);
+	    }
+	  break;
+
+	default:
+	  break;
+	}
+
+      if (len && pos >= 0 && pos + len <= GET_MODE_BITSIZE (GET_MODE (inner)))
+	{
+	  enum machine_mode mode = GET_MODE (SET_SRC (x));
+
+	  /* For unsigned, we have a choice of a shift followed by an
+	     AND or two shifts.  Use two shifts for field sizes where the
+	     constant might be too large.  We assume here that we can
+	     always at least get 8-bit constants in an AND insn, which is
+	     true for every current RISC.  */
+
+	  if (unsignedp && len <= 8)
+	    {
+	      SUBST (SET_SRC (x),
+		     gen_rtx_AND (mode,
+				  gen_rtx_LSHIFTRT
+				  (mode, gen_lowpart_for_combine (mode, inner),
+				   GEN_INT (pos)),
+				  GEN_INT (((HOST_WIDE_INT) 1 << len) - 1)));
+
+	      split = find_split_point (&SET_SRC (x), insn);
+	      if (split && split != &SET_SRC (x))
+		return split;
+	    }
+	  else
+	    {
+	      SUBST (SET_SRC (x),
+		     gen_rtx_fmt_ee
+		     (unsignedp ? LSHIFTRT : ASHIFTRT, mode,
+		      gen_rtx_ASHIFT (mode,
+				      gen_lowpart_for_combine (mode, inner),
+				      GEN_INT (GET_MODE_BITSIZE (mode)
+					       - len - pos)),
+		      GEN_INT (GET_MODE_BITSIZE (mode) - len)));
+
+	      split = find_split_point (&SET_SRC (x), insn);
+	      if (split && split != &SET_SRC (x))
+		return split;
+	    }
+	}
+
+      /* See if this is a simple operation with a constant as the second
+	 operand.  It might be that this constant is out of range and hence
+	 could be used as a split point.  */
+      if ((GET_RTX_CLASS (GET_CODE (SET_SRC (x))) == '2'
+	   || GET_RTX_CLASS (GET_CODE (SET_SRC (x))) == 'c'
+	   || GET_RTX_CLASS (GET_CODE (SET_SRC (x))) == '<')
+	  && CONSTANT_P (XEXP (SET_SRC (x), 1))
+	  && (GET_RTX_CLASS (GET_CODE (XEXP (SET_SRC (x), 0))) == 'o'
+	      || (GET_CODE (XEXP (SET_SRC (x), 0)) == SUBREG
+		  && (GET_RTX_CLASS (GET_CODE (SUBREG_REG (XEXP (SET_SRC (x), 0))))
+		      == 'o'))))
+	return &XEXP (SET_SRC (x), 1);
+
+      /* Finally, see if this is a simple operation with its first operand
+	 not in a register.  The operation might require this operand in a
+	 register, so return it as a split point.  We can always do this
+	 because if the first operand were another operation, we would have
+	 already found it as a split point.  */
+      if ((GET_RTX_CLASS (GET_CODE (SET_SRC (x))) == '2'
+	   || GET_RTX_CLASS (GET_CODE (SET_SRC (x))) == 'c'
+	   || GET_RTX_CLASS (GET_CODE (SET_SRC (x))) == '<'
+	   || GET_RTX_CLASS (GET_CODE (SET_SRC (x))) == '1')
+	  && ! register_operand (XEXP (SET_SRC (x), 0), VOIDmode))
+	return &XEXP (SET_SRC (x), 0);
+
+      return 0;
+
+    case AND:
+    case IOR:
+      /* We write NOR as (and (not A) (not B)), but if we don't have a NOR,
+	 it is better to write this as (not (ior A B)) so we can split it.
+	 Similarly for IOR.  */
+      if (GET_CODE (XEXP (x, 0)) == NOT && GET_CODE (XEXP (x, 1)) == NOT)
+	{
+	  SUBST (*loc,
+		 gen_rtx_NOT (GET_MODE (x),
+			      gen_rtx_fmt_ee (code == IOR ? AND : IOR,
+					      GET_MODE (x),
+					      XEXP (XEXP (x, 0), 0),
+					      XEXP (XEXP (x, 1), 0))));
+	  return find_split_point (loc, insn);
+	}
+
+      /* Many RISC machines have a large set of logical insns.  If the
+	 second operand is a NOT, put it first so we will try to split the
+	 other operand first.  */
+      if (GET_CODE (XEXP (x, 1)) == NOT)
+	{
+	  rtx tem = XEXP (x, 0);
+	  SUBST (XEXP (x, 0), XEXP (x, 1));
+	  SUBST (XEXP (x, 1), tem);
+	}
+      break;
+
+    default:
+      break;
+    }
+
+  /* Otherwise, select our actions depending on our rtx class.  */
+  switch (GET_RTX_CLASS (code))
+    {
+    case 'b':			/* This is ZERO_EXTRACT and SIGN_EXTRACT.  */
+    case '3':
+      split = find_split_point (&XEXP (x, 2), insn);
+      if (split)
+	return split;
+      /* ... fall through ...  */
+    case '2':
+    case 'c':
+    case '<':
+      split = find_split_point (&XEXP (x, 1), insn);
+      if (split)
+	return split;
+      /* ... fall through ...  */
+    case '1':
+      /* Some machines have (and (shift ...) ...) insns.  If X is not
+	 an AND, but XEXP (X, 0) is, use it as our split point.  */
+      if (GET_CODE (x) != AND && GET_CODE (XEXP (x, 0)) == AND)
+	return &XEXP (x, 0);
+
+      split = find_split_point (&XEXP (x, 0), insn);
+      if (split)
+	return split;
+      return loc;
+    }
+
+  /* Otherwise, we don't have a split point.  */
+  return 0;
+}
+
+/* Throughout X, replace FROM with TO, and return the result.
+   The result is TO if X is FROM;
+   otherwise the result is X, but its contents may have been modified.
+   If they were modified, a record was made in undobuf so that
+   undo_all will (among other things) return X to its original state.
+
+   If the number of changes necessary is too much to record to undo,
+   the excess changes are not made, so the result is invalid.
+   The changes already made can still be undone.
+   undobuf.num_undo is incremented for such changes, so by testing that
+   the caller can tell whether the result is valid.
+
+   `n_occurrences' is incremented each time FROM is replaced.
+
+   IN_DEST is nonzero if we are processing the SET_DEST of a SET.
+
+   UNIQUE_COPY is nonzero if each substitution must be unique.  We do this
+   by copying if `n_occurrences' is nonzero.  */
+
+static rtx
+subst (rtx x, rtx from, rtx to, int in_dest, int unique_copy)
+{
+  enum rtx_code code = GET_CODE (x);
+  enum machine_mode op0_mode = VOIDmode;
+  const char *fmt;
+  int len, i;
+  rtx new;
+
+/* Two expressions are equal if they are identical copies of a shared
+   RTX or if they are both registers with the same register number
+   and mode.  */
+
+#define COMBINE_RTX_EQUAL_P(X,Y)			\
+  ((X) == (Y)						\
+   || (GET_CODE (X) == REG && GET_CODE (Y) == REG	\
+       && REGNO (X) == REGNO (Y) && GET_MODE (X) == GET_MODE (Y)))
+
+  if (! in_dest && COMBINE_RTX_EQUAL_P (x, from))
+    {
+      n_occurrences++;
+      return (unique_copy && n_occurrences > 1 ? copy_rtx (to) : to);
+    }
+
+  /* If X and FROM are the same register but different modes, they will
+     not have been seen as equal above.  However, flow.c will make a
+     LOG_LINKS entry for that case.  If we do nothing, we will try to
+     rerecognize our original insn and, when it succeeds, we will
+     delete the feeding insn, which is incorrect.
+
+     So force this insn not to match in this (rare) case.  */
+  if (! in_dest && code == REG && GET_CODE (from) == REG
+      && REGNO (x) == REGNO (from))
+    return gen_rtx_CLOBBER (GET_MODE (x), const0_rtx);
+
+  /* If this is an object, we are done unless it is a MEM or LO_SUM, both
+     of which may contain things that can be combined.  */
+  if (code != MEM && code != LO_SUM && GET_RTX_CLASS (code) == 'o')
+    return x;
+
+  /* It is possible to have a subexpression appear twice in the insn.
+     Suppose that FROM is a register that appears within TO.
+     Then, after that subexpression has been scanned once by `subst',
+     the second time it is scanned, TO may be found.  If we were
+     to scan TO here, we would find FROM within it and create a
+     self-referent rtl structure which is completely wrong.  */
+  if (COMBINE_RTX_EQUAL_P (x, to))
+    return to;
+
+  /* Parallel asm_operands need special attention because all of the
+     inputs are shared across the arms.  Furthermore, unsharing the
+     rtl results in recognition failures.  Failure to handle this case
+     specially can result in circular rtl.
+
+     Solve this by doing a normal pass across the first entry of the
+     parallel, and only processing the SET_DESTs of the subsequent
+     entries.  Ug.  */
+
+  if (code == PARALLEL
+      && GET_CODE (XVECEXP (x, 0, 0)) == SET
+      && GET_CODE (SET_SRC (XVECEXP (x, 0, 0))) == ASM_OPERANDS)
+    {
+      new = subst (XVECEXP (x, 0, 0), from, to, 0, unique_copy);
+
+      /* If this substitution failed, this whole thing fails.  */
+      if (GET_CODE (new) == CLOBBER
+	  && XEXP (new, 0) == const0_rtx)
+	return new;
+
+      SUBST (XVECEXP (x, 0, 0), new);
+
+      for (i = XVECLEN (x, 0) - 1; i >= 1; i--)
+	{
+	  rtx dest = SET_DEST (XVECEXP (x, 0, i));
+
+	  if (GET_CODE (dest) != REG
+	      && GET_CODE (dest) != CC0
+	      && GET_CODE (dest) != PC)
+	    {
+	      new = subst (dest, from, to, 0, unique_copy);
+
+	      /* If this substitution failed, this whole thing fails.  */
+	      if (GET_CODE (new) == CLOBBER
+		  && XEXP (new, 0) == const0_rtx)
+		return new;
+
+	      SUBST (SET_DEST (XVECEXP (x, 0, i)), new);
+	    }
+	}
+    }
+  else
+    {
+      len = GET_RTX_LENGTH (code);
+      fmt = GET_RTX_FORMAT (code);
+
+      /* We don't need to process a SET_DEST that is a register, CC0,
+	 or PC, so set up to skip this common case.  All other cases
+	 where we want to suppress replacing something inside a
+	 SET_SRC are handled via the IN_DEST operand.  */
+      if (code == SET
+	  && (GET_CODE (SET_DEST (x)) == REG
+	      || GET_CODE (SET_DEST (x)) == CC0
+	      || GET_CODE (SET_DEST (x)) == PC))
+	fmt = "ie";
+
+      /* Get the mode of operand 0 in case X is now a SIGN_EXTEND of a
+	 constant.  */
+      if (fmt[0] == 'e')
+	op0_mode = GET_MODE (XEXP (x, 0));
+
+      for (i = 0; i < len; i++)
+	{
+	  if (fmt[i] == 'E')
+	    {
+	      int j;
+	      for (j = XVECLEN (x, i) - 1; j >= 0; j--)
+		{
+		  if (COMBINE_RTX_EQUAL_P (XVECEXP (x, i, j), from))
+		    {
+		      new = (unique_copy && n_occurrences
+			     ? copy_rtx (to) : to);
+		      n_occurrences++;
+		    }
+		  else
+		    {
+		      new = subst (XVECEXP (x, i, j), from, to, 0,
+				   unique_copy);
+
+		      /* If this substitution failed, this whole thing
+			 fails.  */
+		      if (GET_CODE (new) == CLOBBER
+			  && XEXP (new, 0) == const0_rtx)
+			return new;
+		    }
+
+		  SUBST (XVECEXP (x, i, j), new);
+		}
+	    }
+	  else if (fmt[i] == 'e')
+	    {
+	      /* If this is a register being set, ignore it.  */
+	      new = XEXP (x, i);
+	      if (in_dest
+		  && i == 0
+		  && (((code == SUBREG || code == ZERO_EXTRACT)
+		       && GET_CODE (new) == REG)
+		      || code == STRICT_LOW_PART))
+		;
+
+	      else if (COMBINE_RTX_EQUAL_P (XEXP (x, i), from))
+		{
+		  /* In general, don't install a subreg involving two
+		     modes not tieable.  It can worsen register
+		     allocation, and can even make invalid reload
+		     insns, since the reg inside may need to be copied
+		     from in the outside mode, and that may be invalid
+		     if it is an fp reg copied in integer mode.
+
+		     We allow two exceptions to this: It is valid if
+		     it is inside another SUBREG and the mode of that
+		     SUBREG and the mode of the inside of TO is
+		     tieable and it is valid if X is a SET that copies
+		     FROM to CC0.  */
+
+		  if (GET_CODE (to) == SUBREG
+		      && ! MODES_TIEABLE_P (GET_MODE (to),
+					    GET_MODE (SUBREG_REG (to)))
+		      && ! (code == SUBREG
+			    && MODES_TIEABLE_P (GET_MODE (x),
+						GET_MODE (SUBREG_REG (to))))
+#ifdef HAVE_cc0
+		      && ! (code == SET && i == 1 && XEXP (x, 0) == cc0_rtx)
+#endif
+		      )
+		    return gen_rtx_CLOBBER (VOIDmode, const0_rtx);
+
+#ifdef CANNOT_CHANGE_MODE_CLASS
+		  if (code == SUBREG
+		      && GET_CODE (to) == REG
+		      && REGNO (to) < FIRST_PSEUDO_REGISTER
+		      && REG_CANNOT_CHANGE_MODE_P (REGNO (to),
+						   GET_MODE (to),
+						   GET_MODE (x)))
+		    return gen_rtx_CLOBBER (VOIDmode, const0_rtx);
+#endif
+
+		  new = (unique_copy && n_occurrences ? copy_rtx (to) : to);
+		  n_occurrences++;
+		}
+	      else
+		/* If we are in a SET_DEST, suppress most cases unless we
+		   have gone inside a MEM, in which case we want to
+		   simplify the address.  We assume here that things that
+		   are actually part of the destination have their inner
+		   parts in the first expression.  This is true for SUBREG,
+		   STRICT_LOW_PART, and ZERO_EXTRACT, which are the only
+		   things aside from REG and MEM that should appear in a
+		   SET_DEST.  */
+		new = subst (XEXP (x, i), from, to,
+			     (((in_dest
+				&& (code == SUBREG || code == STRICT_LOW_PART
+				    || code == ZERO_EXTRACT))
+			       || code == SET)
+			      && i == 0), unique_copy);
+
+	      /* If we found that we will have to reject this combination,
+		 indicate that by returning the CLOBBER ourselves, rather than
+		 an expression containing it.  This will speed things up as
+		 well as prevent accidents where two CLOBBERs are considered
+		 to be equal, thus producing an incorrect simplification.  */
+
+	      if (GET_CODE (new) == CLOBBER && XEXP (new, 0) == const0_rtx)
+		return new;
+
+	      if (GET_CODE (x) == SUBREG
+		  && (GET_CODE (new) == CONST_INT
+		      || GET_CODE (new) == CONST_DOUBLE))
+		{
+		  enum machine_mode mode = GET_MODE (x);
+
+		  x = simplify_subreg (GET_MODE (x), new,
+				       GET_MODE (SUBREG_REG (x)),
+				       SUBREG_BYTE (x));
+		  if (! x)
+		    x = gen_rtx_CLOBBER (mode, const0_rtx);
+		}
+	      else if (GET_CODE (new) == CONST_INT
+		       && GET_CODE (x) == ZERO_EXTEND)
+		{
+		  x = simplify_unary_operation (ZERO_EXTEND, GET_MODE (x),
+						new, GET_MODE (XEXP (x, 0)));
+		  if (! x)
+		    abort ();
+		}
+	      else
+		SUBST (XEXP (x, i), new);
+	    }
+	}
+    }
+
+  /* Try to simplify X.  If the simplification changed the code, it is likely
+     that further simplification will help, so loop, but limit the number
+     of repetitions that will be performed.  */
+
+  for (i = 0; i < 4; i++)
+    {
+      /* If X is sufficiently simple, don't bother trying to do anything
+	 with it.  */
+      if (code != CONST_INT && code != REG && code != CLOBBER)
+	x = combine_simplify_rtx (x, op0_mode, i == 3, in_dest);
+
+      if (GET_CODE (x) == code)
+	break;
+
+      code = GET_CODE (x);
+
+      /* We no longer know the original mode of operand 0 since we
+	 have changed the form of X)  */
+      op0_mode = VOIDmode;
+    }
+
+  return x;
+}
+
+/* Simplify X, a piece of RTL.  We just operate on the expression at the
+   outer level; call `subst' to simplify recursively.  Return the new
+   expression.
+
+   OP0_MODE is the original mode of XEXP (x, 0); LAST is nonzero if this
+   will be the iteration even if an expression with a code different from
+   X is returned; IN_DEST is nonzero if we are inside a SET_DEST.  */
+
+static rtx
+combine_simplify_rtx (rtx x, enum machine_mode op0_mode, int last,
+		      int in_dest)
+{
+  enum rtx_code code = GET_CODE (x);
+  enum machine_mode mode = GET_MODE (x);
+  rtx temp;
+  rtx reversed;
+  int i;
+
+  /* If this is a commutative operation, put a constant last and a complex
+     expression first.  We don't need to do this for comparisons here.  */
+  if (GET_RTX_CLASS (code) == 'c'
+      && swap_commutative_operands_p (XEXP (x, 0), XEXP (x, 1)))
+    {
+      temp = XEXP (x, 0);
+      SUBST (XEXP (x, 0), XEXP (x, 1));
+      SUBST (XEXP (x, 1), temp);
+    }
+
+  /* If this is a PLUS, MINUS, or MULT, and the first operand is the
+     sign extension of a PLUS with a constant, reverse the order of the sign
+     extension and the addition. Note that this not the same as the original
+     code, but overflow is undefined for signed values.  Also note that the
+     PLUS will have been partially moved "inside" the sign-extension, so that
+     the first operand of X will really look like:
+         (ashiftrt (plus (ashift A C4) C5) C4).
+     We convert this to
+         (plus (ashiftrt (ashift A C4) C2) C4)
+     and replace the first operand of X with that expression.  Later parts
+     of this function may simplify the expression further.
+
+     For example, if we start with (mult (sign_extend (plus A C1)) C2),
+     we swap the SIGN_EXTEND and PLUS.  Later code will apply the
+     distributive law to produce (plus (mult (sign_extend X) C1) C3).
+
+     We do this to simplify address expressions.  */
+
+  if ((code == PLUS || code == MINUS || code == MULT)
+      && GET_CODE (XEXP (x, 0)) == ASHIFTRT
+      && GET_CODE (XEXP (XEXP (x, 0), 0)) == PLUS
+      && GET_CODE (XEXP (XEXP (XEXP (x, 0), 0), 0)) == ASHIFT
+      && GET_CODE (XEXP (XEXP (XEXP (XEXP (x, 0), 0), 0), 1)) == CONST_INT
+      && GET_CODE (XEXP (XEXP (x, 0), 1)) == CONST_INT
+      && XEXP (XEXP (XEXP (XEXP (x, 0), 0), 0), 1) == XEXP (XEXP (x, 0), 1)
+      && GET_CODE (XEXP (XEXP (XEXP (x, 0), 0), 1)) == CONST_INT
+      && (temp = simplify_binary_operation (ASHIFTRT, mode,
+					    XEXP (XEXP (XEXP (x, 0), 0), 1),
+					    XEXP (XEXP (x, 0), 1))) != 0)
+    {
+      rtx new
+	= simplify_shift_const (NULL_RTX, ASHIFT, mode,
+				XEXP (XEXP (XEXP (XEXP (x, 0), 0), 0), 0),
+				INTVAL (XEXP (XEXP (x, 0), 1)));
+
+      new = simplify_shift_const (NULL_RTX, ASHIFTRT, mode, new,
+				  INTVAL (XEXP (XEXP (x, 0), 1)));
+
+      SUBST (XEXP (x, 0), gen_binary (PLUS, mode, new, temp));
+    }
+
+  /* If this is a simple operation applied to an IF_THEN_ELSE, try
+     applying it to the arms of the IF_THEN_ELSE.  This often simplifies
+     things.  Check for cases where both arms are testing the same
+     condition.
+
+     Don't do anything if all operands are very simple.  */
+
+  if (((GET_RTX_CLASS (code) == '2' || GET_RTX_CLASS (code) == 'c'
+	|| GET_RTX_CLASS (code) == '<')
+       && ((GET_RTX_CLASS (GET_CODE (XEXP (x, 0))) != 'o'
+	    && ! (GET_CODE (XEXP (x, 0)) == SUBREG
+		  && (GET_RTX_CLASS (GET_CODE (SUBREG_REG (XEXP (x, 0))))
+		      == 'o')))
+	   || (GET_RTX_CLASS (GET_CODE (XEXP (x, 1))) != 'o'
+	       && ! (GET_CODE (XEXP (x, 1)) == SUBREG
+		     && (GET_RTX_CLASS (GET_CODE (SUBREG_REG (XEXP (x, 1))))
+			 == 'o')))))
+      || (GET_RTX_CLASS (code) == '1'
+	  && ((GET_RTX_CLASS (GET_CODE (XEXP (x, 0))) != 'o'
+	       && ! (GET_CODE (XEXP (x, 0)) == SUBREG
+		     && (GET_RTX_CLASS (GET_CODE (SUBREG_REG (XEXP (x, 0))))
+			 == 'o'))))))
+    {
+      rtx cond, true_rtx, false_rtx;
+
+      cond = if_then_else_cond (x, &true_rtx, &false_rtx);
+      if (cond != 0
+	  /* If everything is a comparison, what we have is highly unlikely
+	     to be simpler, so don't use it.  */
+	  && ! (GET_RTX_CLASS (code) == '<'
+		&& (GET_RTX_CLASS (GET_CODE (true_rtx)) == '<'
+		    || GET_RTX_CLASS (GET_CODE (false_rtx)) == '<')))
+	{
+	  rtx cop1 = const0_rtx;
+	  enum rtx_code cond_code = simplify_comparison (NE, &cond, &cop1);
+
+	  if (cond_code == NE && GET_RTX_CLASS (GET_CODE (cond)) == '<')
+	    return x;
+
+	  /* Simplify the alternative arms; this may collapse the true and
+	     false arms to store-flag values.  Be careful to use copy_rtx
+	     here since true_rtx or false_rtx might share RTL with x as a
+	     result of the if_then_else_cond call above.  */
+	  true_rtx = subst (copy_rtx (true_rtx), pc_rtx, pc_rtx, 0, 0);
+	  false_rtx = subst (copy_rtx (false_rtx), pc_rtx, pc_rtx, 0, 0);
+
+	  /* If true_rtx and false_rtx are not general_operands, an if_then_else
+	     is unlikely to be simpler.  */
+	  if (general_operand (true_rtx, VOIDmode)
+	      && general_operand (false_rtx, VOIDmode))
+	    {
+	      enum rtx_code reversed;
+
+	      /* Restarting if we generate a store-flag expression will cause
+		 us to loop.  Just drop through in this case.  */
+
+	      /* If the result values are STORE_FLAG_VALUE and zero, we can
+		 just make the comparison operation.  */
+	      if (true_rtx == const_true_rtx && false_rtx == const0_rtx)
+		x = gen_binary (cond_code, mode, cond, cop1);
+	      else if (true_rtx == const0_rtx && false_rtx == const_true_rtx
+		       && ((reversed = reversed_comparison_code_parts
+					(cond_code, cond, cop1, NULL))
+		           != UNKNOWN))
+		x = gen_binary (reversed, mode, cond, cop1);
+
+	      /* Likewise, we can make the negate of a comparison operation
+		 if the result values are - STORE_FLAG_VALUE and zero.  */
+	      else if (GET_CODE (true_rtx) == CONST_INT
+		       && INTVAL (true_rtx) == - STORE_FLAG_VALUE
+		       && false_rtx == const0_rtx)
+		x = simplify_gen_unary (NEG, mode,
+					gen_binary (cond_code, mode, cond,
+						    cop1),
+					mode);
+	      else if (GET_CODE (false_rtx) == CONST_INT
+		       && INTVAL (false_rtx) == - STORE_FLAG_VALUE
+		       && true_rtx == const0_rtx
+		       && ((reversed = reversed_comparison_code_parts
+					(cond_code, cond, cop1, NULL))
+		           != UNKNOWN))
+		x = simplify_gen_unary (NEG, mode,
+					gen_binary (reversed, mode,
+						    cond, cop1),
+					mode);
+	      else
+		return gen_rtx_IF_THEN_ELSE (mode,
+					     gen_binary (cond_code, VOIDmode,
+							 cond, cop1),
+					     true_rtx, false_rtx);
+
+	      code = GET_CODE (x);
+	      op0_mode = VOIDmode;
+	    }
+	}
+    }
+
+  /* Try to fold this expression in case we have constants that weren't
+     present before.  */
+  temp = 0;
+  switch (GET_RTX_CLASS (code))
+    {
+    case '1':
+      if (op0_mode == VOIDmode)
+	op0_mode = GET_MODE (XEXP (x, 0));
+      temp = simplify_unary_operation (code, mode, XEXP (x, 0), op0_mode);
+      break;
+    case '<':
+      if (! VECTOR_MODE_P (mode))
+	{
+	  enum machine_mode cmp_mode = GET_MODE (XEXP (x, 0));
+	  if (cmp_mode == VOIDmode)
+	    {
+	      cmp_mode = GET_MODE (XEXP (x, 1));
+	      if (cmp_mode == VOIDmode)
+		cmp_mode = op0_mode;
+	    }
+	  temp = simplify_relational_operation (code, cmp_mode,
+						XEXP (x, 0), XEXP (x, 1));
+#ifdef FLOAT_STORE_FLAG_VALUE
+	  if (temp != 0 && GET_MODE_CLASS (mode) == MODE_FLOAT)
+	    {
+	      if (temp == const0_rtx)
+		temp = CONST0_RTX (mode);
+	      else
+		temp = CONST_DOUBLE_FROM_REAL_VALUE
+			 (FLOAT_STORE_FLAG_VALUE (mode), mode);
+	    }
+#endif
+	}
+      break;
+    case 'c':
+    case '2':
+      temp = simplify_binary_operation (code, mode, XEXP (x, 0), XEXP (x, 1));
+      break;
+    case 'b':
+    case '3':
+      temp = simplify_ternary_operation (code, mode, op0_mode, XEXP (x, 0),
+					 XEXP (x, 1), XEXP (x, 2));
+      break;
+    }
+
+  if (temp)
+    {
+      x = temp;
+      code = GET_CODE (temp);
+      op0_mode = VOIDmode;
+      mode = GET_MODE (temp);
+    }
+
+  /* First see if we can apply the inverse distributive law.  */
+  if (code == PLUS || code == MINUS
+      || code == AND || code == IOR || code == XOR)
+    {
+      x = apply_distributive_law (x);
+      code = GET_CODE (x);
+      op0_mode = VOIDmode;
+    }
+
+  /* If CODE is an associative operation not otherwise handled, see if we
+     can associate some operands.  This can win if they are constants or
+     if they are logically related (i.e. (a & b) & a).  */
+  if ((code == PLUS || code == MINUS || code == MULT || code == DIV
+       || code == AND || code == IOR || code == XOR
+       || code == SMAX || code == SMIN || code == UMAX || code == UMIN)
+      && ((INTEGRAL_MODE_P (mode) && code != DIV)
+	  || (flag_unsafe_math_optimizations && FLOAT_MODE_P (mode))))
+    {
+      if (GET_CODE (XEXP (x, 0)) == code)
+	{
+	  rtx other = XEXP (XEXP (x, 0), 0);
+	  rtx inner_op0 = XEXP (XEXP (x, 0), 1);
+	  rtx inner_op1 = XEXP (x, 1);
+	  rtx inner;
+
+	  /* Make sure we pass the constant operand if any as the second
+	     one if this is a commutative operation.  */
+	  if (CONSTANT_P (inner_op0) && GET_RTX_CLASS (code) == 'c')
+	    {
+	      rtx tem = inner_op0;
+	      inner_op0 = inner_op1;
+	      inner_op1 = tem;
+	    }
+	  inner = simplify_binary_operation (code == MINUS ? PLUS
+					     : code == DIV ? MULT
+					     : code,
+					     mode, inner_op0, inner_op1);
+
+	  /* For commutative operations, try the other pair if that one
+	     didn't simplify.  */
+	  if (inner == 0 && GET_RTX_CLASS (code) == 'c')
+	    {
+	      other = XEXP (XEXP (x, 0), 1);
+	      inner = simplify_binary_operation (code, mode,
+						 XEXP (XEXP (x, 0), 0),
+						 XEXP (x, 1));
+	    }
+
+	  if (inner)
+	    return gen_binary (code, mode, other, inner);
+	}
+    }
+
+  /* A little bit of algebraic simplification here.  */
+  switch (code)
+    {
+    case MEM:
+      /* Ensure that our address has any ASHIFTs converted to MULT in case
+	 address-recognizing predicates are called later.  */
+      temp = make_compound_operation (XEXP (x, 0), MEM);
+      SUBST (XEXP (x, 0), temp);
+      break;
+
+    case SUBREG:
+      if (op0_mode == VOIDmode)
+	op0_mode = GET_MODE (SUBREG_REG (x));
+
+      /* simplify_subreg can't use gen_lowpart_for_combine.  */
+      if (CONSTANT_P (SUBREG_REG (x))
+	  && subreg_lowpart_offset (mode, op0_mode) == SUBREG_BYTE (x)
+	     /* Don't call gen_lowpart_for_combine if the inner mode
+		is VOIDmode and we cannot simplify it, as SUBREG without
+		inner mode is invalid.  */
+	  && (GET_MODE (SUBREG_REG (x)) != VOIDmode
+	      || gen_lowpart_common (mode, SUBREG_REG (x))))
+	return gen_lowpart_for_combine (mode, SUBREG_REG (x));
+
+      if (GET_MODE_CLASS (GET_MODE (SUBREG_REG (x))) == MODE_CC)
+        break;
+      {
+	rtx temp;
+	temp = simplify_subreg (mode, SUBREG_REG (x), op0_mode,
+				SUBREG_BYTE (x));
+	if (temp)
+	  return temp;
+      }
+
+      /* Don't change the mode of the MEM if that would change the meaning
+	 of the address.  */
+      if (GET_CODE (SUBREG_REG (x)) == MEM
+	  && (MEM_VOLATILE_P (SUBREG_REG (x))
+	      || mode_dependent_address_p (XEXP (SUBREG_REG (x), 0))))
+	return gen_rtx_CLOBBER (mode, const0_rtx);
+
+      /* Note that we cannot do any narrowing for non-constants since
+	 we might have been counting on using the fact that some bits were
+	 zero.  We now do this in the SET.  */
+
+      break;
+
+    case NOT:
+      if (GET_CODE (XEXP (x, 0)) == SUBREG
+	  && subreg_lowpart_p (XEXP (x, 0))
+	  && (GET_MODE_SIZE (GET_MODE (XEXP (x, 0)))
+	      < GET_MODE_SIZE (GET_MODE (SUBREG_REG (XEXP (x, 0)))))
+	  && GET_CODE (SUBREG_REG (XEXP (x, 0))) == ASHIFT
+	  && XEXP (SUBREG_REG (XEXP (x, 0)), 0) == const1_rtx)
+	{
+	  enum machine_mode inner_mode = GET_MODE (SUBREG_REG (XEXP (x, 0)));
+
+	  x = gen_rtx_ROTATE (inner_mode,
+			      simplify_gen_unary (NOT, inner_mode, const1_rtx,
+						  inner_mode),
+			      XEXP (SUBREG_REG (XEXP (x, 0)), 1));
+	  return gen_lowpart_for_combine (mode, x);
+	}
+
+      /* Apply De Morgan's laws to reduce number of patterns for machines
+	 with negating logical insns (and-not, nand, etc.).  If result has
+	 only one NOT, put it first, since that is how the patterns are
+	 coded.  */
+
+      if (GET_CODE (XEXP (x, 0)) == IOR || GET_CODE (XEXP (x, 0)) == AND)
+	{
+	  rtx in1 = XEXP (XEXP (x, 0), 0), in2 = XEXP (XEXP (x, 0), 1);
+	  enum machine_mode op_mode;
+
+	  op_mode = GET_MODE (in1);
+	  in1 = simplify_gen_unary (NOT, op_mode, in1, op_mode);
+
+	  op_mode = GET_MODE (in2);
+	  if (op_mode == VOIDmode)
+	    op_mode = mode;
+	  in2 = simplify_gen_unary (NOT, op_mode, in2, op_mode);
+
+	  if (GET_CODE (in2) == NOT && GET_CODE (in1) != NOT)
+	    {
+	      rtx tem = in2;
+	      in2 = in1; in1 = tem;
+	    }
+
+	  return gen_rtx_fmt_ee (GET_CODE (XEXP (x, 0)) == IOR ? AND : IOR,
+				 mode, in1, in2);
+	}
+      break;
+
+    case NEG:
+      /* (neg (xor A 1)) is (plus A -1) if A is known to be either 0 or 1.  */
+      if (GET_CODE (XEXP (x, 0)) == XOR
+	  && XEXP (XEXP (x, 0), 1) == const1_rtx
+	  && nonzero_bits (XEXP (XEXP (x, 0), 0), mode) == 1)
+	return gen_binary (PLUS, mode, XEXP (XEXP (x, 0), 0), constm1_rtx);
+
+      temp = expand_compound_operation (XEXP (x, 0));
+
+      /* For C equal to the width of MODE minus 1, (neg (ashiftrt X C)) can be
+	 replaced by (lshiftrt X C).  This will convert
+	 (neg (sign_extract X 1 Y)) to (zero_extract X 1 Y).  */
+
+      if (GET_CODE (temp) == ASHIFTRT
+	  && GET_CODE (XEXP (temp, 1)) == CONST_INT
+	  && INTVAL (XEXP (temp, 1)) == GET_MODE_BITSIZE (mode) - 1)
+	return simplify_shift_const (temp, LSHIFTRT, mode, XEXP (temp, 0),
+				     INTVAL (XEXP (temp, 1)));
+
+      /* If X has only a single bit that might be nonzero, say, bit I, convert
+	 (neg X) to (ashiftrt (ashift X C-I) C-I) where C is the bitsize of
+	 MODE minus 1.  This will convert (neg (zero_extract X 1 Y)) to
+	 (sign_extract X 1 Y).  But only do this if TEMP isn't a register
+	 or a SUBREG of one since we'd be making the expression more
+	 complex if it was just a register.  */
+
+      if (GET_CODE (temp) != REG
+	  && ! (GET_CODE (temp) == SUBREG
+		&& GET_CODE (SUBREG_REG (temp)) == REG)
+	  && (i = exact_log2 (nonzero_bits (temp, mode))) >= 0)
+	{
+	  rtx temp1 = simplify_shift_const
+	    (NULL_RTX, ASHIFTRT, mode,
+	     simplify_shift_const (NULL_RTX, ASHIFT, mode, temp,
+				   GET_MODE_BITSIZE (mode) - 1 - i),
+	     GET_MODE_BITSIZE (mode) - 1 - i);
+
+	  /* If all we did was surround TEMP with the two shifts, we
+	     haven't improved anything, so don't use it.  Otherwise,
+	     we are better off with TEMP1.  */
+	  if (GET_CODE (temp1) != ASHIFTRT
+	      || GET_CODE (XEXP (temp1, 0)) != ASHIFT
+	      || XEXP (XEXP (temp1, 0), 0) != temp)
+	    return temp1;
+	}
+      break;
+
+    case TRUNCATE:
+      /* We can't handle truncation to a partial integer mode here
+	 because we don't know the real bitsize of the partial
+	 integer mode.  */
+      if (GET_MODE_CLASS (mode) == MODE_PARTIAL_INT)
+	break;
+
+      if (GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT
+	  && TRULY_NOOP_TRUNCATION (GET_MODE_BITSIZE (mode),
+				    GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0)))))
+	SUBST (XEXP (x, 0),
+	       force_to_mode (XEXP (x, 0), GET_MODE (XEXP (x, 0)),
+			      GET_MODE_MASK (mode), NULL_RTX, 0));
+
+      /* (truncate:SI ({sign,zero}_extend:DI foo:SI)) == foo:SI.  */
+      if ((GET_CODE (XEXP (x, 0)) == SIGN_EXTEND
+	   || GET_CODE (XEXP (x, 0)) == ZERO_EXTEND)
+	  && GET_MODE (XEXP (XEXP (x, 0), 0)) == mode)
+	return XEXP (XEXP (x, 0), 0);
+
+      /* (truncate:SI (OP:DI ({sign,zero}_extend:DI foo:SI))) is
+	 (OP:SI foo:SI) if OP is NEG or ABS.  */
+      if ((GET_CODE (XEXP (x, 0)) == ABS
+	   || GET_CODE (XEXP (x, 0)) == NEG)
+	  && (GET_CODE (XEXP (XEXP (x, 0), 0)) == SIGN_EXTEND
+	      || GET_CODE (XEXP (XEXP (x, 0), 0)) == ZERO_EXTEND)
+	  && GET_MODE (XEXP (XEXP (XEXP (x, 0), 0), 0)) == mode)
+	return simplify_gen_unary (GET_CODE (XEXP (x, 0)), mode,
+				   XEXP (XEXP (XEXP (x, 0), 0), 0), mode);
+
+      /* (truncate:SI (subreg:DI (truncate:SI X) 0)) is
+	 (truncate:SI x).  */
+      if (GET_CODE (XEXP (x, 0)) == SUBREG
+	  && GET_CODE (SUBREG_REG (XEXP (x, 0))) == TRUNCATE
+	  && subreg_lowpart_p (XEXP (x, 0)))
+	return SUBREG_REG (XEXP (x, 0));
+
+      /* If we know that the value is already truncated, we can
+         replace the TRUNCATE with a SUBREG if TRULY_NOOP_TRUNCATION
+         is nonzero for the corresponding modes.  But don't do this
+         for an (LSHIFTRT (MULT ...)) since this will cause problems
+         with the umulXi3_highpart patterns.  */
+      if (TRULY_NOOP_TRUNCATION (GET_MODE_BITSIZE (mode),
+				 GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0))))
+	  && num_sign_bit_copies (XEXP (x, 0), GET_MODE (XEXP (x, 0)))
+	     >= (unsigned int) (GET_MODE_BITSIZE (mode) + 1)
+	  && ! (GET_CODE (XEXP (x, 0)) == LSHIFTRT
+		&& GET_CODE (XEXP (XEXP (x, 0), 0)) == MULT))
+	return gen_lowpart_for_combine (mode, XEXP (x, 0));
+
+      /* A truncate of a comparison can be replaced with a subreg if
+         STORE_FLAG_VALUE permits.  This is like the previous test,
+         but it works even if the comparison is done in a mode larger
+         than HOST_BITS_PER_WIDE_INT.  */
+      if (GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT
+	  && GET_RTX_CLASS (GET_CODE (XEXP (x, 0))) == '<'
+	  && ((HOST_WIDE_INT) STORE_FLAG_VALUE & ~GET_MODE_MASK (mode)) == 0)
+	return gen_lowpart_for_combine (mode, XEXP (x, 0));
+
+      /* Similarly, a truncate of a register whose value is a
+         comparison can be replaced with a subreg if STORE_FLAG_VALUE
+         permits.  */
+      if (GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT
+	  && ((HOST_WIDE_INT) STORE_FLAG_VALUE & ~GET_MODE_MASK (mode)) == 0
+	  && (temp = get_last_value (XEXP (x, 0)))
+	  && GET_RTX_CLASS (GET_CODE (temp)) == '<')
+	return gen_lowpart_for_combine (mode, XEXP (x, 0));
+
+      break;
+
+    case FLOAT_TRUNCATE:
+      /* (float_truncate:SF (float_extend:DF foo:SF)) = foo:SF.  */
+      if (GET_CODE (XEXP (x, 0)) == FLOAT_EXTEND
+	  && GET_MODE (XEXP (XEXP (x, 0), 0)) == mode)
+	return XEXP (XEXP (x, 0), 0);
+
+      /* (float_truncate:SF (float_truncate:DF foo:XF))
+         = (float_truncate:SF foo:XF).
+	 This may eliminate double rounding, so it is unsafe.
+
+         (float_truncate:SF (float_extend:XF foo:DF))
+         = (float_truncate:SF foo:DF).
+
+         (float_truncate:DF (float_extend:XF foo:SF))
+         = (float_extend:SF foo:DF).  */
+      if ((GET_CODE (XEXP (x, 0)) == FLOAT_TRUNCATE
+	   && flag_unsafe_math_optimizations)
+	  || GET_CODE (XEXP (x, 0)) == FLOAT_EXTEND)
+	return simplify_gen_unary (GET_MODE_SIZE (GET_MODE (XEXP (XEXP (x, 0),
+							    0)))
+				   > GET_MODE_SIZE (mode)
+				   ? FLOAT_TRUNCATE : FLOAT_EXTEND,
+				   mode,
+				   XEXP (XEXP (x, 0), 0), mode);
+
+      /*  (float_truncate (float x)) is (float x)  */
+      if (GET_CODE (XEXP (x, 0)) == FLOAT
+	  && (flag_unsafe_math_optimizations
+	      || ((unsigned)significand_size (GET_MODE (XEXP (x, 0)))
+		  >= (GET_MODE_BITSIZE (GET_MODE (XEXP (XEXP (x, 0), 0)))
+		      - num_sign_bit_copies (XEXP (XEXP (x, 0), 0),
+					     GET_MODE (XEXP (XEXP (x, 0), 0)))))))
+	return simplify_gen_unary (FLOAT, mode,
+				   XEXP (XEXP (x, 0), 0),
+				   GET_MODE (XEXP (XEXP (x, 0), 0)));
+
+      /* (float_truncate:SF (OP:DF (float_extend:DF foo:sf))) is
+	 (OP:SF foo:SF) if OP is NEG or ABS.  */
+      if ((GET_CODE (XEXP (x, 0)) == ABS
+	   || GET_CODE (XEXP (x, 0)) == NEG)
+	  && GET_CODE (XEXP (XEXP (x, 0), 0)) == FLOAT_EXTEND
+	  && GET_MODE (XEXP (XEXP (XEXP (x, 0), 0), 0)) == mode)
+	return simplify_gen_unary (GET_CODE (XEXP (x, 0)), mode,
+				   XEXP (XEXP (XEXP (x, 0), 0), 0), mode);
+
+      /* (float_truncate:SF (subreg:DF (float_truncate:SF X) 0))
+	 is (float_truncate:SF x).  */
+      if (GET_CODE (XEXP (x, 0)) == SUBREG
+	  && subreg_lowpart_p (XEXP (x, 0))
+	  && GET_CODE (SUBREG_REG (XEXP (x, 0))) == FLOAT_TRUNCATE)
+	return SUBREG_REG (XEXP (x, 0));
+      break;
+    case FLOAT_EXTEND:
+      /*  (float_extend (float_extend x)) is (float_extend x)
+
+	  (float_extend (float x)) is (float x) assuming that double
+	  rounding can't happen.
+          */
+      if (GET_CODE (XEXP (x, 0)) == FLOAT_EXTEND
+	  || (GET_CODE (XEXP (x, 0)) == FLOAT
+	      && ((unsigned)significand_size (GET_MODE (XEXP (x, 0)))
+		  >= (GET_MODE_BITSIZE (GET_MODE (XEXP (XEXP (x, 0), 0)))
+		      - num_sign_bit_copies (XEXP (XEXP (x, 0), 0),
+					     GET_MODE (XEXP (XEXP (x, 0), 0)))))))
+	return simplify_gen_unary (GET_CODE (XEXP (x, 0)), mode,
+				   XEXP (XEXP (x, 0), 0),
+				   GET_MODE (XEXP (XEXP (x, 0), 0)));
+
+      break;
+#ifdef HAVE_cc0
+    case COMPARE:
+      /* Convert (compare FOO (const_int 0)) to FOO unless we aren't
+	 using cc0, in which case we want to leave it as a COMPARE
+	 so we can distinguish it from a register-register-copy.  */
+      if (XEXP (x, 1) == const0_rtx)
+	return XEXP (x, 0);
+
+      /* x - 0 is the same as x unless x's mode has signed zeros and
+	 allows rounding towards -infinity.  Under those conditions,
+	 0 - 0 is -0.  */
+      if (!(HONOR_SIGNED_ZEROS (GET_MODE (XEXP (x, 0)))
+	    && HONOR_SIGN_DEPENDENT_ROUNDING (GET_MODE (XEXP (x, 0))))
+	  && XEXP (x, 1) == CONST0_RTX (GET_MODE (XEXP (x, 0))))
+	return XEXP (x, 0);
+      break;
+#endif
+
+    case CONST:
+      /* (const (const X)) can become (const X).  Do it this way rather than
+	 returning the inner CONST since CONST can be shared with a
+	 REG_EQUAL note.  */
+      if (GET_CODE (XEXP (x, 0)) == CONST)
+	SUBST (XEXP (x, 0), XEXP (XEXP (x, 0), 0));
+      break;
+
+#ifdef HAVE_lo_sum
+    case LO_SUM:
+      /* Convert (lo_sum (high FOO) FOO) to FOO.  This is necessary so we
+	 can add in an offset.  find_split_point will split this address up
+	 again if it doesn't match.  */
+      if (GET_CODE (XEXP (x, 0)) == HIGH
+	  && rtx_equal_p (XEXP (XEXP (x, 0), 0), XEXP (x, 1)))
+	return XEXP (x, 1);
+      break;
+#endif
+
+    case PLUS:
+      /* Canonicalize (plus (mult (neg B) C) A) to (minus A (mult B C)).
+       */
+      if (GET_CODE (XEXP (x, 0)) == MULT
+	  && GET_CODE (XEXP (XEXP (x, 0), 0)) == NEG)
+	{
+	  rtx in1, in2;
+
+	  in1 = XEXP (XEXP (XEXP (x, 0), 0), 0);
+	  in2 = XEXP (XEXP (x, 0), 1);
+	  return gen_binary (MINUS, mode, XEXP (x, 1),
+			     gen_binary (MULT, mode, in1, in2));
+	}
+
+      /* If we have (plus (plus (A const) B)), associate it so that CONST is
+	 outermost.  That's because that's the way indexed addresses are
+	 supposed to appear.  This code used to check many more cases, but
+	 they are now checked elsewhere.  */
+      if (GET_CODE (XEXP (x, 0)) == PLUS
+	  && CONSTANT_ADDRESS_P (XEXP (XEXP (x, 0), 1)))
+	return gen_binary (PLUS, mode,
+			   gen_binary (PLUS, mode, XEXP (XEXP (x, 0), 0),
+				       XEXP (x, 1)),
+			   XEXP (XEXP (x, 0), 1));
+
+      /* (plus (xor (and <foo> (const_int pow2 - 1)) <c>) <-c>)
+	 when c is (const_int (pow2 + 1) / 2) is a sign extension of a
+	 bit-field and can be replaced by either a sign_extend or a
+	 sign_extract.  The `and' may be a zero_extend and the two
+	 <c>, -<c> constants may be reversed.  */
+      if (GET_CODE (XEXP (x, 0)) == XOR
+	  && GET_CODE (XEXP (x, 1)) == CONST_INT
+	  && GET_CODE (XEXP (XEXP (x, 0), 1)) == CONST_INT
+	  && INTVAL (XEXP (x, 1)) == -INTVAL (XEXP (XEXP (x, 0), 1))
+	  && ((i = exact_log2 (INTVAL (XEXP (XEXP (x, 0), 1)))) >= 0
+	      || (i = exact_log2 (INTVAL (XEXP (x, 1)))) >= 0)
+	  && GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT
+	  && ((GET_CODE (XEXP (XEXP (x, 0), 0)) == AND
+	       && GET_CODE (XEXP (XEXP (XEXP (x, 0), 0), 1)) == CONST_INT
+	       && (INTVAL (XEXP (XEXP (XEXP (x, 0), 0), 1))
+		   == ((HOST_WIDE_INT) 1 << (i + 1)) - 1))
+	      || (GET_CODE (XEXP (XEXP (x, 0), 0)) == ZERO_EXTEND
+		  && (GET_MODE_BITSIZE (GET_MODE (XEXP (XEXP (XEXP (x, 0), 0), 0)))
+		      == (unsigned int) i + 1))))
+	return simplify_shift_const
+	  (NULL_RTX, ASHIFTRT, mode,
+	   simplify_shift_const (NULL_RTX, ASHIFT, mode,
+				 XEXP (XEXP (XEXP (x, 0), 0), 0),
+				 GET_MODE_BITSIZE (mode) - (i + 1)),
+	   GET_MODE_BITSIZE (mode) - (i + 1));
+
+      /* (plus (comparison A B) C) can become (neg (rev-comp A B)) if
+	 C is 1 and STORE_FLAG_VALUE is -1 or if C is -1 and STORE_FLAG_VALUE
+	 is 1.  This produces better code than the alternative immediately
+	 below.  */
+      if (GET_RTX_CLASS (GET_CODE (XEXP (x, 0))) == '<'
+	  && ((STORE_FLAG_VALUE == -1 && XEXP (x, 1) == const1_rtx)
+	      || (STORE_FLAG_VALUE == 1 && XEXP (x, 1) == constm1_rtx))
+	  && (reversed = reversed_comparison (XEXP (x, 0), mode,
+					      XEXP (XEXP (x, 0), 0),
+					      XEXP (XEXP (x, 0), 1))))
+	return
+	  simplify_gen_unary (NEG, mode, reversed, mode);
+
+      /* If only the low-order bit of X is possibly nonzero, (plus x -1)
+	 can become (ashiftrt (ashift (xor x 1) C) C) where C is
+	 the bitsize of the mode - 1.  This allows simplification of
+	 "a = (b & 8) == 0;"  */
+      if (XEXP (x, 1) == constm1_rtx
+	  && GET_CODE (XEXP (x, 0)) != REG
+	  && ! (GET_CODE (XEXP (x, 0)) == SUBREG
+		&& GET_CODE (SUBREG_REG (XEXP (x, 0))) == REG)
+	  && nonzero_bits (XEXP (x, 0), mode) == 1)
+	return simplify_shift_const (NULL_RTX, ASHIFTRT, mode,
+	   simplify_shift_const (NULL_RTX, ASHIFT, mode,
+				 gen_rtx_XOR (mode, XEXP (x, 0), const1_rtx),
+				 GET_MODE_BITSIZE (mode) - 1),
+	   GET_MODE_BITSIZE (mode) - 1);
+
+      /* If we are adding two things that have no bits in common, convert
+	 the addition into an IOR.  This will often be further simplified,
+	 for example in cases like ((a & 1) + (a & 2)), which can
+	 become a & 3.  */
+
+      if (GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT
+	  && (nonzero_bits (XEXP (x, 0), mode)
+	      & nonzero_bits (XEXP (x, 1), mode)) == 0)
+	{
+	  /* Try to simplify the expression further.  */
+	  rtx tor = gen_binary (IOR, mode, XEXP (x, 0), XEXP (x, 1));
+	  temp = combine_simplify_rtx (tor, mode, last, in_dest);
+
+	  /* If we could, great.  If not, do not go ahead with the IOR
+	     replacement, since PLUS appears in many special purpose
+	     address arithmetic instructions.  */
+	  if (GET_CODE (temp) != CLOBBER && temp != tor)
+	    return temp;
+	}
+      break;
+
+    case MINUS:
+      /* If STORE_FLAG_VALUE is 1, (minus 1 (comparison foo bar)) can be done
+	 by reversing the comparison code if valid.  */
+      if (STORE_FLAG_VALUE == 1
+	  && XEXP (x, 0) == const1_rtx
+	  && GET_RTX_CLASS (GET_CODE (XEXP (x, 1))) == '<'
+	  && (reversed = reversed_comparison (XEXP (x, 1), mode,
+					      XEXP (XEXP (x, 1), 0),
+					      XEXP (XEXP (x, 1), 1))))
+	return reversed;
+
+      /* (minus <foo> (and <foo> (const_int -pow2))) becomes
+	 (and <foo> (const_int pow2-1))  */
+      if (GET_CODE (XEXP (x, 1)) == AND
+	  && GET_CODE (XEXP (XEXP (x, 1), 1)) == CONST_INT
+	  && exact_log2 (-INTVAL (XEXP (XEXP (x, 1), 1))) >= 0
+	  && rtx_equal_p (XEXP (XEXP (x, 1), 0), XEXP (x, 0)))
+	return simplify_and_const_int (NULL_RTX, mode, XEXP (x, 0),
+				       -INTVAL (XEXP (XEXP (x, 1), 1)) - 1);
+
+      /* Canonicalize (minus A (mult (neg B) C)) to (plus (mult B C) A).
+       */
+      if (GET_CODE (XEXP (x, 1)) == MULT
+	  && GET_CODE (XEXP (XEXP (x, 1), 0)) == NEG)
+	{
+	  rtx in1, in2;
+
+	  in1 = XEXP (XEXP (XEXP (x, 1), 0), 0);
+	  in2 = XEXP (XEXP (x, 1), 1);
+	  return gen_binary (PLUS, mode, gen_binary (MULT, mode, in1, in2),
+			     XEXP (x, 0));
+	}
+
+      /* Canonicalize (minus (neg A) (mult B C)) to
+	 (minus (mult (neg B) C) A).  */
+      if (GET_CODE (XEXP (x, 1)) == MULT
+	  && GET_CODE (XEXP (x, 0)) == NEG)
+	{
+	  rtx in1, in2;
+
+	  in1 = simplify_gen_unary (NEG, mode, XEXP (XEXP (x, 1), 0), mode);
+	  in2 = XEXP (XEXP (x, 1), 1);
+	  return gen_binary (MINUS, mode, gen_binary (MULT, mode, in1, in2),
+			     XEXP (XEXP (x, 0), 0));
+	}
+
+      /* Canonicalize (minus A (plus B C)) to (minus (minus A B) C) for
+	 integers.  */
+      if (GET_CODE (XEXP (x, 1)) == PLUS && INTEGRAL_MODE_P (mode))
+	return gen_binary (MINUS, mode,
+			   gen_binary (MINUS, mode, XEXP (x, 0),
+				       XEXP (XEXP (x, 1), 0)),
+			   XEXP (XEXP (x, 1), 1));
+      break;
+
+    case MULT:
+      /* If we have (mult (plus A B) C), apply the distributive law and then
+	 the inverse distributive law to see if things simplify.  This
+	 occurs mostly in addresses, often when unrolling loops.  */
+
+      if (GET_CODE (XEXP (x, 0)) == PLUS)
+	{
+	  x = apply_distributive_law
+	    (gen_binary (PLUS, mode,
+			 gen_binary (MULT, mode,
+				     XEXP (XEXP (x, 0), 0), XEXP (x, 1)),
+			 gen_binary (MULT, mode,
+				     XEXP (XEXP (x, 0), 1),
+				     copy_rtx (XEXP (x, 1)))));
+
+	  if (GET_CODE (x) != MULT)
+	    return x;
+	}
+      /* Try simplify a*(b/c) as (a*b)/c.  */
+      if (FLOAT_MODE_P (mode) && flag_unsafe_math_optimizations
+	  && GET_CODE (XEXP (x, 0)) == DIV)
+	{
+	  rtx tem = simplify_binary_operation (MULT, mode,
+					       XEXP (XEXP (x, 0), 0),
+					       XEXP (x, 1));
+	  if (tem)
+	    return gen_binary (DIV, mode, tem, XEXP (XEXP (x, 0), 1));
+	}
+      break;
+
+    case UDIV:
+      /* If this is a divide by a power of two, treat it as a shift if
+	 its first operand is a shift.  */
+      if (GET_CODE (XEXP (x, 1)) == CONST_INT
+	  && (i = exact_log2 (INTVAL (XEXP (x, 1)))) >= 0
+	  && (GET_CODE (XEXP (x, 0)) == ASHIFT
+	      || GET_CODE (XEXP (x, 0)) == LSHIFTRT
+	      || GET_CODE (XEXP (x, 0)) == ASHIFTRT
+	      || GET_CODE (XEXP (x, 0)) == ROTATE
+	      || GET_CODE (XEXP (x, 0)) == ROTATERT))
+	return simplify_shift_const (NULL_RTX, LSHIFTRT, mode, XEXP (x, 0), i);
+      break;
+
+    case EQ:  case NE:
+    case GT:  case GTU:  case GE:  case GEU:
+    case LT:  case LTU:  case LE:  case LEU:
+    case UNEQ:  case LTGT:
+    case UNGT:  case UNGE:
+    case UNLT:  case UNLE:
+    case UNORDERED: case ORDERED:
+      /* If the first operand is a condition code, we can't do anything
+	 with it.  */
+      if (GET_CODE (XEXP (x, 0)) == COMPARE
+	  || (GET_MODE_CLASS (GET_MODE (XEXP (x, 0))) != MODE_CC
+	      && ! CC0_P (XEXP (x, 0))))
+	{
+	  rtx op0 = XEXP (x, 0);
+	  rtx op1 = XEXP (x, 1);
+	  enum rtx_code new_code;
+
+	  if (GET_CODE (op0) == COMPARE)
+	    op1 = XEXP (op0, 1), op0 = XEXP (op0, 0);
+
+	  /* Simplify our comparison, if possible.  */
+	  new_code = simplify_comparison (code, &op0, &op1);
+
+	  /* If STORE_FLAG_VALUE is 1, we can convert (ne x 0) to simply X
+	     if only the low-order bit is possibly nonzero in X (such as when
+	     X is a ZERO_EXTRACT of one bit).  Similarly, we can convert EQ to
+	     (xor X 1) or (minus 1 X); we use the former.  Finally, if X is
+	     known to be either 0 or -1, NE becomes a NEG and EQ becomes
+	     (plus X 1).
+
+	     Remove any ZERO_EXTRACT we made when thinking this was a
+	     comparison.  It may now be simpler to use, e.g., an AND.  If a
+	     ZERO_EXTRACT is indeed appropriate, it will be placed back by
+	     the call to make_compound_operation in the SET case.  */
+
+	  if (STORE_FLAG_VALUE == 1
+	      && new_code == NE && GET_MODE_CLASS (mode) == MODE_INT
+	      && op1 == const0_rtx
+	      && mode == GET_MODE (op0)
+	      && nonzero_bits (op0, mode) == 1)
+	    return gen_lowpart_for_combine (mode,
+					    expand_compound_operation (op0));
+
+	  else if (STORE_FLAG_VALUE == 1
+		   && new_code == NE && GET_MODE_CLASS (mode) == MODE_INT
+		   && op1 == const0_rtx
+		   && mode == GET_MODE (op0)
+		   && (num_sign_bit_copies (op0, mode)
+		       == GET_MODE_BITSIZE (mode)))
+	    {
+	      op0 = expand_compound_operation (op0);
+	      return simplify_gen_unary (NEG, mode,
+					 gen_lowpart_for_combine (mode, op0),
+					 mode);
+	    }
+
+	  else if (STORE_FLAG_VALUE == 1
+		   && new_code == EQ && GET_MODE_CLASS (mode) == MODE_INT
+		   && op1 == const0_rtx
+		   && mode == GET_MODE (op0)
+		   && nonzero_bits (op0, mode) == 1)
+	    {
+	      op0 = expand_compound_operation (op0);
+	      return gen_binary (XOR, mode,
+				 gen_lowpart_for_combine (mode, op0),
+				 const1_rtx);
+	    }
+
+	  else if (STORE_FLAG_VALUE == 1
+		   && new_code == EQ && GET_MODE_CLASS (mode) == MODE_INT
+		   && op1 == const0_rtx
+		   && mode == GET_MODE (op0)
+		   && (num_sign_bit_copies (op0, mode)
+		       == GET_MODE_BITSIZE (mode)))
+	    {
+	      op0 = expand_compound_operation (op0);
+	      return plus_constant (gen_lowpart_for_combine (mode, op0), 1);
+	    }
+
+	  /* If STORE_FLAG_VALUE is -1, we have cases similar to
+	     those above.  */
+	  if (STORE_FLAG_VALUE == -1
+	      && new_code == NE && GET_MODE_CLASS (mode) == MODE_INT
+	      && op1 == const0_rtx
+	      && (num_sign_bit_copies (op0, mode)
+		  == GET_MODE_BITSIZE (mode)))
+	    return gen_lowpart_for_combine (mode,
+					    expand_compound_operation (op0));
+
+	  else if (STORE_FLAG_VALUE == -1
+		   && new_code == NE && GET_MODE_CLASS (mode) == MODE_INT
+		   && op1 == const0_rtx
+		   && mode == GET_MODE (op0)
+		   && nonzero_bits (op0, mode) == 1)
+	    {
+	      op0 = expand_compound_operation (op0);
+	      return simplify_gen_unary (NEG, mode,
+					 gen_lowpart_for_combine (mode, op0),
+					 mode);
+	    }
+
+	  else if (STORE_FLAG_VALUE == -1
+		   && new_code == EQ && GET_MODE_CLASS (mode) == MODE_INT
+		   && op1 == const0_rtx
+		   && mode == GET_MODE (op0)
+		   && (num_sign_bit_copies (op0, mode)
+		       == GET_MODE_BITSIZE (mode)))
+	    {
+	      op0 = expand_compound_operation (op0);
+	      return simplify_gen_unary (NOT, mode,
+					 gen_lowpart_for_combine (mode, op0),
+					 mode);
+	    }
+
+	  /* If X is 0/1, (eq X 0) is X-1.  */
+	  else if (STORE_FLAG_VALUE == -1
+		   && new_code == EQ && GET_MODE_CLASS (mode) == MODE_INT
+		   && op1 == const0_rtx
+		   && mode == GET_MODE (op0)
+		   && nonzero_bits (op0, mode) == 1)
+	    {
+	      op0 = expand_compound_operation (op0);
+	      return plus_constant (gen_lowpart_for_combine (mode, op0), -1);
+	    }
+
+	  /* If STORE_FLAG_VALUE says to just test the sign bit and X has just
+	     one bit that might be nonzero, we can convert (ne x 0) to
+	     (ashift x c) where C puts the bit in the sign bit.  Remove any
+	     AND with STORE_FLAG_VALUE when we are done, since we are only
+	     going to test the sign bit.  */
+	  if (new_code == NE && GET_MODE_CLASS (mode) == MODE_INT
+	      && GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT
+	      && ((STORE_FLAG_VALUE & GET_MODE_MASK (mode))
+		  == (unsigned HOST_WIDE_INT) 1 << (GET_MODE_BITSIZE (mode) - 1))
+	      && op1 == const0_rtx
+	      && mode == GET_MODE (op0)
+	      && (i = exact_log2 (nonzero_bits (op0, mode))) >= 0)
+	    {
+	      x = simplify_shift_const (NULL_RTX, ASHIFT, mode,
+					expand_compound_operation (op0),
+					GET_MODE_BITSIZE (mode) - 1 - i);
+	      if (GET_CODE (x) == AND && XEXP (x, 1) == const_true_rtx)
+		return XEXP (x, 0);
+	      else
+		return x;
+	    }
+
+	  /* If the code changed, return a whole new comparison.  */
+	  if (new_code != code)
+	    return gen_rtx_fmt_ee (new_code, mode, op0, op1);
+
+	  /* Otherwise, keep this operation, but maybe change its operands.
+	     This also converts (ne (compare FOO BAR) 0) to (ne FOO BAR).  */
+	  SUBST (XEXP (x, 0), op0);
+	  SUBST (XEXP (x, 1), op1);
+	}
+      break;
+
+    case IF_THEN_ELSE:
+      return simplify_if_then_else (x);
+
+    case ZERO_EXTRACT:
+    case SIGN_EXTRACT:
+    case ZERO_EXTEND:
+    case SIGN_EXTEND:
+      /* If we are processing SET_DEST, we are done.  */
+      if (in_dest)
+	return x;
+
+      return expand_compound_operation (x);
+
+    case SET:
+      return simplify_set (x);
+
+    case AND:
+    case IOR:
+    case XOR:
+      return simplify_logical (x, last);
+
+    case ABS:
+      /* (abs (neg <foo>)) -> (abs <foo>) */
+      if (GET_CODE (XEXP (x, 0)) == NEG)
+	SUBST (XEXP (x, 0), XEXP (XEXP (x, 0), 0));
+
+      /* If the mode of the operand is VOIDmode (i.e. if it is ASM_OPERANDS),
+         do nothing.  */
+      if (GET_MODE (XEXP (x, 0)) == VOIDmode)
+	break;
+
+      /* If operand is something known to be positive, ignore the ABS.  */
+      if (GET_CODE (XEXP (x, 0)) == FFS || GET_CODE (XEXP (x, 0)) == ABS
+	  || ((GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0)))
+	       <= HOST_BITS_PER_WIDE_INT)
+	      && ((nonzero_bits (XEXP (x, 0), GET_MODE (XEXP (x, 0)))
+		   & ((HOST_WIDE_INT) 1
+		      << (GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0))) - 1)))
+		  == 0)))
+	return XEXP (x, 0);
+
+      /* If operand is known to be only -1 or 0, convert ABS to NEG.  */
+      if (num_sign_bit_copies (XEXP (x, 0), mode) == GET_MODE_BITSIZE (mode))
+	return gen_rtx_NEG (mode, XEXP (x, 0));
+
+      break;
+
+    case FFS:
+      /* (ffs (*_extend <X>)) = (ffs <X>) */
+      if (GET_CODE (XEXP (x, 0)) == SIGN_EXTEND
+	  || GET_CODE (XEXP (x, 0)) == ZERO_EXTEND)
+	SUBST (XEXP (x, 0), XEXP (XEXP (x, 0), 0));
+      break;
+
+    case POPCOUNT:
+    case PARITY:
+      /* (pop* (zero_extend <X>)) = (pop* <X>) */
+      if (GET_CODE (XEXP (x, 0)) == ZERO_EXTEND)
+	SUBST (XEXP (x, 0), XEXP (XEXP (x, 0), 0));
+      break;
+
+    case FLOAT:
+      /* (float (sign_extend <X>)) = (float <X>).  */
+      if (GET_CODE (XEXP (x, 0)) == SIGN_EXTEND)
+	SUBST (XEXP (x, 0), XEXP (XEXP (x, 0), 0));
+      break;
+
+    case ASHIFT:
+    case LSHIFTRT:
+    case ASHIFTRT:
+    case ROTATE:
+    case ROTATERT:
+      /* If this is a shift by a constant amount, simplify it.  */
+      if (GET_CODE (XEXP (x, 1)) == CONST_INT)
+	return simplify_shift_const (x, code, mode, XEXP (x, 0),
+				     INTVAL (XEXP (x, 1)));
+
+      else if (SHIFT_COUNT_TRUNCATED && GET_CODE (XEXP (x, 1)) != REG)
+	SUBST (XEXP (x, 1),
+	       force_to_mode (XEXP (x, 1), GET_MODE (XEXP (x, 1)),
+			      ((HOST_WIDE_INT) 1
+			       << exact_log2 (GET_MODE_BITSIZE (GET_MODE (x))))
+			      - 1,
+			      NULL_RTX, 0));
+      break;
+
+    case VEC_SELECT:
+      {
+	rtx op0 = XEXP (x, 0);
+	rtx op1 = XEXP (x, 1);
+	int len;
+
+	if (GET_CODE (op1) != PARALLEL)
+	  abort ();
+	len = XVECLEN (op1, 0);
+	if (len == 1
+	    && GET_CODE (XVECEXP (op1, 0, 0)) == CONST_INT
+	    && GET_CODE (op0) == VEC_CONCAT)
+	  {
+	    int offset = INTVAL (XVECEXP (op1, 0, 0)) * GET_MODE_SIZE (GET_MODE (x));
+
+	    /* Try to find the element in the VEC_CONCAT.  */
+	    for (;;)
+	      {
+		if (GET_MODE (op0) == GET_MODE (x))
+		  return op0;
+		if (GET_CODE (op0) == VEC_CONCAT)
+		  {
+		    HOST_WIDE_INT op0_size = GET_MODE_SIZE (GET_MODE (XEXP (op0, 0)));
+		    if (op0_size < offset)
+		      op0 = XEXP (op0, 0);
+		    else
+		      {
+			offset -= op0_size;
+			op0 = XEXP (op0, 1);
+		      }
+		  }
+		else
+		  break;
+	      }
+	  }
+      }
+
+      break;
+
+    default:
+      break;
+    }
+
+  return x;
+}
+
+/* Simplify X, an IF_THEN_ELSE expression.  Return the new expression.  */
+
+static rtx
+simplify_if_then_else (rtx x)
+{
+  enum machine_mode mode = GET_MODE (x);
+  rtx cond = XEXP (x, 0);
+  rtx true_rtx = XEXP (x, 1);
+  rtx false_rtx = XEXP (x, 2);
+  enum rtx_code true_code = GET_CODE (cond);
+  int comparison_p = GET_RTX_CLASS (true_code) == '<';
+  rtx temp;
+  int i;
+  enum rtx_code false_code;
+  rtx reversed;
+
+  /* Simplify storing of the truth value.  */
+  if (comparison_p && true_rtx == const_true_rtx && false_rtx == const0_rtx)
+    return gen_binary (true_code, mode, XEXP (cond, 0), XEXP (cond, 1));
+
+  /* Also when the truth value has to be reversed.  */
+  if (comparison_p
+      && true_rtx == const0_rtx && false_rtx == const_true_rtx
+      && (reversed = reversed_comparison (cond, mode, XEXP (cond, 0),
+					  XEXP (cond, 1))))
+    return reversed;
+
+  /* Sometimes we can simplify the arm of an IF_THEN_ELSE if a register used
+     in it is being compared against certain values.  Get the true and false
+     comparisons and see if that says anything about the value of each arm.  */
+
+  if (comparison_p
+      && ((false_code = combine_reversed_comparison_code (cond))
+	  != UNKNOWN)
+      && GET_CODE (XEXP (cond, 0)) == REG)
+    {
+      HOST_WIDE_INT nzb;
+      rtx from = XEXP (cond, 0);
+      rtx true_val = XEXP (cond, 1);
+      rtx false_val = true_val;
+      int swapped = 0;
+
+      /* If FALSE_CODE is EQ, swap the codes and arms.  */
+
+      if (false_code == EQ)
+	{
+	  swapped = 1, true_code = EQ, false_code = NE;
+	  temp = true_rtx, true_rtx = false_rtx, false_rtx = temp;
+	}
+
+      /* If we are comparing against zero and the expression being tested has
+	 only a single bit that might be nonzero, that is its value when it is
+	 not equal to zero.  Similarly if it is known to be -1 or 0.  */
+
+      if (true_code == EQ && true_val == const0_rtx
+	  && exact_log2 (nzb = nonzero_bits (from, GET_MODE (from))) >= 0)
+	false_code = EQ, false_val = GEN_INT (nzb);
+      else if (true_code == EQ && true_val == const0_rtx
+	       && (num_sign_bit_copies (from, GET_MODE (from))
+		   == GET_MODE_BITSIZE (GET_MODE (from))))
+	false_code = EQ, false_val = constm1_rtx;
+
+      /* Now simplify an arm if we know the value of the register in the
+	 branch and it is used in the arm.  Be careful due to the potential
+	 of locally-shared RTL.  */
+
+      if (reg_mentioned_p (from, true_rtx))
+	true_rtx = subst (known_cond (copy_rtx (true_rtx), true_code,
+				      from, true_val),
+		      pc_rtx, pc_rtx, 0, 0);
+      if (reg_mentioned_p (from, false_rtx))
+	false_rtx = subst (known_cond (copy_rtx (false_rtx), false_code,
+				   from, false_val),
+		       pc_rtx, pc_rtx, 0, 0);
+
+      SUBST (XEXP (x, 1), swapped ? false_rtx : true_rtx);
+      SUBST (XEXP (x, 2), swapped ? true_rtx : false_rtx);
+
+      true_rtx = XEXP (x, 1);
+      false_rtx = XEXP (x, 2);
+      true_code = GET_CODE (cond);
+    }
+
+  /* If we have (if_then_else FOO (pc) (label_ref BAR)) and FOO can be
+     reversed, do so to avoid needing two sets of patterns for
+     subtract-and-branch insns.  Similarly if we have a constant in the true
+     arm, the false arm is the same as the first operand of the comparison, or
+     the false arm is more complicated than the true arm.  */
+
+  if (comparison_p
+      && combine_reversed_comparison_code (cond) != UNKNOWN
+      && (true_rtx == pc_rtx
+	  || (CONSTANT_P (true_rtx)
+	      && GET_CODE (false_rtx) != CONST_INT && false_rtx != pc_rtx)
+	  || true_rtx == const0_rtx
+	  || (GET_RTX_CLASS (GET_CODE (true_rtx)) == 'o'
+	      && GET_RTX_CLASS (GET_CODE (false_rtx)) != 'o')
+	  || (GET_CODE (true_rtx) == SUBREG
+	      && GET_RTX_CLASS (GET_CODE (SUBREG_REG (true_rtx))) == 'o'
+	      && GET_RTX_CLASS (GET_CODE (false_rtx)) != 'o')
+	  || reg_mentioned_p (true_rtx, false_rtx)
+	  || rtx_equal_p (false_rtx, XEXP (cond, 0))))
+    {
+      true_code = reversed_comparison_code (cond, NULL);
+      SUBST (XEXP (x, 0),
+	     reversed_comparison (cond, GET_MODE (cond), XEXP (cond, 0),
+				  XEXP (cond, 1)));
+
+      SUBST (XEXP (x, 1), false_rtx);
+      SUBST (XEXP (x, 2), true_rtx);
+
+      temp = true_rtx, true_rtx = false_rtx, false_rtx = temp;
+      cond = XEXP (x, 0);
+
+      /* It is possible that the conditional has been simplified out.  */
+      true_code = GET_CODE (cond);
+      comparison_p = GET_RTX_CLASS (true_code) == '<';
+    }
+
+  /* If the two arms are identical, we don't need the comparison.  */
+
+  if (rtx_equal_p (true_rtx, false_rtx) && ! side_effects_p (cond))
+    return true_rtx;
+
+  /* Convert a == b ? b : a to "a".  */
+  if (true_code == EQ && ! side_effects_p (cond)
+      && !HONOR_NANS (mode)
+      && rtx_equal_p (XEXP (cond, 0), false_rtx)
+      && rtx_equal_p (XEXP (cond, 1), true_rtx))
+    return false_rtx;
+  else if (true_code == NE && ! side_effects_p (cond)
+	   && !HONOR_NANS (mode)
+	   && rtx_equal_p (XEXP (cond, 0), true_rtx)
+	   && rtx_equal_p (XEXP (cond, 1), false_rtx))
+    return true_rtx;
+
+  /* Look for cases where we have (abs x) or (neg (abs X)).  */
+
+  if (GET_MODE_CLASS (mode) == MODE_INT
+      && GET_CODE (false_rtx) == NEG
+      && rtx_equal_p (true_rtx, XEXP (false_rtx, 0))
+      && comparison_p
+      && rtx_equal_p (true_rtx, XEXP (cond, 0))
+      && ! side_effects_p (true_rtx))
+    switch (true_code)
+      {
+      case GT:
+      case GE:
+	return simplify_gen_unary (ABS, mode, true_rtx, mode);
+      case LT:
+      case LE:
+	return
+	  simplify_gen_unary (NEG, mode,
+			      simplify_gen_unary (ABS, mode, true_rtx, mode),
+			      mode);
+      default:
+	break;
+      }
+
+  /* Look for MIN or MAX.  */
+
+  if ((! FLOAT_MODE_P (mode) || flag_unsafe_math_optimizations)
+      && comparison_p
+      && rtx_equal_p (XEXP (cond, 0), true_rtx)
+      && rtx_equal_p (XEXP (cond, 1), false_rtx)
+      && ! side_effects_p (cond))
+    switch (true_code)
+      {
+      case GE:
+      case GT:
+	return gen_binary (SMAX, mode, true_rtx, false_rtx);
+      case LE:
+      case LT:
+	return gen_binary (SMIN, mode, true_rtx, false_rtx);
+      case GEU:
+      case GTU:
+	return gen_binary (UMAX, mode, true_rtx, false_rtx);
+      case LEU:
+      case LTU:
+	return gen_binary (UMIN, mode, true_rtx, false_rtx);
+      default:
+	break;
+      }
+
+  /* If we have (if_then_else COND (OP Z C1) Z) and OP is an identity when its
+     second operand is zero, this can be done as (OP Z (mult COND C2)) where
+     C2 = C1 * STORE_FLAG_VALUE. Similarly if OP has an outer ZERO_EXTEND or
+     SIGN_EXTEND as long as Z is already extended (so we don't destroy it).
+     We can do this kind of thing in some cases when STORE_FLAG_VALUE is
+     neither 1 or -1, but it isn't worth checking for.  */
+
+  if ((STORE_FLAG_VALUE == 1 || STORE_FLAG_VALUE == -1)
+      && comparison_p
+      && GET_MODE_CLASS (mode) == MODE_INT
+      && ! side_effects_p (x))
+    {
+      rtx t = make_compound_operation (true_rtx, SET);
+      rtx f = make_compound_operation (false_rtx, SET);
+      rtx cond_op0 = XEXP (cond, 0);
+      rtx cond_op1 = XEXP (cond, 1);
+      enum rtx_code op = NIL, extend_op = NIL;
+      enum machine_mode m = mode;
+      rtx z = 0, c1 = NULL_RTX;
+
+      if ((GET_CODE (t) == PLUS || GET_CODE (t) == MINUS
+	   || GET_CODE (t) == IOR || GET_CODE (t) == XOR
+	   || GET_CODE (t) == ASHIFT
+	   || GET_CODE (t) == LSHIFTRT || GET_CODE (t) == ASHIFTRT)
+	  && rtx_equal_p (XEXP (t, 0), f))
+	c1 = XEXP (t, 1), op = GET_CODE (t), z = f;
+
+      /* If an identity-zero op is commutative, check whether there
+	 would be a match if we swapped the operands.  */
+      else if ((GET_CODE (t) == PLUS || GET_CODE (t) == IOR
+		|| GET_CODE (t) == XOR)
+	       && rtx_equal_p (XEXP (t, 1), f))
+	c1 = XEXP (t, 0), op = GET_CODE (t), z = f;
+      else if (GET_CODE (t) == SIGN_EXTEND
+	       && (GET_CODE (XEXP (t, 0)) == PLUS
+		   || GET_CODE (XEXP (t, 0)) == MINUS
+		   || GET_CODE (XEXP (t, 0)) == IOR
+		   || GET_CODE (XEXP (t, 0)) == XOR
+		   || GET_CODE (XEXP (t, 0)) == ASHIFT
+		   || GET_CODE (XEXP (t, 0)) == LSHIFTRT
+		   || GET_CODE (XEXP (t, 0)) == ASHIFTRT)
+	       && GET_CODE (XEXP (XEXP (t, 0), 0)) == SUBREG
+	       && subreg_lowpart_p (XEXP (XEXP (t, 0), 0))
+	       && rtx_equal_p (SUBREG_REG (XEXP (XEXP (t, 0), 0)), f)
+	       && (num_sign_bit_copies (f, GET_MODE (f))
+		   > (unsigned int)
+		     (GET_MODE_BITSIZE (mode)
+		      - GET_MODE_BITSIZE (GET_MODE (XEXP (XEXP (t, 0), 0))))))
+	{
+	  c1 = XEXP (XEXP (t, 0), 1); z = f; op = GET_CODE (XEXP (t, 0));
+	  extend_op = SIGN_EXTEND;
+	  m = GET_MODE (XEXP (t, 0));
+	}
+      else if (GET_CODE (t) == SIGN_EXTEND
+	       && (GET_CODE (XEXP (t, 0)) == PLUS
+		   || GET_CODE (XEXP (t, 0)) == IOR
+		   || GET_CODE (XEXP (t, 0)) == XOR)
+	       && GET_CODE (XEXP (XEXP (t, 0), 1)) == SUBREG
+	       && subreg_lowpart_p (XEXP (XEXP (t, 0), 1))
+	       && rtx_equal_p (SUBREG_REG (XEXP (XEXP (t, 0), 1)), f)
+	       && (num_sign_bit_copies (f, GET_MODE (f))
+		   > (unsigned int)
+		     (GET_MODE_BITSIZE (mode)
+		      - GET_MODE_BITSIZE (GET_MODE (XEXP (XEXP (t, 0), 1))))))
+	{
+	  c1 = XEXP (XEXP (t, 0), 0); z = f; op = GET_CODE (XEXP (t, 0));
+	  extend_op = SIGN_EXTEND;
+	  m = GET_MODE (XEXP (t, 0));
+	}
+      else if (GET_CODE (t) == ZERO_EXTEND
+	       && (GET_CODE (XEXP (t, 0)) == PLUS
+		   || GET_CODE (XEXP (t, 0)) == MINUS
+		   || GET_CODE (XEXP (t, 0)) == IOR
+		   || GET_CODE (XEXP (t, 0)) == XOR
+		   || GET_CODE (XEXP (t, 0)) == ASHIFT
+		   || GET_CODE (XEXP (t, 0)) == LSHIFTRT
+		   || GET_CODE (XEXP (t, 0)) == ASHIFTRT)
+	       && GET_CODE (XEXP (XEXP (t, 0), 0)) == SUBREG
+	       && GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT
+	       && subreg_lowpart_p (XEXP (XEXP (t, 0), 0))
+	       && rtx_equal_p (SUBREG_REG (XEXP (XEXP (t, 0), 0)), f)
+	       && ((nonzero_bits (f, GET_MODE (f))
+		    & ~GET_MODE_MASK (GET_MODE (XEXP (XEXP (t, 0), 0))))
+		   == 0))
+	{
+	  c1 = XEXP (XEXP (t, 0), 1); z = f; op = GET_CODE (XEXP (t, 0));
+	  extend_op = ZERO_EXTEND;
+	  m = GET_MODE (XEXP (t, 0));
+	}
+      else if (GET_CODE (t) == ZERO_EXTEND
+	       && (GET_CODE (XEXP (t, 0)) == PLUS
+		   || GET_CODE (XEXP (t, 0)) == IOR
+		   || GET_CODE (XEXP (t, 0)) == XOR)
+	       && GET_CODE (XEXP (XEXP (t, 0), 1)) == SUBREG
+	       && GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT
+	       && subreg_lowpart_p (XEXP (XEXP (t, 0), 1))
+	       && rtx_equal_p (SUBREG_REG (XEXP (XEXP (t, 0), 1)), f)
+	       && ((nonzero_bits (f, GET_MODE (f))
+		    & ~GET_MODE_MASK (GET_MODE (XEXP (XEXP (t, 0), 1))))
+		   == 0))
+	{
+	  c1 = XEXP (XEXP (t, 0), 0); z = f; op = GET_CODE (XEXP (t, 0));
+	  extend_op = ZERO_EXTEND;
+	  m = GET_MODE (XEXP (t, 0));
+	}
+
+      if (z)
+	{
+	  temp = subst (gen_binary (true_code, m, cond_op0, cond_op1),
+			pc_rtx, pc_rtx, 0, 0);
+	  temp = gen_binary (MULT, m, temp,
+			     gen_binary (MULT, m, c1, const_true_rtx));
+	  temp = subst (temp, pc_rtx, pc_rtx, 0, 0);
+	  temp = gen_binary (op, m, gen_lowpart_for_combine (m, z), temp);
+
+	  if (extend_op != NIL)
+	    temp = simplify_gen_unary (extend_op, mode, temp, m);
+
+	  return temp;
+	}
+    }
+
+  /* If we have (if_then_else (ne A 0) C1 0) and either A is known to be 0 or
+     1 and C1 is a single bit or A is known to be 0 or -1 and C1 is the
+     negation of a single bit, we can convert this operation to a shift.  We
+     can actually do this more generally, but it doesn't seem worth it.  */
+
+  if (true_code == NE && XEXP (cond, 1) == const0_rtx
+      && false_rtx == const0_rtx && GET_CODE (true_rtx) == CONST_INT
+      && ((1 == nonzero_bits (XEXP (cond, 0), mode)
+	   && (i = exact_log2 (INTVAL (true_rtx))) >= 0)
+	  || ((num_sign_bit_copies (XEXP (cond, 0), mode)
+	       == GET_MODE_BITSIZE (mode))
+	      && (i = exact_log2 (-INTVAL (true_rtx))) >= 0)))
+    return
+      simplify_shift_const (NULL_RTX, ASHIFT, mode,
+			    gen_lowpart_for_combine (mode, XEXP (cond, 0)), i);
+
+  /* (IF_THEN_ELSE (NE REG 0) (0) (8)) is REG for nonzero_bits (REG) == 8.  */
+  if (true_code == NE && XEXP (cond, 1) == const0_rtx
+      && false_rtx == const0_rtx && GET_CODE (true_rtx) == CONST_INT
+      && GET_MODE (XEXP (cond, 0)) == mode
+      && (INTVAL (true_rtx) & GET_MODE_MASK (mode))
+	  == nonzero_bits (XEXP (cond, 0), mode)
+      && (i = exact_log2 (INTVAL (true_rtx) & GET_MODE_MASK (mode))) >= 0)
+    return XEXP (cond, 0);
+
+  return x;
+}
+
+/* Simplify X, a SET expression.  Return the new expression.  */
+
+static rtx
+simplify_set (rtx x)
+{
+  rtx src = SET_SRC (x);
+  rtx dest = SET_DEST (x);
+  enum machine_mode mode
+    = GET_MODE (src) != VOIDmode ? GET_MODE (src) : GET_MODE (dest);
+  rtx other_insn;
+  rtx *cc_use;
+
+  /* (set (pc) (return)) gets written as (return).  */
+  if (GET_CODE (dest) == PC && GET_CODE (src) == RETURN)
+    return src;
+
+  /* Now that we know for sure which bits of SRC we are using, see if we can
+     simplify the expression for the object knowing that we only need the
+     low-order bits.  */
+
+  if (GET_MODE_CLASS (mode) == MODE_INT
+      && GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT)
+    {
+      src = force_to_mode (src, mode, ~(HOST_WIDE_INT) 0, NULL_RTX, 0);
+      SUBST (SET_SRC (x), src);
+    }
+
+  /* If we are setting CC0 or if the source is a COMPARE, look for the use of
+     the comparison result and try to simplify it unless we already have used
+     undobuf.other_insn.  */
+  if ((GET_MODE_CLASS (mode) == MODE_CC
+       || GET_CODE (src) == COMPARE
+       || CC0_P (dest))
+      && (cc_use = find_single_use (dest, subst_insn, &other_insn)) != 0
+      && (undobuf.other_insn == 0 || other_insn == undobuf.other_insn)
+      && GET_RTX_CLASS (GET_CODE (*cc_use)) == '<'
+      && rtx_equal_p (XEXP (*cc_use, 0), dest))
+    {
+      enum rtx_code old_code = GET_CODE (*cc_use);
+      enum rtx_code new_code;
+      rtx op0, op1, tmp;
+      int other_changed = 0;
+      enum machine_mode compare_mode = GET_MODE (dest);
+      enum machine_mode tmp_mode;
+
+      if (GET_CODE (src) == COMPARE)
+	op0 = XEXP (src, 0), op1 = XEXP (src, 1);
+      else
+	op0 = src, op1 = const0_rtx;
+
+      /* Check whether the comparison is known at compile time.  */
+      if (GET_MODE (op0) != VOIDmode)
+	tmp_mode = GET_MODE (op0);
+      else if (GET_MODE (op1) != VOIDmode)
+	tmp_mode = GET_MODE (op1);
+      else
+	tmp_mode = compare_mode;
+      tmp = simplify_relational_operation (old_code, tmp_mode, op0, op1);
+      if (tmp != NULL_RTX)
+	{
+	  rtx pat = PATTERN (other_insn);
+	  undobuf.other_insn = other_insn;
+	  SUBST (*cc_use, tmp);
+
+	  /* Attempt to simplify CC user.  */
+	  if (GET_CODE (pat) == SET)
+	    {
+	      rtx new = simplify_rtx (SET_SRC (pat));
+	      if (new != NULL_RTX)
+		SUBST (SET_SRC (pat), new);
+	    }
+
+	  /* Convert X into a no-op move.  */
+	  SUBST (SET_DEST (x), pc_rtx);
+	  SUBST (SET_SRC (x), pc_rtx);
+	  return x;
+	}
+
+      /* Simplify our comparison, if possible.  */
+      new_code = simplify_comparison (old_code, &op0, &op1);
+
+#ifdef SELECT_CC_MODE
+      /* If this machine has CC modes other than CCmode, check to see if we
+	 need to use a different CC mode here.  */
+      compare_mode = SELECT_CC_MODE (new_code, op0, op1);
+
+#ifndef HAVE_cc0
+      /* If the mode changed, we have to change SET_DEST, the mode in the
+	 compare, and the mode in the place SET_DEST is used.  If SET_DEST is
+	 a hard register, just build new versions with the proper mode.  If it
+	 is a pseudo, we lose unless it is only time we set the pseudo, in
+	 which case we can safely change its mode.  */
+      if (compare_mode != GET_MODE (dest))
+	{
+	  unsigned int regno = REGNO (dest);
+	  rtx new_dest = gen_rtx_REG (compare_mode, regno);
+
+	  if (regno < FIRST_PSEUDO_REGISTER
+	      || (REG_N_SETS (regno) == 1 && ! REG_USERVAR_P (dest)))
+	    {
+	      if (regno >= FIRST_PSEUDO_REGISTER)
+		SUBST (regno_reg_rtx[regno], new_dest);
+
+	      SUBST (SET_DEST (x), new_dest);
+	      SUBST (XEXP (*cc_use, 0), new_dest);
+	      other_changed = 1;
+
+	      dest = new_dest;
+	    }
+	}
+#endif  /* cc0 */
+#endif  /* SELECT_CC_MODE */
+
+      /* If the code changed, we have to build a new comparison in
+	 undobuf.other_insn.  */
+      if (new_code != old_code)
+	{
+	  int other_changed_previously = other_changed;
+	  unsigned HOST_WIDE_INT mask;
+
+	  SUBST (*cc_use, gen_rtx_fmt_ee (new_code, GET_MODE (*cc_use),
+					  dest, const0_rtx));
+	  other_changed = 1;
+
+	  /* If the only change we made was to change an EQ into an NE or
+	     vice versa, OP0 has only one bit that might be nonzero, and OP1
+	     is zero, check if changing the user of the condition code will
+	     produce a valid insn.  If it won't, we can keep the original code
+	     in that insn by surrounding our operation with an XOR.  */
+
+	  if (((old_code == NE && new_code == EQ)
+	       || (old_code == EQ && new_code == NE))
+	      && ! other_changed_previously && op1 == const0_rtx
+	      && GET_MODE_BITSIZE (GET_MODE (op0)) <= HOST_BITS_PER_WIDE_INT
+	      && exact_log2 (mask = nonzero_bits (op0, GET_MODE (op0))) >= 0)
+	    {
+	      rtx pat = PATTERN (other_insn), note = 0;
+
+	      if ((recog_for_combine (&pat, other_insn, &note) < 0
+		   && ! check_asm_operands (pat)))
+		{
+		  PUT_CODE (*cc_use, old_code);
+		  other_changed = 0;
+
+		  op0 = gen_binary (XOR, GET_MODE (op0), op0, GEN_INT (mask));
+		}
+	    }
+	}
+
+      if (other_changed)
+	undobuf.other_insn = other_insn;
+
+#ifdef HAVE_cc0
+      /* If we are now comparing against zero, change our source if
+	 needed.  If we do not use cc0, we always have a COMPARE.  */
+      if (op1 == const0_rtx && dest == cc0_rtx)
+	{
+	  SUBST (SET_SRC (x), op0);
+	  src = op0;
+	}
+      else
+#endif
+
+      /* Otherwise, if we didn't previously have a COMPARE in the
+	 correct mode, we need one.  */
+      if (GET_CODE (src) != COMPARE || GET_MODE (src) != compare_mode)
+	{
+	  SUBST (SET_SRC (x), gen_rtx_COMPARE (compare_mode, op0, op1));
+	  src = SET_SRC (x);
+	}
+      else
+	{
+	  /* Otherwise, update the COMPARE if needed.  */
+	  SUBST (XEXP (src, 0), op0);
+	  SUBST (XEXP (src, 1), op1);
+	}
+    }
+  else
+    {
+      /* Get SET_SRC in a form where we have placed back any
+	 compound expressions.  Then do the checks below.  */
+      src = make_compound_operation (src, SET);
+      SUBST (SET_SRC (x), src);
+    }
+
+  /* If we have (set x (subreg:m1 (op:m2 ...) 0)) with OP being some operation,
+     and X being a REG or (subreg (reg)), we may be able to convert this to
+     (set (subreg:m2 x) (op)).
+
+     We can always do this if M1 is narrower than M2 because that means that
+     we only care about the low bits of the result.
+
+     However, on machines without WORD_REGISTER_OPERATIONS defined, we cannot
+     perform a narrower operation than requested since the high-order bits will
+     be undefined.  On machine where it is defined, this transformation is safe
+     as long as M1 and M2 have the same number of words.  */
+
+  if (GET_CODE (src) == SUBREG && subreg_lowpart_p (src)
+      && GET_RTX_CLASS (GET_CODE (SUBREG_REG (src))) != 'o'
+      && (((GET_MODE_SIZE (GET_MODE (src)) + (UNITS_PER_WORD - 1))
+	   / UNITS_PER_WORD)
+	  == ((GET_MODE_SIZE (GET_MODE (SUBREG_REG (src)))
+	       + (UNITS_PER_WORD - 1)) / UNITS_PER_WORD))
+#ifndef WORD_REGISTER_OPERATIONS
+      && (GET_MODE_SIZE (GET_MODE (src))
+        < GET_MODE_SIZE (GET_MODE (SUBREG_REG (src))))
+#endif
+#ifdef CANNOT_CHANGE_MODE_CLASS
+      && ! (GET_CODE (dest) == REG && REGNO (dest) < FIRST_PSEUDO_REGISTER
+	    && REG_CANNOT_CHANGE_MODE_P (REGNO (dest),
+					 GET_MODE (SUBREG_REG (src)),
+					 GET_MODE (src)))
+#endif
+      && (GET_CODE (dest) == REG
+	  || (GET_CODE (dest) == SUBREG
+	      && GET_CODE (SUBREG_REG (dest)) == REG)))
+    {
+      SUBST (SET_DEST (x),
+	     gen_lowpart_for_combine (GET_MODE (SUBREG_REG (src)),
+				      dest));
+      SUBST (SET_SRC (x), SUBREG_REG (src));
+
+      src = SET_SRC (x), dest = SET_DEST (x);
+    }
+
+#ifdef HAVE_cc0
+  /* If we have (set (cc0) (subreg ...)), we try to remove the subreg
+     in SRC.  */
+  if (dest == cc0_rtx
+      && GET_CODE (src) == SUBREG
+      && subreg_lowpart_p (src)
+      && (GET_MODE_BITSIZE (GET_MODE (src))
+	  < GET_MODE_BITSIZE (GET_MODE (SUBREG_REG (src)))))
+    {
+      rtx inner = SUBREG_REG (src);
+      enum machine_mode inner_mode = GET_MODE (inner);
+
+      /* Here we make sure that we don't have a sign bit on.  */
+      if (GET_MODE_BITSIZE (inner_mode) <= HOST_BITS_PER_WIDE_INT
+	  && (nonzero_bits (inner, inner_mode)
+	      < ((unsigned HOST_WIDE_INT) 1
+		 << (GET_MODE_BITSIZE (GET_MODE (src)) - 1))))
+	{
+	  SUBST (SET_SRC (x), inner);
+	  src = SET_SRC (x);
+	}
+    }
+#endif
+
+#ifdef LOAD_EXTEND_OP
+  /* If we have (set FOO (subreg:M (mem:N BAR) 0)) with M wider than N, this
+     would require a paradoxical subreg.  Replace the subreg with a
+     zero_extend to avoid the reload that would otherwise be required.  */
+
+  if (GET_CODE (src) == SUBREG && subreg_lowpart_p (src)
+      && LOAD_EXTEND_OP (GET_MODE (SUBREG_REG (src))) != NIL
+      && SUBREG_BYTE (src) == 0
+      && (GET_MODE_SIZE (GET_MODE (src))
+	  > GET_MODE_SIZE (GET_MODE (SUBREG_REG (src))))
+      && GET_CODE (SUBREG_REG (src)) == MEM)
+    {
+      SUBST (SET_SRC (x),
+	     gen_rtx (LOAD_EXTEND_OP (GET_MODE (SUBREG_REG (src))),
+		      GET_MODE (src), SUBREG_REG (src)));
+
+      src = SET_SRC (x);
+    }
+#endif
+
+  /* If we don't have a conditional move, SET_SRC is an IF_THEN_ELSE, and we
+     are comparing an item known to be 0 or -1 against 0, use a logical
+     operation instead. Check for one of the arms being an IOR of the other
+     arm with some value.  We compute three terms to be IOR'ed together.  In
+     practice, at most two will be nonzero.  Then we do the IOR's.  */
+
+  if (GET_CODE (dest) != PC
+      && GET_CODE (src) == IF_THEN_ELSE
+      && GET_MODE_CLASS (GET_MODE (src)) == MODE_INT
+      && (GET_CODE (XEXP (src, 0)) == EQ || GET_CODE (XEXP (src, 0)) == NE)
+      && XEXP (XEXP (src, 0), 1) == const0_rtx
+      && GET_MODE (src) == GET_MODE (XEXP (XEXP (src, 0), 0))
+#ifdef HAVE_conditional_move
+      && ! can_conditionally_move_p (GET_MODE (src))
+#endif
+      && (num_sign_bit_copies (XEXP (XEXP (src, 0), 0),
+			       GET_MODE (XEXP (XEXP (src, 0), 0)))
+	  == GET_MODE_BITSIZE (GET_MODE (XEXP (XEXP (src, 0), 0))))
+      && ! side_effects_p (src))
+    {
+      rtx true_rtx = (GET_CODE (XEXP (src, 0)) == NE
+		      ? XEXP (src, 1) : XEXP (src, 2));
+      rtx false_rtx = (GET_CODE (XEXP (src, 0)) == NE
+		   ? XEXP (src, 2) : XEXP (src, 1));
+      rtx term1 = const0_rtx, term2, term3;
+
+      if (GET_CODE (true_rtx) == IOR
+	  && rtx_equal_p (XEXP (true_rtx, 0), false_rtx))
+	term1 = false_rtx, true_rtx = XEXP (true_rtx, 1), false_rtx = const0_rtx;
+      else if (GET_CODE (true_rtx) == IOR
+	       && rtx_equal_p (XEXP (true_rtx, 1), false_rtx))
+	term1 = false_rtx, true_rtx = XEXP (true_rtx, 0), false_rtx = const0_rtx;
+      else if (GET_CODE (false_rtx) == IOR
+	       && rtx_equal_p (XEXP (false_rtx, 0), true_rtx))
+	term1 = true_rtx, false_rtx = XEXP (false_rtx, 1), true_rtx = const0_rtx;
+      else if (GET_CODE (false_rtx) == IOR
+	       && rtx_equal_p (XEXP (false_rtx, 1), true_rtx))
+	term1 = true_rtx, false_rtx = XEXP (false_rtx, 0), true_rtx = const0_rtx;
+
+      term2 = gen_binary (AND, GET_MODE (src),
+			  XEXP (XEXP (src, 0), 0), true_rtx);
+      term3 = gen_binary (AND, GET_MODE (src),
+			  simplify_gen_unary (NOT, GET_MODE (src),
+					      XEXP (XEXP (src, 0), 0),
+					      GET_MODE (src)),
+			  false_rtx);
+
+      SUBST (SET_SRC (x),
+	     gen_binary (IOR, GET_MODE (src),
+			 gen_binary (IOR, GET_MODE (src), term1, term2),
+			 term3));
+
+      src = SET_SRC (x);
+    }
+
+  /* If either SRC or DEST is a CLOBBER of (const_int 0), make this
+     whole thing fail.  */
+  if (GET_CODE (src) == CLOBBER && XEXP (src, 0) == const0_rtx)
+    return src;
+  else if (GET_CODE (dest) == CLOBBER && XEXP (dest, 0) == const0_rtx)
+    return dest;
+  else
+    /* Convert this into a field assignment operation, if possible.  */
+    return make_field_assignment (x);
+}
+
+/* Simplify, X, and AND, IOR, or XOR operation, and return the simplified
+   result.  LAST is nonzero if this is the last retry.  */
+
+static rtx
+simplify_logical (rtx x, int last)
+{
+  enum machine_mode mode = GET_MODE (x);
+  rtx op0 = XEXP (x, 0);
+  rtx op1 = XEXP (x, 1);
+  rtx reversed;
+
+  switch (GET_CODE (x))
+    {
+    case AND:
+      /* Convert (A ^ B) & A to A & (~B) since the latter is often a single
+	 insn (and may simplify more).  */
+      if (GET_CODE (op0) == XOR
+	  && rtx_equal_p (XEXP (op0, 0), op1)
+	  && ! side_effects_p (op1))
+	x = gen_binary (AND, mode,
+			simplify_gen_unary (NOT, mode, XEXP (op0, 1), mode),
+			op1);
+
+      if (GET_CODE (op0) == XOR
+	  && rtx_equal_p (XEXP (op0, 1), op1)
+	  && ! side_effects_p (op1))
+	x = gen_binary (AND, mode,
+			simplify_gen_unary (NOT, mode, XEXP (op0, 0), mode),
+			op1);
+
+      /* Similarly for (~(A ^ B)) & A.  */
+      if (GET_CODE (op0) == NOT
+	  && GET_CODE (XEXP (op0, 0)) == XOR
+	  && rtx_equal_p (XEXP (XEXP (op0, 0), 0), op1)
+	  && ! side_effects_p (op1))
+	x = gen_binary (AND, mode, XEXP (XEXP (op0, 0), 1), op1);
+
+      if (GET_CODE (op0) == NOT
+	  && GET_CODE (XEXP (op0, 0)) == XOR
+	  && rtx_equal_p (XEXP (XEXP (op0, 0), 1), op1)
+	  && ! side_effects_p (op1))
+	x = gen_binary (AND, mode, XEXP (XEXP (op0, 0), 0), op1);
+
+      /* We can call simplify_and_const_int only if we don't lose
+	 any (sign) bits when converting INTVAL (op1) to
+	 "unsigned HOST_WIDE_INT".  */
+      if (GET_CODE (op1) == CONST_INT
+	  && (GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT
+	      || INTVAL (op1) > 0))
+	{
+	  x = simplify_and_const_int (x, mode, op0, INTVAL (op1));
+
+	  /* If we have (ior (and (X C1) C2)) and the next restart would be
+	     the last, simplify this by making C1 as small as possible
+	     and then exit.  */
+	  if (last
+	      && GET_CODE (x) == IOR && GET_CODE (op0) == AND
+	      && GET_CODE (XEXP (op0, 1)) == CONST_INT
+	      && GET_CODE (op1) == CONST_INT)
+	    return gen_binary (IOR, mode,
+			       gen_binary (AND, mode, XEXP (op0, 0),
+					   GEN_INT (INTVAL (XEXP (op0, 1))
+						    & ~INTVAL (op1))), op1);
+
+	  if (GET_CODE (x) != AND)
+	    return x;
+
+	  if (GET_RTX_CLASS (GET_CODE (x)) == 'c'
+	      || GET_RTX_CLASS (GET_CODE (x)) == '2')
+	    op0 = XEXP (x, 0), op1 = XEXP (x, 1);
+	}
+
+      /* Convert (A | B) & A to A.  */
+      if (GET_CODE (op0) == IOR
+	  && (rtx_equal_p (XEXP (op0, 0), op1)
+	      || rtx_equal_p (XEXP (op0, 1), op1))
+	  && ! side_effects_p (XEXP (op0, 0))
+	  && ! side_effects_p (XEXP (op0, 1)))
+	return op1;
+
+      /* In the following group of tests (and those in case IOR below),
+	 we start with some combination of logical operations and apply
+	 the distributive law followed by the inverse distributive law.
+	 Most of the time, this results in no change.  However, if some of
+	 the operands are the same or inverses of each other, simplifications
+	 will result.
+
+	 For example, (and (ior A B) (not B)) can occur as the result of
+	 expanding a bit field assignment.  When we apply the distributive
+	 law to this, we get (ior (and (A (not B))) (and (B (not B)))),
+	 which then simplifies to (and (A (not B))).
+
+	 If we have (and (ior A B) C), apply the distributive law and then
+	 the inverse distributive law to see if things simplify.  */
+
+      if (GET_CODE (op0) == IOR || GET_CODE (op0) == XOR)
+	{
+	  x = apply_distributive_law
+	    (gen_binary (GET_CODE (op0), mode,
+			 gen_binary (AND, mode, XEXP (op0, 0), op1),
+			 gen_binary (AND, mode, XEXP (op0, 1),
+				     copy_rtx (op1))));
+	  if (GET_CODE (x) != AND)
+	    return x;
+	}
+
+      if (GET_CODE (op1) == IOR || GET_CODE (op1) == XOR)
+	return apply_distributive_law
+	  (gen_binary (GET_CODE (op1), mode,
+		       gen_binary (AND, mode, XEXP (op1, 0), op0),
+		       gen_binary (AND, mode, XEXP (op1, 1),
+				   copy_rtx (op0))));
+
+      /* Similarly, taking advantage of the fact that
+	 (and (not A) (xor B C)) == (xor (ior A B) (ior A C))  */
+
+      if (GET_CODE (op0) == NOT && GET_CODE (op1) == XOR)
+	return apply_distributive_law
+	  (gen_binary (XOR, mode,
+		       gen_binary (IOR, mode, XEXP (op0, 0), XEXP (op1, 0)),
+		       gen_binary (IOR, mode, copy_rtx (XEXP (op0, 0)),
+				   XEXP (op1, 1))));
+
+      else if (GET_CODE (op1) == NOT && GET_CODE (op0) == XOR)
+	return apply_distributive_law
+	  (gen_binary (XOR, mode,
+		       gen_binary (IOR, mode, XEXP (op1, 0), XEXP (op0, 0)),
+		       gen_binary (IOR, mode, copy_rtx (XEXP (op1, 0)), XEXP (op0, 1))));
+      break;
+
+    case IOR:
+      /* (ior A C) is C if all bits of A that might be nonzero are on in C.  */
+      if (GET_CODE (op1) == CONST_INT
+	  && GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT
+	  && (nonzero_bits (op0, mode) & ~INTVAL (op1)) == 0)
+	return op1;
+
+      /* Convert (A & B) | A to A.  */
+      if (GET_CODE (op0) == AND
+	  && (rtx_equal_p (XEXP (op0, 0), op1)
+	      || rtx_equal_p (XEXP (op0, 1), op1))
+	  && ! side_effects_p (XEXP (op0, 0))
+	  && ! side_effects_p (XEXP (op0, 1)))
+	return op1;
+
+      /* If we have (ior (and A B) C), apply the distributive law and then
+	 the inverse distributive law to see if things simplify.  */
+
+      if (GET_CODE (op0) == AND)
+	{
+	  x = apply_distributive_law
+	    (gen_binary (AND, mode,
+			 gen_binary (IOR, mode, XEXP (op0, 0), op1),
+			 gen_binary (IOR, mode, XEXP (op0, 1),
+				     copy_rtx (op1))));
+
+	  if (GET_CODE (x) != IOR)
+	    return x;
+	}
+
+      if (GET_CODE (op1) == AND)
+	{
+	  x = apply_distributive_law
+	    (gen_binary (AND, mode,
+			 gen_binary (IOR, mode, XEXP (op1, 0), op0),
+			 gen_binary (IOR, mode, XEXP (op1, 1),
+				     copy_rtx (op0))));
+
+	  if (GET_CODE (x) != IOR)
+	    return x;
+	}
+
+      /* Convert (ior (ashift A CX) (lshiftrt A CY)) where CX+CY equals the
+	 mode size to (rotate A CX).  */
+
+      if (((GET_CODE (op0) == ASHIFT && GET_CODE (op1) == LSHIFTRT)
+	   || (GET_CODE (op1) == ASHIFT && GET_CODE (op0) == LSHIFTRT))
+	  && rtx_equal_p (XEXP (op0, 0), XEXP (op1, 0))
+	  && GET_CODE (XEXP (op0, 1)) == CONST_INT
+	  && GET_CODE (XEXP (op1, 1)) == CONST_INT
+	  && (INTVAL (XEXP (op0, 1)) + INTVAL (XEXP (op1, 1))
+	      == GET_MODE_BITSIZE (mode)))
+	return gen_rtx_ROTATE (mode, XEXP (op0, 0),
+			       (GET_CODE (op0) == ASHIFT
+				? XEXP (op0, 1) : XEXP (op1, 1)));
+
+      /* If OP0 is (ashiftrt (plus ...) C), it might actually be
+	 a (sign_extend (plus ...)).  If so, OP1 is a CONST_INT, and the PLUS
+	 does not affect any of the bits in OP1, it can really be done
+	 as a PLUS and we can associate.  We do this by seeing if OP1
+	 can be safely shifted left C bits.  */
+      if (GET_CODE (op1) == CONST_INT && GET_CODE (op0) == ASHIFTRT
+	  && GET_CODE (XEXP (op0, 0)) == PLUS
+	  && GET_CODE (XEXP (XEXP (op0, 0), 1)) == CONST_INT
+	  && GET_CODE (XEXP (op0, 1)) == CONST_INT
+	  && INTVAL (XEXP (op0, 1)) < HOST_BITS_PER_WIDE_INT)
+	{
+	  int count = INTVAL (XEXP (op0, 1));
+	  HOST_WIDE_INT mask = INTVAL (op1) << count;
+
+	  if (mask >> count == INTVAL (op1)
+	      && (mask & nonzero_bits (XEXP (op0, 0), mode)) == 0)
+	    {
+	      SUBST (XEXP (XEXP (op0, 0), 1),
+		     GEN_INT (INTVAL (XEXP (XEXP (op0, 0), 1)) | mask));
+	      return op0;
+	    }
+	}
+      break;
+
+    case XOR:
+      /* If we are XORing two things that have no bits in common,
+	 convert them into an IOR.  This helps to detect rotation encoded
+	 using those methods and possibly other simplifications.  */
+
+      if (GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT
+	  && (nonzero_bits (op0, mode)
+	      & nonzero_bits (op1, mode)) == 0)
+	return (gen_binary (IOR, mode, op0, op1));
+
+      /* Convert (XOR (NOT x) (NOT y)) to (XOR x y).
+	 Also convert (XOR (NOT x) y) to (NOT (XOR x y)), similarly for
+	 (NOT y).  */
+      {
+	int num_negated = 0;
+
+	if (GET_CODE (op0) == NOT)
+	  num_negated++, op0 = XEXP (op0, 0);
+	if (GET_CODE (op1) == NOT)
+	  num_negated++, op1 = XEXP (op1, 0);
+
+	if (num_negated == 2)
+	  {
+	    SUBST (XEXP (x, 0), op0);
+	    SUBST (XEXP (x, 1), op1);
+	  }
+	else if (num_negated == 1)
+	  return
+	    simplify_gen_unary (NOT, mode, gen_binary (XOR, mode, op0, op1),
+				mode);
+      }
+
+      /* Convert (xor (and A B) B) to (and (not A) B).  The latter may
+	 correspond to a machine insn or result in further simplifications
+	 if B is a constant.  */
+
+      if (GET_CODE (op0) == AND
+	  && rtx_equal_p (XEXP (op0, 1), op1)
+	  && ! side_effects_p (op1))
+	return gen_binary (AND, mode,
+			   simplify_gen_unary (NOT, mode, XEXP (op0, 0), mode),
+			   op1);
+
+      else if (GET_CODE (op0) == AND
+	       && rtx_equal_p (XEXP (op0, 0), op1)
+	       && ! side_effects_p (op1))
+	return gen_binary (AND, mode,
+			   simplify_gen_unary (NOT, mode, XEXP (op0, 1), mode),
+			   op1);
+
+      /* (xor (comparison foo bar) (const_int 1)) can become the reversed
+	 comparison if STORE_FLAG_VALUE is 1.  */
+      if (STORE_FLAG_VALUE == 1
+	  && op1 == const1_rtx
+	  && GET_RTX_CLASS (GET_CODE (op0)) == '<'
+	  && (reversed = reversed_comparison (op0, mode, XEXP (op0, 0),
+					      XEXP (op0, 1))))
+	return reversed;
+
+      /* (lshiftrt foo C) where C is the number of bits in FOO minus 1
+	 is (lt foo (const_int 0)), so we can perform the above
+	 simplification if STORE_FLAG_VALUE is 1.  */
+
+      if (STORE_FLAG_VALUE == 1
+	  && op1 == const1_rtx
+	  && GET_CODE (op0) == LSHIFTRT
+	  && GET_CODE (XEXP (op0, 1)) == CONST_INT
+	  && INTVAL (XEXP (op0, 1)) == GET_MODE_BITSIZE (mode) - 1)
+	return gen_rtx_GE (mode, XEXP (op0, 0), const0_rtx);
+
+      /* (xor (comparison foo bar) (const_int sign-bit))
+	 when STORE_FLAG_VALUE is the sign bit.  */
+      if (GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT
+	  && ((STORE_FLAG_VALUE & GET_MODE_MASK (mode))
+	      == (unsigned HOST_WIDE_INT) 1 << (GET_MODE_BITSIZE (mode) - 1))
+	  && op1 == const_true_rtx
+	  && GET_RTX_CLASS (GET_CODE (op0)) == '<'
+	  && (reversed = reversed_comparison (op0, mode, XEXP (op0, 0),
+					      XEXP (op0, 1))))
+	return reversed;
+
+      break;
+
+    default:
+      abort ();
+    }
+
+  return x;
+}
+
+/* We consider ZERO_EXTRACT, SIGN_EXTRACT, and SIGN_EXTEND as "compound
+   operations" because they can be replaced with two more basic operations.
+   ZERO_EXTEND is also considered "compound" because it can be replaced with
+   an AND operation, which is simpler, though only one operation.
+
+   The function expand_compound_operation is called with an rtx expression
+   and will convert it to the appropriate shifts and AND operations,
+   simplifying at each stage.
+
+   The function make_compound_operation is called to convert an expression
+   consisting of shifts and ANDs into the equivalent compound expression.
+   It is the inverse of this function, loosely speaking.  */
+
+static rtx
+expand_compound_operation (rtx x)
+{
+  unsigned HOST_WIDE_INT pos = 0, len;
+  int unsignedp = 0;
+  unsigned int modewidth;
+  rtx tem;
+
+  switch (GET_CODE (x))
+    {
+    case ZERO_EXTEND:
+      unsignedp = 1;
+    case SIGN_EXTEND:
+      /* We can't necessarily use a const_int for a multiword mode;
+	 it depends on implicitly extending the value.
+	 Since we don't know the right way to extend it,
+	 we can't tell whether the implicit way is right.
+
+	 Even for a mode that is no wider than a const_int,
+	 we can't win, because we need to sign extend one of its bits through
+	 the rest of it, and we don't know which bit.  */
+      if (GET_CODE (XEXP (x, 0)) == CONST_INT)
+	return x;
+
+      /* Return if (subreg:MODE FROM 0) is not a safe replacement for
+	 (zero_extend:MODE FROM) or (sign_extend:MODE FROM).  It is for any MEM
+	 because (SUBREG (MEM...)) is guaranteed to cause the MEM to be
+	 reloaded. If not for that, MEM's would very rarely be safe.
+
+	 Reject MODEs bigger than a word, because we might not be able
+	 to reference a two-register group starting with an arbitrary register
+	 (and currently gen_lowpart might crash for a SUBREG).  */
+
+      if (GET_MODE_SIZE (GET_MODE (XEXP (x, 0))) > UNITS_PER_WORD)
+	return x;
+
+      /* Reject MODEs that aren't scalar integers because turning vector
+	 or complex modes into shifts causes problems.  */
+
+      if (! SCALAR_INT_MODE_P (GET_MODE (XEXP (x, 0))))
+	return x;
+
+      len = GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0)));
+      /* If the inner object has VOIDmode (the only way this can happen
+	 is if it is an ASM_OPERANDS), we can't do anything since we don't
+	 know how much masking to do.  */
+      if (len == 0)
+	return x;
+
+      break;
+
+    case ZERO_EXTRACT:
+      unsignedp = 1;
+    case SIGN_EXTRACT:
+      /* If the operand is a CLOBBER, just return it.  */
+      if (GET_CODE (XEXP (x, 0)) == CLOBBER)
+	return XEXP (x, 0);
+
+      if (GET_CODE (XEXP (x, 1)) != CONST_INT
+	  || GET_CODE (XEXP (x, 2)) != CONST_INT
+	  || GET_MODE (XEXP (x, 0)) == VOIDmode)
+	return x;
+
+      /* Reject MODEs that aren't scalar integers because turning vector
+	 or complex modes into shifts causes problems.  */
+
+      if (! SCALAR_INT_MODE_P (GET_MODE (XEXP (x, 0))))
+	return x;
+
+      len = INTVAL (XEXP (x, 1));
+      pos = INTVAL (XEXP (x, 2));
+
+      /* If this goes outside the object being extracted, replace the object
+	 with a (use (mem ...)) construct that only combine understands
+	 and is used only for this purpose.  */
+      if (len + pos > GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0))))
+	SUBST (XEXP (x, 0), gen_rtx_USE (GET_MODE (x), XEXP (x, 0)));
+
+      if (BITS_BIG_ENDIAN)
+	pos = GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0))) - len - pos;
+
+      break;
+
+    default:
+      return x;
+    }
+  /* Convert sign extension to zero extension, if we know that the high
+     bit is not set, as this is easier to optimize.  It will be converted
+     back to cheaper alternative in make_extraction.  */
+  if (GET_CODE (x) == SIGN_EXTEND
+      && (GET_MODE_BITSIZE (GET_MODE (x)) <= HOST_BITS_PER_WIDE_INT
+	  && ((nonzero_bits (XEXP (x, 0), GET_MODE (XEXP (x, 0)))
+		& ~(((unsigned HOST_WIDE_INT)
+		      GET_MODE_MASK (GET_MODE (XEXP (x, 0))))
+		     >> 1))
+	       == 0)))
+    {
+      rtx temp = gen_rtx_ZERO_EXTEND (GET_MODE (x), XEXP (x, 0));
+      rtx temp2 = expand_compound_operation (temp);
+
+      /* Make sure this is a profitable operation.  */
+      if (rtx_cost (x, SET) > rtx_cost (temp2, SET))
+       return temp2;
+      else if (rtx_cost (x, SET) > rtx_cost (temp, SET))
+       return temp;
+      else
+       return x;
+    }
+
+  /* We can optimize some special cases of ZERO_EXTEND.  */
+  if (GET_CODE (x) == ZERO_EXTEND)
+    {
+      /* (zero_extend:DI (truncate:SI foo:DI)) is just foo:DI if we
+         know that the last value didn't have any inappropriate bits
+         set.  */
+      if (GET_CODE (XEXP (x, 0)) == TRUNCATE
+	  && GET_MODE (XEXP (XEXP (x, 0), 0)) == GET_MODE (x)
+	  && GET_MODE_BITSIZE (GET_MODE (x)) <= HOST_BITS_PER_WIDE_INT
+	  && (nonzero_bits (XEXP (XEXP (x, 0), 0), GET_MODE (x))
+	      & ~GET_MODE_MASK (GET_MODE (XEXP (x, 0)))) == 0)
+	return XEXP (XEXP (x, 0), 0);
+
+      /* Likewise for (zero_extend:DI (subreg:SI foo:DI 0)).  */
+      if (GET_CODE (XEXP (x, 0)) == SUBREG
+	  && GET_MODE (SUBREG_REG (XEXP (x, 0))) == GET_MODE (x)
+	  && subreg_lowpart_p (XEXP (x, 0))
+	  && GET_MODE_BITSIZE (GET_MODE (x)) <= HOST_BITS_PER_WIDE_INT
+	  && (nonzero_bits (SUBREG_REG (XEXP (x, 0)), GET_MODE (x))
+	      & ~GET_MODE_MASK (GET_MODE (XEXP (x, 0)))) == 0)
+	return SUBREG_REG (XEXP (x, 0));
+
+      /* (zero_extend:DI (truncate:SI foo:DI)) is just foo:DI when foo
+         is a comparison and STORE_FLAG_VALUE permits.  This is like
+         the first case, but it works even when GET_MODE (x) is larger
+         than HOST_WIDE_INT.  */
+      if (GET_CODE (XEXP (x, 0)) == TRUNCATE
+	  && GET_MODE (XEXP (XEXP (x, 0), 0)) == GET_MODE (x)
+	  && GET_RTX_CLASS (GET_CODE (XEXP (XEXP (x, 0), 0))) == '<'
+	  && (GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0)))
+	      <= HOST_BITS_PER_WIDE_INT)
+	  && ((HOST_WIDE_INT) STORE_FLAG_VALUE
+	      & ~GET_MODE_MASK (GET_MODE (XEXP (x, 0)))) == 0)
+	return XEXP (XEXP (x, 0), 0);
+
+      /* Likewise for (zero_extend:DI (subreg:SI foo:DI 0)).  */
+      if (GET_CODE (XEXP (x, 0)) == SUBREG
+	  && GET_MODE (SUBREG_REG (XEXP (x, 0))) == GET_MODE (x)
+	  && subreg_lowpart_p (XEXP (x, 0))
+	  && GET_RTX_CLASS (GET_CODE (SUBREG_REG (XEXP (x, 0)))) == '<'
+	  && (GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0)))
+	      <= HOST_BITS_PER_WIDE_INT)
+	  && ((HOST_WIDE_INT) STORE_FLAG_VALUE
+	      & ~GET_MODE_MASK (GET_MODE (XEXP (x, 0)))) == 0)
+	return SUBREG_REG (XEXP (x, 0));
+
+    }
+
+  /* If we reach here, we want to return a pair of shifts.  The inner
+     shift is a left shift of BITSIZE - POS - LEN bits.  The outer
+     shift is a right shift of BITSIZE - LEN bits.  It is arithmetic or
+     logical depending on the value of UNSIGNEDP.
+
+     If this was a ZERO_EXTEND or ZERO_EXTRACT, this pair of shifts will be
+     converted into an AND of a shift.
+
+     We must check for the case where the left shift would have a negative
+     count.  This can happen in a case like (x >> 31) & 255 on machines
+     that can't shift by a constant.  On those machines, we would first
+     combine the shift with the AND to produce a variable-position
+     extraction.  Then the constant of 31 would be substituted in to produce
+     a such a position.  */
+
+  modewidth = GET_MODE_BITSIZE (GET_MODE (x));
+  if (modewidth + len >= pos)
+    tem = simplify_shift_const (NULL_RTX, unsignedp ? LSHIFTRT : ASHIFTRT,
+				GET_MODE (x),
+				simplify_shift_const (NULL_RTX, ASHIFT,
+						      GET_MODE (x),
+						      XEXP (x, 0),
+						      modewidth - pos - len),
+				modewidth - len);
+
+  else if (unsignedp && len < HOST_BITS_PER_WIDE_INT)
+    tem = simplify_and_const_int (NULL_RTX, GET_MODE (x),
+				  simplify_shift_const (NULL_RTX, LSHIFTRT,
+							GET_MODE (x),
+							XEXP (x, 0), pos),
+				  ((HOST_WIDE_INT) 1 << len) - 1);
+  else
+    /* Any other cases we can't handle.  */
+    return x;
+
+  /* If we couldn't do this for some reason, return the original
+     expression.  */
+  if (GET_CODE (tem) == CLOBBER)
+    return x;
+
+  return tem;
+}
+
+/* X is a SET which contains an assignment of one object into
+   a part of another (such as a bit-field assignment, STRICT_LOW_PART,
+   or certain SUBREGS). If possible, convert it into a series of
+   logical operations.
+
+   We half-heartedly support variable positions, but do not at all
+   support variable lengths.  */
+
+static rtx
+expand_field_assignment (rtx x)
+{
+  rtx inner;
+  rtx pos;			/* Always counts from low bit.  */
+  int len;
+  rtx mask;
+  enum machine_mode compute_mode;
+
+  /* Loop until we find something we can't simplify.  */
+  while (1)
+    {
+      if (GET_CODE (SET_DEST (x)) == STRICT_LOW_PART
+	  && GET_CODE (XEXP (SET_DEST (x), 0)) == SUBREG)
+	{
+	  inner = SUBREG_REG (XEXP (SET_DEST (x), 0));
+	  len = GET_MODE_BITSIZE (GET_MODE (XEXP (SET_DEST (x), 0)));
+	  pos = GEN_INT (subreg_lsb (XEXP (SET_DEST (x), 0)));
+	}
+      else if (GET_CODE (SET_DEST (x)) == ZERO_EXTRACT
+	       && GET_CODE (XEXP (SET_DEST (x), 1)) == CONST_INT)
+	{
+	  inner = XEXP (SET_DEST (x), 0);
+	  len = INTVAL (XEXP (SET_DEST (x), 1));
+	  pos = XEXP (SET_DEST (x), 2);
+
+	  /* If the position is constant and spans the width of INNER,
+	     surround INNER  with a USE to indicate this.  */
+	  if (GET_CODE (pos) == CONST_INT
+	      && INTVAL (pos) + len > GET_MODE_BITSIZE (GET_MODE (inner)))
+	    inner = gen_rtx_USE (GET_MODE (SET_DEST (x)), inner);
+
+	  if (BITS_BIG_ENDIAN)
+	    {
+	      if (GET_CODE (pos) == CONST_INT)
+		pos = GEN_INT (GET_MODE_BITSIZE (GET_MODE (inner)) - len
+			       - INTVAL (pos));
+	      else if (GET_CODE (pos) == MINUS
+		       && GET_CODE (XEXP (pos, 1)) == CONST_INT
+		       && (INTVAL (XEXP (pos, 1))
+			   == GET_MODE_BITSIZE (GET_MODE (inner)) - len))
+		/* If position is ADJUST - X, new position is X.  */
+		pos = XEXP (pos, 0);
+	      else
+		pos = gen_binary (MINUS, GET_MODE (pos),
+				  GEN_INT (GET_MODE_BITSIZE (GET_MODE (inner))
+					   - len),
+				  pos);
+	    }
+	}
+
+      /* A SUBREG between two modes that occupy the same numbers of words
+	 can be done by moving the SUBREG to the source.  */
+      else if (GET_CODE (SET_DEST (x)) == SUBREG
+	       /* We need SUBREGs to compute nonzero_bits properly.  */
+	       && nonzero_sign_valid
+	       && (((GET_MODE_SIZE (GET_MODE (SET_DEST (x)))
+		     + (UNITS_PER_WORD - 1)) / UNITS_PER_WORD)
+		   == ((GET_MODE_SIZE (GET_MODE (SUBREG_REG (SET_DEST (x))))
+			+ (UNITS_PER_WORD - 1)) / UNITS_PER_WORD)))
+	{
+	  x = gen_rtx_SET (VOIDmode, SUBREG_REG (SET_DEST (x)),
+			   gen_lowpart_for_combine
+			   (GET_MODE (SUBREG_REG (SET_DEST (x))),
+			    SET_SRC (x)));
+	  continue;
+	}
+      else
+	break;
+
+      while (GET_CODE (inner) == SUBREG && subreg_lowpart_p (inner))
+	inner = SUBREG_REG (inner);
+
+      compute_mode = GET_MODE (inner);
+
+      /* Don't attempt bitwise arithmetic on non scalar integer modes.  */
+      if (! SCALAR_INT_MODE_P (compute_mode))
+	{
+	  enum machine_mode imode;
+
+	  /* Don't do anything for vector or complex integral types.  */
+	  if (! FLOAT_MODE_P (compute_mode))
+	    break;
+
+	  /* Try to find an integral mode to pun with.  */
+	  imode = mode_for_size (GET_MODE_BITSIZE (compute_mode), MODE_INT, 0);
+	  if (imode == BLKmode)
+	    break;
+
+	  compute_mode = imode;
+	  inner = gen_lowpart_for_combine (imode, inner);
+	}
+
+      /* Compute a mask of LEN bits, if we can do this on the host machine.  */
+      if (len < HOST_BITS_PER_WIDE_INT)
+	mask = GEN_INT (((HOST_WIDE_INT) 1 << len) - 1);
+      else
+	break;
+
+      /* Now compute the equivalent expression.  Make a copy of INNER
+	 for the SET_DEST in case it is a MEM into which we will substitute;
+	 we don't want shared RTL in that case.  */
+      x = gen_rtx_SET
+	(VOIDmode, copy_rtx (inner),
+	 gen_binary (IOR, compute_mode,
+		     gen_binary (AND, compute_mode,
+				 simplify_gen_unary (NOT, compute_mode,
+						     gen_binary (ASHIFT,
+								 compute_mode,
+								 mask, pos),
+						     compute_mode),
+				 inner),
+		     gen_binary (ASHIFT, compute_mode,
+				 gen_binary (AND, compute_mode,
+					     gen_lowpart_for_combine
+					     (compute_mode, SET_SRC (x)),
+					     mask),
+				 pos)));
+    }
+
+  return x;
+}
+
+/* Return an RTX for a reference to LEN bits of INNER.  If POS_RTX is nonzero,
+   it is an RTX that represents a variable starting position; otherwise,
+   POS is the (constant) starting bit position (counted from the LSB).
+
+   INNER may be a USE.  This will occur when we started with a bitfield
+   that went outside the boundary of the object in memory, which is
+   allowed on most machines.  To isolate this case, we produce a USE
+   whose mode is wide enough and surround the MEM with it.  The only
+   code that understands the USE is this routine.  If it is not removed,
+   it will cause the resulting insn not to match.
+
+   UNSIGNEDP is nonzero for an unsigned reference and zero for a
+   signed reference.
+
+   IN_DEST is nonzero if this is a reference in the destination of a
+   SET.  This is used when a ZERO_ or SIGN_EXTRACT isn't needed.  If nonzero,
+   a STRICT_LOW_PART will be used, if zero, ZERO_EXTEND or SIGN_EXTEND will
+   be used.
+
+   IN_COMPARE is nonzero if we are in a COMPARE.  This means that a
+   ZERO_EXTRACT should be built even for bits starting at bit 0.
+
+   MODE is the desired mode of the result (if IN_DEST == 0).
+
+   The result is an RTX for the extraction or NULL_RTX if the target
+   can't handle it.  */
+
+static rtx
+make_extraction (enum machine_mode mode, rtx inner, HOST_WIDE_INT pos,
+		 rtx pos_rtx, unsigned HOST_WIDE_INT len, int unsignedp,
+		 int in_dest, int in_compare)
+{
+  /* This mode describes the size of the storage area
+     to fetch the overall value from.  Within that, we
+     ignore the POS lowest bits, etc.  */
+  enum machine_mode is_mode = GET_MODE (inner);
+  enum machine_mode inner_mode;
+  enum machine_mode wanted_inner_mode = byte_mode;
+  enum machine_mode wanted_inner_reg_mode = word_mode;
+  enum machine_mode pos_mode = word_mode;
+  enum machine_mode extraction_mode = word_mode;
+  enum machine_mode tmode = mode_for_size (len, MODE_INT, 1);
+  int spans_byte = 0;
+  rtx new = 0;
+  rtx orig_pos_rtx = pos_rtx;
+  HOST_WIDE_INT orig_pos;
+
+  /* Get some information about INNER and get the innermost object.  */
+  if (GET_CODE (inner) == USE)
+    /* (use:SI (mem:QI foo)) stands for (mem:SI foo).  */
+    /* We don't need to adjust the position because we set up the USE
+       to pretend that it was a full-word object.  */
+    spans_byte = 1, inner = XEXP (inner, 0);
+  else if (GET_CODE (inner) == SUBREG && subreg_lowpart_p (inner))
+    {
+      /* If going from (subreg:SI (mem:QI ...)) to (mem:QI ...),
+	 consider just the QI as the memory to extract from.
+	 The subreg adds or removes high bits; its mode is
+	 irrelevant to the meaning of this extraction,
+	 since POS and LEN count from the lsb.  */
+      if (GET_CODE (SUBREG_REG (inner)) == MEM)
+	is_mode = GET_MODE (SUBREG_REG (inner));
+      inner = SUBREG_REG (inner);
+    }
+  else if (GET_CODE (inner) == ASHIFT
+	   && GET_CODE (XEXP (inner, 1)) == CONST_INT
+	   && pos_rtx == 0 && pos == 0
+	   && len > (unsigned HOST_WIDE_INT) INTVAL (XEXP (inner, 1)))
+    {
+      /* We're extracting the least significant bits of an rtx
+	 (ashift X (const_int C)), where LEN > C.  Extract the
+	 least significant (LEN - C) bits of X, giving an rtx
+	 whose mode is MODE, then shift it left C times.  */
+      new = make_extraction (mode, XEXP (inner, 0),
+			     0, 0, len - INTVAL (XEXP (inner, 1)),
+			     unsignedp, in_dest, in_compare);
+      if (new != 0)
+	return gen_rtx_ASHIFT (mode, new, XEXP (inner, 1));
+    }
+
+  inner_mode = GET_MODE (inner);
+
+  if (pos_rtx && GET_CODE (pos_rtx) == CONST_INT)
+    pos = INTVAL (pos_rtx), pos_rtx = 0;
+
+  /* See if this can be done without an extraction.  We never can if the
+     width of the field is not the same as that of some integer mode. For
+     registers, we can only avoid the extraction if the position is at the
+     low-order bit and this is either not in the destination or we have the
+     appropriate STRICT_LOW_PART operation available.
+
+     For MEM, we can avoid an extract if the field starts on an appropriate
+     boundary and we can change the mode of the memory reference.  However,
+     we cannot directly access the MEM if we have a USE and the underlying
+     MEM is not TMODE.  This combination means that MEM was being used in a
+     context where bits outside its mode were being referenced; that is only
+     valid in bit-field insns.  */
+
+  if (tmode != BLKmode
+      && ! (spans_byte && inner_mode != tmode)
+      && ((pos_rtx == 0 && (pos % BITS_PER_WORD) == 0
+	   && GET_CODE (inner) != MEM
+	   && (! in_dest
+	       || (GET_CODE (inner) == REG
+		   && have_insn_for (STRICT_LOW_PART, tmode))))
+	  || (GET_CODE (inner) == MEM && pos_rtx == 0
+	      && (pos
+		  % (STRICT_ALIGNMENT ? GET_MODE_ALIGNMENT (tmode)
+		     : BITS_PER_UNIT)) == 0
+	      /* We can't do this if we are widening INNER_MODE (it
+		 may not be aligned, for one thing).  */
+	      && GET_MODE_BITSIZE (inner_mode) >= GET_MODE_BITSIZE (tmode)
+	      && (inner_mode == tmode
+		  || (! mode_dependent_address_p (XEXP (inner, 0))
+		      && ! MEM_VOLATILE_P (inner))))))
+    {
+      /* If INNER is a MEM, make a new MEM that encompasses just the desired
+	 field.  If the original and current mode are the same, we need not
+	 adjust the offset.  Otherwise, we do if bytes big endian.
+
+	 If INNER is not a MEM, get a piece consisting of just the field
+	 of interest (in this case POS % BITS_PER_WORD must be 0).  */
+
+      if (GET_CODE (inner) == MEM)
+	{
+	  HOST_WIDE_INT offset;
+
+	  /* POS counts from lsb, but make OFFSET count in memory order.  */
+	  if (BYTES_BIG_ENDIAN)
+	    offset = (GET_MODE_BITSIZE (is_mode) - len - pos) / BITS_PER_UNIT;
+	  else
+	    offset = pos / BITS_PER_UNIT;
+
+	  new = adjust_address_nv (inner, tmode, offset);
+	}
+      else if (GET_CODE (inner) == REG)
+	{
+	  if (tmode != inner_mode)
+	    {
+	      /* We can't call gen_lowpart_for_combine in a DEST since we
+		 always want a SUBREG (see below) and it would sometimes
+		 return a new hard register.  */
+	      if (pos || in_dest)
+		{
+		  HOST_WIDE_INT final_word = pos / BITS_PER_WORD;
+
+		  if (WORDS_BIG_ENDIAN
+		      && GET_MODE_SIZE (inner_mode) > UNITS_PER_WORD)
+		    final_word = ((GET_MODE_SIZE (inner_mode)
+				   - GET_MODE_SIZE (tmode))
+				  / UNITS_PER_WORD) - final_word;
+
+		  final_word *= UNITS_PER_WORD;
+		  if (BYTES_BIG_ENDIAN &&
+		      GET_MODE_SIZE (inner_mode) > GET_MODE_SIZE (tmode))
+		    final_word += (GET_MODE_SIZE (inner_mode)
+				   - GET_MODE_SIZE (tmode)) % UNITS_PER_WORD;
+
+		  /* Avoid creating invalid subregs, for example when
+		     simplifying (x>>32)&255.  */
+		  if (final_word >= GET_MODE_SIZE (inner_mode))
+		    return NULL_RTX;
+
+		  new = gen_rtx_SUBREG (tmode, inner, final_word);
+		}
+	      else
+		new = gen_lowpart_for_combine (tmode, inner);
+	    }
+	  else
+	    new = inner;
+	}
+      else
+	new = force_to_mode (inner, tmode,
+			     len >= HOST_BITS_PER_WIDE_INT
+			     ? ~(unsigned HOST_WIDE_INT) 0
+			     : ((unsigned HOST_WIDE_INT) 1 << len) - 1,
+			     NULL_RTX, 0);
+
+      /* If this extraction is going into the destination of a SET,
+	 make a STRICT_LOW_PART unless we made a MEM.  */
+
+      if (in_dest)
+	return (GET_CODE (new) == MEM ? new
+		: (GET_CODE (new) != SUBREG
+		   ? gen_rtx_CLOBBER (tmode, const0_rtx)
+		   : gen_rtx_STRICT_LOW_PART (VOIDmode, new)));
+
+      if (mode == tmode)
+	return new;
+
+      if (GET_CODE (new) == CONST_INT)
+	return gen_int_mode (INTVAL (new), mode);
+
+      /* If we know that no extraneous bits are set, and that the high
+	 bit is not set, convert the extraction to the cheaper of
+	 sign and zero extension, that are equivalent in these cases.  */
+      if (flag_expensive_optimizations
+	  && (GET_MODE_BITSIZE (tmode) <= HOST_BITS_PER_WIDE_INT
+	      && ((nonzero_bits (new, tmode)
+		   & ~(((unsigned HOST_WIDE_INT)
+			GET_MODE_MASK (tmode))
+		       >> 1))
+		  == 0)))
+	{
+	  rtx temp = gen_rtx_ZERO_EXTEND (mode, new);
+	  rtx temp1 = gen_rtx_SIGN_EXTEND (mode, new);
+
+	  /* Prefer ZERO_EXTENSION, since it gives more information to
+	     backends.  */
+	  if (rtx_cost (temp, SET) <= rtx_cost (temp1, SET))
+	    return temp;
+	  return temp1;
+	}
+
+      /* Otherwise, sign- or zero-extend unless we already are in the
+	 proper mode.  */
+
+      return (gen_rtx_fmt_e (unsignedp ? ZERO_EXTEND : SIGN_EXTEND,
+			     mode, new));
+    }
+
+  /* Unless this is a COMPARE or we have a funny memory reference,
+     don't do anything with zero-extending field extracts starting at
+     the low-order bit since they are simple AND operations.  */
+  if (pos_rtx == 0 && pos == 0 && ! in_dest
+      && ! in_compare && ! spans_byte && unsignedp)
+    return 0;
+
+  /* Unless we are allowed to span bytes or INNER is not MEM, reject this if
+     we would be spanning bytes or if the position is not a constant and the
+     length is not 1.  In all other cases, we would only be going outside
+     our object in cases when an original shift would have been
+     undefined.  */
+  if (! spans_byte && GET_CODE (inner) == MEM
+      && ((pos_rtx == 0 && pos + len > GET_MODE_BITSIZE (is_mode))
+	  || (pos_rtx != 0 && len != 1)))
+    return 0;
+
+  /* Get the mode to use should INNER not be a MEM, the mode for the position,
+     and the mode for the result.  */
+  if (in_dest && mode_for_extraction (EP_insv, -1) != MAX_MACHINE_MODE)
+    {
+      wanted_inner_reg_mode = mode_for_extraction (EP_insv, 0);
+      pos_mode = mode_for_extraction (EP_insv, 2);
+      extraction_mode = mode_for_extraction (EP_insv, 3);
+    }
+
+  if (! in_dest && unsignedp
+      && mode_for_extraction (EP_extzv, -1) != MAX_MACHINE_MODE)
+    {
+      wanted_inner_reg_mode = mode_for_extraction (EP_extzv, 1);
+      pos_mode = mode_for_extraction (EP_extzv, 3);
+      extraction_mode = mode_for_extraction (EP_extzv, 0);
+    }
+
+  if (! in_dest && ! unsignedp
+      && mode_for_extraction (EP_extv, -1) != MAX_MACHINE_MODE)
+    {
+      wanted_inner_reg_mode = mode_for_extraction (EP_extv, 1);
+      pos_mode = mode_for_extraction (EP_extv, 3);
+      extraction_mode = mode_for_extraction (EP_extv, 0);
+    }
+
+  /* Never narrow an object, since that might not be safe.  */
+
+  if (mode != VOIDmode
+      && GET_MODE_SIZE (extraction_mode) < GET_MODE_SIZE (mode))
+    extraction_mode = mode;
+
+  if (pos_rtx && GET_MODE (pos_rtx) != VOIDmode
+      && GET_MODE_SIZE (pos_mode) < GET_MODE_SIZE (GET_MODE (pos_rtx)))
+    pos_mode = GET_MODE (pos_rtx);
+
+  /* If this is not from memory, the desired mode is wanted_inner_reg_mode;
+     if we have to change the mode of memory and cannot, the desired mode is
+     EXTRACTION_MODE.  */
+  if (GET_CODE (inner) != MEM)
+    wanted_inner_mode = wanted_inner_reg_mode;
+  else if (inner_mode != wanted_inner_mode
+	   && (mode_dependent_address_p (XEXP (inner, 0))
+	       || MEM_VOLATILE_P (inner)))
+    wanted_inner_mode = extraction_mode;
+
+  orig_pos = pos;
+
+  if (BITS_BIG_ENDIAN)
+    {
+      /* POS is passed as if BITS_BIG_ENDIAN == 0, so we need to convert it to
+	 BITS_BIG_ENDIAN style.  If position is constant, compute new
+	 position.  Otherwise, build subtraction.
+	 Note that POS is relative to the mode of the original argument.
+	 If it's a MEM we need to recompute POS relative to that.
+	 However, if we're extracting from (or inserting into) a register,
+	 we want to recompute POS relative to wanted_inner_mode.  */
+      int width = (GET_CODE (inner) == MEM
+		   ? GET_MODE_BITSIZE (is_mode)
+		   : GET_MODE_BITSIZE (wanted_inner_mode));
+
+      if (pos_rtx == 0)
+	pos = width - len - pos;
+      else
+	pos_rtx
+	  = gen_rtx_MINUS (GET_MODE (pos_rtx), GEN_INT (width - len), pos_rtx);
+      /* POS may be less than 0 now, but we check for that below.
+	 Note that it can only be less than 0 if GET_CODE (inner) != MEM.  */
+    }
+
+  /* If INNER has a wider mode, make it smaller.  If this is a constant
+     extract, try to adjust the byte to point to the byte containing
+     the value.  */
+  if (wanted_inner_mode != VOIDmode
+      && GET_MODE_SIZE (wanted_inner_mode) < GET_MODE_SIZE (is_mode)
+      && ((GET_CODE (inner) == MEM
+	   && (inner_mode == wanted_inner_mode
+	       || (! mode_dependent_address_p (XEXP (inner, 0))
+		   && ! MEM_VOLATILE_P (inner))))))
+    {
+      int offset = 0;
+
+      /* The computations below will be correct if the machine is big
+	 endian in both bits and bytes or little endian in bits and bytes.
+	 If it is mixed, we must adjust.  */
+
+      /* If bytes are big endian and we had a paradoxical SUBREG, we must
+	 adjust OFFSET to compensate.  */
+      if (BYTES_BIG_ENDIAN
+	  && ! spans_byte
+	  && GET_MODE_SIZE (inner_mode) < GET_MODE_SIZE (is_mode))
+	offset -= GET_MODE_SIZE (is_mode) - GET_MODE_SIZE (inner_mode);
+
+      /* If this is a constant position, we can move to the desired byte.  */
+      if (pos_rtx == 0)
+	{
+	  offset += pos / BITS_PER_UNIT;
+	  pos %= GET_MODE_BITSIZE (wanted_inner_mode);
+	}
+
+      if (BYTES_BIG_ENDIAN != BITS_BIG_ENDIAN
+	  && ! spans_byte
+	  && is_mode != wanted_inner_mode)
+	offset = (GET_MODE_SIZE (is_mode)
+		  - GET_MODE_SIZE (wanted_inner_mode) - offset);
+
+      if (offset != 0 || inner_mode != wanted_inner_mode)
+	inner = adjust_address_nv (inner, wanted_inner_mode, offset);
+    }
+
+  /* If INNER is not memory, we can always get it into the proper mode.  If we
+     are changing its mode, POS must be a constant and smaller than the size
+     of the new mode.  */
+  else if (GET_CODE (inner) != MEM)
+    {
+      if (GET_MODE (inner) != wanted_inner_mode
+	  && (pos_rtx != 0
+	      || orig_pos + len > GET_MODE_BITSIZE (wanted_inner_mode)))
+	return 0;
+
+      inner = force_to_mode (inner, wanted_inner_mode,
+			     pos_rtx
+			     || len + orig_pos >= HOST_BITS_PER_WIDE_INT
+			     ? ~(unsigned HOST_WIDE_INT) 0
+			     : ((((unsigned HOST_WIDE_INT) 1 << len) - 1)
+				<< orig_pos),
+			     NULL_RTX, 0);
+    }
+
+  /* Adjust mode of POS_RTX, if needed.  If we want a wider mode, we
+     have to zero extend.  Otherwise, we can just use a SUBREG.  */
+  if (pos_rtx != 0
+      && GET_MODE_SIZE (pos_mode) > GET_MODE_SIZE (GET_MODE (pos_rtx)))
+    {
+      rtx temp = gen_rtx_ZERO_EXTEND (pos_mode, pos_rtx);
+
+      /* If we know that no extraneous bits are set, and that the high
+	 bit is not set, convert extraction to cheaper one - either
+	 SIGN_EXTENSION or ZERO_EXTENSION, that are equivalent in these
+	 cases.  */
+      if (flag_expensive_optimizations
+	  && (GET_MODE_BITSIZE (GET_MODE (pos_rtx)) <= HOST_BITS_PER_WIDE_INT
+	      && ((nonzero_bits (pos_rtx, GET_MODE (pos_rtx))
+		   & ~(((unsigned HOST_WIDE_INT)
+			GET_MODE_MASK (GET_MODE (pos_rtx)))
+		       >> 1))
+		  == 0)))
+	{
+	  rtx temp1 = gen_rtx_SIGN_EXTEND (pos_mode, pos_rtx);
+
+	  /* Prefer ZERO_EXTENSION, since it gives more information to
+	     backends.  */
+	  if (rtx_cost (temp1, SET) < rtx_cost (temp, SET))
+	    temp = temp1;
+	}
+      pos_rtx = temp;
+    }
+  else if (pos_rtx != 0
+	   && GET_MODE_SIZE (pos_mode) < GET_MODE_SIZE (GET_MODE (pos_rtx)))
+    pos_rtx = gen_lowpart_for_combine (pos_mode, pos_rtx);
+
+  /* Make POS_RTX unless we already have it and it is correct.  If we don't
+     have a POS_RTX but we do have an ORIG_POS_RTX, the latter must
+     be a CONST_INT.  */
+  if (pos_rtx == 0 && orig_pos_rtx != 0 && INTVAL (orig_pos_rtx) == pos)
+    pos_rtx = orig_pos_rtx;
+
+  else if (pos_rtx == 0)
+    pos_rtx = GEN_INT (pos);
+
+  /* Make the required operation.  See if we can use existing rtx.  */
+  new = gen_rtx_fmt_eee (unsignedp ? ZERO_EXTRACT : SIGN_EXTRACT,
+			 extraction_mode, inner, GEN_INT (len), pos_rtx);
+  if (! in_dest)
+    new = gen_lowpart_for_combine (mode, new);
+
+  return new;
+}
+
+/* See if X contains an ASHIFT of COUNT or more bits that can be commuted
+   with any other operations in X.  Return X without that shift if so.  */
+
+static rtx
+extract_left_shift (rtx x, int count)
+{
+  enum rtx_code code = GET_CODE (x);
+  enum machine_mode mode = GET_MODE (x);
+  rtx tem;
+
+  switch (code)
+    {
+    case ASHIFT:
+      /* This is the shift itself.  If it is wide enough, we will return
+	 either the value being shifted if the shift count is equal to
+	 COUNT or a shift for the difference.  */
+      if (GET_CODE (XEXP (x, 1)) == CONST_INT
+	  && INTVAL (XEXP (x, 1)) >= count)
+	return simplify_shift_const (NULL_RTX, ASHIFT, mode, XEXP (x, 0),
+				     INTVAL (XEXP (x, 1)) - count);
+      break;
+
+    case NEG:  case NOT:
+      if ((tem = extract_left_shift (XEXP (x, 0), count)) != 0)
+	return simplify_gen_unary (code, mode, tem, mode);
+
+      break;
+
+    case PLUS:  case IOR:  case XOR:  case AND:
+      /* If we can safely shift this constant and we find the inner shift,
+	 make a new operation.  */
+      if (GET_CODE (XEXP (x, 1)) == CONST_INT
+	  && (INTVAL (XEXP (x, 1)) & ((((HOST_WIDE_INT) 1 << count)) - 1)) == 0
+	  && (tem = extract_left_shift (XEXP (x, 0), count)) != 0)
+	return gen_binary (code, mode, tem,
+			   GEN_INT (INTVAL (XEXP (x, 1)) >> count));
+
+      break;
+
+    default:
+      break;
+    }
+
+  return 0;
+}
+
+/* Look at the expression rooted at X.  Look for expressions
+   equivalent to ZERO_EXTRACT, SIGN_EXTRACT, ZERO_EXTEND, SIGN_EXTEND.
+   Form these expressions.
+
+   Return the new rtx, usually just X.
+
+   Also, for machines like the VAX that don't have logical shift insns,
+   try to convert logical to arithmetic shift operations in cases where
+   they are equivalent.  This undoes the canonicalizations to logical
+   shifts done elsewhere.
+
+   We try, as much as possible, to re-use rtl expressions to save memory.
+
+   IN_CODE says what kind of expression we are processing.  Normally, it is
+   SET.  In a memory address (inside a MEM, PLUS or minus, the latter two
+   being kludges), it is MEM.  When processing the arguments of a comparison
+   or a COMPARE against zero, it is COMPARE.  */
+
+static rtx
+make_compound_operation (rtx x, enum rtx_code in_code)
+{
+  enum rtx_code code = GET_CODE (x);
+  enum machine_mode mode = GET_MODE (x);
+  int mode_width = GET_MODE_BITSIZE (mode);
+  rtx rhs, lhs;
+  enum rtx_code next_code;
+  int i;
+  rtx new = 0;
+  rtx tem;
+  const char *fmt;
+
+  /* Select the code to be used in recursive calls.  Once we are inside an
+     address, we stay there.  If we have a comparison, set to COMPARE,
+     but once inside, go back to our default of SET.  */
+
+  next_code = (code == MEM || code == PLUS || code == MINUS ? MEM
+	       : ((code == COMPARE || GET_RTX_CLASS (code) == '<')
+		  && XEXP (x, 1) == const0_rtx) ? COMPARE
+	       : in_code == COMPARE ? SET : in_code);
+
+  /* Process depending on the code of this operation.  If NEW is set
+     nonzero, it will be returned.  */
+
+  switch (code)
+    {
+    case ASHIFT:
+      /* Convert shifts by constants into multiplications if inside
+	 an address.  */
+      if (in_code == MEM && GET_CODE (XEXP (x, 1)) == CONST_INT
+	  && INTVAL (XEXP (x, 1)) < HOST_BITS_PER_WIDE_INT
+	  && INTVAL (XEXP (x, 1)) >= 0)
+	{
+	  new = make_compound_operation (XEXP (x, 0), next_code);
+	  new = gen_rtx_MULT (mode, new,
+			      GEN_INT ((HOST_WIDE_INT) 1
+				       << INTVAL (XEXP (x, 1))));
+	}
+      break;
+
+    case AND:
+      /* If the second operand is not a constant, we can't do anything
+	 with it.  */
+      if (GET_CODE (XEXP (x, 1)) != CONST_INT)
+	break;
+
+      /* If the constant is a power of two minus one and the first operand
+	 is a logical right shift, make an extraction.  */
+      if (GET_CODE (XEXP (x, 0)) == LSHIFTRT
+	  && (i = exact_log2 (INTVAL (XEXP (x, 1)) + 1)) >= 0)
+	{
+	  new = make_compound_operation (XEXP (XEXP (x, 0), 0), next_code);
+	  new = make_extraction (mode, new, 0, XEXP (XEXP (x, 0), 1), i, 1,
+				 0, in_code == COMPARE);
+	}
+
+      /* Same as previous, but for (subreg (lshiftrt ...)) in first op.  */
+      else if (GET_CODE (XEXP (x, 0)) == SUBREG
+	       && subreg_lowpart_p (XEXP (x, 0))
+	       && GET_CODE (SUBREG_REG (XEXP (x, 0))) == LSHIFTRT
+	       && (i = exact_log2 (INTVAL (XEXP (x, 1)) + 1)) >= 0)
+	{
+	  new = make_compound_operation (XEXP (SUBREG_REG (XEXP (x, 0)), 0),
+					 next_code);
+	  new = make_extraction (GET_MODE (SUBREG_REG (XEXP (x, 0))), new, 0,
+				 XEXP (SUBREG_REG (XEXP (x, 0)), 1), i, 1,
+				 0, in_code == COMPARE);
+	}
+      /* Same as previous, but for (xor/ior (lshiftrt...) (lshiftrt...)).  */
+      else if ((GET_CODE (XEXP (x, 0)) == XOR
+		|| GET_CODE (XEXP (x, 0)) == IOR)
+	       && GET_CODE (XEXP (XEXP (x, 0), 0)) == LSHIFTRT
+	       && GET_CODE (XEXP (XEXP (x, 0), 1)) == LSHIFTRT
+	       && (i = exact_log2 (INTVAL (XEXP (x, 1)) + 1)) >= 0)
+	{
+	  /* Apply the distributive law, and then try to make extractions.  */
+	  new = gen_rtx_fmt_ee (GET_CODE (XEXP (x, 0)), mode,
+				gen_rtx_AND (mode, XEXP (XEXP (x, 0), 0),
+					     XEXP (x, 1)),
+				gen_rtx_AND (mode, XEXP (XEXP (x, 0), 1),
+					     XEXP (x, 1)));
+	  new = make_compound_operation (new, in_code);
+	}
+
+      /* If we are have (and (rotate X C) M) and C is larger than the number
+	 of bits in M, this is an extraction.  */
+
+      else if (GET_CODE (XEXP (x, 0)) == ROTATE
+	       && GET_CODE (XEXP (XEXP (x, 0), 1)) == CONST_INT
+	       && (i = exact_log2 (INTVAL (XEXP (x, 1)) + 1)) >= 0
+	       && i <= INTVAL (XEXP (XEXP (x, 0), 1)))
+	{
+	  new = make_compound_operation (XEXP (XEXP (x, 0), 0), next_code);
+	  new = make_extraction (mode, new,
+				 (GET_MODE_BITSIZE (mode)
+				  - INTVAL (XEXP (XEXP (x, 0), 1))),
+				 NULL_RTX, i, 1, 0, in_code == COMPARE);
+	}
+
+      /* On machines without logical shifts, if the operand of the AND is
+	 a logical shift and our mask turns off all the propagated sign
+	 bits, we can replace the logical shift with an arithmetic shift.  */
+      else if (GET_CODE (XEXP (x, 0)) == LSHIFTRT
+	       && !have_insn_for (LSHIFTRT, mode)
+	       && have_insn_for (ASHIFTRT, mode)
+	       && GET_CODE (XEXP (XEXP (x, 0), 1)) == CONST_INT
+	       && INTVAL (XEXP (XEXP (x, 0), 1)) >= 0
+	       && INTVAL (XEXP (XEXP (x, 0), 1)) < HOST_BITS_PER_WIDE_INT
+	       && mode_width <= HOST_BITS_PER_WIDE_INT)
+	{
+	  unsigned HOST_WIDE_INT mask = GET_MODE_MASK (mode);
+
+	  mask >>= INTVAL (XEXP (XEXP (x, 0), 1));
+	  if ((INTVAL (XEXP (x, 1)) & ~mask) == 0)
+	    SUBST (XEXP (x, 0),
+		   gen_rtx_ASHIFTRT (mode,
+				     make_compound_operation
+				     (XEXP (XEXP (x, 0), 0), next_code),
+				     XEXP (XEXP (x, 0), 1)));
+	}
+
+      /* If the constant is one less than a power of two, this might be
+	 representable by an extraction even if no shift is present.
+	 If it doesn't end up being a ZERO_EXTEND, we will ignore it unless
+	 we are in a COMPARE.  */
+      else if ((i = exact_log2 (INTVAL (XEXP (x, 1)) + 1)) >= 0)
+	new = make_extraction (mode,
+			       make_compound_operation (XEXP (x, 0),
+							next_code),
+			       0, NULL_RTX, i, 1, 0, in_code == COMPARE);
+
+      /* If we are in a comparison and this is an AND with a power of two,
+	 convert this into the appropriate bit extract.  */
+      else if (in_code == COMPARE
+	       && (i = exact_log2 (INTVAL (XEXP (x, 1)))) >= 0)
+	new = make_extraction (mode,
+			       make_compound_operation (XEXP (x, 0),
+							next_code),
+			       i, NULL_RTX, 1, 1, 0, 1);
+
+      break;
+
+    case LSHIFTRT:
+      /* If the sign bit is known to be zero, replace this with an
+	 arithmetic shift.  */
+      if (have_insn_for (ASHIFTRT, mode)
+	  && ! have_insn_for (LSHIFTRT, mode)
+	  && mode_width <= HOST_BITS_PER_WIDE_INT
+	  && (nonzero_bits (XEXP (x, 0), mode) & (1 << (mode_width - 1))) == 0)
+	{
+	  new = gen_rtx_ASHIFTRT (mode,
+				  make_compound_operation (XEXP (x, 0),
+							   next_code),
+				  XEXP (x, 1));
+	  break;
+	}
+
+      /* ... fall through ...  */
+
+    case ASHIFTRT:
+      lhs = XEXP (x, 0);
+      rhs = XEXP (x, 1);
+
+      /* If we have (ashiftrt (ashift foo C1) C2) with C2 >= C1,
+	 this is a SIGN_EXTRACT.  */
+      if (GET_CODE (rhs) == CONST_INT
+	  && GET_CODE (lhs) == ASHIFT
+	  && GET_CODE (XEXP (lhs, 1)) == CONST_INT
+	  && INTVAL (rhs) >= INTVAL (XEXP (lhs, 1)))
+	{
+	  new = make_compound_operation (XEXP (lhs, 0), next_code);
+	  new = make_extraction (mode, new,
+				 INTVAL (rhs) - INTVAL (XEXP (lhs, 1)),
+				 NULL_RTX, mode_width - INTVAL (rhs),
+				 code == LSHIFTRT, 0, in_code == COMPARE);
+	  break;
+	}
+
+      /* See if we have operations between an ASHIFTRT and an ASHIFT.
+	 If so, try to merge the shifts into a SIGN_EXTEND.  We could
+	 also do this for some cases of SIGN_EXTRACT, but it doesn't
+	 seem worth the effort; the case checked for occurs on Alpha.  */
+
+      if (GET_RTX_CLASS (GET_CODE (lhs)) != 'o'
+	  && ! (GET_CODE (lhs) == SUBREG
+		&& (GET_RTX_CLASS (GET_CODE (SUBREG_REG (lhs))) == 'o'))
+	  && GET_CODE (rhs) == CONST_INT
+	  && INTVAL (rhs) < HOST_BITS_PER_WIDE_INT
+	  && (new = extract_left_shift (lhs, INTVAL (rhs))) != 0)
+	new = make_extraction (mode, make_compound_operation (new, next_code),
+			       0, NULL_RTX, mode_width - INTVAL (rhs),
+			       code == LSHIFTRT, 0, in_code == COMPARE);
+
+      break;
+
+    case SUBREG:
+      /* Call ourselves recursively on the inner expression.  If we are
+	 narrowing the object and it has a different RTL code from
+	 what it originally did, do this SUBREG as a force_to_mode.  */
+
+      tem = make_compound_operation (SUBREG_REG (x), in_code);
+      if (GET_CODE (tem) != GET_CODE (SUBREG_REG (x))
+	  && GET_MODE_SIZE (mode) < GET_MODE_SIZE (GET_MODE (tem))
+	  && subreg_lowpart_p (x))
+	{
+	  rtx newer = force_to_mode (tem, mode, ~(HOST_WIDE_INT) 0,
+				     NULL_RTX, 0);
+
+	  /* If we have something other than a SUBREG, we might have
+	     done an expansion, so rerun ourselves.  */
+	  if (GET_CODE (newer) != SUBREG)
+	    newer = make_compound_operation (newer, in_code);
+
+	  return newer;
+	}
+
+      /* If this is a paradoxical subreg, and the new code is a sign or
+	 zero extension, omit the subreg and widen the extension.  If it
+	 is a regular subreg, we can still get rid of the subreg by not
+	 widening so much, or in fact removing the extension entirely.  */
+      if ((GET_CODE (tem) == SIGN_EXTEND
+	   || GET_CODE (tem) == ZERO_EXTEND)
+	  && subreg_lowpart_p (x))
+	{
+	  if (GET_MODE_SIZE (mode) > GET_MODE_SIZE (GET_MODE (tem))
+	      || (GET_MODE_SIZE (mode) >
+		  GET_MODE_SIZE (GET_MODE (XEXP (tem, 0)))))
+	    {
+	      if (! SCALAR_INT_MODE_P (mode))
+		break;
+	      tem = gen_rtx_fmt_e (GET_CODE (tem), mode, XEXP (tem, 0));
+	    }
+	  else
+	    tem = gen_lowpart_for_combine (mode, XEXP (tem, 0));
+	  return tem;
+	}
+      break;
+
+    default:
+      break;
+    }
+
+  if (new)
+    {
+      x = gen_lowpart_for_combine (mode, new);
+      code = GET_CODE (x);
+    }
+
+  /* Now recursively process each operand of this operation.  */
+  fmt = GET_RTX_FORMAT (code);
+  for (i = 0; i < GET_RTX_LENGTH (code); i++)
+    if (fmt[i] == 'e')
+      {
+	new = make_compound_operation (XEXP (x, i), next_code);
+	SUBST (XEXP (x, i), new);
+      }
+
+  return x;
+}
+
+/* Given M see if it is a value that would select a field of bits
+   within an item, but not the entire word.  Return -1 if not.
+   Otherwise, return the starting position of the field, where 0 is the
+   low-order bit.
+
+   *PLEN is set to the length of the field.  */
+
+static int
+get_pos_from_mask (unsigned HOST_WIDE_INT m, unsigned HOST_WIDE_INT *plen)
+{
+  /* Get the bit number of the first 1 bit from the right, -1 if none.  */
+  int pos = exact_log2 (m & -m);
+  int len;
+
+  if (pos < 0)
+    return -1;
+
+  /* Now shift off the low-order zero bits and see if we have a power of
+     two minus 1.  */
+  len = exact_log2 ((m >> pos) + 1);
+
+  if (len <= 0)
+    return -1;
+
+  *plen = len;
+  return pos;
+}
+
+/* See if X can be simplified knowing that we will only refer to it in
+   MODE and will only refer to those bits that are nonzero in MASK.
+   If other bits are being computed or if masking operations are done
+   that select a superset of the bits in MASK, they can sometimes be
+   ignored.
+
+   Return a possibly simplified expression, but always convert X to
+   MODE.  If X is a CONST_INT, AND the CONST_INT with MASK.
+
+   Also, if REG is nonzero and X is a register equal in value to REG,
+   replace X with REG.
+
+   If JUST_SELECT is nonzero, don't optimize by noticing that bits in MASK
+   are all off in X.  This is used when X will be complemented, by either
+   NOT, NEG, or XOR.  */
+
+static rtx
+force_to_mode (rtx x, enum machine_mode mode, unsigned HOST_WIDE_INT mask,
+	       rtx reg, int just_select)
+{
+  enum rtx_code code = GET_CODE (x);
+  int next_select = just_select || code == XOR || code == NOT || code == NEG;
+  enum machine_mode op_mode;
+  unsigned HOST_WIDE_INT fuller_mask, nonzero;
+  rtx op0, op1, temp;
+
+  /* If this is a CALL or ASM_OPERANDS, don't do anything.  Some of the
+     code below will do the wrong thing since the mode of such an
+     expression is VOIDmode.
+
+     Also do nothing if X is a CLOBBER; this can happen if X was
+     the return value from a call to gen_lowpart_for_combine.  */
+  if (code == CALL || code == ASM_OPERANDS || code == CLOBBER)
+    return x;
+
+  /* We want to perform the operation is its present mode unless we know
+     that the operation is valid in MODE, in which case we do the operation
+     in MODE.  */
+  op_mode = ((GET_MODE_CLASS (mode) == GET_MODE_CLASS (GET_MODE (x))
+	      && have_insn_for (code, mode))
+	     ? mode : GET_MODE (x));
+
+  /* It is not valid to do a right-shift in a narrower mode
+     than the one it came in with.  */
+  if ((code == LSHIFTRT || code == ASHIFTRT)
+      && GET_MODE_BITSIZE (mode) < GET_MODE_BITSIZE (GET_MODE (x)))
+    op_mode = GET_MODE (x);
+
+  /* Truncate MASK to fit OP_MODE.  */
+  if (op_mode)
+    mask &= GET_MODE_MASK (op_mode);
+
+  /* When we have an arithmetic operation, or a shift whose count we
+     do not know, we need to assume that all bits up to the highest-order
+     bit in MASK will be needed.  This is how we form such a mask.  */
+  if (mask & ((unsigned HOST_WIDE_INT) 1 << (HOST_BITS_PER_WIDE_INT - 1)))
+    fuller_mask = ~(unsigned HOST_WIDE_INT) 0;
+  else
+    fuller_mask = (((unsigned HOST_WIDE_INT) 1 << (floor_log2 (mask) + 1))
+		   - 1);
+
+  /* Determine what bits of X are guaranteed to be (non)zero.  */
+  nonzero = nonzero_bits (x, mode);
+
+  /* If none of the bits in X are needed, return a zero.  */
+  if (! just_select && (nonzero & mask) == 0)
+    x = const0_rtx;
+
+  /* If X is a CONST_INT, return a new one.  Do this here since the
+     test below will fail.  */
+  if (GET_CODE (x) == CONST_INT)
+    {
+      if (SCALAR_INT_MODE_P (mode))
+        return gen_int_mode (INTVAL (x) & mask, mode);
+      else
+	{
+	  x = GEN_INT (INTVAL (x) & mask);
+	  return gen_lowpart_common (mode, x);
+	}
+    }
+
+  /* If X is narrower than MODE and we want all the bits in X's mode, just
+     get X in the proper mode.  */
+  if (GET_MODE_SIZE (GET_MODE (x)) < GET_MODE_SIZE (mode)
+      && (GET_MODE_MASK (GET_MODE (x)) & ~mask) == 0)
+    return gen_lowpart_for_combine (mode, x);
+
+  /* If we aren't changing the mode, X is not a SUBREG, and all zero bits in
+     MASK are already known to be zero in X, we need not do anything.  */
+  if (GET_MODE (x) == mode && code != SUBREG && (~mask & nonzero) == 0)
+    return x;
+
+  switch (code)
+    {
+    case CLOBBER:
+      /* If X is a (clobber (const_int)), return it since we know we are
+	 generating something that won't match.  */
+      return x;
+
+    case USE:
+      /* X is a (use (mem ..)) that was made from a bit-field extraction that
+	 spanned the boundary of the MEM.  If we are now masking so it is
+	 within that boundary, we don't need the USE any more.  */
+      if (! BITS_BIG_ENDIAN
+	  && (mask & ~GET_MODE_MASK (GET_MODE (XEXP (x, 0)))) == 0)
+	return force_to_mode (XEXP (x, 0), mode, mask, reg, next_select);
+      break;
+
+    case SIGN_EXTEND:
+    case ZERO_EXTEND:
+    case ZERO_EXTRACT:
+    case SIGN_EXTRACT:
+      x = expand_compound_operation (x);
+      if (GET_CODE (x) != code)
+	return force_to_mode (x, mode, mask, reg, next_select);
+      break;
+
+    case REG:
+      if (reg != 0 && (rtx_equal_p (get_last_value (reg), x)
+		       || rtx_equal_p (reg, get_last_value (x))))
+	x = reg;
+      break;
+
+    case SUBREG:
+      if (subreg_lowpart_p (x)
+	  /* We can ignore the effect of this SUBREG if it narrows the mode or
+	     if the constant masks to zero all the bits the mode doesn't
+	     have.  */
+	  && ((GET_MODE_SIZE (GET_MODE (x))
+	       < GET_MODE_SIZE (GET_MODE (SUBREG_REG (x))))
+	      || (0 == (mask
+			& GET_MODE_MASK (GET_MODE (x))
+			& ~GET_MODE_MASK (GET_MODE (SUBREG_REG (x)))))))
+	return force_to_mode (SUBREG_REG (x), mode, mask, reg, next_select);
+      break;
+
+    case AND:
+      /* If this is an AND with a constant, convert it into an AND
+	 whose constant is the AND of that constant with MASK.  If it
+	 remains an AND of MASK, delete it since it is redundant.  */
+
+      if (GET_CODE (XEXP (x, 1)) == CONST_INT)
+	{
+	  x = simplify_and_const_int (x, op_mode, XEXP (x, 0),
+				      mask & INTVAL (XEXP (x, 1)));
+
+	  /* If X is still an AND, see if it is an AND with a mask that
+	     is just some low-order bits.  If so, and it is MASK, we don't
+	     need it.  */
+
+	  if (GET_CODE (x) == AND && GET_CODE (XEXP (x, 1)) == CONST_INT
+	      && ((INTVAL (XEXP (x, 1)) & GET_MODE_MASK (GET_MODE (x)))
+		  == mask))
+	    x = XEXP (x, 0);
+
+	  /* If it remains an AND, try making another AND with the bits
+	     in the mode mask that aren't in MASK turned on.  If the
+	     constant in the AND is wide enough, this might make a
+	     cheaper constant.  */
+
+	  if (GET_CODE (x) == AND && GET_CODE (XEXP (x, 1)) == CONST_INT
+	      && GET_MODE_MASK (GET_MODE (x)) != mask
+	      && GET_MODE_BITSIZE (GET_MODE (x)) <= HOST_BITS_PER_WIDE_INT)
+	    {
+	      HOST_WIDE_INT cval = (INTVAL (XEXP (x, 1))
+				    | (GET_MODE_MASK (GET_MODE (x)) & ~mask));
+	      int width = GET_MODE_BITSIZE (GET_MODE (x));
+	      rtx y;
+
+	      /* If MODE is narrower that HOST_WIDE_INT and CVAL is a negative
+		 number, sign extend it.  */
+	      if (width > 0 && width < HOST_BITS_PER_WIDE_INT
+		  && (cval & ((HOST_WIDE_INT) 1 << (width - 1))) != 0)
+		cval |= (HOST_WIDE_INT) -1 << width;
+
+	      y = gen_binary (AND, GET_MODE (x), XEXP (x, 0), GEN_INT (cval));
+	      if (rtx_cost (y, SET) < rtx_cost (x, SET))
+		x = y;
+	    }
+
+	  break;
+	}
+
+      goto binop;
+
+    case PLUS:
+      /* In (and (plus FOO C1) M), if M is a mask that just turns off
+	 low-order bits (as in an alignment operation) and FOO is already
+	 aligned to that boundary, mask C1 to that boundary as well.
+	 This may eliminate that PLUS and, later, the AND.  */
+
+      {
+	unsigned int width = GET_MODE_BITSIZE (mode);
+	unsigned HOST_WIDE_INT smask = mask;
+
+	/* If MODE is narrower than HOST_WIDE_INT and mask is a negative
+	   number, sign extend it.  */
+
+	if (width < HOST_BITS_PER_WIDE_INT
+	    && (smask & ((HOST_WIDE_INT) 1 << (width - 1))) != 0)
+	  smask |= (HOST_WIDE_INT) -1 << width;
+
+	if (GET_CODE (XEXP (x, 1)) == CONST_INT
+	    && exact_log2 (- smask) >= 0
+	    && (nonzero_bits (XEXP (x, 0), mode) & ~smask) == 0
+	    && (INTVAL (XEXP (x, 1)) & ~smask) != 0)
+	  return force_to_mode (plus_constant (XEXP (x, 0),
+					       (INTVAL (XEXP (x, 1)) & smask)),
+				mode, smask, reg, next_select);
+      }
+
+      /* ... fall through ...  */
+
+    case MULT:
+      /* For PLUS, MINUS and MULT, we need any bits less significant than the
+	 most significant bit in MASK since carries from those bits will
+	 affect the bits we are interested in.  */
+      mask = fuller_mask;
+      goto binop;
+
+    case MINUS:
+      /* If X is (minus C Y) where C's least set bit is larger than any bit
+	 in the mask, then we may replace with (neg Y).  */
+      if (GET_CODE (XEXP (x, 0)) == CONST_INT
+	  && (((unsigned HOST_WIDE_INT) (INTVAL (XEXP (x, 0))
+					& -INTVAL (XEXP (x, 0))))
+	      > mask))
+	{
+	  x = simplify_gen_unary (NEG, GET_MODE (x), XEXP (x, 1),
+				  GET_MODE (x));
+	  return force_to_mode (x, mode, mask, reg, next_select);
+	}
+
+      /* Similarly, if C contains every bit in the fuller_mask, then we may
+	 replace with (not Y).  */
+      if (GET_CODE (XEXP (x, 0)) == CONST_INT
+	  && ((INTVAL (XEXP (x, 0)) | (HOST_WIDE_INT) fuller_mask)
+	      == INTVAL (XEXP (x, 0))))
+	{
+	  x = simplify_gen_unary (NOT, GET_MODE (x),
+				  XEXP (x, 1), GET_MODE (x));
+	  return force_to_mode (x, mode, mask, reg, next_select);
+	}
+
+      mask = fuller_mask;
+      goto binop;
+
+    case IOR:
+    case XOR:
+      /* If X is (ior (lshiftrt FOO C1) C2), try to commute the IOR and
+	 LSHIFTRT so we end up with an (and (lshiftrt (ior ...) ...) ...)
+	 operation which may be a bitfield extraction.  Ensure that the
+	 constant we form is not wider than the mode of X.  */
+
+      if (GET_CODE (XEXP (x, 0)) == LSHIFTRT
+	  && GET_CODE (XEXP (XEXP (x, 0), 1)) == CONST_INT
+	  && INTVAL (XEXP (XEXP (x, 0), 1)) >= 0
+	  && INTVAL (XEXP (XEXP (x, 0), 1)) < HOST_BITS_PER_WIDE_INT
+	  && GET_CODE (XEXP (x, 1)) == CONST_INT
+	  && ((INTVAL (XEXP (XEXP (x, 0), 1))
+	       + floor_log2 (INTVAL (XEXP (x, 1))))
+	      < GET_MODE_BITSIZE (GET_MODE (x)))
+	  && (INTVAL (XEXP (x, 1))
+	      & ~nonzero_bits (XEXP (x, 0), GET_MODE (x))) == 0)
+	{
+	  temp = GEN_INT ((INTVAL (XEXP (x, 1)) & mask)
+			  << INTVAL (XEXP (XEXP (x, 0), 1)));
+	  temp = gen_binary (GET_CODE (x), GET_MODE (x),
+			     XEXP (XEXP (x, 0), 0), temp);
+	  x = gen_binary (LSHIFTRT, GET_MODE (x), temp,
+			  XEXP (XEXP (x, 0), 1));
+	  return force_to_mode (x, mode, mask, reg, next_select);
+	}
+
+    binop:
+      /* For most binary operations, just propagate into the operation and
+	 change the mode if we have an operation of that mode.  */
+
+      op0 = gen_lowpart_for_combine (op_mode,
+				     force_to_mode (XEXP (x, 0), mode, mask,
+						    reg, next_select));
+      op1 = gen_lowpart_for_combine (op_mode,
+				     force_to_mode (XEXP (x, 1), mode, mask,
+						    reg, next_select));
+
+      if (op_mode != GET_MODE (x) || op0 != XEXP (x, 0) || op1 != XEXP (x, 1))
+	x = gen_binary (code, op_mode, op0, op1);
+      break;
+
+    case ASHIFT:
+      /* For left shifts, do the same, but just for the first operand.
+	 However, we cannot do anything with shifts where we cannot
+	 guarantee that the counts are smaller than the size of the mode
+	 because such a count will have a different meaning in a
+	 wider mode.  */
+
+      if (! (GET_CODE (XEXP (x, 1)) == CONST_INT
+	     && INTVAL (XEXP (x, 1)) >= 0
+	     && INTVAL (XEXP (x, 1)) < GET_MODE_BITSIZE (mode))
+	  && ! (GET_MODE (XEXP (x, 1)) != VOIDmode
+		&& (nonzero_bits (XEXP (x, 1), GET_MODE (XEXP (x, 1)))
+		    < (unsigned HOST_WIDE_INT) GET_MODE_BITSIZE (mode))))
+	break;
+
+      /* If the shift count is a constant and we can do arithmetic in
+	 the mode of the shift, refine which bits we need.  Otherwise, use the
+	 conservative form of the mask.  */
+      if (GET_CODE (XEXP (x, 1)) == CONST_INT
+	  && INTVAL (XEXP (x, 1)) >= 0
+	  && INTVAL (XEXP (x, 1)) < GET_MODE_BITSIZE (op_mode)
+	  && GET_MODE_BITSIZE (op_mode) <= HOST_BITS_PER_WIDE_INT)
+	mask >>= INTVAL (XEXP (x, 1));
+      else
+	mask = fuller_mask;
+
+      op0 = gen_lowpart_for_combine (op_mode,
+				     force_to_mode (XEXP (x, 0), op_mode,
+						    mask, reg, next_select));
+
+      if (op_mode != GET_MODE (x) || op0 != XEXP (x, 0))
+	x = gen_binary (code, op_mode, op0, XEXP (x, 1));
+      break;
+
+    case LSHIFTRT:
+      /* Here we can only do something if the shift count is a constant,
+	 this shift constant is valid for the host, and we can do arithmetic
+	 in OP_MODE.  */
+
+      if (GET_CODE (XEXP (x, 1)) == CONST_INT
+	  && INTVAL (XEXP (x, 1)) < HOST_BITS_PER_WIDE_INT
+	  && GET_MODE_BITSIZE (op_mode) <= HOST_BITS_PER_WIDE_INT)
+	{
+	  rtx inner = XEXP (x, 0);
+	  unsigned HOST_WIDE_INT inner_mask;
+
+	  /* Select the mask of the bits we need for the shift operand.  */
+	  inner_mask = mask << INTVAL (XEXP (x, 1));
+
+	  /* We can only change the mode of the shift if we can do arithmetic
+	     in the mode of the shift and INNER_MASK is no wider than the
+	     width of OP_MODE.  */
+	  if (GET_MODE_BITSIZE (op_mode) > HOST_BITS_PER_WIDE_INT
+	      || (inner_mask & ~GET_MODE_MASK (op_mode)) != 0)
+	    op_mode = GET_MODE (x);
+
+	  inner = force_to_mode (inner, op_mode, inner_mask, reg, next_select);
+
+	  if (GET_MODE (x) != op_mode || inner != XEXP (x, 0))
+	    x = gen_binary (LSHIFTRT, op_mode, inner, XEXP (x, 1));
+	}
+
+      /* If we have (and (lshiftrt FOO C1) C2) where the combination of the
+	 shift and AND produces only copies of the sign bit (C2 is one less
+	 than a power of two), we can do this with just a shift.  */
+
+      if (GET_CODE (x) == LSHIFTRT
+	  && GET_CODE (XEXP (x, 1)) == CONST_INT
+	  /* The shift puts one of the sign bit copies in the least significant
+	     bit.  */
+	  && ((INTVAL (XEXP (x, 1))
+	       + num_sign_bit_copies (XEXP (x, 0), GET_MODE (XEXP (x, 0))))
+	      >= GET_MODE_BITSIZE (GET_MODE (x)))
+	  && exact_log2 (mask + 1) >= 0
+	  /* Number of bits left after the shift must be more than the mask
+	     needs.  */
+	  && ((INTVAL (XEXP (x, 1)) + exact_log2 (mask + 1))
+	      <= GET_MODE_BITSIZE (GET_MODE (x)))
+	  /* Must be more sign bit copies than the mask needs.  */
+	  && ((int) num_sign_bit_copies (XEXP (x, 0), GET_MODE (XEXP (x, 0)))
+	      >= exact_log2 (mask + 1)))
+	x = gen_binary (LSHIFTRT, GET_MODE (x), XEXP (x, 0),
+			GEN_INT (GET_MODE_BITSIZE (GET_MODE (x))
+				 - exact_log2 (mask + 1)));
+
+      goto shiftrt;
+
+    case ASHIFTRT:
+      /* If we are just looking for the sign bit, we don't need this shift at
+	 all, even if it has a variable count.  */
+      if (GET_MODE_BITSIZE (GET_MODE (x)) <= HOST_BITS_PER_WIDE_INT
+	  && (mask == ((unsigned HOST_WIDE_INT) 1
+		       << (GET_MODE_BITSIZE (GET_MODE (x)) - 1))))
+	return force_to_mode (XEXP (x, 0), mode, mask, reg, next_select);
+
+      /* If this is a shift by a constant, get a mask that contains those bits
+	 that are not copies of the sign bit.  We then have two cases:  If
+	 MASK only includes those bits, this can be a logical shift, which may
+	 allow simplifications.  If MASK is a single-bit field not within
+	 those bits, we are requesting a copy of the sign bit and hence can
+	 shift the sign bit to the appropriate location.  */
+
+      if (GET_CODE (XEXP (x, 1)) == CONST_INT && INTVAL (XEXP (x, 1)) >= 0
+	  && INTVAL (XEXP (x, 1)) < HOST_BITS_PER_WIDE_INT)
+	{
+	  int i = -1;
+
+	  /* If the considered data is wider than HOST_WIDE_INT, we can't
+	     represent a mask for all its bits in a single scalar.
+	     But we only care about the lower bits, so calculate these.  */
+
+	  if (GET_MODE_BITSIZE (GET_MODE (x)) > HOST_BITS_PER_WIDE_INT)
+	    {
+	      nonzero = ~(HOST_WIDE_INT) 0;
+
+	      /* GET_MODE_BITSIZE (GET_MODE (x)) - INTVAL (XEXP (x, 1))
+		 is the number of bits a full-width mask would have set.
+		 We need only shift if these are fewer than nonzero can
+		 hold.  If not, we must keep all bits set in nonzero.  */
+
+	      if (GET_MODE_BITSIZE (GET_MODE (x)) - INTVAL (XEXP (x, 1))
+		  < HOST_BITS_PER_WIDE_INT)
+		nonzero >>= INTVAL (XEXP (x, 1))
+			    + HOST_BITS_PER_WIDE_INT
+			    - GET_MODE_BITSIZE (GET_MODE (x)) ;
+	    }
+	  else
+	    {
+	      nonzero = GET_MODE_MASK (GET_MODE (x));
+	      nonzero >>= INTVAL (XEXP (x, 1));
+	    }
+
+	  if ((mask & ~nonzero) == 0
+	      || (i = exact_log2 (mask)) >= 0)
+	    {
+	      x = simplify_shift_const
+		(x, LSHIFTRT, GET_MODE (x), XEXP (x, 0),
+		 i < 0 ? INTVAL (XEXP (x, 1))
+		 : GET_MODE_BITSIZE (GET_MODE (x)) - 1 - i);
+
+	      if (GET_CODE (x) != ASHIFTRT)
+		return force_to_mode (x, mode, mask, reg, next_select);
+	    }
+	}
+
+      /* If MASK is 1, convert this to an LSHIFTRT.  This can be done
+	 even if the shift count isn't a constant.  */
+      if (mask == 1)
+	x = gen_binary (LSHIFTRT, GET_MODE (x), XEXP (x, 0), XEXP (x, 1));
+
+    shiftrt:
+
+      /* If this is a zero- or sign-extension operation that just affects bits
+	 we don't care about, remove it.  Be sure the call above returned
+	 something that is still a shift.  */
+
+      if ((GET_CODE (x) == LSHIFTRT || GET_CODE (x) == ASHIFTRT)
+	  && GET_CODE (XEXP (x, 1)) == CONST_INT
+	  && INTVAL (XEXP (x, 1)) >= 0
+	  && (INTVAL (XEXP (x, 1))
+	      <= GET_MODE_BITSIZE (GET_MODE (x)) - (floor_log2 (mask) + 1))
+	  && GET_CODE (XEXP (x, 0)) == ASHIFT
+	  && XEXP (XEXP (x, 0), 1) == XEXP (x, 1))
+	return force_to_mode (XEXP (XEXP (x, 0), 0), mode, mask,
+			      reg, next_select);
+
+      break;
+
+    case ROTATE:
+    case ROTATERT:
+      /* If the shift count is constant and we can do computations
+	 in the mode of X, compute where the bits we care about are.
+	 Otherwise, we can't do anything.  Don't change the mode of
+	 the shift or propagate MODE into the shift, though.  */
+      if (GET_CODE (XEXP (x, 1)) == CONST_INT
+	  && INTVAL (XEXP (x, 1)) >= 0)
+	{
+	  temp = simplify_binary_operation (code == ROTATE ? ROTATERT : ROTATE,
+					    GET_MODE (x), GEN_INT (mask),
+					    XEXP (x, 1));
+	  if (temp && GET_CODE (temp) == CONST_INT)
+	    SUBST (XEXP (x, 0),
+		   force_to_mode (XEXP (x, 0), GET_MODE (x),
+				  INTVAL (temp), reg, next_select));
+	}
+      break;
+
+    case NEG:
+      /* If we just want the low-order bit, the NEG isn't needed since it
+	 won't change the low-order bit.  */
+      if (mask == 1)
+	return force_to_mode (XEXP (x, 0), mode, mask, reg, just_select);
+
+      /* We need any bits less significant than the most significant bit in
+	 MASK since carries from those bits will affect the bits we are
+	 interested in.  */
+      mask = fuller_mask;
+      goto unop;
+
+    case NOT:
+      /* (not FOO) is (xor FOO CONST), so if FOO is an LSHIFTRT, we can do the
+	 same as the XOR case above.  Ensure that the constant we form is not
+	 wider than the mode of X.  */
+
+      if (GET_CODE (XEXP (x, 0)) == LSHIFTRT
+	  && GET_CODE (XEXP (XEXP (x, 0), 1)) == CONST_INT
+	  && INTVAL (XEXP (XEXP (x, 0), 1)) >= 0
+	  && (INTVAL (XEXP (XEXP (x, 0), 1)) + floor_log2 (mask)
+	      < GET_MODE_BITSIZE (GET_MODE (x)))
+	  && INTVAL (XEXP (XEXP (x, 0), 1)) < HOST_BITS_PER_WIDE_INT)
+	{
+	  temp = gen_int_mode (mask << INTVAL (XEXP (XEXP (x, 0), 1)),
+			       GET_MODE (x));
+	  temp = gen_binary (XOR, GET_MODE (x), XEXP (XEXP (x, 0), 0), temp);
+	  x = gen_binary (LSHIFTRT, GET_MODE (x), temp, XEXP (XEXP (x, 0), 1));
+
+	  return force_to_mode (x, mode, mask, reg, next_select);
+	}
+
+      /* (and (not FOO) CONST) is (not (or FOO (not CONST))), so we must
+	 use the full mask inside the NOT.  */
+      mask = fuller_mask;
+
+    unop:
+      op0 = gen_lowpart_for_combine (op_mode,
+				     force_to_mode (XEXP (x, 0), mode, mask,
+						    reg, next_select));
+      if (op_mode != GET_MODE (x) || op0 != XEXP (x, 0))
+	x = simplify_gen_unary (code, op_mode, op0, op_mode);
+      break;
+
+    case NE:
+      /* (and (ne FOO 0) CONST) can be (and FOO CONST) if CONST is included
+	 in STORE_FLAG_VALUE and FOO has a single bit that might be nonzero,
+	 which is equal to STORE_FLAG_VALUE.  */
+      if ((mask & ~STORE_FLAG_VALUE) == 0 && XEXP (x, 1) == const0_rtx
+	  && exact_log2 (nonzero_bits (XEXP (x, 0), mode)) >= 0
+	  && (nonzero_bits (XEXP (x, 0), mode)
+	      == (unsigned HOST_WIDE_INT) STORE_FLAG_VALUE))
+	return force_to_mode (XEXP (x, 0), mode, mask, reg, next_select);
+
+      break;
+
+    case IF_THEN_ELSE:
+      /* We have no way of knowing if the IF_THEN_ELSE can itself be
+	 written in a narrower mode.  We play it safe and do not do so.  */
+
+      SUBST (XEXP (x, 1),
+	     gen_lowpart_for_combine (GET_MODE (x),
+				      force_to_mode (XEXP (x, 1), mode,
+						     mask, reg, next_select)));
+      SUBST (XEXP (x, 2),
+	     gen_lowpart_for_combine (GET_MODE (x),
+				      force_to_mode (XEXP (x, 2), mode,
+						     mask, reg, next_select)));
+      break;
+
+    default:
+      break;
+    }
+
+  /* Ensure we return a value of the proper mode.  */
+  return gen_lowpart_for_combine (mode, x);
+}
+
+/* Return nonzero if X is an expression that has one of two values depending on
+   whether some other value is zero or nonzero.  In that case, we return the
+   value that is being tested, *PTRUE is set to the value if the rtx being
+   returned has a nonzero value, and *PFALSE is set to the other alternative.
+
+   If we return zero, we set *PTRUE and *PFALSE to X.  */
+
+static rtx
+if_then_else_cond (rtx x, rtx *ptrue, rtx *pfalse)
+{
+  enum machine_mode mode = GET_MODE (x);
+  enum rtx_code code = GET_CODE (x);
+  rtx cond0, cond1, true0, true1, false0, false1;
+  unsigned HOST_WIDE_INT nz;
+
+  /* If we are comparing a value against zero, we are done.  */
+  if ((code == NE || code == EQ)
+      && XEXP (x, 1) == const0_rtx)
+    {
+      *ptrue = (code == NE) ? const_true_rtx : const0_rtx;
+      *pfalse = (code == NE) ? const0_rtx : const_true_rtx;
+      return XEXP (x, 0);
+    }
+
+  /* If this is a unary operation whose operand has one of two values, apply
+     our opcode to compute those values.  */
+  else if (GET_RTX_CLASS (code) == '1'
+	   && (cond0 = if_then_else_cond (XEXP (x, 0), &true0, &false0)) != 0)
+    {
+      *ptrue = simplify_gen_unary (code, mode, true0, GET_MODE (XEXP (x, 0)));
+      *pfalse = simplify_gen_unary (code, mode, false0,
+				    GET_MODE (XEXP (x, 0)));
+      return cond0;
+    }
+
+  /* If this is a COMPARE, do nothing, since the IF_THEN_ELSE we would
+     make can't possibly match and would suppress other optimizations.  */
+  else if (code == COMPARE)
+    ;
+
+  /* If this is a binary operation, see if either side has only one of two
+     values.  If either one does or if both do and they are conditional on
+     the same value, compute the new true and false values.  */
+  else if (GET_RTX_CLASS (code) == 'c' || GET_RTX_CLASS (code) == '2'
+	   || GET_RTX_CLASS (code) == '<')
+    {
+      cond0 = if_then_else_cond (XEXP (x, 0), &true0, &false0);
+      cond1 = if_then_else_cond (XEXP (x, 1), &true1, &false1);
+
+      if ((cond0 != 0 || cond1 != 0)
+	  && ! (cond0 != 0 && cond1 != 0 && ! rtx_equal_p (cond0, cond1)))
+	{
+	  /* If if_then_else_cond returned zero, then true/false are the
+	     same rtl.  We must copy one of them to prevent invalid rtl
+	     sharing.  */
+	  if (cond0 == 0)
+	    true0 = copy_rtx (true0);
+	  else if (cond1 == 0)
+	    true1 = copy_rtx (true1);
+
+	  *ptrue = gen_binary (code, mode, true0, true1);
+	  *pfalse = gen_binary (code, mode, false0, false1);
+	  return cond0 ? cond0 : cond1;
+	}
+
+      /* See if we have PLUS, IOR, XOR, MINUS or UMAX, where one of the
+	 operands is zero when the other is nonzero, and vice-versa,
+	 and STORE_FLAG_VALUE is 1 or -1.  */
+
+      if ((STORE_FLAG_VALUE == 1 || STORE_FLAG_VALUE == -1)
+	  && (code == PLUS || code == IOR || code == XOR || code == MINUS
+	      || code == UMAX)
+	  && GET_CODE (XEXP (x, 0)) == MULT && GET_CODE (XEXP (x, 1)) == MULT)
+	{
+	  rtx op0 = XEXP (XEXP (x, 0), 1);
+	  rtx op1 = XEXP (XEXP (x, 1), 1);
+
+	  cond0 = XEXP (XEXP (x, 0), 0);
+	  cond1 = XEXP (XEXP (x, 1), 0);
+
+	  if (GET_RTX_CLASS (GET_CODE (cond0)) == '<'
+	      && GET_RTX_CLASS (GET_CODE (cond1)) == '<'
+	      && ((GET_CODE (cond0) == combine_reversed_comparison_code (cond1)
+		   && rtx_equal_p (XEXP (cond0, 0), XEXP (cond1, 0))
+		   && rtx_equal_p (XEXP (cond0, 1), XEXP (cond1, 1)))
+		  || ((swap_condition (GET_CODE (cond0))
+		       == combine_reversed_comparison_code (cond1))
+		      && rtx_equal_p (XEXP (cond0, 0), XEXP (cond1, 1))
+		      && rtx_equal_p (XEXP (cond0, 1), XEXP (cond1, 0))))
+	      && ! side_effects_p (x))
+	    {
+	      *ptrue = gen_binary (MULT, mode, op0, const_true_rtx);
+	      *pfalse = gen_binary (MULT, mode,
+				    (code == MINUS
+				     ? simplify_gen_unary (NEG, mode, op1,
+							   mode)
+				     : op1),
+				    const_true_rtx);
+	      return cond0;
+	    }
+	}
+
+      /* Similarly for MULT, AND and UMIN, except that for these the result
+	 is always zero.  */
+      if ((STORE_FLAG_VALUE == 1 || STORE_FLAG_VALUE == -1)
+	  && (code == MULT || code == AND || code == UMIN)
+	  && GET_CODE (XEXP (x, 0)) == MULT && GET_CODE (XEXP (x, 1)) == MULT)
+	{
+	  cond0 = XEXP (XEXP (x, 0), 0);
+	  cond1 = XEXP (XEXP (x, 1), 0);
+
+	  if (GET_RTX_CLASS (GET_CODE (cond0)) == '<'
+	      && GET_RTX_CLASS (GET_CODE (cond1)) == '<'
+	      && ((GET_CODE (cond0) == combine_reversed_comparison_code (cond1)
+		   && rtx_equal_p (XEXP (cond0, 0), XEXP (cond1, 0))
+		   && rtx_equal_p (XEXP (cond0, 1), XEXP (cond1, 1)))
+		  || ((swap_condition (GET_CODE (cond0))
+		       == combine_reversed_comparison_code (cond1))
+		      && rtx_equal_p (XEXP (cond0, 0), XEXP (cond1, 1))
+		      && rtx_equal_p (XEXP (cond0, 1), XEXP (cond1, 0))))
+	      && ! side_effects_p (x))
+	    {
+	      *ptrue = *pfalse = const0_rtx;
+	      return cond0;
+	    }
+	}
+    }
+
+  else if (code == IF_THEN_ELSE)
+    {
+      /* If we have IF_THEN_ELSE already, extract the condition and
+	 canonicalize it if it is NE or EQ.  */
+      cond0 = XEXP (x, 0);
+      *ptrue = XEXP (x, 1), *pfalse = XEXP (x, 2);
+      if (GET_CODE (cond0) == NE && XEXP (cond0, 1) == const0_rtx)
+	return XEXP (cond0, 0);
+      else if (GET_CODE (cond0) == EQ && XEXP (cond0, 1) == const0_rtx)
+	{
+	  *ptrue = XEXP (x, 2), *pfalse = XEXP (x, 1);
+	  return XEXP (cond0, 0);
+	}
+      else
+	return cond0;
+    }
+
+  /* If X is a SUBREG, we can narrow both the true and false values
+     if the inner expression, if there is a condition.  */
+  else if (code == SUBREG
+	   && 0 != (cond0 = if_then_else_cond (SUBREG_REG (x),
+					       &true0, &false0)))
+    {
+      true0 = simplify_gen_subreg (mode, true0,
+				   GET_MODE (SUBREG_REG (x)), SUBREG_BYTE (x));
+      false0 = simplify_gen_subreg (mode, false0,
+				    GET_MODE (SUBREG_REG (x)), SUBREG_BYTE (x));
+      if (true0 && false0)
+	{
+	  *ptrue = true0;
+	  *pfalse = false0;
+	  return cond0;
+	}
+    }
+
+  /* If X is a constant, this isn't special and will cause confusions
+     if we treat it as such.  Likewise if it is equivalent to a constant.  */
+  else if (CONSTANT_P (x)
+	   || ((cond0 = get_last_value (x)) != 0 && CONSTANT_P (cond0)))
+    ;
+
+  /* If we're in BImode, canonicalize on 0 and STORE_FLAG_VALUE, as that
+     will be least confusing to the rest of the compiler.  */
+  else if (mode == BImode)
+    {
+      *ptrue = GEN_INT (STORE_FLAG_VALUE), *pfalse = const0_rtx;
+      return x;
+    }
+
+  /* If X is known to be either 0 or -1, those are the true and
+     false values when testing X.  */
+  else if (x == constm1_rtx || x == const0_rtx
+	   || (mode != VOIDmode
+	       && num_sign_bit_copies (x, mode) == GET_MODE_BITSIZE (mode)))
+    {
+      *ptrue = constm1_rtx, *pfalse = const0_rtx;
+      return x;
+    }
+
+  /* Likewise for 0 or a single bit.  */
+  else if (SCALAR_INT_MODE_P (mode)
+	   && GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT
+	   && exact_log2 (nz = nonzero_bits (x, mode)) >= 0)
+    {
+      *ptrue = gen_int_mode (nz, mode), *pfalse = const0_rtx;
+      return x;
+    }
+
+  /* Otherwise fail; show no condition with true and false values the same.  */
+  *ptrue = *pfalse = x;
+  return 0;
+}
+
+/* Return the value of expression X given the fact that condition COND
+   is known to be true when applied to REG as its first operand and VAL
+   as its second.  X is known to not be shared and so can be modified in
+   place.
+
+   We only handle the simplest cases, and specifically those cases that
+   arise with IF_THEN_ELSE expressions.  */
+
+static rtx
+known_cond (rtx x, enum rtx_code cond, rtx reg, rtx val)
+{
+  enum rtx_code code = GET_CODE (x);
+  rtx temp;
+  const char *fmt;
+  int i, j;
+
+  if (side_effects_p (x))
+    return x;
+
+  /* If either operand of the condition is a floating point value,
+     then we have to avoid collapsing an EQ comparison.  */
+  if (cond == EQ
+      && rtx_equal_p (x, reg)
+      && ! FLOAT_MODE_P (GET_MODE (x))
+      && ! FLOAT_MODE_P (GET_MODE (val)))
+    return val;
+
+  if (cond == UNEQ && rtx_equal_p (x, reg))
+    return val;
+
+  /* If X is (abs REG) and we know something about REG's relationship
+     with zero, we may be able to simplify this.  */
+
+  if (code == ABS && rtx_equal_p (XEXP (x, 0), reg) && val == const0_rtx)
+    switch (cond)
+      {
+      case GE:  case GT:  case EQ:
+	return XEXP (x, 0);
+      case LT:  case LE:
+	return simplify_gen_unary (NEG, GET_MODE (XEXP (x, 0)),
+				   XEXP (x, 0),
+				   GET_MODE (XEXP (x, 0)));
+      default:
+	break;
+      }
+
+  /* The only other cases we handle are MIN, MAX, and comparisons if the
+     operands are the same as REG and VAL.  */
+
+  else if (GET_RTX_CLASS (code) == '<' || GET_RTX_CLASS (code) == 'c')
+    {
+      if (rtx_equal_p (XEXP (x, 0), val))
+	cond = swap_condition (cond), temp = val, val = reg, reg = temp;
+
+      if (rtx_equal_p (XEXP (x, 0), reg) && rtx_equal_p (XEXP (x, 1), val))
+	{
+	  if (GET_RTX_CLASS (code) == '<')
+	    {
+	      if (comparison_dominates_p (cond, code))
+		return const_true_rtx;
+
+	      code = combine_reversed_comparison_code (x);
+	      if (code != UNKNOWN
+		  && comparison_dominates_p (cond, code))
+		return const0_rtx;
+	      else
+		return x;
+	    }
+	  else if (code == SMAX || code == SMIN
+		   || code == UMIN || code == UMAX)
+	    {
+	      int unsignedp = (code == UMIN || code == UMAX);
+
+	      /* Do not reverse the condition when it is NE or EQ.
+		 This is because we cannot conclude anything about
+		 the value of 'SMAX (x, y)' when x is not equal to y,
+		 but we can when x equals y.  */
+	      if ((code == SMAX || code == UMAX)
+		  && ! (cond == EQ || cond == NE))
+		cond = reverse_condition (cond);
+
+	      switch (cond)
+		{
+		case GE:   case GT:
+		  return unsignedp ? x : XEXP (x, 1);
+		case LE:   case LT:
+		  return unsignedp ? x : XEXP (x, 0);
+		case GEU:  case GTU:
+		  return unsignedp ? XEXP (x, 1) : x;
+		case LEU:  case LTU:
+		  return unsignedp ? XEXP (x, 0) : x;
+		default:
+		  break;
+		}
+	    }
+	}
+    }
+  else if (code == SUBREG)
+    {
+      enum machine_mode inner_mode = GET_MODE (SUBREG_REG (x));
+      rtx new, r = known_cond (SUBREG_REG (x), cond, reg, val);
+
+      if (SUBREG_REG (x) != r)
+	{
+	  /* We must simplify subreg here, before we lose track of the
+	     original inner_mode.  */
+	  new = simplify_subreg (GET_MODE (x), r,
+				 inner_mode, SUBREG_BYTE (x));
+	  if (new)
+	    return new;
+	  else
+	    SUBST (SUBREG_REG (x), r);
+	}
+
+      return x;
+    }
+  /* We don't have to handle SIGN_EXTEND here, because even in the
+     case of replacing something with a modeless CONST_INT, a
+     CONST_INT is already (supposed to be) a valid sign extension for
+     its narrower mode, which implies it's already properly
+     sign-extended for the wider mode.  Now, for ZERO_EXTEND, the
+     story is different.  */
+  else if (code == ZERO_EXTEND)
+    {
+      enum machine_mode inner_mode = GET_MODE (XEXP (x, 0));
+      rtx new, r = known_cond (XEXP (x, 0), cond, reg, val);
+
+      if (XEXP (x, 0) != r)
+	{
+	  /* We must simplify the zero_extend here, before we lose
+             track of the original inner_mode.  */
+	  new = simplify_unary_operation (ZERO_EXTEND, GET_MODE (x),
+					  r, inner_mode);
+	  if (new)
+	    return new;
+	  else
+	    SUBST (XEXP (x, 0), r);
+	}
+
+      return x;
+    }
+
+  fmt = GET_RTX_FORMAT (code);
+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)
+    {
+      if (fmt[i] == 'e')
+	SUBST (XEXP (x, i), known_cond (XEXP (x, i), cond, reg, val));
+      else if (fmt[i] == 'E')
+	for (j = XVECLEN (x, i) - 1; j >= 0; j--)
+	  SUBST (XVECEXP (x, i, j), known_cond (XVECEXP (x, i, j),
+						cond, reg, val));
+    }
+
+  return x;
+}
+
+/* See if X and Y are equal for the purposes of seeing if we can rewrite an
+   assignment as a field assignment.  */
+
+static int
+rtx_equal_for_field_assignment_p (rtx x, rtx y)
+{
+  if (x == y || rtx_equal_p (x, y))
+    return 1;
+
+  if (x == 0 || y == 0 || GET_MODE (x) != GET_MODE (y))
+    return 0;
+
+  /* Check for a paradoxical SUBREG of a MEM compared with the MEM.
+     Note that all SUBREGs of MEM are paradoxical; otherwise they
+     would have been rewritten.  */
+  if (GET_CODE (x) == MEM && GET_CODE (y) == SUBREG
+      && GET_CODE (SUBREG_REG (y)) == MEM
+      && rtx_equal_p (SUBREG_REG (y),
+		      gen_lowpart_for_combine (GET_MODE (SUBREG_REG (y)), x)))
+    return 1;
+
+  if (GET_CODE (y) == MEM && GET_CODE (x) == SUBREG
+      && GET_CODE (SUBREG_REG (x)) == MEM
+      && rtx_equal_p (SUBREG_REG (x),
+		      gen_lowpart_for_combine (GET_MODE (SUBREG_REG (x)), y)))
+    return 1;
+
+  /* We used to see if get_last_value of X and Y were the same but that's
+     not correct.  In one direction, we'll cause the assignment to have
+     the wrong destination and in the case, we'll import a register into this
+     insn that might have already have been dead.   So fail if none of the
+     above cases are true.  */
+  return 0;
+}
+
+/* See if X, a SET operation, can be rewritten as a bit-field assignment.
+   Return that assignment if so.
+
+   We only handle the most common cases.  */
+
+static rtx
+make_field_assignment (rtx x)
+{
+  rtx dest = SET_DEST (x);
+  rtx src = SET_SRC (x);
+  rtx assign;
+  rtx rhs, lhs;
+  HOST_WIDE_INT c1;
+  HOST_WIDE_INT pos;
+  unsigned HOST_WIDE_INT len;
+  rtx other;
+  enum machine_mode mode;
+
+  /* If SRC was (and (not (ashift (const_int 1) POS)) DEST), this is
+     a clear of a one-bit field.  We will have changed it to
+     (and (rotate (const_int -2) POS) DEST), so check for that.  Also check
+     for a SUBREG.  */
+
+  if (GET_CODE (src) == AND && GET_CODE (XEXP (src, 0)) == ROTATE
+      && GET_CODE (XEXP (XEXP (src, 0), 0)) == CONST_INT
+      && INTVAL (XEXP (XEXP (src, 0), 0)) == -2
+      && rtx_equal_for_field_assignment_p (dest, XEXP (src, 1)))
+    {
+      assign = make_extraction (VOIDmode, dest, 0, XEXP (XEXP (src, 0), 1),
+				1, 1, 1, 0);
+      if (assign != 0)
+	return gen_rtx_SET (VOIDmode, assign, const0_rtx);
+      return x;
+    }
+
+  else if (GET_CODE (src) == AND && GET_CODE (XEXP (src, 0)) == SUBREG
+	   && subreg_lowpart_p (XEXP (src, 0))
+	   && (GET_MODE_SIZE (GET_MODE (XEXP (src, 0)))
+	       < GET_MODE_SIZE (GET_MODE (SUBREG_REG (XEXP (src, 0)))))
+	   && GET_CODE (SUBREG_REG (XEXP (src, 0))) == ROTATE
+	   && GET_CODE (XEXP (SUBREG_REG (XEXP (src, 0)), 0)) == CONST_INT
+	   && INTVAL (XEXP (SUBREG_REG (XEXP (src, 0)), 0)) == -2
+	   && rtx_equal_for_field_assignment_p (dest, XEXP (src, 1)))
+    {
+      assign = make_extraction (VOIDmode, dest, 0,
+				XEXP (SUBREG_REG (XEXP (src, 0)), 1),
+				1, 1, 1, 0);
+      if (assign != 0)
+	return gen_rtx_SET (VOIDmode, assign, const0_rtx);
+      return x;
+    }
+
+  /* If SRC is (ior (ashift (const_int 1) POS) DEST), this is a set of a
+     one-bit field.  */
+  else if (GET_CODE (src) == IOR && GET_CODE (XEXP (src, 0)) == ASHIFT
+	   && XEXP (XEXP (src, 0), 0) == const1_rtx
+	   && rtx_equal_for_field_assignment_p (dest, XEXP (src, 1)))
+    {
+      assign = make_extraction (VOIDmode, dest, 0, XEXP (XEXP (src, 0), 1),
+				1, 1, 1, 0);
+      if (assign != 0)
+	return gen_rtx_SET (VOIDmode, assign, const1_rtx);
+      return x;
+    }
+
+  /* The other case we handle is assignments into a constant-position
+     field.  They look like (ior/xor (and DEST C1) OTHER).  If C1 represents
+     a mask that has all one bits except for a group of zero bits and
+     OTHER is known to have zeros where C1 has ones, this is such an
+     assignment.  Compute the position and length from C1.  Shift OTHER
+     to the appropriate position, force it to the required mode, and
+     make the extraction.  Check for the AND in both operands.  */
+
+  if (GET_CODE (src) != IOR && GET_CODE (src) != XOR)
+    return x;
+
+  rhs = expand_compound_operation (XEXP (src, 0));
+  lhs = expand_compound_operation (XEXP (src, 1));
+
+  if (GET_CODE (rhs) == AND
+      && GET_CODE (XEXP (rhs, 1)) == CONST_INT
+      && rtx_equal_for_field_assignment_p (XEXP (rhs, 0), dest))
+    c1 = INTVAL (XEXP (rhs, 1)), other = lhs;
+  else if (GET_CODE (lhs) == AND
+	   && GET_CODE (XEXP (lhs, 1)) == CONST_INT
+	   && rtx_equal_for_field_assignment_p (XEXP (lhs, 0), dest))
+    c1 = INTVAL (XEXP (lhs, 1)), other = rhs;
+  else
+    return x;
+
+  pos = get_pos_from_mask ((~c1) & GET_MODE_MASK (GET_MODE (dest)), &len);
+  if (pos < 0 || pos + len > GET_MODE_BITSIZE (GET_MODE (dest))
+      || GET_MODE_BITSIZE (GET_MODE (dest)) > HOST_BITS_PER_WIDE_INT
+      || (c1 & nonzero_bits (other, GET_MODE (dest))) != 0)
+    return x;
+
+  assign = make_extraction (VOIDmode, dest, pos, NULL_RTX, len, 1, 1, 0);
+  if (assign == 0)
+    return x;
+
+  /* The mode to use for the source is the mode of the assignment, or of
+     what is inside a possible STRICT_LOW_PART.  */
+  mode = (GET_CODE (assign) == STRICT_LOW_PART
+	  ? GET_MODE (XEXP (assign, 0)) : GET_MODE (assign));
+
+  /* Shift OTHER right POS places and make it the source, restricting it
+     to the proper length and mode.  */
+
+  src = force_to_mode (simplify_shift_const (NULL_RTX, LSHIFTRT,
+					     GET_MODE (src), other, pos),
+		       mode,
+		       GET_MODE_BITSIZE (mode) >= HOST_BITS_PER_WIDE_INT
+		       ? ~(unsigned HOST_WIDE_INT) 0
+		       : ((unsigned HOST_WIDE_INT) 1 << len) - 1,
+		       dest, 0);
+
+  /* If SRC is masked by an AND that does not make a difference in
+     the value being stored, strip it.  */
+  if (GET_CODE (assign) == ZERO_EXTRACT
+      && GET_CODE (XEXP (assign, 1)) == CONST_INT
+      && INTVAL (XEXP (assign, 1)) < HOST_BITS_PER_WIDE_INT
+      && GET_CODE (src) == AND
+      && GET_CODE (XEXP (src, 1)) == CONST_INT
+      && ((unsigned HOST_WIDE_INT) INTVAL (XEXP (src, 1))
+	  == ((unsigned HOST_WIDE_INT) 1 << INTVAL (XEXP (assign, 1))) - 1))
+    src = XEXP (src, 0);
+
+  return gen_rtx_SET (VOIDmode, assign, src);
+}
+
+/* See if X is of the form (+ (* a c) (* b c)) and convert to (* (+ a b) c)
+   if so.  */
+
+static rtx
+apply_distributive_law (rtx x)
+{
+  enum rtx_code code = GET_CODE (x);
+  enum rtx_code inner_code;
+  rtx lhs, rhs, other;
+  rtx tem;
+
+  /* Distributivity is not true for floating point as it can change the
+     value.  So we don't do it unless -funsafe-math-optimizations.  */
+  if (FLOAT_MODE_P (GET_MODE (x))
+      && ! flag_unsafe_math_optimizations)
+    return x;
+
+  /* The outer operation can only be one of the following:  */
+  if (code != IOR && code != AND && code != XOR
+      && code != PLUS && code != MINUS)
+    return x;
+
+  lhs = XEXP (x, 0);
+  rhs = XEXP (x, 1);
+
+  /* If either operand is a primitive we can't do anything, so get out
+     fast.  */
+  if (GET_RTX_CLASS (GET_CODE (lhs)) == 'o'
+      || GET_RTX_CLASS (GET_CODE (rhs)) == 'o')
+    return x;
+
+  lhs = expand_compound_operation (lhs);
+  rhs = expand_compound_operation (rhs);
+  inner_code = GET_CODE (lhs);
+  if (inner_code != GET_CODE (rhs))
+    return x;
+
+  /* See if the inner and outer operations distribute.  */
+  switch (inner_code)
+    {
+    case LSHIFTRT:
+    case ASHIFTRT:
+    case AND:
+    case IOR:
+      /* These all distribute except over PLUS.  */
+      if (code == PLUS || code == MINUS)
+	return x;
+      break;
+
+    case MULT:
+      if (code != PLUS && code != MINUS)
+	return x;
+      break;
+
+    case ASHIFT:
+      /* This is also a multiply, so it distributes over everything.  */
+      break;
+
+    case SUBREG:
+      /* Non-paradoxical SUBREGs distributes over all operations, provided
+	 the inner modes and byte offsets are the same, this is an extraction
+	 of a low-order part, we don't convert an fp operation to int or
+	 vice versa, and we would not be converting a single-word
+	 operation into a multi-word operation.  The latter test is not
+	 required, but it prevents generating unneeded multi-word operations.
+	 Some of the previous tests are redundant given the latter test, but
+	 are retained because they are required for correctness.
+
+	 We produce the result slightly differently in this case.  */
+
+      if (GET_MODE (SUBREG_REG (lhs)) != GET_MODE (SUBREG_REG (rhs))
+	  || SUBREG_BYTE (lhs) != SUBREG_BYTE (rhs)
+	  || ! subreg_lowpart_p (lhs)
+	  || (GET_MODE_CLASS (GET_MODE (lhs))
+	      != GET_MODE_CLASS (GET_MODE (SUBREG_REG (lhs))))
+	  || (GET_MODE_SIZE (GET_MODE (lhs))
+	      > GET_MODE_SIZE (GET_MODE (SUBREG_REG (lhs))))
+	  || GET_MODE_SIZE (GET_MODE (SUBREG_REG (lhs))) > UNITS_PER_WORD)
+	return x;
+
+      tem = gen_binary (code, GET_MODE (SUBREG_REG (lhs)),
+			SUBREG_REG (lhs), SUBREG_REG (rhs));
+      return gen_lowpart_for_combine (GET_MODE (x), tem);
+
+    default:
+      return x;
+    }
+
+  /* Set LHS and RHS to the inner operands (A and B in the example
+     above) and set OTHER to the common operand (C in the example).
+     These is only one way to do this unless the inner operation is
+     commutative.  */
+  if (GET_RTX_CLASS (inner_code) == 'c'
+      && rtx_equal_p (XEXP (lhs, 0), XEXP (rhs, 0)))
+    other = XEXP (lhs, 0), lhs = XEXP (lhs, 1), rhs = XEXP (rhs, 1);
+  else if (GET_RTX_CLASS (inner_code) == 'c'
+	   && rtx_equal_p (XEXP (lhs, 0), XEXP (rhs, 1)))
+    other = XEXP (lhs, 0), lhs = XEXP (lhs, 1), rhs = XEXP (rhs, 0);
+  else if (GET_RTX_CLASS (inner_code) == 'c'
+	   && rtx_equal_p (XEXP (lhs, 1), XEXP (rhs, 0)))
+    other = XEXP (lhs, 1), lhs = XEXP (lhs, 0), rhs = XEXP (rhs, 1);
+  else if (rtx_equal_p (XEXP (lhs, 1), XEXP (rhs, 1)))
+    other = XEXP (lhs, 1), lhs = XEXP (lhs, 0), rhs = XEXP (rhs, 0);
+  else
+    return x;
+
+  /* Form the new inner operation, seeing if it simplifies first.  */
+  tem = gen_binary (code, GET_MODE (x), lhs, rhs);
+
+  /* There is one exception to the general way of distributing:
+     (a | c) ^ (b | c) -> (a ^ b) & ~c  */
+  if (code == XOR && inner_code == IOR)
+    {
+      inner_code = AND;
+      other = simplify_gen_unary (NOT, GET_MODE (x), other, GET_MODE (x));
+    }
+
+  /* We may be able to continuing distributing the result, so call
+     ourselves recursively on the inner operation before forming the
+     outer operation, which we return.  */
+  return gen_binary (inner_code, GET_MODE (x),
+		     apply_distributive_law (tem), other);
+}
+
+/* We have X, a logical `and' of VAROP with the constant CONSTOP, to be done
+   in MODE.
+
+   Return an equivalent form, if different from X.  Otherwise, return X.  If
+   X is zero, we are to always construct the equivalent form.  */
+
+static rtx
+simplify_and_const_int (rtx x, enum machine_mode mode, rtx varop,
+			unsigned HOST_WIDE_INT constop)
+{
+  unsigned HOST_WIDE_INT nonzero;
+  int i;
+
+  /* Simplify VAROP knowing that we will be only looking at some of the
+     bits in it.
+
+     Note by passing in CONSTOP, we guarantee that the bits not set in
+     CONSTOP are not significant and will never be examined.  We must
+     ensure that is the case by explicitly masking out those bits
+     before returning.  */
+  varop = force_to_mode (varop, mode, constop, NULL_RTX, 0);
+
+  /* If VAROP is a CLOBBER, we will fail so return it.  */
+  if (GET_CODE (varop) == CLOBBER)
+    return varop;
+
+  /* If VAROP is a CONST_INT, then we need to apply the mask in CONSTOP
+     to VAROP and return the new constant.  */
+  if (GET_CODE (varop) == CONST_INT)
+    return GEN_INT (trunc_int_for_mode (INTVAL (varop) & constop, mode));
+
+  /* See what bits may be nonzero in VAROP.  Unlike the general case of
+     a call to nonzero_bits, here we don't care about bits outside
+     MODE.  */
+
+  nonzero = nonzero_bits (varop, mode) & GET_MODE_MASK (mode);
+
+  /* Turn off all bits in the constant that are known to already be zero.
+     Thus, if the AND isn't needed at all, we will have CONSTOP == NONZERO_BITS
+     which is tested below.  */
+
+  constop &= nonzero;
+
+  /* If we don't have any bits left, return zero.  */
+  if (constop == 0)
+    return const0_rtx;
+
+  /* If VAROP is a NEG of something known to be zero or 1 and CONSTOP is
+     a power of two, we can replace this with an ASHIFT.  */
+  if (GET_CODE (varop) == NEG && nonzero_bits (XEXP (varop, 0), mode) == 1
+      && (i = exact_log2 (constop)) >= 0)
+    return simplify_shift_const (NULL_RTX, ASHIFT, mode, XEXP (varop, 0), i);
+
+  /* If VAROP is an IOR or XOR, apply the AND to both branches of the IOR
+     or XOR, then try to apply the distributive law.  This may eliminate
+     operations if either branch can be simplified because of the AND.
+     It may also make some cases more complex, but those cases probably
+     won't match a pattern either with or without this.  */
+
+  if (GET_CODE (varop) == IOR || GET_CODE (varop) == XOR)
+    return
+      gen_lowpart_for_combine
+	(mode,
+	 apply_distributive_law
+	 (gen_binary (GET_CODE (varop), GET_MODE (varop),
+		      simplify_and_const_int (NULL_RTX, GET_MODE (varop),
+					      XEXP (varop, 0), constop),
+		      simplify_and_const_int (NULL_RTX, GET_MODE (varop),
+					      XEXP (varop, 1), constop))));
+
+  /* If VAROP is PLUS, and the constant is a mask of low bite, distribute
+     the AND and see if one of the operands simplifies to zero.  If so, we
+     may eliminate it.  */
+
+  if (GET_CODE (varop) == PLUS
+      && exact_log2 (constop + 1) >= 0)
+    {
+      rtx o0, o1;
+
+      o0 = simplify_and_const_int (NULL_RTX, mode, XEXP (varop, 0), constop);
+      o1 = simplify_and_const_int (NULL_RTX, mode, XEXP (varop, 1), constop);
+      if (o0 == const0_rtx)
+	return o1;
+      if (o1 == const0_rtx)
+	return o0;
+    }
+
+  /* Get VAROP in MODE.  Try to get a SUBREG if not.  Don't make a new SUBREG
+     if we already had one (just check for the simplest cases).  */
+  if (x && GET_CODE (XEXP (x, 0)) == SUBREG
+      && GET_MODE (XEXP (x, 0)) == mode
+      && SUBREG_REG (XEXP (x, 0)) == varop)
+    varop = XEXP (x, 0);
+  else
+    varop = gen_lowpart_for_combine (mode, varop);
+
+  /* If we can't make the SUBREG, try to return what we were given.  */
+  if (GET_CODE (varop) == CLOBBER)
+    return x ? x : varop;
+
+  /* If we are only masking insignificant bits, return VAROP.  */
+  if (constop == nonzero)
+    x = varop;
+  else
+    {
+      /* Otherwise, return an AND.  */
+      constop = trunc_int_for_mode (constop, mode);
+      /* See how much, if any, of X we can use.  */
+      if (x == 0 || GET_CODE (x) != AND || GET_MODE (x) != mode)
+	x = gen_binary (AND, mode, varop, GEN_INT (constop));
+
+      else
+	{
+	  if (GET_CODE (XEXP (x, 1)) != CONST_INT
+	      || (unsigned HOST_WIDE_INT) INTVAL (XEXP (x, 1)) != constop)
+	    SUBST (XEXP (x, 1), GEN_INT (constop));
+
+	  SUBST (XEXP (x, 0), varop);
+	}
+    }
+
+  return x;
+}
+
+#define nonzero_bits_with_known(X, MODE) \
+  cached_nonzero_bits (X, MODE, known_x, known_mode, known_ret)
+
+/* The function cached_nonzero_bits is a wrapper around nonzero_bits1.
+   It avoids exponential behavior in nonzero_bits1 when X has
+   identical subexpressions on the first or the second level.  */
+
+static unsigned HOST_WIDE_INT
+cached_nonzero_bits (rtx x, enum machine_mode mode, rtx known_x,
+		     enum machine_mode known_mode,
+		     unsigned HOST_WIDE_INT known_ret)
+{
+  if (x == known_x && mode == known_mode)
+    return known_ret;
+
+  /* Try to find identical subexpressions.  If found call
+     nonzero_bits1 on X with the subexpressions as KNOWN_X and the
+     precomputed value for the subexpression as KNOWN_RET.  */
+
+  if (GET_RTX_CLASS (GET_CODE (x)) == '2'
+      || GET_RTX_CLASS (GET_CODE (x)) == 'c')
+    {
+      rtx x0 = XEXP (x, 0);
+      rtx x1 = XEXP (x, 1);
+
+      /* Check the first level.  */
+      if (x0 == x1)
+	return nonzero_bits1 (x, mode, x0, mode,
+			      nonzero_bits_with_known (x0, mode));
+
+      /* Check the second level.  */
+      if ((GET_RTX_CLASS (GET_CODE (x0)) == '2'
+	   || GET_RTX_CLASS (GET_CODE (x0)) == 'c')
+	  && (x1 == XEXP (x0, 0) || x1 == XEXP (x0, 1)))
+	return nonzero_bits1 (x, mode, x1, mode,
+			      nonzero_bits_with_known (x1, mode));
+
+      if ((GET_RTX_CLASS (GET_CODE (x1)) == '2'
+	   || GET_RTX_CLASS (GET_CODE (x1)) == 'c')
+	  && (x0 == XEXP (x1, 0) || x0 == XEXP (x1, 1)))
+	return nonzero_bits1 (x, mode, x0, mode,
+			 nonzero_bits_with_known (x0, mode));
+    }
+
+  return nonzero_bits1 (x, mode, known_x, known_mode, known_ret);
+}
+
+/* We let num_sign_bit_copies recur into nonzero_bits as that is useful.
+   We don't let nonzero_bits recur into num_sign_bit_copies, because that
+   is less useful.  We can't allow both, because that results in exponential
+   run time recursion.  There is a nullstone testcase that triggered
+   this.  This macro avoids accidental uses of num_sign_bit_copies.  */
+#define cached_num_sign_bit_copies()
+
+/* Given an expression, X, compute which bits in X can be nonzero.
+   We don't care about bits outside of those defined in MODE.
+
+   For most X this is simply GET_MODE_MASK (GET_MODE (MODE)), but if X is
+   a shift, AND, or zero_extract, we can do better.  */
+
+static unsigned HOST_WIDE_INT
+nonzero_bits1 (rtx x, enum machine_mode mode, rtx known_x,
+	       enum machine_mode known_mode,
+	       unsigned HOST_WIDE_INT known_ret)
+{
+  unsigned HOST_WIDE_INT nonzero = GET_MODE_MASK (mode);
+  unsigned HOST_WIDE_INT inner_nz;
+  enum rtx_code code;
+  unsigned int mode_width = GET_MODE_BITSIZE (mode);
+  rtx tem;
+
+  /* For floating-point values, assume all bits are needed.  */
+  if (FLOAT_MODE_P (GET_MODE (x)) || FLOAT_MODE_P (mode))
+    return nonzero;
+
+  /* If X is wider than MODE, use its mode instead.  */
+  if (GET_MODE_BITSIZE (GET_MODE (x)) > mode_width)
+    {
+      mode = GET_MODE (x);
+      nonzero = GET_MODE_MASK (mode);
+      mode_width = GET_MODE_BITSIZE (mode);
+    }
+
+  if (mode_width > HOST_BITS_PER_WIDE_INT)
+    /* Our only callers in this case look for single bit values.  So
+       just return the mode mask.  Those tests will then be false.  */
+    return nonzero;
+
+#ifndef WORD_REGISTER_OPERATIONS
+  /* If MODE is wider than X, but both are a single word for both the host
+     and target machines, we can compute this from which bits of the
+     object might be nonzero in its own mode, taking into account the fact
+     that on many CISC machines, accessing an object in a wider mode
+     causes the high-order bits to become undefined.  So they are
+     not known to be zero.  */
+
+  if (GET_MODE (x) != VOIDmode && GET_MODE (x) != mode
+      && GET_MODE_BITSIZE (GET_MODE (x)) <= BITS_PER_WORD
+      && GET_MODE_BITSIZE (GET_MODE (x)) <= HOST_BITS_PER_WIDE_INT
+      && GET_MODE_BITSIZE (mode) > GET_MODE_BITSIZE (GET_MODE (x)))
+    {
+      nonzero &= nonzero_bits_with_known (x, GET_MODE (x));
+      nonzero |= GET_MODE_MASK (mode) & ~GET_MODE_MASK (GET_MODE (x));
+      return nonzero;
+    }
+#endif
+
+  code = GET_CODE (x);
+  switch (code)
+    {
+    case REG:
+#if defined(POINTERS_EXTEND_UNSIGNED) && !defined(HAVE_ptr_extend)
+      /* If pointers extend unsigned and this is a pointer in Pmode, say that
+	 all the bits above ptr_mode are known to be zero.  */
+      if (POINTERS_EXTEND_UNSIGNED && GET_MODE (x) == Pmode
+	  && REG_POINTER (x))
+	nonzero &= GET_MODE_MASK (ptr_mode);
+#endif
+
+      /* Include declared information about alignment of pointers.  */
+      /* ??? We don't properly preserve REG_POINTER changes across
+	 pointer-to-integer casts, so we can't trust it except for
+	 things that we know must be pointers.  See execute/960116-1.c.  */
+      if ((x == stack_pointer_rtx
+	   || x == frame_pointer_rtx
+	   || x == arg_pointer_rtx)
+	  && REGNO_POINTER_ALIGN (REGNO (x)))
+	{
+	  unsigned HOST_WIDE_INT alignment
+	    = REGNO_POINTER_ALIGN (REGNO (x)) / BITS_PER_UNIT;
+
+#ifdef PUSH_ROUNDING
+	  /* If PUSH_ROUNDING is defined, it is possible for the
+	     stack to be momentarily aligned only to that amount,
+	     so we pick the least alignment.  */
+	  if (x == stack_pointer_rtx && PUSH_ARGS)
+	    alignment = MIN ((unsigned HOST_WIDE_INT) PUSH_ROUNDING (1),
+			     alignment);
+#endif
+
+	  nonzero &= ~(alignment - 1);
+	}
+
+      /* If X is a register whose nonzero bits value is current, use it.
+	 Otherwise, if X is a register whose value we can find, use that
+	 value.  Otherwise, use the previously-computed global nonzero bits
+	 for this register.  */
+
+      if (reg_last_set_value[REGNO (x)] != 0
+	  && (reg_last_set_mode[REGNO (x)] == mode
+	      || (GET_MODE_CLASS (reg_last_set_mode[REGNO (x)]) == MODE_INT
+		  && GET_MODE_CLASS (mode) == MODE_INT))
+	  && (reg_last_set_label[REGNO (x)] == label_tick
+	      || (REGNO (x) >= FIRST_PSEUDO_REGISTER
+		  && REG_N_SETS (REGNO (x)) == 1
+		  && ! REGNO_REG_SET_P (ENTRY_BLOCK_PTR->next_bb->global_live_at_start,
+					REGNO (x))))
+	  && INSN_CUID (reg_last_set[REGNO (x)]) < subst_low_cuid)
+	return reg_last_set_nonzero_bits[REGNO (x)] & nonzero;
+
+      tem = get_last_value (x);
+
+      if (tem)
+	{
+#ifdef SHORT_IMMEDIATES_SIGN_EXTEND
+	  /* If X is narrower than MODE and TEM is a non-negative
+	     constant that would appear negative in the mode of X,
+	     sign-extend it for use in reg_nonzero_bits because some
+	     machines (maybe most) will actually do the sign-extension
+	     and this is the conservative approach.
+
+	     ??? For 2.5, try to tighten up the MD files in this regard
+	     instead of this kludge.  */
+
+	  if (GET_MODE_BITSIZE (GET_MODE (x)) < mode_width
+	      && GET_CODE (tem) == CONST_INT
+	      && INTVAL (tem) > 0
+	      && 0 != (INTVAL (tem)
+		       & ((HOST_WIDE_INT) 1
+			  << (GET_MODE_BITSIZE (GET_MODE (x)) - 1))))
+	    tem = GEN_INT (INTVAL (tem)
+			   | ((HOST_WIDE_INT) (-1)
+			      << GET_MODE_BITSIZE (GET_MODE (x))));
+#endif
+	  return nonzero_bits_with_known (tem, mode) & nonzero;
+	}
+      else if (nonzero_sign_valid && reg_nonzero_bits[REGNO (x)])
+	{
+	  unsigned HOST_WIDE_INT mask = reg_nonzero_bits[REGNO (x)];
+
+	  if (GET_MODE_BITSIZE (GET_MODE (x)) < mode_width)
+	    /* We don't know anything about the upper bits.  */
+	    mask |= GET_MODE_MASK (mode) ^ GET_MODE_MASK (GET_MODE (x));
+	  return nonzero & mask;
+	}
+      else
+	return nonzero;
+
+    case CONST_INT:
+#ifdef SHORT_IMMEDIATES_SIGN_EXTEND
+      /* If X is negative in MODE, sign-extend the value.  */
+      if (INTVAL (x) > 0 && mode_width < BITS_PER_WORD
+	  && 0 != (INTVAL (x) & ((HOST_WIDE_INT) 1 << (mode_width - 1))))
+	return (INTVAL (x) | ((HOST_WIDE_INT) (-1) << mode_width));
+#endif
+
+      return INTVAL (x);
+
+    case MEM:
+#ifdef LOAD_EXTEND_OP
+      /* In many, if not most, RISC machines, reading a byte from memory
+	 zeros the rest of the register.  Noticing that fact saves a lot
+	 of extra zero-extends.  */
+      if (LOAD_EXTEND_OP (GET_MODE (x)) == ZERO_EXTEND)
+	nonzero &= GET_MODE_MASK (GET_MODE (x));
+#endif
+      break;
+
+    case EQ:  case NE:
+    case UNEQ:  case LTGT:
+    case GT:  case GTU:  case UNGT:
+    case LT:  case LTU:  case UNLT:
+    case GE:  case GEU:  case UNGE:
+    case LE:  case LEU:  case UNLE:
+    case UNORDERED: case ORDERED:
+
+      /* If this produces an integer result, we know which bits are set.
+	 Code here used to clear bits outside the mode of X, but that is
+	 now done above.  */
+
+      if (GET_MODE_CLASS (mode) == MODE_INT
+	  && mode_width <= HOST_BITS_PER_WIDE_INT)
+	nonzero = STORE_FLAG_VALUE;
+      break;
+
+    case NEG:
+#if 0
+      /* Disabled to avoid exponential mutual recursion between nonzero_bits
+	 and num_sign_bit_copies.  */
+      if (num_sign_bit_copies (XEXP (x, 0), GET_MODE (x))
+	  == GET_MODE_BITSIZE (GET_MODE (x)))
+	nonzero = 1;
+#endif
+
+      if (GET_MODE_SIZE (GET_MODE (x)) < mode_width)
+	nonzero |= (GET_MODE_MASK (mode) & ~GET_MODE_MASK (GET_MODE (x)));
+      break;
+
+    case ABS:
+#if 0
+      /* Disabled to avoid exponential mutual recursion between nonzero_bits
+	 and num_sign_bit_copies.  */
+      if (num_sign_bit_copies (XEXP (x, 0), GET_MODE (x))
+	  == GET_MODE_BITSIZE (GET_MODE (x)))
+	nonzero = 1;
+#endif
+      break;
+
+    case TRUNCATE:
+      nonzero &= (nonzero_bits_with_known (XEXP (x, 0), mode)
+		  & GET_MODE_MASK (mode));
+      break;
+
+    case ZERO_EXTEND:
+      nonzero &= nonzero_bits_with_known (XEXP (x, 0), mode);
+      if (GET_MODE (XEXP (x, 0)) != VOIDmode)
+	nonzero &= GET_MODE_MASK (GET_MODE (XEXP (x, 0)));
+      break;
+
+    case SIGN_EXTEND:
+      /* If the sign bit is known clear, this is the same as ZERO_EXTEND.
+	 Otherwise, show all the bits in the outer mode but not the inner
+	 may be nonzero.  */
+      inner_nz = nonzero_bits_with_known (XEXP (x, 0), mode);
+      if (GET_MODE (XEXP (x, 0)) != VOIDmode)
+	{
+	  inner_nz &= GET_MODE_MASK (GET_MODE (XEXP (x, 0)));
+	  if (inner_nz
+	      & (((HOST_WIDE_INT) 1
+		  << (GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0))) - 1))))
+	    inner_nz |= (GET_MODE_MASK (mode)
+			 & ~GET_MODE_MASK (GET_MODE (XEXP (x, 0))));
+	}
+
+      nonzero &= inner_nz;
+      break;
+
+    case AND:
+      nonzero &= (nonzero_bits_with_known (XEXP (x, 0), mode)
+		  & nonzero_bits_with_known (XEXP (x, 1), mode));
+      break;
+
+    case XOR:   case IOR:
+    case UMIN:  case UMAX:  case SMIN:  case SMAX:
+      {
+	unsigned HOST_WIDE_INT nonzero0 =
+	  nonzero_bits_with_known (XEXP (x, 0), mode);
+
+	/* Don't call nonzero_bits for the second time if it cannot change
+	   anything.  */
+	if ((nonzero & nonzero0) != nonzero)
+	  nonzero &= (nonzero0
+		      | nonzero_bits_with_known (XEXP (x, 1), mode));
+      }
+      break;
+
+    case PLUS:  case MINUS:
+    case MULT:
+    case DIV:   case UDIV:
+    case MOD:   case UMOD:
+      /* We can apply the rules of arithmetic to compute the number of
+	 high- and low-order zero bits of these operations.  We start by
+	 computing the width (position of the highest-order nonzero bit)
+	 and the number of low-order zero bits for each value.  */
+      {
+	unsigned HOST_WIDE_INT nz0 =
+	  nonzero_bits_with_known (XEXP (x, 0), mode);
+	unsigned HOST_WIDE_INT nz1 =
+	  nonzero_bits_with_known (XEXP (x, 1), mode);
+	int sign_index = GET_MODE_BITSIZE (GET_MODE (x)) - 1;
+	int width0 = floor_log2 (nz0) + 1;
+	int width1 = floor_log2 (nz1) + 1;
+	int low0 = floor_log2 (nz0 & -nz0);
+	int low1 = floor_log2 (nz1 & -nz1);
+	HOST_WIDE_INT op0_maybe_minusp
+	  = (nz0 & ((HOST_WIDE_INT) 1 << sign_index));
+	HOST_WIDE_INT op1_maybe_minusp
+	  = (nz1 & ((HOST_WIDE_INT) 1 << sign_index));
+	unsigned int result_width = mode_width;
+	int result_low = 0;
+
+	switch (code)
+	  {
+	  case PLUS:
+	    result_width = MAX (width0, width1) + 1;
+	    result_low = MIN (low0, low1);
+	    break;
+	  case MINUS:
+	    result_low = MIN (low0, low1);
+	    break;
+	  case MULT:
+	    result_width = width0 + width1;
+	    result_low = low0 + low1;
+	    break;
+	  case DIV:
+	    if (width1 == 0)
+	      break;
+	    if (! op0_maybe_minusp && ! op1_maybe_minusp)
+	      result_width = width0;
+	    break;
+	  case UDIV:
+	    if (width1 == 0)
+	      break;
+	    result_width = width0;
+	    break;
+	  case MOD:
+	    if (width1 == 0)
+	      break;
+	    if (! op0_maybe_minusp && ! op1_maybe_minusp)
+	      result_width = MIN (width0, width1);
+	    result_low = MIN (low0, low1);
+	    break;
+	  case UMOD:
+	    if (width1 == 0)
+	      break;
+	    result_width = MIN (width0, width1);
+	    result_low = MIN (low0, low1);
+	    break;
+	  default:
+	    abort ();
+	  }
+
+	if (result_width < mode_width)
+	  nonzero &= ((HOST_WIDE_INT) 1 << result_width) - 1;
+
+	if (result_low > 0)
+	  nonzero &= ~(((HOST_WIDE_INT) 1 << result_low) - 1);
+
+#ifdef POINTERS_EXTEND_UNSIGNED
+	/* If pointers extend unsigned and this is an addition or subtraction
+	   to a pointer in Pmode, all the bits above ptr_mode are known to be
+	   zero.  */
+	if (POINTERS_EXTEND_UNSIGNED > 0 && GET_MODE (x) == Pmode
+	    && (code == PLUS || code == MINUS)
+	    && GET_CODE (XEXP (x, 0)) == REG && REG_POINTER (XEXP (x, 0)))
+	  nonzero &= GET_MODE_MASK (ptr_mode);
+#endif
+      }
+      break;
+
+    case ZERO_EXTRACT:
+      if (GET_CODE (XEXP (x, 1)) == CONST_INT
+	  && INTVAL (XEXP (x, 1)) < HOST_BITS_PER_WIDE_INT)
+	nonzero &= ((HOST_WIDE_INT) 1 << INTVAL (XEXP (x, 1))) - 1;
+      break;
+
+    case SUBREG:
+      /* If this is a SUBREG formed for a promoted variable that has
+	 been zero-extended, we know that at least the high-order bits
+	 are zero, though others might be too.  */
+
+      if (SUBREG_PROMOTED_VAR_P (x) && SUBREG_PROMOTED_UNSIGNED_P (x) > 0)
+	nonzero = (GET_MODE_MASK (GET_MODE (x))
+		   & nonzero_bits_with_known (SUBREG_REG (x), GET_MODE (x)));
+
+      /* If the inner mode is a single word for both the host and target
+	 machines, we can compute this from which bits of the inner
+	 object might be nonzero.  */
+      if (GET_MODE_BITSIZE (GET_MODE (SUBREG_REG (x))) <= BITS_PER_WORD
+	  && (GET_MODE_BITSIZE (GET_MODE (SUBREG_REG (x)))
+	      <= HOST_BITS_PER_WIDE_INT))
+	{
+	  nonzero &= nonzero_bits_with_known (SUBREG_REG (x), mode);
+
+#if defined (WORD_REGISTER_OPERATIONS) && defined (LOAD_EXTEND_OP)
+	  /* If this is a typical RISC machine, we only have to worry
+	     about the way loads are extended.  */
+	  if ((LOAD_EXTEND_OP (GET_MODE (SUBREG_REG (x))) == SIGN_EXTEND
+	       ? (((nonzero
+		    & (((unsigned HOST_WIDE_INT) 1
+			<< (GET_MODE_BITSIZE (GET_MODE (SUBREG_REG (x))) - 1))))
+		   != 0))
+	       : LOAD_EXTEND_OP (GET_MODE (SUBREG_REG (x))) != ZERO_EXTEND)
+	      || GET_CODE (SUBREG_REG (x)) != MEM)
+#endif
+	    {
+	      /* On many CISC machines, accessing an object in a wider mode
+		 causes the high-order bits to become undefined.  So they are
+		 not known to be zero.  */
+	      if (GET_MODE_SIZE (GET_MODE (x))
+		  > GET_MODE_SIZE (GET_MODE (SUBREG_REG (x))))
+		nonzero |= (GET_MODE_MASK (GET_MODE (x))
+			    & ~GET_MODE_MASK (GET_MODE (SUBREG_REG (x))));
+	    }
+	}
+      break;
+
+    case ASHIFTRT:
+    case LSHIFTRT:
+    case ASHIFT:
+    case ROTATE:
+      /* The nonzero bits are in two classes: any bits within MODE
+	 that aren't in GET_MODE (x) are always significant.  The rest of the
+	 nonzero bits are those that are significant in the operand of
+	 the shift when shifted the appropriate number of bits.  This
+	 shows that high-order bits are cleared by the right shift and
+	 low-order bits by left shifts.  */
+      if (GET_CODE (XEXP (x, 1)) == CONST_INT
+	  && INTVAL (XEXP (x, 1)) >= 0
+	  && INTVAL (XEXP (x, 1)) < HOST_BITS_PER_WIDE_INT)
+	{
+	  enum machine_mode inner_mode = GET_MODE (x);
+	  unsigned int width = GET_MODE_BITSIZE (inner_mode);
+	  int count = INTVAL (XEXP (x, 1));
+	  unsigned HOST_WIDE_INT mode_mask = GET_MODE_MASK (inner_mode);
+	  unsigned HOST_WIDE_INT op_nonzero =
+	    nonzero_bits_with_known (XEXP (x, 0), mode);
+	  unsigned HOST_WIDE_INT inner = op_nonzero & mode_mask;
+	  unsigned HOST_WIDE_INT outer = 0;
+
+	  if (mode_width > width)
+	    outer = (op_nonzero & nonzero & ~mode_mask);
+
+	  if (code == LSHIFTRT)
+	    inner >>= count;
+	  else if (code == ASHIFTRT)
+	    {
+	      inner >>= count;
+
+	      /* If the sign bit may have been nonzero before the shift, we
+		 need to mark all the places it could have been copied to
+		 by the shift as possibly nonzero.  */
+	      if (inner & ((HOST_WIDE_INT) 1 << (width - 1 - count)))
+		inner |= (((HOST_WIDE_INT) 1 << count) - 1) << (width - count);
+	    }
+	  else if (code == ASHIFT)
+	    inner <<= count;
+	  else
+	    inner = ((inner << (count % width)
+		      | (inner >> (width - (count % width)))) & mode_mask);
+
+	  nonzero &= (outer | inner);
+	}
+      break;
+
+    case FFS:
+    case POPCOUNT:
+      /* This is at most the number of bits in the mode.  */
+      nonzero = ((HOST_WIDE_INT) 2 << (floor_log2 (mode_width))) - 1;
+      break;
+
+    case CLZ:
+      /* If CLZ has a known value at zero, then the nonzero bits are
+	 that value, plus the number of bits in the mode minus one.  */
+      if (CLZ_DEFINED_VALUE_AT_ZERO (mode, nonzero))
+	nonzero |= ((HOST_WIDE_INT) 1 << (floor_log2 (mode_width))) - 1;
+      else
+	nonzero = -1;
+      break;
+
+    case CTZ:
+      /* If CTZ has a known value at zero, then the nonzero bits are
+	 that value, plus the number of bits in the mode minus one.  */
+      if (CTZ_DEFINED_VALUE_AT_ZERO (mode, nonzero))
+	nonzero |= ((HOST_WIDE_INT) 1 << (floor_log2 (mode_width))) - 1;
+      else
+	nonzero = -1;
+      break;
+
+    case PARITY:
+      nonzero = 1;
+      break;
+
+    case IF_THEN_ELSE:
+      nonzero &= (nonzero_bits_with_known (XEXP (x, 1), mode)
+		  | nonzero_bits_with_known (XEXP (x, 2), mode));
+      break;
+
+    default:
+      break;
+    }
+
+  return nonzero;
+}
+
+/* See the macro definition above.  */
+#undef cached_num_sign_bit_copies
+
+#define num_sign_bit_copies_with_known(X, M) \
+  cached_num_sign_bit_copies (X, M, known_x, known_mode, known_ret)
+
+/* The function cached_num_sign_bit_copies is a wrapper around
+   num_sign_bit_copies1.  It avoids exponential behavior in
+   num_sign_bit_copies1 when X has identical subexpressions on the
+   first or the second level.  */
+
+static unsigned int
+cached_num_sign_bit_copies (rtx x, enum machine_mode mode, rtx known_x,
+			    enum machine_mode known_mode,
+			    unsigned int known_ret)
+{
+  if (x == known_x && mode == known_mode)
+    return known_ret;
+
+  /* Try to find identical subexpressions.  If found call
+     num_sign_bit_copies1 on X with the subexpressions as KNOWN_X and
+     the precomputed value for the subexpression as KNOWN_RET.  */
+
+  if (GET_RTX_CLASS (GET_CODE (x)) == '2'
+      || GET_RTX_CLASS (GET_CODE (x)) == 'c')
+    {
+      rtx x0 = XEXP (x, 0);
+      rtx x1 = XEXP (x, 1);
+
+      /* Check the first level.  */
+      if (x0 == x1)
+	return
+	  num_sign_bit_copies1 (x, mode, x0, mode,
+				num_sign_bit_copies_with_known (x0, mode));
+
+      /* Check the second level.  */
+      if ((GET_RTX_CLASS (GET_CODE (x0)) == '2'
+	   || GET_RTX_CLASS (GET_CODE (x0)) == 'c')
+	  && (x1 == XEXP (x0, 0) || x1 == XEXP (x0, 1)))
+	return
+	  num_sign_bit_copies1 (x, mode, x1, mode,
+				num_sign_bit_copies_with_known (x1, mode));
+
+      if ((GET_RTX_CLASS (GET_CODE (x1)) == '2'
+	   || GET_RTX_CLASS (GET_CODE (x1)) == 'c')
+	  && (x0 == XEXP (x1, 0) || x0 == XEXP (x1, 1)))
+	return
+	  num_sign_bit_copies1 (x, mode, x0, mode,
+				num_sign_bit_copies_with_known (x0, mode));
+    }
+
+  return num_sign_bit_copies1 (x, mode, known_x, known_mode, known_ret);
+}
+
+/* Return the number of bits at the high-order end of X that are known to
+   be equal to the sign bit.  X will be used in mode MODE; if MODE is
+   VOIDmode, X will be used in its own mode.  The returned value  will always
+   be between 1 and the number of bits in MODE.  */
+
+static unsigned int
+num_sign_bit_copies1 (rtx x, enum machine_mode mode, rtx known_x,
+		      enum machine_mode known_mode,
+		      unsigned int known_ret)
+{
+  enum rtx_code code = GET_CODE (x);
+  unsigned int bitwidth;
+  int num0, num1, result;
+  unsigned HOST_WIDE_INT nonzero;
+  rtx tem;
+
+  /* If we weren't given a mode, use the mode of X.  If the mode is still
+     VOIDmode, we don't know anything.  Likewise if one of the modes is
+     floating-point.  */
+
+  if (mode == VOIDmode)
+    mode = GET_MODE (x);
+
+  if (mode == VOIDmode || FLOAT_MODE_P (mode) || FLOAT_MODE_P (GET_MODE (x)))
+    return 1;
+
+  bitwidth = GET_MODE_BITSIZE (mode);
+
+  /* For a smaller object, just ignore the high bits.  */
+  if (bitwidth < GET_MODE_BITSIZE (GET_MODE (x)))
+    {
+      num0 = num_sign_bit_copies_with_known (x, GET_MODE (x));
+      return MAX (1,
+		  num0 - (int) (GET_MODE_BITSIZE (GET_MODE (x)) - bitwidth));
+    }
+
+  if (GET_MODE (x) != VOIDmode && bitwidth > GET_MODE_BITSIZE (GET_MODE (x)))
+    {
+#ifndef WORD_REGISTER_OPERATIONS
+  /* If this machine does not do all register operations on the entire
+     register and MODE is wider than the mode of X, we can say nothing
+     at all about the high-order bits.  */
+      return 1;
+#else
+      /* Likewise on machines that do, if the mode of the object is smaller
+	 than a word and loads of that size don't sign extend, we can say
+	 nothing about the high order bits.  */
+      if (GET_MODE_BITSIZE (GET_MODE (x)) < BITS_PER_WORD
+#ifdef LOAD_EXTEND_OP
+	  && LOAD_EXTEND_OP (GET_MODE (x)) != SIGN_EXTEND
+#endif
+	  )
+	return 1;
+#endif
+    }
+
+  switch (code)
+    {
+    case REG:
+
+#if defined(POINTERS_EXTEND_UNSIGNED) && !defined(HAVE_ptr_extend)
+      /* If pointers extend signed and this is a pointer in Pmode, say that
+	 all the bits above ptr_mode are known to be sign bit copies.  */
+      if (! POINTERS_EXTEND_UNSIGNED && GET_MODE (x) == Pmode && mode == Pmode
+	  && REG_POINTER (x))
+	return GET_MODE_BITSIZE (Pmode) - GET_MODE_BITSIZE (ptr_mode) + 1;
+#endif
+
+      if (reg_last_set_value[REGNO (x)] != 0
+	  && reg_last_set_mode[REGNO (x)] == mode
+	  && (reg_last_set_label[REGNO (x)] == label_tick
+	      || (REGNO (x) >= FIRST_PSEUDO_REGISTER
+		  && REG_N_SETS (REGNO (x)) == 1
+		  && ! REGNO_REG_SET_P (ENTRY_BLOCK_PTR->next_bb->global_live_at_start,
+					REGNO (x))))
+	  && INSN_CUID (reg_last_set[REGNO (x)]) < subst_low_cuid)
+	return reg_last_set_sign_bit_copies[REGNO (x)];
+
+      tem = get_last_value (x);
+      if (tem != 0)
+	return num_sign_bit_copies_with_known (tem, mode);
+
+      if (nonzero_sign_valid && reg_sign_bit_copies[REGNO (x)] != 0
+	  && GET_MODE_BITSIZE (GET_MODE (x)) == bitwidth)
+	return reg_sign_bit_copies[REGNO (x)];
+      break;
+
+    case MEM:
+#ifdef LOAD_EXTEND_OP
+      /* Some RISC machines sign-extend all loads of smaller than a word.  */
+      if (LOAD_EXTEND_OP (GET_MODE (x)) == SIGN_EXTEND)
+	return MAX (1, ((int) bitwidth
+			- (int) GET_MODE_BITSIZE (GET_MODE (x)) + 1));
+#endif
+      break;
+
+    case CONST_INT:
+      /* If the constant is negative, take its 1's complement and remask.
+	 Then see how many zero bits we have.  */
+      nonzero = INTVAL (x) & GET_MODE_MASK (mode);
+      if (bitwidth <= HOST_BITS_PER_WIDE_INT
+	  && (nonzero & ((HOST_WIDE_INT) 1 << (bitwidth - 1))) != 0)
+	nonzero = (~nonzero) & GET_MODE_MASK (mode);
+
+      return (nonzero == 0 ? bitwidth : bitwidth - floor_log2 (nonzero) - 1);
+
+    case SUBREG:
+      /* If this is a SUBREG for a promoted object that is sign-extended
+	 and we are looking at it in a wider mode, we know that at least the
+	 high-order bits are known to be sign bit copies.  */
+
+      if (SUBREG_PROMOTED_VAR_P (x) && ! SUBREG_PROMOTED_UNSIGNED_P (x))
+	{
+	  num0 = num_sign_bit_copies_with_known (SUBREG_REG (x), mode);
+	  return MAX ((int) bitwidth
+		      - (int) GET_MODE_BITSIZE (GET_MODE (x)) + 1,
+		      num0);
+	}
+
+      /* For a smaller object, just ignore the high bits.  */
+      if (bitwidth <= GET_MODE_BITSIZE (GET_MODE (SUBREG_REG (x))))
+	{
+	  num0 = num_sign_bit_copies_with_known (SUBREG_REG (x), VOIDmode);
+	  return MAX (1, (num0
+			  - (int) (GET_MODE_BITSIZE (GET_MODE (SUBREG_REG (x)))
+				   - bitwidth)));
+	}
+
+#ifdef WORD_REGISTER_OPERATIONS
+#ifdef LOAD_EXTEND_OP
+      /* For paradoxical SUBREGs on machines where all register operations
+	 affect the entire register, just look inside.  Note that we are
+	 passing MODE to the recursive call, so the number of sign bit copies
+	 will remain relative to that mode, not the inner mode.  */
+
+      /* This works only if loads sign extend.  Otherwise, if we get a
+	 reload for the inner part, it may be loaded from the stack, and
+	 then we lose all sign bit copies that existed before the store
+	 to the stack.  */
+
+      if ((GET_MODE_SIZE (GET_MODE (x))
+	   > GET_MODE_SIZE (GET_MODE (SUBREG_REG (x))))
+	  && LOAD_EXTEND_OP (GET_MODE (SUBREG_REG (x))) == SIGN_EXTEND
+	  && GET_CODE (SUBREG_REG (x)) == MEM)
+	return num_sign_bit_copies_with_known (SUBREG_REG (x), mode);
+#endif
+#endif
+      break;
+
+    case SIGN_EXTRACT:
+      if (GET_CODE (XEXP (x, 1)) == CONST_INT)
+	return MAX (1, (int) bitwidth - INTVAL (XEXP (x, 1)));
+      break;
+
+    case SIGN_EXTEND:
+      return (bitwidth - GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0)))
+	      + num_sign_bit_copies_with_known (XEXP (x, 0), VOIDmode));
+
+    case TRUNCATE:
+      /* For a smaller object, just ignore the high bits.  */
+      num0 = num_sign_bit_copies_with_known (XEXP (x, 0), VOIDmode);
+      return MAX (1, (num0 - (int) (GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0)))
+				    - bitwidth)));
+
+    case NOT:
+      return num_sign_bit_copies_with_known (XEXP (x, 0), mode);
+
+    case ROTATE:       case ROTATERT:
+      /* If we are rotating left by a number of bits less than the number
+	 of sign bit copies, we can just subtract that amount from the
+	 number.  */
+      if (GET_CODE (XEXP (x, 1)) == CONST_INT
+	  && INTVAL (XEXP (x, 1)) >= 0
+	  && INTVAL (XEXP (x, 1)) < (int) bitwidth)
+	{
+	  num0 = num_sign_bit_copies_with_known (XEXP (x, 0), mode);
+	  return MAX (1, num0 - (code == ROTATE ? INTVAL (XEXP (x, 1))
+				 : (int) bitwidth - INTVAL (XEXP (x, 1))));
+	}
+      break;
+
+    case NEG:
+      /* In general, this subtracts one sign bit copy.  But if the value
+	 is known to be positive, the number of sign bit copies is the
+	 same as that of the input.  Finally, if the input has just one bit
+	 that might be nonzero, all the bits are copies of the sign bit.  */
+      num0 = num_sign_bit_copies_with_known (XEXP (x, 0), mode);
+      if (bitwidth > HOST_BITS_PER_WIDE_INT)
+	return num0 > 1 ? num0 - 1 : 1;
+
+      nonzero = nonzero_bits (XEXP (x, 0), mode);
+      if (nonzero == 1)
+	return bitwidth;
+
+      if (num0 > 1
+	  && (((HOST_WIDE_INT) 1 << (bitwidth - 1)) & nonzero))
+	num0--;
+
+      return num0;
+
+    case IOR:   case AND:   case XOR:
+    case SMIN:  case SMAX:  case UMIN:  case UMAX:
+      /* Logical operations will preserve the number of sign-bit copies.
+	 MIN and MAX operations always return one of the operands.  */
+      num0 = num_sign_bit_copies_with_known (XEXP (x, 0), mode);
+      num1 = num_sign_bit_copies_with_known (XEXP (x, 1), mode);
+      return MIN (num0, num1);
+
+    case PLUS:  case MINUS:
+      /* For addition and subtraction, we can have a 1-bit carry.  However,
+	 if we are subtracting 1 from a positive number, there will not
+	 be such a carry.  Furthermore, if the positive number is known to
+	 be 0 or 1, we know the result is either -1 or 0.  */
+
+      if (code == PLUS && XEXP (x, 1) == constm1_rtx
+	  && bitwidth <= HOST_BITS_PER_WIDE_INT)
+	{
+	  nonzero = nonzero_bits (XEXP (x, 0), mode);
+	  if ((((HOST_WIDE_INT) 1 << (bitwidth - 1)) & nonzero) == 0)
+	    return (nonzero == 1 || nonzero == 0 ? bitwidth
+		    : bitwidth - floor_log2 (nonzero) - 1);
+	}
+
+      num0 = num_sign_bit_copies_with_known (XEXP (x, 0), mode);
+      num1 = num_sign_bit_copies_with_known (XEXP (x, 1), mode);
+      result = MAX (1, MIN (num0, num1) - 1);
+
+#ifdef POINTERS_EXTEND_UNSIGNED
+      /* If pointers extend signed and this is an addition or subtraction
+	 to a pointer in Pmode, all the bits above ptr_mode are known to be
+	 sign bit copies.  */
+      if (! POINTERS_EXTEND_UNSIGNED && GET_MODE (x) == Pmode
+	  && (code == PLUS || code == MINUS)
+	  && GET_CODE (XEXP (x, 0)) == REG && REG_POINTER (XEXP (x, 0)))
+	result = MAX ((int) (GET_MODE_BITSIZE (Pmode)
+			     - GET_MODE_BITSIZE (ptr_mode) + 1),
+		      result);
+#endif
+      return result;
+
+    case MULT:
+      /* The number of bits of the product is the sum of the number of
+	 bits of both terms.  However, unless one of the terms if known
+	 to be positive, we must allow for an additional bit since negating
+	 a negative number can remove one sign bit copy.  */
+
+      num0 = num_sign_bit_copies_with_known (XEXP (x, 0), mode);
+      num1 = num_sign_bit_copies_with_known (XEXP (x, 1), mode);
+
+      result = bitwidth - (bitwidth - num0) - (bitwidth - num1);
+      if (result > 0
+	  && (bitwidth > HOST_BITS_PER_WIDE_INT
+	      || (((nonzero_bits (XEXP (x, 0), mode)
+		    & ((HOST_WIDE_INT) 1 << (bitwidth - 1))) != 0)
+		  && ((nonzero_bits (XEXP (x, 1), mode)
+		       & ((HOST_WIDE_INT) 1 << (bitwidth - 1))) != 0))))
+	result--;
+
+      return MAX (1, result);
+
+    case UDIV:
+      /* The result must be <= the first operand.  If the first operand
+         has the high bit set, we know nothing about the number of sign
+         bit copies.  */
+      if (bitwidth > HOST_BITS_PER_WIDE_INT)
+	return 1;
+      else if ((nonzero_bits (XEXP (x, 0), mode)
+		& ((HOST_WIDE_INT) 1 << (bitwidth - 1))) != 0)
+	return 1;
+      else
+	return num_sign_bit_copies_with_known (XEXP (x, 0), mode);
+
+    case UMOD:
+      /* The result must be <= the second operand.  */
+      return num_sign_bit_copies_with_known (XEXP (x, 1), mode);
+
+    case DIV:
+      /* Similar to unsigned division, except that we have to worry about
+	 the case where the divisor is negative, in which case we have
+	 to add 1.  */
+      result = num_sign_bit_copies_with_known (XEXP (x, 0), mode);
+      if (result > 1
+	  && (bitwidth > HOST_BITS_PER_WIDE_INT
+	      || (nonzero_bits (XEXP (x, 1), mode)
+		  & ((HOST_WIDE_INT) 1 << (bitwidth - 1))) != 0))
+	result--;
+
+      return result;
+
+    case MOD:
+      result = num_sign_bit_copies_with_known (XEXP (x, 1), mode);
+      if (result > 1
+	  && (bitwidth > HOST_BITS_PER_WIDE_INT
+	      || (nonzero_bits (XEXP (x, 1), mode)
+		  & ((HOST_WIDE_INT) 1 << (bitwidth - 1))) != 0))
+	result--;
+
+      return result;
+
+    case ASHIFTRT:
+      /* Shifts by a constant add to the number of bits equal to the
+	 sign bit.  */
+      num0 = num_sign_bit_copies_with_known (XEXP (x, 0), mode);
+      if (GET_CODE (XEXP (x, 1)) == CONST_INT
+	  && INTVAL (XEXP (x, 1)) > 0)
+	num0 = MIN ((int) bitwidth, num0 + INTVAL (XEXP (x, 1)));
+
+      return num0;
+
+    case ASHIFT:
+      /* Left shifts destroy copies.  */
+      if (GET_CODE (XEXP (x, 1)) != CONST_INT
+	  || INTVAL (XEXP (x, 1)) < 0
+	  || INTVAL (XEXP (x, 1)) >= (int) bitwidth)
+	return 1;
+
+      num0 = num_sign_bit_copies_with_known (XEXP (x, 0), mode);
+      return MAX (1, num0 - INTVAL (XEXP (x, 1)));
+
+    case IF_THEN_ELSE:
+      num0 = num_sign_bit_copies_with_known (XEXP (x, 1), mode);
+      num1 = num_sign_bit_copies_with_known (XEXP (x, 2), mode);
+      return MIN (num0, num1);
+
+    case EQ:  case NE:  case GE:  case GT:  case LE:  case LT:
+    case UNEQ:  case LTGT:  case UNGE:  case UNGT:  case UNLE:  case UNLT:
+    case GEU: case GTU: case LEU: case LTU:
+    case UNORDERED: case ORDERED:
+      /* If the constant is negative, take its 1's complement and remask.
+	 Then see how many zero bits we have.  */
+      nonzero = STORE_FLAG_VALUE;
+      if (bitwidth <= HOST_BITS_PER_WIDE_INT
+	  && (nonzero & ((HOST_WIDE_INT) 1 << (bitwidth - 1))) != 0)
+	nonzero = (~nonzero) & GET_MODE_MASK (mode);
+
+      return (nonzero == 0 ? bitwidth : bitwidth - floor_log2 (nonzero) - 1);
+      break;
+
+    default:
+      break;
+    }
+
+  /* If we haven't been able to figure it out by one of the above rules,
+     see if some of the high-order bits are known to be zero.  If so,
+     count those bits and return one less than that amount.  If we can't
+     safely compute the mask for this mode, always return BITWIDTH.  */
+
+  if (bitwidth > HOST_BITS_PER_WIDE_INT)
+    return 1;
+
+  nonzero = nonzero_bits (x, mode);
+  return (nonzero & ((HOST_WIDE_INT) 1 << (bitwidth - 1))
+	  ? 1 : bitwidth - floor_log2 (nonzero) - 1);
+}
+
+/* Return the number of "extended" bits there are in X, when interpreted
+   as a quantity in MODE whose signedness is indicated by UNSIGNEDP.  For
+   unsigned quantities, this is the number of high-order zero bits.
+   For signed quantities, this is the number of copies of the sign bit
+   minus 1.  In both case, this function returns the number of "spare"
+   bits.  For example, if two quantities for which this function returns
+   at least 1 are added, the addition is known not to overflow.
+
+   This function will always return 0 unless called during combine, which
+   implies that it must be called from a define_split.  */
+
+unsigned int
+extended_count (rtx x, enum machine_mode mode, int unsignedp)
+{
+  if (nonzero_sign_valid == 0)
+    return 0;
+
+  return (unsignedp
+	  ? (GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT
+	     ? (unsigned int) (GET_MODE_BITSIZE (mode) - 1
+			       - floor_log2 (nonzero_bits (x, mode)))
+	     : 0)
+	  : num_sign_bit_copies (x, mode) - 1);
+}
+
+/* This function is called from `simplify_shift_const' to merge two
+   outer operations.  Specifically, we have already found that we need
+   to perform operation *POP0 with constant *PCONST0 at the outermost
+   position.  We would now like to also perform OP1 with constant CONST1
+   (with *POP0 being done last).
+
+   Return 1 if we can do the operation and update *POP0 and *PCONST0 with
+   the resulting operation.  *PCOMP_P is set to 1 if we would need to
+   complement the innermost operand, otherwise it is unchanged.
+
+   MODE is the mode in which the operation will be done.  No bits outside
+   the width of this mode matter.  It is assumed that the width of this mode
+   is smaller than or equal to HOST_BITS_PER_WIDE_INT.
+
+   If *POP0 or OP1 are NIL, it means no operation is required.  Only NEG, PLUS,
+   IOR, XOR, and AND are supported.  We may set *POP0 to SET if the proper
+   result is simply *PCONST0.
+
+   If the resulting operation cannot be expressed as one operation, we
+   return 0 and do not change *POP0, *PCONST0, and *PCOMP_P.  */
+
+static int
+merge_outer_ops (enum rtx_code *pop0, HOST_WIDE_INT *pconst0, enum rtx_code op1, HOST_WIDE_INT const1, enum machine_mode mode, int *pcomp_p)
+{
+  enum rtx_code op0 = *pop0;
+  HOST_WIDE_INT const0 = *pconst0;
+
+  const0 &= GET_MODE_MASK (mode);
+  const1 &= GET_MODE_MASK (mode);
+
+  /* If OP0 is an AND, clear unimportant bits in CONST1.  */
+  if (op0 == AND)
+    const1 &= const0;
+
+  /* If OP0 or OP1 is NIL, this is easy.  Similarly if they are the same or
+     if OP0 is SET.  */
+
+  if (op1 == NIL || op0 == SET)
+    return 1;
+
+  else if (op0 == NIL)
+    op0 = op1, const0 = const1;
+
+  else if (op0 == op1)
+    {
+      switch (op0)
+	{
+	case AND:
+	  const0 &= const1;
+	  break;
+	case IOR:
+	  const0 |= const1;
+	  break;
+	case XOR:
+	  const0 ^= const1;
+	  break;
+	case PLUS:
+	  const0 += const1;
+	  break;
+	case NEG:
+	  op0 = NIL;
+	  break;
+	default:
+	  break;
+	}
+    }
+
+  /* Otherwise, if either is a PLUS or NEG, we can't do anything.  */
+  else if (op0 == PLUS || op1 == PLUS || op0 == NEG || op1 == NEG)
+    return 0;
+
+  /* If the two constants aren't the same, we can't do anything.  The
+     remaining six cases can all be done.  */
+  else if (const0 != const1)
+    return 0;
+
+  else
+    switch (op0)
+      {
+      case IOR:
+	if (op1 == AND)
+	  /* (a & b) | b == b */
+	  op0 = SET;
+	else /* op1 == XOR */
+	  /* (a ^ b) | b == a | b */
+	  {;}
+	break;
+
+      case XOR:
+	if (op1 == AND)
+	  /* (a & b) ^ b == (~a) & b */
+	  op0 = AND, *pcomp_p = 1;
+	else /* op1 == IOR */
+	  /* (a | b) ^ b == a & ~b */
+	  op0 = AND, const0 = ~const0;
+	break;
+
+      case AND:
+	if (op1 == IOR)
+	  /* (a | b) & b == b */
+	op0 = SET;
+	else /* op1 == XOR */
+	  /* (a ^ b) & b) == (~a) & b */
+	  *pcomp_p = 1;
+	break;
+      default:
+	break;
+      }
+
+  /* Check for NO-OP cases.  */
+  const0 &= GET_MODE_MASK (mode);
+  if (const0 == 0
+      && (op0 == IOR || op0 == XOR || op0 == PLUS))
+    op0 = NIL;
+  else if (const0 == 0 && op0 == AND)
+    op0 = SET;
+  else if ((unsigned HOST_WIDE_INT) const0 == GET_MODE_MASK (mode)
+	   && op0 == AND)
+    op0 = NIL;
+
+  /* ??? Slightly redundant with the above mask, but not entirely.
+     Moving this above means we'd have to sign-extend the mode mask
+     for the final test.  */
+  const0 = trunc_int_for_mode (const0, mode);
+
+  *pop0 = op0;
+  *pconst0 = const0;
+
+  return 1;
+}
+
+/* Simplify a shift of VAROP by COUNT bits.  CODE says what kind of shift.
+   The result of the shift is RESULT_MODE.  X, if nonzero, is an expression
+   that we started with.
+
+   The shift is normally computed in the widest mode we find in VAROP, as
+   long as it isn't a different number of words than RESULT_MODE.  Exceptions
+   are ASHIFTRT and ROTATE, which are always done in their original mode,  */
+
+static rtx
+simplify_shift_const (rtx x, enum rtx_code code,
+		      enum machine_mode result_mode, rtx varop,
+		      int orig_count)
+{
+  enum rtx_code orig_code = code;
+  unsigned int count;
+  int signed_count;
+  enum machine_mode mode = result_mode;
+  enum machine_mode shift_mode, tmode;
+  unsigned int mode_words
+    = (GET_MODE_SIZE (mode) + (UNITS_PER_WORD - 1)) / UNITS_PER_WORD;
+  /* We form (outer_op (code varop count) (outer_const)).  */
+  enum rtx_code outer_op = NIL;
+  HOST_WIDE_INT outer_const = 0;
+  rtx const_rtx;
+  int complement_p = 0;
+  rtx new;
+
+  /* Make sure and truncate the "natural" shift on the way in.  We don't
+     want to do this inside the loop as it makes it more difficult to
+     combine shifts.  */
+  if (SHIFT_COUNT_TRUNCATED)
+    orig_count &= GET_MODE_BITSIZE (mode) - 1;
+
+  /* If we were given an invalid count, don't do anything except exactly
+     what was requested.  */
+
+  if (orig_count < 0 || orig_count >= (int) GET_MODE_BITSIZE (mode))
+    {
+      if (x)
+	return x;
+
+      return gen_rtx_fmt_ee (code, mode, varop, GEN_INT (orig_count));
+    }
+
+  count = orig_count;
+
+  /* Unless one of the branches of the `if' in this loop does a `continue',
+     we will `break' the loop after the `if'.  */
+
+  while (count != 0)
+    {
+      /* If we have an operand of (clobber (const_int 0)), just return that
+	 value.  */
+      if (GET_CODE (varop) == CLOBBER)
+	return varop;
+
+      /* If we discovered we had to complement VAROP, leave.  Making a NOT
+	 here would cause an infinite loop.  */
+      if (complement_p)
+	break;
+
+      /* Convert ROTATERT to ROTATE.  */
+      if (code == ROTATERT)
+	{
+	  unsigned int bitsize = GET_MODE_BITSIZE (result_mode);;
+	  code = ROTATE;
+	  if (VECTOR_MODE_P (result_mode))
+	    count = bitsize / GET_MODE_NUNITS (result_mode) - count;
+	  else
+	    count = bitsize - count;
+	}
+
+      /* We need to determine what mode we will do the shift in.  If the
+	 shift is a right shift or a ROTATE, we must always do it in the mode
+	 it was originally done in.  Otherwise, we can do it in MODE, the
+	 widest mode encountered.  */
+      shift_mode
+	= (code == ASHIFTRT || code == LSHIFTRT || code == ROTATE
+	   ? result_mode : mode);
+
+      /* Handle cases where the count is greater than the size of the mode
+	 minus 1.  For ASHIFT, use the size minus one as the count (this can
+	 occur when simplifying (lshiftrt (ashiftrt ..))).  For rotates,
+	 take the count modulo the size.  For other shifts, the result is
+	 zero.
+
+	 Since these shifts are being produced by the compiler by combining
+	 multiple operations, each of which are defined, we know what the
+	 result is supposed to be.  */
+
+      if (count > (unsigned int) (GET_MODE_BITSIZE (shift_mode) - 1))
+	{
+	  if (code == ASHIFTRT)
+	    count = GET_MODE_BITSIZE (shift_mode) - 1;
+	  else if (code == ROTATE || code == ROTATERT)
+	    count %= GET_MODE_BITSIZE (shift_mode);
+	  else
+	    {
+	      /* We can't simply return zero because there may be an
+		 outer op.  */
+	      varop = const0_rtx;
+	      count = 0;
+	      break;
+	    }
+	}
+
+      /* An arithmetic right shift of a quantity known to be -1 or 0
+	 is a no-op.  */
+      if (code == ASHIFTRT
+	  && (num_sign_bit_copies (varop, shift_mode)
+	      == GET_MODE_BITSIZE (shift_mode)))
+	{
+	  count = 0;
+	  break;
+	}
+
+      /* If we are doing an arithmetic right shift and discarding all but
+	 the sign bit copies, this is equivalent to doing a shift by the
+	 bitsize minus one.  Convert it into that shift because it will often
+	 allow other simplifications.  */
+
+      if (code == ASHIFTRT
+	  && (count + num_sign_bit_copies (varop, shift_mode)
+	      >= GET_MODE_BITSIZE (shift_mode)))
+	count = GET_MODE_BITSIZE (shift_mode) - 1;
+
+      /* We simplify the tests below and elsewhere by converting
+	 ASHIFTRT to LSHIFTRT if we know the sign bit is clear.
+	 `make_compound_operation' will convert it to an ASHIFTRT for
+	 those machines (such as VAX) that don't have an LSHIFTRT.  */
+      if (GET_MODE_BITSIZE (shift_mode) <= HOST_BITS_PER_WIDE_INT
+	  && code == ASHIFTRT
+	  && ((nonzero_bits (varop, shift_mode)
+	       & ((HOST_WIDE_INT) 1 << (GET_MODE_BITSIZE (shift_mode) - 1)))
+	      == 0))
+	code = LSHIFTRT;
+
+      if (code == LSHIFTRT
+	  && GET_MODE_BITSIZE (shift_mode) <= HOST_BITS_PER_WIDE_INT
+	  && !(nonzero_bits (varop, shift_mode) >> count))
+	varop = const0_rtx;
+      if (code == ASHIFT
+	  && GET_MODE_BITSIZE (shift_mode) <= HOST_BITS_PER_WIDE_INT
+	  && !((nonzero_bits (varop, shift_mode) << count)
+	       & GET_MODE_MASK (shift_mode)))
+	varop = const0_rtx;
+
+      switch (GET_CODE (varop))
+	{
+	case SIGN_EXTEND:
+	case ZERO_EXTEND:
+	case SIGN_EXTRACT:
+	case ZERO_EXTRACT:
+	  new = expand_compound_operation (varop);
+	  if (new != varop)
+	    {
+	      varop = new;
+	      continue;
+	    }
+	  break;
+
+	case MEM:
+	  /* If we have (xshiftrt (mem ...) C) and C is MODE_WIDTH
+	     minus the width of a smaller mode, we can do this with a
+	     SIGN_EXTEND or ZERO_EXTEND from the narrower memory location.  */
+	  if ((code == ASHIFTRT || code == LSHIFTRT)
+	      && ! mode_dependent_address_p (XEXP (varop, 0))
+	      && ! MEM_VOLATILE_P (varop)
+	      && (tmode = mode_for_size (GET_MODE_BITSIZE (mode) - count,
+					 MODE_INT, 1)) != BLKmode)
+	    {
+	      new = adjust_address_nv (varop, tmode,
+				       BYTES_BIG_ENDIAN ? 0
+				       : count / BITS_PER_UNIT);
+
+	      varop = gen_rtx_fmt_e (code == ASHIFTRT ? SIGN_EXTEND
+				     : ZERO_EXTEND, mode, new);
+	      count = 0;
+	      continue;
+	    }
+	  break;
+
+	case USE:
+	  /* Similar to the case above, except that we can only do this if
+	     the resulting mode is the same as that of the underlying
+	     MEM and adjust the address depending on the *bits* endianness
+	     because of the way that bit-field extract insns are defined.  */
+	  if ((code == ASHIFTRT || code == LSHIFTRT)
+	      && (tmode = mode_for_size (GET_MODE_BITSIZE (mode) - count,
+					 MODE_INT, 1)) != BLKmode
+	      && tmode == GET_MODE (XEXP (varop, 0)))
+	    {
+	      if (BITS_BIG_ENDIAN)
+		new = XEXP (varop, 0);
+	      else
+		{
+		  new = copy_rtx (XEXP (varop, 0));
+		  SUBST (XEXP (new, 0),
+			 plus_constant (XEXP (new, 0),
+					count / BITS_PER_UNIT));
+		}
+
+	      varop = gen_rtx_fmt_e (code == ASHIFTRT ? SIGN_EXTEND
+				     : ZERO_EXTEND, mode, new);
+	      count = 0;
+	      continue;
+	    }
+	  break;
+
+	case SUBREG:
+	  /* If VAROP is a SUBREG, strip it as long as the inner operand has
+	     the same number of words as what we've seen so far.  Then store
+	     the widest mode in MODE.  */
+	  if (subreg_lowpart_p (varop)
+	      && (GET_MODE_SIZE (GET_MODE (SUBREG_REG (varop)))
+		  > GET_MODE_SIZE (GET_MODE (varop)))
+	      && (unsigned int) ((GET_MODE_SIZE (GET_MODE (SUBREG_REG (varop)))
+				  + (UNITS_PER_WORD - 1)) / UNITS_PER_WORD)
+		 == mode_words)
+	    {
+	      varop = SUBREG_REG (varop);
+	      if (GET_MODE_SIZE (GET_MODE (varop)) > GET_MODE_SIZE (mode))
+		mode = GET_MODE (varop);
+	      continue;
+	    }
+	  break;
+
+	case MULT:
+	  /* Some machines use MULT instead of ASHIFT because MULT
+	     is cheaper.  But it is still better on those machines to
+	     merge two shifts into one.  */
+	  if (GET_CODE (XEXP (varop, 1)) == CONST_INT
+	      && exact_log2 (INTVAL (XEXP (varop, 1))) >= 0)
+	    {
+	      varop
+		= gen_binary (ASHIFT, GET_MODE (varop), XEXP (varop, 0),
+			      GEN_INT (exact_log2 (INTVAL (XEXP (varop, 1)))));
+	      continue;
+	    }
+	  break;
+
+	case UDIV:
+	  /* Similar, for when divides are cheaper.  */
+	  if (GET_CODE (XEXP (varop, 1)) == CONST_INT
+	      && exact_log2 (INTVAL (XEXP (varop, 1))) >= 0)
+	    {
+	      varop
+		= gen_binary (LSHIFTRT, GET_MODE (varop), XEXP (varop, 0),
+			      GEN_INT (exact_log2 (INTVAL (XEXP (varop, 1)))));
+	      continue;
+	    }
+	  break;
+
+	case ASHIFTRT:
+	  /* If we are extracting just the sign bit of an arithmetic
+	     right shift, that shift is not needed.  However, the sign
+	     bit of a wider mode may be different from what would be
+	     interpreted as the sign bit in a narrower mode, so, if
+	     the result is narrower, don't discard the shift.  */
+	  if (code == LSHIFTRT
+	      && count == (unsigned int) (GET_MODE_BITSIZE (result_mode) - 1)
+	      && (GET_MODE_BITSIZE (result_mode)
+		  >= GET_MODE_BITSIZE (GET_MODE (varop))))
+	    {
+	      varop = XEXP (varop, 0);
+	      continue;
+	    }
+
+	  /* ... fall through ...  */
+
+	case LSHIFTRT:
+	case ASHIFT:
+	case ROTATE:
+	  /* Here we have two nested shifts.  The result is usually the
+	     AND of a new shift with a mask.  We compute the result below.  */
+	  if (GET_CODE (XEXP (varop, 1)) == CONST_INT
+	      && INTVAL (XEXP (varop, 1)) >= 0
+	      && INTVAL (XEXP (varop, 1)) < GET_MODE_BITSIZE (GET_MODE (varop))
+	      && GET_MODE_BITSIZE (result_mode) <= HOST_BITS_PER_WIDE_INT
+	      && GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT)
+	    {
+	      enum rtx_code first_code = GET_CODE (varop);
+	      unsigned int first_count = INTVAL (XEXP (varop, 1));
+	      unsigned HOST_WIDE_INT mask;
+	      rtx mask_rtx;
+
+	      /* We have one common special case.  We can't do any merging if
+		 the inner code is an ASHIFTRT of a smaller mode.  However, if
+		 we have (ashift:M1 (subreg:M1 (ashiftrt:M2 FOO C1) 0) C2)
+		 with C2 == GET_MODE_BITSIZE (M1) - GET_MODE_BITSIZE (M2),
+		 we can convert it to
+		 (ashiftrt:M1 (ashift:M1 (and:M1 (subreg:M1 FOO 0 C2) C3) C1).
+		 This simplifies certain SIGN_EXTEND operations.  */
+	      if (code == ASHIFT && first_code == ASHIFTRT
+		  && count == (unsigned int)
+			      (GET_MODE_BITSIZE (result_mode)
+			       - GET_MODE_BITSIZE (GET_MODE (varop))))
+		{
+		  /* C3 has the low-order C1 bits zero.  */
+
+		  mask = (GET_MODE_MASK (mode)
+			  & ~(((HOST_WIDE_INT) 1 << first_count) - 1));
+
+		  varop = simplify_and_const_int (NULL_RTX, result_mode,
+						  XEXP (varop, 0), mask);
+		  varop = simplify_shift_const (NULL_RTX, ASHIFT, result_mode,
+						varop, count);
+		  count = first_count;
+		  code = ASHIFTRT;
+		  continue;
+		}
+
+	      /* If this was (ashiftrt (ashift foo C1) C2) and FOO has more
+		 than C1 high-order bits equal to the sign bit, we can convert
+		 this to either an ASHIFT or an ASHIFTRT depending on the
+		 two counts.
+
+		 We cannot do this if VAROP's mode is not SHIFT_MODE.  */
+
+	      if (code == ASHIFTRT && first_code == ASHIFT
+		  && GET_MODE (varop) == shift_mode
+		  && (num_sign_bit_copies (XEXP (varop, 0), shift_mode)
+		      > first_count))
+		{
+		  varop = XEXP (varop, 0);
+
+		  signed_count = count - first_count;
+		  if (signed_count < 0)
+		    count = -signed_count, code = ASHIFT;
+		  else
+		    count = signed_count;
+
+		  continue;
+		}
+
+	      /* There are some cases we can't do.  If CODE is ASHIFTRT,
+		 we can only do this if FIRST_CODE is also ASHIFTRT.
+
+		 We can't do the case when CODE is ROTATE and FIRST_CODE is
+		 ASHIFTRT.
+
+		 If the mode of this shift is not the mode of the outer shift,
+		 we can't do this if either shift is a right shift or ROTATE.
+
+		 Finally, we can't do any of these if the mode is too wide
+		 unless the codes are the same.
+
+		 Handle the case where the shift codes are the same
+		 first.  */
+
+	      if (code == first_code)
+		{
+		  if (GET_MODE (varop) != result_mode
+		      && (code == ASHIFTRT || code == LSHIFTRT
+			  || code == ROTATE))
+		    break;
+
+		  count += first_count;
+		  varop = XEXP (varop, 0);
+		  continue;
+		}
+
+	      if (code == ASHIFTRT
+		  || (code == ROTATE && first_code == ASHIFTRT)
+		  || GET_MODE_BITSIZE (mode) > HOST_BITS_PER_WIDE_INT
+		  || (GET_MODE (varop) != result_mode
+		      && (first_code == ASHIFTRT || first_code == LSHIFTRT
+			  || first_code == ROTATE
+			  || code == ROTATE)))
+		break;
+
+	      /* To compute the mask to apply after the shift, shift the
+		 nonzero bits of the inner shift the same way the
+		 outer shift will.  */
+
+	      mask_rtx = GEN_INT (nonzero_bits (varop, GET_MODE (varop)));
+
+	      mask_rtx
+		= simplify_binary_operation (code, result_mode, mask_rtx,
+					     GEN_INT (count));
+
+	      /* Give up if we can't compute an outer operation to use.  */
+	      if (mask_rtx == 0
+		  || GET_CODE (mask_rtx) != CONST_INT
+		  || ! merge_outer_ops (&outer_op, &outer_const, AND,
+					INTVAL (mask_rtx),
+					result_mode, &complement_p))
+		break;
+
+	      /* If the shifts are in the same direction, we add the
+		 counts.  Otherwise, we subtract them.  */
+	      signed_count = count;
+	      if ((code == ASHIFTRT || code == LSHIFTRT)
+		  == (first_code == ASHIFTRT || first_code == LSHIFTRT))
+		signed_count += first_count;
+	      else
+		signed_count -= first_count;
+
+	      /* If COUNT is positive, the new shift is usually CODE,
+		 except for the two exceptions below, in which case it is
+		 FIRST_CODE.  If the count is negative, FIRST_CODE should
+		 always be used  */
+	      if (signed_count > 0
+		  && ((first_code == ROTATE && code == ASHIFT)
+		      || (first_code == ASHIFTRT && code == LSHIFTRT)))
+		code = first_code, count = signed_count;
+	      else if (signed_count < 0)
+		code = first_code, count = -signed_count;
+	      else
+		count = signed_count;
+
+	      varop = XEXP (varop, 0);
+	      continue;
+	    }
+
+	  /* If we have (A << B << C) for any shift, we can convert this to
+	     (A << C << B).  This wins if A is a constant.  Only try this if
+	     B is not a constant.  */
+
+	  else if (GET_CODE (varop) == code
+		   && GET_CODE (XEXP (varop, 1)) != CONST_INT
+		   && 0 != (new
+			    = simplify_binary_operation (code, mode,
+							 XEXP (varop, 0),
+							 GEN_INT (count))))
+	    {
+	      varop = gen_rtx_fmt_ee (code, mode, new, XEXP (varop, 1));
+	      count = 0;
+	      continue;
+	    }
+	  break;
+
+	case NOT:
+	  /* Make this fit the case below.  */
+	  varop = gen_rtx_XOR (mode, XEXP (varop, 0),
+			       GEN_INT (GET_MODE_MASK (mode)));
+	  continue;
+
+	case IOR:
+	case AND:
+	case XOR:
+	  /* If we have (xshiftrt (ior (plus X (const_int -1)) X) C)
+	     with C the size of VAROP - 1 and the shift is logical if
+	     STORE_FLAG_VALUE is 1 and arithmetic if STORE_FLAG_VALUE is -1,
+	     we have an (le X 0) operation.   If we have an arithmetic shift
+	     and STORE_FLAG_VALUE is 1 or we have a logical shift with
+	     STORE_FLAG_VALUE of -1, we have a (neg (le X 0)) operation.  */
+
+	  if (GET_CODE (varop) == IOR && GET_CODE (XEXP (varop, 0)) == PLUS
+	      && XEXP (XEXP (varop, 0), 1) == constm1_rtx
+	      && (STORE_FLAG_VALUE == 1 || STORE_FLAG_VALUE == -1)
+	      && (code == LSHIFTRT || code == ASHIFTRT)
+	      && count == (unsigned int)
+			  (GET_MODE_BITSIZE (GET_MODE (varop)) - 1)
+	      && rtx_equal_p (XEXP (XEXP (varop, 0), 0), XEXP (varop, 1)))
+	    {
+	      count = 0;
+	      varop = gen_rtx_LE (GET_MODE (varop), XEXP (varop, 1),
+				  const0_rtx);
+
+	      if (STORE_FLAG_VALUE == 1 ? code == ASHIFTRT : code == LSHIFTRT)
+		varop = gen_rtx_NEG (GET_MODE (varop), varop);
+
+	      continue;
+	    }
+
+	  /* If we have (shift (logical)), move the logical to the outside
+	     to allow it to possibly combine with another logical and the
+	     shift to combine with another shift.  This also canonicalizes to
+	     what a ZERO_EXTRACT looks like.  Also, some machines have
+	     (and (shift)) insns.  */
+
+	  if (GET_CODE (XEXP (varop, 1)) == CONST_INT
+	      && (new = simplify_binary_operation (code, result_mode,
+						   XEXP (varop, 1),
+						   GEN_INT (count))) != 0
+	      && GET_CODE (new) == CONST_INT
+	      && merge_outer_ops (&outer_op, &outer_const, GET_CODE (varop),
+				  INTVAL (new), result_mode, &complement_p))
+	    {
+	      varop = XEXP (varop, 0);
+	      continue;
+	    }
+
+	  /* If we can't do that, try to simplify the shift in each arm of the
+	     logical expression, make a new logical expression, and apply
+	     the inverse distributive law.  */
+	  {
+	    rtx lhs = simplify_shift_const (NULL_RTX, code, shift_mode,
+					    XEXP (varop, 0), count);
+	    rtx rhs = simplify_shift_const (NULL_RTX, code, shift_mode,
+					    XEXP (varop, 1), count);
+
+	    varop = gen_binary (GET_CODE (varop), shift_mode, lhs, rhs);
+	    varop = apply_distributive_law (varop);
+
+	    count = 0;
+	  }
+	  break;
+
+	case EQ:
+	  /* Convert (lshiftrt (eq FOO 0) C) to (xor FOO 1) if STORE_FLAG_VALUE
+	     says that the sign bit can be tested, FOO has mode MODE, C is
+	     GET_MODE_BITSIZE (MODE) - 1, and FOO has only its low-order bit
+	     that may be nonzero.  */
+	  if (code == LSHIFTRT
+	      && XEXP (varop, 1) == const0_rtx
+	      && GET_MODE (XEXP (varop, 0)) == result_mode
+	      && count == (unsigned int) (GET_MODE_BITSIZE (result_mode) - 1)
+	      && GET_MODE_BITSIZE (result_mode) <= HOST_BITS_PER_WIDE_INT
+	      && ((STORE_FLAG_VALUE
+		   & ((HOST_WIDE_INT) 1
+		      < (GET_MODE_BITSIZE (result_mode) - 1))))
+	      && nonzero_bits (XEXP (varop, 0), result_mode) == 1
+	      && merge_outer_ops (&outer_op, &outer_const, XOR,
+				  (HOST_WIDE_INT) 1, result_mode,
+				  &complement_p))
+	    {
+	      varop = XEXP (varop, 0);
+	      count = 0;
+	      continue;
+	    }
+	  break;
+
+	case NEG:
+	  /* (lshiftrt (neg A) C) where A is either 0 or 1 and C is one less
+	     than the number of bits in the mode is equivalent to A.  */
+	  if (code == LSHIFTRT
+	      && count == (unsigned int) (GET_MODE_BITSIZE (result_mode) - 1)
+	      && nonzero_bits (XEXP (varop, 0), result_mode) == 1)
+	    {
+	      varop = XEXP (varop, 0);
+	      count = 0;
+	      continue;
+	    }
+
+	  /* NEG commutes with ASHIFT since it is multiplication.  Move the
+	     NEG outside to allow shifts to combine.  */
+	  if (code == ASHIFT
+	      && merge_outer_ops (&outer_op, &outer_const, NEG,
+				  (HOST_WIDE_INT) 0, result_mode,
+				  &complement_p))
+	    {
+	      varop = XEXP (varop, 0);
+	      continue;
+	    }
+	  break;
+
+	case PLUS:
+	  /* (lshiftrt (plus A -1) C) where A is either 0 or 1 and C
+	     is one less than the number of bits in the mode is
+	     equivalent to (xor A 1).  */
+	  if (code == LSHIFTRT
+	      && count == (unsigned int) (GET_MODE_BITSIZE (result_mode) - 1)
+	      && XEXP (varop, 1) == constm1_rtx
+	      && nonzero_bits (XEXP (varop, 0), result_mode) == 1
+	      && merge_outer_ops (&outer_op, &outer_const, XOR,
+				  (HOST_WIDE_INT) 1, result_mode,
+				  &complement_p))
+	    {
+	      count = 0;
+	      varop = XEXP (varop, 0);
+	      continue;
+	    }
+
+	  /* If we have (xshiftrt (plus FOO BAR) C), and the only bits
+	     that might be nonzero in BAR are those being shifted out and those
+	     bits are known zero in FOO, we can replace the PLUS with FOO.
+	     Similarly in the other operand order.  This code occurs when
+	     we are computing the size of a variable-size array.  */
+
+	  if ((code == ASHIFTRT || code == LSHIFTRT)
+	      && count < HOST_BITS_PER_WIDE_INT
+	      && nonzero_bits (XEXP (varop, 1), result_mode) >> count == 0
+	      && (nonzero_bits (XEXP (varop, 1), result_mode)
+		  & nonzero_bits (XEXP (varop, 0), result_mode)) == 0)
+	    {
+	      varop = XEXP (varop, 0);
+	      continue;
+	    }
+	  else if ((code == ASHIFTRT || code == LSHIFTRT)
+		   && count < HOST_BITS_PER_WIDE_INT
+		   && GET_MODE_BITSIZE (result_mode) <= HOST_BITS_PER_WIDE_INT
+		   && 0 == (nonzero_bits (XEXP (varop, 0), result_mode)
+			    >> count)
+		   && 0 == (nonzero_bits (XEXP (varop, 0), result_mode)
+			    & nonzero_bits (XEXP (varop, 1),
+						 result_mode)))
+	    {
+	      varop = XEXP (varop, 1);
+	      continue;
+	    }
+
+	  /* (ashift (plus foo C) N) is (plus (ashift foo N) C').  */
+	  if (code == ASHIFT
+	      && GET_CODE (XEXP (varop, 1)) == CONST_INT
+	      && (new = simplify_binary_operation (ASHIFT, result_mode,
+						   XEXP (varop, 1),
+						   GEN_INT (count))) != 0
+	      && GET_CODE (new) == CONST_INT
+	      && merge_outer_ops (&outer_op, &outer_const, PLUS,
+				  INTVAL (new), result_mode, &complement_p))
+	    {
+	      varop = XEXP (varop, 0);
+	      continue;
+	    }
+	  break;
+
+	case MINUS:
+	  /* If we have (xshiftrt (minus (ashiftrt X C)) X) C)
+	     with C the size of VAROP - 1 and the shift is logical if
+	     STORE_FLAG_VALUE is 1 and arithmetic if STORE_FLAG_VALUE is -1,
+	     we have a (gt X 0) operation.  If the shift is arithmetic with
+	     STORE_FLAG_VALUE of 1 or logical with STORE_FLAG_VALUE == -1,
+	     we have a (neg (gt X 0)) operation.  */
+
+	  if ((STORE_FLAG_VALUE == 1 || STORE_FLAG_VALUE == -1)
+	      && GET_CODE (XEXP (varop, 0)) == ASHIFTRT
+	      && count == (unsigned int)
+			  (GET_MODE_BITSIZE (GET_MODE (varop)) - 1)
+	      && (code == LSHIFTRT || code == ASHIFTRT)
+	      && GET_CODE (XEXP (XEXP (varop, 0), 1)) == CONST_INT
+	      && (unsigned HOST_WIDE_INT) INTVAL (XEXP (XEXP (varop, 0), 1))
+		 == count
+	      && rtx_equal_p (XEXP (XEXP (varop, 0), 0), XEXP (varop, 1)))
+	    {
+	      count = 0;
+	      varop = gen_rtx_GT (GET_MODE (varop), XEXP (varop, 1),
+				  const0_rtx);
+
+	      if (STORE_FLAG_VALUE == 1 ? code == ASHIFTRT : code == LSHIFTRT)
+		varop = gen_rtx_NEG (GET_MODE (varop), varop);
+
+	      continue;
+	    }
+	  break;
+
+	case TRUNCATE:
+	  /* Change (lshiftrt (truncate (lshiftrt))) to (truncate (lshiftrt))
+	     if the truncate does not affect the value.  */
+	  if (code == LSHIFTRT
+	      && GET_CODE (XEXP (varop, 0)) == LSHIFTRT
+	      && GET_CODE (XEXP (XEXP (varop, 0), 1)) == CONST_INT
+	      && (INTVAL (XEXP (XEXP (varop, 0), 1))
+		  >= (GET_MODE_BITSIZE (GET_MODE (XEXP (varop, 0)))
+		      - GET_MODE_BITSIZE (GET_MODE (varop)))))
+	    {
+	      rtx varop_inner = XEXP (varop, 0);
+
+	      varop_inner
+		= gen_rtx_LSHIFTRT (GET_MODE (varop_inner),
+				    XEXP (varop_inner, 0),
+				    GEN_INT
+				    (count + INTVAL (XEXP (varop_inner, 1))));
+	      varop = gen_rtx_TRUNCATE (GET_MODE (varop), varop_inner);
+	      count = 0;
+	      continue;
+	    }
+	  break;
+
+	default:
+	  break;
+	}
+
+      break;
+    }
+
+  /* We need to determine what mode to do the shift in.  If the shift is
+     a right shift or ROTATE, we must always do it in the mode it was
+     originally done in.  Otherwise, we can do it in MODE, the widest mode
+     encountered.  The code we care about is that of the shift that will
+     actually be done, not the shift that was originally requested.  */
+  shift_mode
+    = (code == ASHIFTRT || code == LSHIFTRT || code == ROTATE
+       ? result_mode : mode);
+
+  /* We have now finished analyzing the shift.  The result should be
+     a shift of type CODE with SHIFT_MODE shifting VAROP COUNT places.  If
+     OUTER_OP is non-NIL, it is an operation that needs to be applied
+     to the result of the shift.  OUTER_CONST is the relevant constant,
+     but we must turn off all bits turned off in the shift.
+
+     If we were passed a value for X, see if we can use any pieces of
+     it.  If not, make new rtx.  */
+
+  if (x && GET_RTX_CLASS (GET_CODE (x)) == '2'
+      && GET_CODE (XEXP (x, 1)) == CONST_INT
+      && (unsigned HOST_WIDE_INT) INTVAL (XEXP (x, 1)) == count)
+    const_rtx = XEXP (x, 1);
+  else
+    const_rtx = GEN_INT (count);
+
+  if (x && GET_CODE (XEXP (x, 0)) == SUBREG
+      && GET_MODE (XEXP (x, 0)) == shift_mode
+      && SUBREG_REG (XEXP (x, 0)) == varop)
+    varop = XEXP (x, 0);
+  else if (GET_MODE (varop) != shift_mode)
+    varop = gen_lowpart_for_combine (shift_mode, varop);
+
+  /* If we can't make the SUBREG, try to return what we were given.  */
+  if (GET_CODE (varop) == CLOBBER)
+    return x ? x : varop;
+
+  new = simplify_binary_operation (code, shift_mode, varop, const_rtx);
+  if (new != 0)
+    x = new;
+  else
+    x = gen_rtx_fmt_ee (code, shift_mode, varop, const_rtx);
+
+  /* If we have an outer operation and we just made a shift, it is
+     possible that we could have simplified the shift were it not
+     for the outer operation.  So try to do the simplification
+     recursively.  */
+
+  if (outer_op != NIL && GET_CODE (x) == code
+      && GET_CODE (XEXP (x, 1)) == CONST_INT)
+    x = simplify_shift_const (x, code, shift_mode, XEXP (x, 0),
+			      INTVAL (XEXP (x, 1)));
+
+  /* If we were doing an LSHIFTRT in a wider mode than it was originally,
+     turn off all the bits that the shift would have turned off.  */
+  if (orig_code == LSHIFTRT && result_mode != shift_mode)
+    x = simplify_and_const_int (NULL_RTX, shift_mode, x,
+				GET_MODE_MASK (result_mode) >> orig_count);
+
+  /* Do the remainder of the processing in RESULT_MODE.  */
+  x = gen_lowpart_for_combine (result_mode, x);
+
+  /* If COMPLEMENT_P is set, we have to complement X before doing the outer
+     operation.  */
+  if (complement_p)
+    x = simplify_gen_unary (NOT, result_mode, x, result_mode);
+
+  if (outer_op != NIL)
+    {
+      if (GET_MODE_BITSIZE (result_mode) < HOST_BITS_PER_WIDE_INT)
+	outer_const = trunc_int_for_mode (outer_const, result_mode);
+
+      if (outer_op == AND)
+	x = simplify_and_const_int (NULL_RTX, result_mode, x, outer_const);
+      else if (outer_op == SET)
+	/* This means that we have determined that the result is
+	   equivalent to a constant.  This should be rare.  */
+	x = GEN_INT (outer_const);
+      else if (GET_RTX_CLASS (outer_op) == '1')
+	x = simplify_gen_unary (outer_op, result_mode, x, result_mode);
+      else
+	x = gen_binary (outer_op, result_mode, x, GEN_INT (outer_const));
+    }
+
+  return x;
+}
+
+/* Like recog, but we receive the address of a pointer to a new pattern.
+   We try to match the rtx that the pointer points to.
+   If that fails, we may try to modify or replace the pattern,
+   storing the replacement into the same pointer object.
+
+   Modifications include deletion or addition of CLOBBERs.
+
+   PNOTES is a pointer to a location where any REG_UNUSED notes added for
+   the CLOBBERs are placed.
+
+   The value is the final insn code from the pattern ultimately matched,
+   or -1.  */
+
+static int
+recog_for_combine (rtx *pnewpat, rtx insn, rtx *pnotes)
+{
+  rtx pat = *pnewpat;
+  int insn_code_number;
+  int num_clobbers_to_add = 0;
+  int i;
+  rtx notes = 0;
+  rtx old_notes, old_pat;
+
+  /* If PAT is a PARALLEL, check to see if it contains the CLOBBER
+     we use to indicate that something didn't match.  If we find such a
+     thing, force rejection.  */
+  if (GET_CODE (pat) == PARALLEL)
+    for (i = XVECLEN (pat, 0) - 1; i >= 0; i--)
+      if (GET_CODE (XVECEXP (pat, 0, i)) == CLOBBER
+	  && XEXP (XVECEXP (pat, 0, i), 0) == const0_rtx)
+	return -1;
+
+  old_pat = PATTERN (insn);
+  old_notes = REG_NOTES (insn);
+  PATTERN (insn) = pat;
+  REG_NOTES (insn) = 0;
+
+  insn_code_number = recog (pat, insn, &num_clobbers_to_add);
+
+  /* If it isn't, there is the possibility that we previously had an insn
+     that clobbered some register as a side effect, but the combined
+     insn doesn't need to do that.  So try once more without the clobbers
+     unless this represents an ASM insn.  */
+
+  if (insn_code_number < 0 && ! check_asm_operands (pat)
+      && GET_CODE (pat) == PARALLEL)
+    {
+      int pos;
+
+      for (pos = 0, i = 0; i < XVECLEN (pat, 0); i++)
+	if (GET_CODE (XVECEXP (pat, 0, i)) != CLOBBER)
+	  {
+	    if (i != pos)
+	      SUBST (XVECEXP (pat, 0, pos), XVECEXP (pat, 0, i));
+	    pos++;
+	  }
+
+      SUBST_INT (XVECLEN (pat, 0), pos);
+
+      if (pos == 1)
+	pat = XVECEXP (pat, 0, 0);
+
+      PATTERN (insn) = pat;
+      insn_code_number = recog (pat, insn, &num_clobbers_to_add);
+    }
+  PATTERN (insn) = old_pat;
+  REG_NOTES (insn) = old_notes;
+
+  /* Recognize all noop sets, these will be killed by followup pass.  */
+  if (insn_code_number < 0 && GET_CODE (pat) == SET && set_noop_p (pat))
+    insn_code_number = NOOP_MOVE_INSN_CODE, num_clobbers_to_add = 0;
+
+  /* If we had any clobbers to add, make a new pattern than contains
+     them.  Then check to make sure that all of them are dead.  */
+  if (num_clobbers_to_add)
+    {
+      rtx newpat = gen_rtx_PARALLEL (VOIDmode,
+				     rtvec_alloc (GET_CODE (pat) == PARALLEL
+						  ? (XVECLEN (pat, 0)
+						     + num_clobbers_to_add)
+						  : num_clobbers_to_add + 1));
+
+      if (GET_CODE (pat) == PARALLEL)
+	for (i = 0; i < XVECLEN (pat, 0); i++)
+	  XVECEXP (newpat, 0, i) = XVECEXP (pat, 0, i);
+      else
+	XVECEXP (newpat, 0, 0) = pat;
+
+      add_clobbers (newpat, insn_code_number);
+
+      for (i = XVECLEN (newpat, 0) - num_clobbers_to_add;
+	   i < XVECLEN (newpat, 0); i++)
+	{
+	  if (GET_CODE (XEXP (XVECEXP (newpat, 0, i), 0)) == REG
+	      && ! reg_dead_at_p (XEXP (XVECEXP (newpat, 0, i), 0), insn))
+	    return -1;
+	  notes = gen_rtx_EXPR_LIST (REG_UNUSED,
+				     XEXP (XVECEXP (newpat, 0, i), 0), notes);
+	}
+      pat = newpat;
+    }
+
+  *pnewpat = pat;
+  *pnotes = notes;
+
+  return insn_code_number;
+}
+
+/* Like gen_lowpart but for use by combine.  In combine it is not possible
+   to create any new pseudoregs.  However, it is safe to create
+   invalid memory addresses, because combine will try to recognize
+   them and all they will do is make the combine attempt fail.
+
+   If for some reason this cannot do its job, an rtx
+   (clobber (const_int 0)) is returned.
+   An insn containing that will not be recognized.  */
+
+#undef gen_lowpart
+
+static rtx
+gen_lowpart_for_combine (enum machine_mode mode, rtx x)
+{
+  rtx result;
+
+  if (GET_MODE (x) == mode)
+    return x;
+
+  /* Return identity if this is a CONST or symbolic
+     reference.  */
+  if (mode == Pmode
+      && (GET_CODE (x) == CONST
+	  || GET_CODE (x) == SYMBOL_REF
+	  || GET_CODE (x) == LABEL_REF))
+    return x;
+
+  /* We can only support MODE being wider than a word if X is a
+     constant integer or has a mode the same size.  */
+
+  if (GET_MODE_SIZE (mode) > UNITS_PER_WORD
+      && ! ((GET_MODE (x) == VOIDmode
+	     && (GET_CODE (x) == CONST_INT
+		 || GET_CODE (x) == CONST_DOUBLE))
+	    || GET_MODE_SIZE (GET_MODE (x)) == GET_MODE_SIZE (mode)))
+    return gen_rtx_CLOBBER (GET_MODE (x), const0_rtx);
+
+  /* X might be a paradoxical (subreg (mem)).  In that case, gen_lowpart
+     won't know what to do.  So we will strip off the SUBREG here and
+     process normally.  */
+  if (GET_CODE (x) == SUBREG && GET_CODE (SUBREG_REG (x)) == MEM)
+    {
+      x = SUBREG_REG (x);
+      if (GET_MODE (x) == mode)
+	return x;
+    }
+
+  result = gen_lowpart_common (mode, x);
+#ifdef CANNOT_CHANGE_MODE_CLASS
+  if (result != 0 && GET_CODE (result) == SUBREG)
+    record_subregs_of_mode (result);
+#endif
+
+  if (result)
+    return result;
+
+  if (GET_CODE (x) == MEM)
+    {
+      int offset = 0;
+
+      /* Refuse to work on a volatile memory ref or one with a mode-dependent
+	 address.  */
+      if (MEM_VOLATILE_P (x) || mode_dependent_address_p (XEXP (x, 0)))
+	return gen_rtx_CLOBBER (GET_MODE (x), const0_rtx);
+
+      /* If we want to refer to something bigger than the original memref,
+	 generate a perverse subreg instead.  That will force a reload
+	 of the original memref X.  */
+      if (GET_MODE_SIZE (GET_MODE (x)) < GET_MODE_SIZE (mode))
+	return gen_rtx_SUBREG (mode, x, 0);
+
+      if (WORDS_BIG_ENDIAN)
+	offset = (MAX (GET_MODE_SIZE (GET_MODE (x)), UNITS_PER_WORD)
+		  - MAX (GET_MODE_SIZE (mode), UNITS_PER_WORD));
+
+      if (BYTES_BIG_ENDIAN)
+	{
+	  /* Adjust the address so that the address-after-the-data is
+	     unchanged.  */
+	  offset -= (MIN (UNITS_PER_WORD, GET_MODE_SIZE (mode))
+		     - MIN (UNITS_PER_WORD, GET_MODE_SIZE (GET_MODE (x))));
+	}
+
+      return adjust_address_nv (x, mode, offset);
+    }
+
+  /* If X is a comparison operator, rewrite it in a new mode.  This
+     probably won't match, but may allow further simplifications.  */
+  else if (GET_RTX_CLASS (GET_CODE (x)) == '<')
+    return gen_rtx_fmt_ee (GET_CODE (x), mode, XEXP (x, 0), XEXP (x, 1));
+
+  /* If we couldn't simplify X any other way, just enclose it in a
+     SUBREG.  Normally, this SUBREG won't match, but some patterns may
+     include an explicit SUBREG or we may simplify it further in combine.  */
+  else
+    {
+      int offset = 0;
+      rtx res;
+      enum machine_mode sub_mode = GET_MODE (x);
+
+      offset = subreg_lowpart_offset (mode, sub_mode);
+      if (sub_mode == VOIDmode)
+	{
+	  sub_mode = int_mode_for_mode (mode);
+	  x = gen_lowpart_common (sub_mode, x);
+	  if (x == 0)
+	    return gen_rtx_CLOBBER (VOIDmode, const0_rtx);
+	}
+      res = simplify_gen_subreg (mode, x, sub_mode, offset);
+      if (res)
+	return res;
+      return gen_rtx_CLOBBER (GET_MODE (x), const0_rtx);
+    }
+}
+
+/* These routines make binary and unary operations by first seeing if they
+   fold; if not, a new expression is allocated.  */
+
+static rtx
+gen_binary (enum rtx_code code, enum machine_mode mode, rtx op0, rtx op1)
+{
+  rtx result;
+  rtx tem;
+
+  if (GET_CODE (op0) == CLOBBER)
+    return op0;
+  else if (GET_CODE (op1) == CLOBBER)
+    return op1;
+
+  if (GET_RTX_CLASS (code) == 'c'
+      && swap_commutative_operands_p (op0, op1))
+    tem = op0, op0 = op1, op1 = tem;
+
+  if (GET_RTX_CLASS (code) == '<')
+    {
+      enum machine_mode op_mode = GET_MODE (op0);
+
+      /* Strip the COMPARE from (REL_OP (compare X Y) 0) to get
+	 just (REL_OP X Y).  */
+      if (GET_CODE (op0) == COMPARE && op1 == const0_rtx)
+	{
+	  op1 = XEXP (op0, 1);
+	  op0 = XEXP (op0, 0);
+	  op_mode = GET_MODE (op0);
+	}
+
+      if (op_mode == VOIDmode)
+	op_mode = GET_MODE (op1);
+      result = simplify_relational_operation (code, op_mode, op0, op1);
+    }
+  else
+    result = simplify_binary_operation (code, mode, op0, op1);
+
+  if (result)
+    return result;
+
+  /* Put complex operands first and constants second.  */
+  if (GET_RTX_CLASS (code) == 'c'
+      && swap_commutative_operands_p (op0, op1))
+    return gen_rtx_fmt_ee (code, mode, op1, op0);
+
+  /* If we are turning off bits already known off in OP0, we need not do
+     an AND.  */
+  else if (code == AND && GET_CODE (op1) == CONST_INT
+	   && GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT
+	   && (nonzero_bits (op0, mode) & ~INTVAL (op1)) == 0)
+    return op0;
+
+  return gen_rtx_fmt_ee (code, mode, op0, op1);
+}
+
+/* Simplify a comparison between *POP0 and *POP1 where CODE is the
+   comparison code that will be tested.
+
+   The result is a possibly different comparison code to use.  *POP0 and
+   *POP1 may be updated.
+
+   It is possible that we might detect that a comparison is either always
+   true or always false.  However, we do not perform general constant
+   folding in combine, so this knowledge isn't useful.  Such tautologies
+   should have been detected earlier.  Hence we ignore all such cases.  */
+
+static enum rtx_code
+simplify_comparison (enum rtx_code code, rtx *pop0, rtx *pop1)
+{
+  rtx op0 = *pop0;
+  rtx op1 = *pop1;
+  rtx tem, tem1;
+  int i;
+  enum machine_mode mode, tmode;
+
+  /* Try a few ways of applying the same transformation to both operands.  */
+  while (1)
+    {
+#ifndef WORD_REGISTER_OPERATIONS
+      /* The test below this one won't handle SIGN_EXTENDs on these machines,
+	 so check specially.  */
+      if (code != GTU && code != GEU && code != LTU && code != LEU
+	  && GET_CODE (op0) == ASHIFTRT && GET_CODE (op1) == ASHIFTRT
+	  && GET_CODE (XEXP (op0, 0)) == ASHIFT
+	  && GET_CODE (XEXP (op1, 0)) == ASHIFT
+	  && GET_CODE (XEXP (XEXP (op0, 0), 0)) == SUBREG
+	  && GET_CODE (XEXP (XEXP (op1, 0), 0)) == SUBREG
+	  && (GET_MODE (SUBREG_REG (XEXP (XEXP (op0, 0), 0)))
+	      == GET_MODE (SUBREG_REG (XEXP (XEXP (op1, 0), 0))))
+	  && GET_CODE (XEXP (op0, 1)) == CONST_INT
+	  && XEXP (op0, 1) == XEXP (op1, 1)
+	  && XEXP (op0, 1) == XEXP (XEXP (op0, 0), 1)
+	  && XEXP (op0, 1) == XEXP (XEXP (op1, 0), 1)
+	  && (INTVAL (XEXP (op0, 1))
+	      == (GET_MODE_BITSIZE (GET_MODE (op0))
+		  - (GET_MODE_BITSIZE
+		     (GET_MODE (SUBREG_REG (XEXP (XEXP (op0, 0), 0))))))))
+	{
+	  op0 = SUBREG_REG (XEXP (XEXP (op0, 0), 0));
+	  op1 = SUBREG_REG (XEXP (XEXP (op1, 0), 0));
+	}
+#endif
+
+      /* If both operands are the same constant shift, see if we can ignore the
+	 shift.  We can if the shift is a rotate or if the bits shifted out of
+	 this shift are known to be zero for both inputs and if the type of
+	 comparison is compatible with the shift.  */
+      if (GET_CODE (op0) == GET_CODE (op1)
+	  && GET_MODE_BITSIZE (GET_MODE (op0)) <= HOST_BITS_PER_WIDE_INT
+	  && ((GET_CODE (op0) == ROTATE && (code == NE || code == EQ))
+	      || ((GET_CODE (op0) == LSHIFTRT || GET_CODE (op0) == ASHIFT)
+		  && (code != GT && code != LT && code != GE && code != LE))
+	      || (GET_CODE (op0) == ASHIFTRT
+		  && (code != GTU && code != LTU
+		      && code != GEU && code != LEU)))
+	  && GET_CODE (XEXP (op0, 1)) == CONST_INT
+	  && INTVAL (XEXP (op0, 1)) >= 0
+	  && INTVAL (XEXP (op0, 1)) < HOST_BITS_PER_WIDE_INT
+	  && XEXP (op0, 1) == XEXP (op1, 1))
+	{
+	  enum machine_mode mode = GET_MODE (op0);
+	  unsigned HOST_WIDE_INT mask = GET_MODE_MASK (mode);
+	  int shift_count = INTVAL (XEXP (op0, 1));
+
+	  if (GET_CODE (op0) == LSHIFTRT || GET_CODE (op0) == ASHIFTRT)
+	    mask &= (mask >> shift_count) << shift_count;
+	  else if (GET_CODE (op0) == ASHIFT)
+	    mask = (mask & (mask << shift_count)) >> shift_count;
+
+	  if ((nonzero_bits (XEXP (op0, 0), mode) & ~mask) == 0
+	      && (nonzero_bits (XEXP (op1, 0), mode) & ~mask) == 0)
+	    op0 = XEXP (op0, 0), op1 = XEXP (op1, 0);
+	  else
+	    break;
+	}
+
+      /* If both operands are AND's of a paradoxical SUBREG by constant, the
+	 SUBREGs are of the same mode, and, in both cases, the AND would
+	 be redundant if the comparison was done in the narrower mode,
+	 do the comparison in the narrower mode (e.g., we are AND'ing with 1
+	 and the operand's possibly nonzero bits are 0xffffff01; in that case
+	 if we only care about QImode, we don't need the AND).  This case
+	 occurs if the output mode of an scc insn is not SImode and
+	 STORE_FLAG_VALUE == 1 (e.g., the 386).
+
+	 Similarly, check for a case where the AND's are ZERO_EXTEND
+	 operations from some narrower mode even though a SUBREG is not
+	 present.  */
+
+      else if (GET_CODE (op0) == AND && GET_CODE (op1) == AND
+	       && GET_CODE (XEXP (op0, 1)) == CONST_INT
+	       && GET_CODE (XEXP (op1, 1)) == CONST_INT)
+	{
+	  rtx inner_op0 = XEXP (op0, 0);
+	  rtx inner_op1 = XEXP (op1, 0);
+	  HOST_WIDE_INT c0 = INTVAL (XEXP (op0, 1));
+	  HOST_WIDE_INT c1 = INTVAL (XEXP (op1, 1));
+	  int changed = 0;
+
+	  if (GET_CODE (inner_op0) == SUBREG && GET_CODE (inner_op1) == SUBREG
+	      && (GET_MODE_SIZE (GET_MODE (inner_op0))
+		  > GET_MODE_SIZE (GET_MODE (SUBREG_REG (inner_op0))))
+	      && (GET_MODE (SUBREG_REG (inner_op0))
+		  == GET_MODE (SUBREG_REG (inner_op1)))
+	      && (GET_MODE_BITSIZE (GET_MODE (SUBREG_REG (inner_op0)))
+		  <= HOST_BITS_PER_WIDE_INT)
+	      && (0 == ((~c0) & nonzero_bits (SUBREG_REG (inner_op0),
+					     GET_MODE (SUBREG_REG (inner_op0)))))
+	      && (0 == ((~c1) & nonzero_bits (SUBREG_REG (inner_op1),
+					     GET_MODE (SUBREG_REG (inner_op1))))))
+	    {
+	      op0 = SUBREG_REG (inner_op0);
+	      op1 = SUBREG_REG (inner_op1);
+
+	      /* The resulting comparison is always unsigned since we masked
+		 off the original sign bit.  */
+	      code = unsigned_condition (code);
+
+	      changed = 1;
+	    }
+
+	  else if (c0 == c1)
+	    for (tmode = GET_CLASS_NARROWEST_MODE
+		 (GET_MODE_CLASS (GET_MODE (op0)));
+		 tmode != GET_MODE (op0); tmode = GET_MODE_WIDER_MODE (tmode))
+	      if ((unsigned HOST_WIDE_INT) c0 == GET_MODE_MASK (tmode))
+		{
+		  op0 = gen_lowpart_for_combine (tmode, inner_op0);
+		  op1 = gen_lowpart_for_combine (tmode, inner_op1);
+		  code = unsigned_condition (code);
+		  changed = 1;
+		  break;
+		}
+
+	  if (! changed)
+	    break;
+	}
+
+      /* If both operands are NOT, we can strip off the outer operation
+	 and adjust the comparison code for swapped operands; similarly for
+	 NEG, except that this must be an equality comparison.  */
+      else if ((GET_CODE (op0) == NOT && GET_CODE (op1) == NOT)
+	       || (GET_CODE (op0) == NEG && GET_CODE (op1) == NEG
+		   && (code == EQ || code == NE)))
+	op0 = XEXP (op0, 0), op1 = XEXP (op1, 0), code = swap_condition (code);
+
+      else
+	break;
+    }
+
+  /* If the first operand is a constant, swap the operands and adjust the
+     comparison code appropriately, but don't do this if the second operand
+     is already a constant integer.  */
+  if (swap_commutative_operands_p (op0, op1))
+    {
+      tem = op0, op0 = op1, op1 = tem;
+      code = swap_condition (code);
+    }
+
+  /* We now enter a loop during which we will try to simplify the comparison.
+     For the most part, we only are concerned with comparisons with zero,
+     but some things may really be comparisons with zero but not start
+     out looking that way.  */
+
+  while (GET_CODE (op1) == CONST_INT)
+    {
+      enum machine_mode mode = GET_MODE (op0);
+      unsigned int mode_width = GET_MODE_BITSIZE (mode);
+      unsigned HOST_WIDE_INT mask = GET_MODE_MASK (mode);
+      int equality_comparison_p;
+      int sign_bit_comparison_p;
+      int unsigned_comparison_p;
+      HOST_WIDE_INT const_op;
+
+      /* We only want to handle integral modes.  This catches VOIDmode,
+	 CCmode, and the floating-point modes.  An exception is that we
+	 can handle VOIDmode if OP0 is a COMPARE or a comparison
+	 operation.  */
+
+      if (GET_MODE_CLASS (mode) != MODE_INT
+	  && ! (mode == VOIDmode
+		&& (GET_CODE (op0) == COMPARE
+		    || GET_RTX_CLASS (GET_CODE (op0)) == '<')))
+	break;
+
+      /* Get the constant we are comparing against and turn off all bits
+	 not on in our mode.  */
+      const_op = INTVAL (op1);
+      if (mode != VOIDmode)
+	const_op = trunc_int_for_mode (const_op, mode);
+      op1 = GEN_INT (const_op);
+
+      /* If we are comparing against a constant power of two and the value
+	 being compared can only have that single bit nonzero (e.g., it was
+	 `and'ed with that bit), we can replace this with a comparison
+	 with zero.  */
+      if (const_op
+	  && (code == EQ || code == NE || code == GE || code == GEU
+	      || code == LT || code == LTU)
+	  && mode_width <= HOST_BITS_PER_WIDE_INT
+	  && exact_log2 (const_op) >= 0
+	  && nonzero_bits (op0, mode) == (unsigned HOST_WIDE_INT) const_op)
+	{
+	  code = (code == EQ || code == GE || code == GEU ? NE : EQ);
+	  op1 = const0_rtx, const_op = 0;
+	}
+
+      /* Similarly, if we are comparing a value known to be either -1 or
+	 0 with -1, change it to the opposite comparison against zero.  */
+
+      if (const_op == -1
+	  && (code == EQ || code == NE || code == GT || code == LE
+	      || code == GEU || code == LTU)
+	  && num_sign_bit_copies (op0, mode) == mode_width)
+	{
+	  code = (code == EQ || code == LE || code == GEU ? NE : EQ);
+	  op1 = const0_rtx, const_op = 0;
+	}
+
+      /* Do some canonicalizations based on the comparison code.  We prefer
+	 comparisons against zero and then prefer equality comparisons.
+	 If we can reduce the size of a constant, we will do that too.  */
+
+      switch (code)
+	{
+	case LT:
+	  /* < C is equivalent to <= (C - 1) */
+	  if (const_op > 0)
+	    {
+	      const_op -= 1;
+	      op1 = GEN_INT (const_op);
+	      code = LE;
+	      /* ... fall through to LE case below.  */
+	    }
+	  else
+	    break;
+
+	case LE:
+	  /* <= C is equivalent to < (C + 1); we do this for C < 0  */
+	  if (const_op < 0)
+	    {
+	      const_op += 1;
+	      op1 = GEN_INT (const_op);
+	      code = LT;
+	    }
+
+	  /* If we are doing a <= 0 comparison on a value known to have
+	     a zero sign bit, we can replace this with == 0.  */
+	  else if (const_op == 0
+		   && mode_width <= HOST_BITS_PER_WIDE_INT
+		   && (nonzero_bits (op0, mode)
+		       & ((HOST_WIDE_INT) 1 << (mode_width - 1))) == 0)
+	    code = EQ;
+	  break;
+
+	case GE:
+	  /* >= C is equivalent to > (C - 1).  */
+	  if (const_op > 0)
+	    {
+	      const_op -= 1;
+	      op1 = GEN_INT (const_op);
+	      code = GT;
+	      /* ... fall through to GT below.  */
+	    }
+	  else
+	    break;
+
+	case GT:
+	  /* > C is equivalent to >= (C + 1); we do this for C < 0.  */
+	  if (const_op < 0)
+	    {
+	      const_op += 1;
+	      op1 = GEN_INT (const_op);
+	      code = GE;
+	    }
+
+	  /* If we are doing a > 0 comparison on a value known to have
+	     a zero sign bit, we can replace this with != 0.  */
+	  else if (const_op == 0
+		   && mode_width <= HOST_BITS_PER_WIDE_INT
+		   && (nonzero_bits (op0, mode)
+		       & ((HOST_WIDE_INT) 1 << (mode_width - 1))) == 0)
+	    code = NE;
+	  break;
+
+	case LTU:
+	  /* < C is equivalent to <= (C - 1).  */
+	  if (const_op > 0)
+	    {
+	      const_op -= 1;
+	      op1 = GEN_INT (const_op);
+	      code = LEU;
+	      /* ... fall through ...  */
+	    }
+
+	  /* (unsigned) < 0x80000000 is equivalent to >= 0.  */
+	  else if ((mode_width <= HOST_BITS_PER_WIDE_INT)
+		   && (const_op == (HOST_WIDE_INT) 1 << (mode_width - 1)))
+	    {
+	      const_op = 0, op1 = const0_rtx;
+	      code = GE;
+	      break;
+	    }
+	  else
+	    break;
+
+	case LEU:
+	  /* unsigned <= 0 is equivalent to == 0 */
+	  if (const_op == 0)
+	    code = EQ;
+
+	  /* (unsigned) <= 0x7fffffff is equivalent to >= 0.  */
+	  else if ((mode_width <= HOST_BITS_PER_WIDE_INT)
+		   && (const_op == ((HOST_WIDE_INT) 1 << (mode_width - 1)) - 1))
+	    {
+	      const_op = 0, op1 = const0_rtx;
+	      code = GE;
+	    }
+	  break;
+
+	case GEU:
+	  /* >= C is equivalent to < (C - 1).  */
+	  if (const_op > 1)
+	    {
+	      const_op -= 1;
+	      op1 = GEN_INT (const_op);
+	      code = GTU;
+	      /* ... fall through ...  */
+	    }
+
+	  /* (unsigned) >= 0x80000000 is equivalent to < 0.  */
+	  else if ((mode_width <= HOST_BITS_PER_WIDE_INT)
+		   && (const_op == (HOST_WIDE_INT) 1 << (mode_width - 1)))
+	    {
+	      const_op = 0, op1 = const0_rtx;
+	      code = LT;
+	      break;
+	    }
+	  else
+	    break;
+
+	case GTU:
+	  /* unsigned > 0 is equivalent to != 0 */
+	  if (const_op == 0)
+	    code = NE;
+
+	  /* (unsigned) > 0x7fffffff is equivalent to < 0.  */
+	  else if ((mode_width <= HOST_BITS_PER_WIDE_INT)
+		   && (const_op == ((HOST_WIDE_INT) 1 << (mode_width - 1)) - 1))
+	    {
+	      const_op = 0, op1 = const0_rtx;
+	      code = LT;
+	    }
+	  break;
+
+	default:
+	  break;
+	}
+
+      /* Compute some predicates to simplify code below.  */
+
+      equality_comparison_p = (code == EQ || code == NE);
+      sign_bit_comparison_p = ((code == LT || code == GE) && const_op == 0);
+      unsigned_comparison_p = (code == LTU || code == LEU || code == GTU
+			       || code == GEU);
+
+      /* If this is a sign bit comparison and we can do arithmetic in
+	 MODE, say that we will only be needing the sign bit of OP0.  */
+      if (sign_bit_comparison_p
+	  && GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT)
+	op0 = force_to_mode (op0, mode,
+			     ((HOST_WIDE_INT) 1
+			      << (GET_MODE_BITSIZE (mode) - 1)),
+			     NULL_RTX, 0);
+
+      /* Now try cases based on the opcode of OP0.  If none of the cases
+	 does a "continue", we exit this loop immediately after the
+	 switch.  */
+
+      switch (GET_CODE (op0))
+	{
+	case ZERO_EXTRACT:
+	  /* If we are extracting a single bit from a variable position in
+	     a constant that has only a single bit set and are comparing it
+	     with zero, we can convert this into an equality comparison
+	     between the position and the location of the single bit.  */
+	  /* Except we can't if SHIFT_COUNT_TRUNCATED is set, since we might
+	     have already reduced the shift count modulo the word size.  */
+	  if (!SHIFT_COUNT_TRUNCATED
+	      && GET_CODE (XEXP (op0, 0)) == CONST_INT
+	      && XEXP (op0, 1) == const1_rtx
+	      && equality_comparison_p && const_op == 0
+	      && (i = exact_log2 (INTVAL (XEXP (op0, 0)))) >= 0)
+	    {
+	      if (BITS_BIG_ENDIAN)
+		{
+		  enum machine_mode new_mode
+		    = mode_for_extraction (EP_extzv, 1);
+		  if (new_mode == MAX_MACHINE_MODE)
+		    i = BITS_PER_WORD - 1 - i;
+		  else
+		    {
+		      mode = new_mode;
+		      i = (GET_MODE_BITSIZE (mode) - 1 - i);
+		    }
+		}
+
+	      op0 = XEXP (op0, 2);
+	      op1 = GEN_INT (i);
+	      const_op = i;
+
+	      /* Result is nonzero iff shift count is equal to I.  */
+	      code = reverse_condition (code);
+	      continue;
+	    }
+
+	  /* ... fall through ...  */
+
+	case SIGN_EXTRACT:
+	  tem = expand_compound_operation (op0);
+	  if (tem != op0)
+	    {
+	      op0 = tem;
+	      continue;
+	    }
+	  break;
+
+	case NOT:
+	  /* If testing for equality, we can take the NOT of the constant.  */
+	  if (equality_comparison_p
+	      && (tem = simplify_unary_operation (NOT, mode, op1, mode)) != 0)
+	    {
+	      op0 = XEXP (op0, 0);
+	      op1 = tem;
+	      continue;
+	    }
+
+	  /* If just looking at the sign bit, reverse the sense of the
+	     comparison.  */
+	  if (sign_bit_comparison_p)
+	    {
+	      op0 = XEXP (op0, 0);
+	      code = (code == GE ? LT : GE);
+	      continue;
+	    }
+	  break;
+
+	case NEG:
+	  /* If testing for equality, we can take the NEG of the constant.  */
+	  if (equality_comparison_p
+	      && (tem = simplify_unary_operation (NEG, mode, op1, mode)) != 0)
+	    {
+	      op0 = XEXP (op0, 0);
+	      op1 = tem;
+	      continue;
+	    }
+
+	  /* The remaining cases only apply to comparisons with zero.  */
+	  if (const_op != 0)
+	    break;
+
+	  /* When X is ABS or is known positive,
+	     (neg X) is < 0 if and only if X != 0.  */
+
+	  if (sign_bit_comparison_p
+	      && (GET_CODE (XEXP (op0, 0)) == ABS
+		  || (mode_width <= HOST_BITS_PER_WIDE_INT
+		      && (nonzero_bits (XEXP (op0, 0), mode)
+			  & ((HOST_WIDE_INT) 1 << (mode_width - 1))) == 0)))
+	    {
+	      op0 = XEXP (op0, 0);
+	      code = (code == LT ? NE : EQ);
+	      continue;
+	    }
+
+	  /* If we have NEG of something whose two high-order bits are the
+	     same, we know that "(-a) < 0" is equivalent to "a > 0".  */
+	  if (num_sign_bit_copies (op0, mode) >= 2)
+	    {
+	      op0 = XEXP (op0, 0);
+	      code = swap_condition (code);
+	      continue;
+	    }
+	  break;
+
+	case ROTATE:
+	  /* If we are testing equality and our count is a constant, we
+	     can perform the inverse operation on our RHS.  */
+	  if (equality_comparison_p && GET_CODE (XEXP (op0, 1)) == CONST_INT
+	      && (tem = simplify_binary_operation (ROTATERT, mode,
+						   op1, XEXP (op0, 1))) != 0)
+	    {
+	      op0 = XEXP (op0, 0);
+	      op1 = tem;
+	      continue;
+	    }
+
+	  /* If we are doing a < 0 or >= 0 comparison, it means we are testing
+	     a particular bit.  Convert it to an AND of a constant of that
+	     bit.  This will be converted into a ZERO_EXTRACT.  */
+	  if (const_op == 0 && sign_bit_comparison_p
+	      && GET_CODE (XEXP (op0, 1)) == CONST_INT
+	      && mode_width <= HOST_BITS_PER_WIDE_INT)
+	    {
+	      op0 = simplify_and_const_int (NULL_RTX, mode, XEXP (op0, 0),
+					    ((HOST_WIDE_INT) 1
+					     << (mode_width - 1
+						 - INTVAL (XEXP (op0, 1)))));
+	      code = (code == LT ? NE : EQ);
+	      continue;
+	    }
+
+	  /* Fall through.  */
+
+	case ABS:
+	  /* ABS is ignorable inside an equality comparison with zero.  */
+	  if (const_op == 0 && equality_comparison_p)
+	    {
+	      op0 = XEXP (op0, 0);
+	      continue;
+	    }
+	  break;
+
+	case SIGN_EXTEND:
+	  /* Can simplify (compare (zero/sign_extend FOO) CONST)
+	     to (compare FOO CONST) if CONST fits in FOO's mode and we
+	     are either testing inequality or have an unsigned comparison
+	     with ZERO_EXTEND or a signed comparison with SIGN_EXTEND.  */
+	  if (! unsigned_comparison_p
+	      && (GET_MODE_BITSIZE (GET_MODE (XEXP (op0, 0)))
+		  <= HOST_BITS_PER_WIDE_INT)
+	      && ((unsigned HOST_WIDE_INT) const_op
+		  < (((unsigned HOST_WIDE_INT) 1
+		      << (GET_MODE_BITSIZE (GET_MODE (XEXP (op0, 0))) - 1)))))
+	    {
+	      op0 = XEXP (op0, 0);
+	      continue;
+	    }
+	  break;
+
+	case SUBREG:
+	  /* Check for the case where we are comparing A - C1 with C2, that is
+
+	       (subreg:MODE (plus (A) (-C1))) op (C2)
+
+	     with C1 a constant, and try to lift the SUBREG, i.e. to do the
+	     comparison in the wider mode.  One of the following two conditions
+	     must be true in order for this to be valid:
+
+	       1. The mode extension results in the same bit pattern being added
+		  on both sides and the comparison is equality or unsigned.  As
+		  C2 has been truncated to fit in MODE, the pattern can only be
+		  all 0s or all 1s.
+
+	       2. The mode extension results in the sign bit being copied on
+		  each side.
+
+	     The difficulty here is that we have predicates for A but not for
+	     (A - C1) so we need to check that C1 is within proper bounds so
+	     as to perturbate A as little as possible.  */
+
+	  if (mode_width <= HOST_BITS_PER_WIDE_INT
+	      && subreg_lowpart_p (op0)
+	      && GET_MODE_BITSIZE (GET_MODE (SUBREG_REG (op0))) > mode_width
+	      && GET_CODE (SUBREG_REG (op0)) == PLUS
+	      && GET_CODE (XEXP (SUBREG_REG (op0), 1)) == CONST_INT)
+	    {
+	      enum machine_mode inner_mode = GET_MODE (SUBREG_REG (op0));
+	      rtx a = XEXP (SUBREG_REG (op0), 0);
+	      HOST_WIDE_INT c1 = -INTVAL (XEXP (SUBREG_REG (op0), 1));
+
+	      if ((c1 > 0
+	           && (unsigned HOST_WIDE_INT) c1
+		       < (unsigned HOST_WIDE_INT) 1 << (mode_width - 1)
+		   && (equality_comparison_p || unsigned_comparison_p)
+		   /* (A - C1) zero-extends if it is positive and sign-extends
+		      if it is negative, C2 both zero- and sign-extends.  */
+		   && ((0 == (nonzero_bits (a, inner_mode)
+			      & ~GET_MODE_MASK (mode))
+			&& const_op >= 0)
+		       /* (A - C1) sign-extends if it is positive and 1-extends
+			  if it is negative, C2 both sign- and 1-extends.  */
+		       || (num_sign_bit_copies (a, inner_mode)
+			   > (unsigned int) (GET_MODE_BITSIZE (inner_mode)
+					     - mode_width)
+			   && const_op < 0)))
+		  || ((unsigned HOST_WIDE_INT) c1
+		       < (unsigned HOST_WIDE_INT) 1 << (mode_width - 2)
+		      /* (A - C1) always sign-extends, like C2.  */
+		      && num_sign_bit_copies (a, inner_mode)
+			 > (unsigned int) (GET_MODE_BITSIZE (inner_mode)
+					   - mode_width - 1)))
+		{
+		  op0 = SUBREG_REG (op0);
+		  continue;
+	        }
+	    }
+
+	  /* If the inner mode is narrower and we are extracting the low part,
+	     we can treat the SUBREG as if it were a ZERO_EXTEND.  */
+	  if (subreg_lowpart_p (op0)
+	      && GET_MODE_BITSIZE (GET_MODE (SUBREG_REG (op0))) < mode_width)
+	    /* Fall through */ ;
+	  else
+	    break;
+
+	  /* ... fall through ...  */
+
+	case ZERO_EXTEND:
+	  if ((unsigned_comparison_p || equality_comparison_p)
+	      && (GET_MODE_BITSIZE (GET_MODE (XEXP (op0, 0)))
+		  <= HOST_BITS_PER_WIDE_INT)
+	      && ((unsigned HOST_WIDE_INT) const_op
+		  < GET_MODE_MASK (GET_MODE (XEXP (op0, 0)))))
+	    {
+	      op0 = XEXP (op0, 0);
+	      continue;
+	    }
+	  break;
+
+	case PLUS:
+	  /* (eq (plus X A) B) -> (eq X (minus B A)).  We can only do
+	     this for equality comparisons due to pathological cases involving
+	     overflows.  */
+	  if (equality_comparison_p
+	      && 0 != (tem = simplify_binary_operation (MINUS, mode,
+							op1, XEXP (op0, 1))))
+	    {
+	      op0 = XEXP (op0, 0);
+	      op1 = tem;
+	      continue;
+	    }
+
+	  /* (plus (abs X) (const_int -1)) is < 0 if and only if X == 0.  */
+	  if (const_op == 0 && XEXP (op0, 1) == constm1_rtx
+	      && GET_CODE (XEXP (op0, 0)) == ABS && sign_bit_comparison_p)
+	    {
+	      op0 = XEXP (XEXP (op0, 0), 0);
+	      code = (code == LT ? EQ : NE);
+	      continue;
+	    }
+	  break;
+
+	case MINUS:
+	  /* We used to optimize signed comparisons against zero, but that
+	     was incorrect.  Unsigned comparisons against zero (GTU, LEU)
+	     arrive here as equality comparisons, or (GEU, LTU) are
+	     optimized away.  No need to special-case them.  */
+
+	  /* (eq (minus A B) C) -> (eq A (plus B C)) or
+	     (eq B (minus A C)), whichever simplifies.  We can only do
+	     this for equality comparisons due to pathological cases involving
+	     overflows.  */
+	  if (equality_comparison_p
+	      && 0 != (tem = simplify_binary_operation (PLUS, mode,
+							XEXP (op0, 1), op1)))
+	    {
+	      op0 = XEXP (op0, 0);
+	      op1 = tem;
+	      continue;
+	    }
+
+	  if (equality_comparison_p
+	      && 0 != (tem = simplify_binary_operation (MINUS, mode,
+							XEXP (op0, 0), op1)))
+	    {
+	      op0 = XEXP (op0, 1);
+	      op1 = tem;
+	      continue;
+	    }
+
+	  /* The sign bit of (minus (ashiftrt X C) X), where C is the number
+	     of bits in X minus 1, is one iff X > 0.  */
+	  if (sign_bit_comparison_p && GET_CODE (XEXP (op0, 0)) == ASHIFTRT
+	      && GET_CODE (XEXP (XEXP (op0, 0), 1)) == CONST_INT
+	      && (unsigned HOST_WIDE_INT) INTVAL (XEXP (XEXP (op0, 0), 1))
+		 == mode_width - 1
+	      && rtx_equal_p (XEXP (XEXP (op0, 0), 0), XEXP (op0, 1)))
+	    {
+	      op0 = XEXP (op0, 1);
+	      code = (code == GE ? LE : GT);
+	      continue;
+	    }
+	  break;
+
+	case XOR:
+	  /* (eq (xor A B) C) -> (eq A (xor B C)).  This is a simplification
+	     if C is zero or B is a constant.  */
+	  if (equality_comparison_p
+	      && 0 != (tem = simplify_binary_operation (XOR, mode,
+							XEXP (op0, 1), op1)))
+	    {
+	      op0 = XEXP (op0, 0);
+	      op1 = tem;
+	      continue;
+	    }
+	  break;
+
+	case EQ:  case NE:
+	case UNEQ:  case LTGT:
+	case LT:  case LTU:  case UNLT:  case LE:  case LEU:  case UNLE:
+	case GT:  case GTU:  case UNGT:  case GE:  case GEU:  case UNGE:
+        case UNORDERED: case ORDERED:
+	  /* We can't do anything if OP0 is a condition code value, rather
+	     than an actual data value.  */
+	  if (const_op != 0
+	      || CC0_P (XEXP (op0, 0))
+	      || GET_MODE_CLASS (GET_MODE (XEXP (op0, 0))) == MODE_CC)
+	    break;
+
+	  /* Get the two operands being compared.  */
+	  if (GET_CODE (XEXP (op0, 0)) == COMPARE)
+	    tem = XEXP (XEXP (op0, 0), 0), tem1 = XEXP (XEXP (op0, 0), 1);
+	  else
+	    tem = XEXP (op0, 0), tem1 = XEXP (op0, 1);
+
+	  /* Check for the cases where we simply want the result of the
+	     earlier test or the opposite of that result.  */
+	  if (code == NE || code == EQ
+	      || (GET_MODE_BITSIZE (GET_MODE (op0)) <= HOST_BITS_PER_WIDE_INT
+		  && GET_MODE_CLASS (GET_MODE (op0)) == MODE_INT
+		  && (STORE_FLAG_VALUE
+		      & (((HOST_WIDE_INT) 1
+			  << (GET_MODE_BITSIZE (GET_MODE (op0)) - 1))))
+		  && (code == LT || code == GE)))
+	    {
+	      enum rtx_code new_code;
+	      if (code == LT || code == NE)
+		new_code = GET_CODE (op0);
+	      else
+		new_code = combine_reversed_comparison_code (op0);
+
+	      if (new_code != UNKNOWN)
+		{
+		  code = new_code;
+		  op0 = tem;
+		  op1 = tem1;
+		  continue;
+		}
+	    }
+	  break;
+
+	case IOR:
+	  /* The sign bit of (ior (plus X (const_int -1)) X) is nonzero
+	     iff X <= 0.  */
+	  if (sign_bit_comparison_p && GET_CODE (XEXP (op0, 0)) == PLUS
+	      && XEXP (XEXP (op0, 0), 1) == constm1_rtx
+	      && rtx_equal_p (XEXP (XEXP (op0, 0), 0), XEXP (op0, 1)))
+	    {
+	      op0 = XEXP (op0, 1);
+	      code = (code == GE ? GT : LE);
+	      continue;
+	    }
+	  break;
+
+	case AND:
+	  /* Convert (and (xshift 1 X) Y) to (and (lshiftrt Y X) 1).  This
+	     will be converted to a ZERO_EXTRACT later.  */
+	  if (const_op == 0 && equality_comparison_p
+	      && GET_CODE (XEXP (op0, 0)) == ASHIFT
+	      && XEXP (XEXP (op0, 0), 0) == const1_rtx)
+	    {
+	      op0 = simplify_and_const_int
+		(op0, mode, gen_rtx_LSHIFTRT (mode,
+					      XEXP (op0, 1),
+					      XEXP (XEXP (op0, 0), 1)),
+		 (HOST_WIDE_INT) 1);
+	      continue;
+	    }
+
+	  /* If we are comparing (and (lshiftrt X C1) C2) for equality with
+	     zero and X is a comparison and C1 and C2 describe only bits set
+	     in STORE_FLAG_VALUE, we can compare with X.  */
+	  if (const_op == 0 && equality_comparison_p
+	      && mode_width <= HOST_BITS_PER_WIDE_INT
+	      && GET_CODE (XEXP (op0, 1)) == CONST_INT
+	      && GET_CODE (XEXP (op0, 0)) == LSHIFTRT
+	      && GET_CODE (XEXP (XEXP (op0, 0), 1)) == CONST_INT
+	      && INTVAL (XEXP (XEXP (op0, 0), 1)) >= 0
+	      && INTVAL (XEXP (XEXP (op0, 0), 1)) < HOST_BITS_PER_WIDE_INT)
+	    {
+	      mask = ((INTVAL (XEXP (op0, 1)) & GET_MODE_MASK (mode))
+		      << INTVAL (XEXP (XEXP (op0, 0), 1)));
+	      if ((~STORE_FLAG_VALUE & mask) == 0
+		  && (GET_RTX_CLASS (GET_CODE (XEXP (XEXP (op0, 0), 0))) == '<'
+		      || ((tem = get_last_value (XEXP (XEXP (op0, 0), 0))) != 0
+			  && GET_RTX_CLASS (GET_CODE (tem)) == '<')))
+		{
+		  op0 = XEXP (XEXP (op0, 0), 0);
+		  continue;
+		}
+	    }
+
+	  /* If we are doing an equality comparison of an AND of a bit equal
+	     to the sign bit, replace this with a LT or GE comparison of
+	     the underlying value.  */
+	  if (equality_comparison_p
+	      && const_op == 0
+	      && GET_CODE (XEXP (op0, 1)) == CONST_INT
+	      && mode_width <= HOST_BITS_PER_WIDE_INT
+	      && ((INTVAL (XEXP (op0, 1)) & GET_MODE_MASK (mode))
+		  == (unsigned HOST_WIDE_INT) 1 << (mode_width - 1)))
+	    {
+	      op0 = XEXP (op0, 0);
+	      code = (code == EQ ? GE : LT);
+	      continue;
+	    }
+
+	  /* If this AND operation is really a ZERO_EXTEND from a narrower
+	     mode, the constant fits within that mode, and this is either an
+	     equality or unsigned comparison, try to do this comparison in
+	     the narrower mode.  */
+	  if ((equality_comparison_p || unsigned_comparison_p)
+	      && GET_CODE (XEXP (op0, 1)) == CONST_INT
+	      && (i = exact_log2 ((INTVAL (XEXP (op0, 1))
+				   & GET_MODE_MASK (mode))
+				  + 1)) >= 0
+	      && const_op >> i == 0
+	      && (tmode = mode_for_size (i, MODE_INT, 1)) != BLKmode)
+	    {
+	      op0 = gen_lowpart_for_combine (tmode, XEXP (op0, 0));
+	      continue;
+	    }
+
+	  /* If this is (and:M1 (subreg:M2 X 0) (const_int C1)) where C1
+	     fits in both M1 and M2 and the SUBREG is either paradoxical
+	     or represents the low part, permute the SUBREG and the AND
+	     and try again.  */
+	  if (GET_CODE (XEXP (op0, 0)) == SUBREG)
+	    {
+	      unsigned HOST_WIDE_INT c1;
+	      tmode = GET_MODE (SUBREG_REG (XEXP (op0, 0)));
+	      /* Require an integral mode, to avoid creating something like
+		 (AND:SF ...).  */
+	      if (SCALAR_INT_MODE_P (tmode)
+		  /* It is unsafe to commute the AND into the SUBREG if the
+		     SUBREG is paradoxical and WORD_REGISTER_OPERATIONS is
+		     not defined.  As originally written the upper bits
+		     have a defined value due to the AND operation.
+		     However, if we commute the AND inside the SUBREG then
+		     they no longer have defined values and the meaning of
+		     the code has been changed.  */
+		  && (0
+#ifdef WORD_REGISTER_OPERATIONS
+		      || (mode_width > GET_MODE_BITSIZE (tmode)
+			  && mode_width <= BITS_PER_WORD)
+#endif
+		      || (mode_width <= GET_MODE_BITSIZE (tmode)
+			  && subreg_lowpart_p (XEXP (op0, 0))))
+		  && GET_CODE (XEXP (op0, 1)) == CONST_INT
+		  && mode_width <= HOST_BITS_PER_WIDE_INT
+		  && GET_MODE_BITSIZE (tmode) <= HOST_BITS_PER_WIDE_INT
+		  && ((c1 = INTVAL (XEXP (op0, 1))) & ~mask) == 0
+		  && (c1 & ~GET_MODE_MASK (tmode)) == 0
+		  && c1 != mask
+		  && c1 != GET_MODE_MASK (tmode))
+		{
+		  op0 = gen_binary (AND, tmode,
+				    SUBREG_REG (XEXP (op0, 0)),
+				    gen_int_mode (c1, tmode));
+		  op0 = gen_lowpart_for_combine (mode, op0);
+		  continue;
+		}
+	    }
+
+	  /* Convert (ne (and (not X) 1) 0) to (eq (and X 1) 0).  */
+	  if (const_op == 0 && equality_comparison_p
+	      && XEXP (op0, 1) == const1_rtx
+	      && GET_CODE (XEXP (op0, 0)) == NOT)
+	    {
+	      op0 = simplify_and_const_int
+		(NULL_RTX, mode, XEXP (XEXP (op0, 0), 0), (HOST_WIDE_INT) 1);
+	      code = (code == NE ? EQ : NE);
+	      continue;
+	    }
+
+	  /* Convert (ne (and (lshiftrt (not X)) 1) 0) to
+	     (eq (and (lshiftrt X) 1) 0).
+	     Also handle the case where (not X) is expressed using xor.  */
+	  if (const_op == 0 && equality_comparison_p
+	      && XEXP (op0, 1) == const1_rtx
+	      && GET_CODE (XEXP (op0, 0)) == LSHIFTRT)
+	    {
+	      rtx shift_op = XEXP (XEXP (op0, 0), 0);
+	      rtx shift_count = XEXP (XEXP (op0, 0), 1);
+
+	      if (GET_CODE (shift_op) == NOT
+		  || (GET_CODE (shift_op) == XOR
+		      && GET_CODE (XEXP (shift_op, 1)) == CONST_INT
+		      && GET_CODE (shift_count) == CONST_INT
+		      && GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT
+		      && (INTVAL (XEXP (shift_op, 1))
+			  == (HOST_WIDE_INT) 1 << INTVAL (shift_count))))
+		{
+		  op0 = simplify_and_const_int
+		    (NULL_RTX, mode,
+		     gen_rtx_LSHIFTRT (mode, XEXP (shift_op, 0), shift_count),
+		     (HOST_WIDE_INT) 1);
+		  code = (code == NE ? EQ : NE);
+		  continue;
+		}
+	    }
+	  break;
+
+	case ASHIFT:
+	  /* If we have (compare (ashift FOO N) (const_int C)) and
+	     the high order N bits of FOO (N+1 if an inequality comparison)
+	     are known to be zero, we can do this by comparing FOO with C
+	     shifted right N bits so long as the low-order N bits of C are
+	     zero.  */
+	  if (GET_CODE (XEXP (op0, 1)) == CONST_INT
+	      && INTVAL (XEXP (op0, 1)) >= 0
+	      && ((INTVAL (XEXP (op0, 1)) + ! equality_comparison_p)
+		  < HOST_BITS_PER_WIDE_INT)
+	      && ((const_op
+		   & (((HOST_WIDE_INT) 1 << INTVAL (XEXP (op0, 1))) - 1)) == 0)
+	      && mode_width <= HOST_BITS_PER_WIDE_INT
+	      && (nonzero_bits (XEXP (op0, 0), mode)
+		  & ~(mask >> (INTVAL (XEXP (op0, 1))
+			       + ! equality_comparison_p))) == 0)
+	    {
+	      /* We must perform a logical shift, not an arithmetic one,
+		 as we want the top N bits of C to be zero.  */
+	      unsigned HOST_WIDE_INT temp = const_op & GET_MODE_MASK (mode);
+
+	      temp >>= INTVAL (XEXP (op0, 1));
+	      op1 = gen_int_mode (temp, mode);
+	      op0 = XEXP (op0, 0);
+	      continue;
+	    }
+
+	  /* If we are doing a sign bit comparison, it means we are testing
+	     a particular bit.  Convert it to the appropriate AND.  */
+	  if (sign_bit_comparison_p && GET_CODE (XEXP (op0, 1)) == CONST_INT
+	      && mode_width <= HOST_BITS_PER_WIDE_INT)
+	    {
+	      op0 = simplify_and_const_int (NULL_RTX, mode, XEXP (op0, 0),
+					    ((HOST_WIDE_INT) 1
+					     << (mode_width - 1
+						 - INTVAL (XEXP (op0, 1)))));
+	      code = (code == LT ? NE : EQ);
+	      continue;
+	    }
+
+	  /* If this an equality comparison with zero and we are shifting
+	     the low bit to the sign bit, we can convert this to an AND of the
+	     low-order bit.  */
+	  if (const_op == 0 && equality_comparison_p
+	      && GET_CODE (XEXP (op0, 1)) == CONST_INT
+	      && (unsigned HOST_WIDE_INT) INTVAL (XEXP (op0, 1))
+		 == mode_width - 1)
+	    {
+	      op0 = simplify_and_const_int (NULL_RTX, mode, XEXP (op0, 0),
+					    (HOST_WIDE_INT) 1);
+	      continue;
+	    }
+	  break;
+
+	case ASHIFTRT:
+	  /* If this is an equality comparison with zero, we can do this
+	     as a logical shift, which might be much simpler.  */
+	  if (equality_comparison_p && const_op == 0
+	      && GET_CODE (XEXP (op0, 1)) == CONST_INT)
+	    {
+	      op0 = simplify_shift_const (NULL_RTX, LSHIFTRT, mode,
+					  XEXP (op0, 0),
+					  INTVAL (XEXP (op0, 1)));
+	      continue;
+	    }
+
+	  /* If OP0 is a sign extension and CODE is not an unsigned comparison,
+	     do the comparison in a narrower mode.  */
+	  if (! unsigned_comparison_p
+	      && GET_CODE (XEXP (op0, 1)) == CONST_INT
+	      && GET_CODE (XEXP (op0, 0)) == ASHIFT
+	      && XEXP (op0, 1) == XEXP (XEXP (op0, 0), 1)
+	      && (tmode = mode_for_size (mode_width - INTVAL (XEXP (op0, 1)),
+					 MODE_INT, 1)) != BLKmode
+	      && (((unsigned HOST_WIDE_INT) const_op
+		   + (GET_MODE_MASK (tmode) >> 1) + 1)
+		  <= GET_MODE_MASK (tmode)))
+	    {
+	      op0 = gen_lowpart_for_combine (tmode, XEXP (XEXP (op0, 0), 0));
+	      continue;
+	    }
+
+	  /* Likewise if OP0 is a PLUS of a sign extension with a
+	     constant, which is usually represented with the PLUS
+	     between the shifts.  */
+	  if (! unsigned_comparison_p
+	      && GET_CODE (XEXP (op0, 1)) == CONST_INT
+	      && GET_CODE (XEXP (op0, 0)) == PLUS
+	      && GET_CODE (XEXP (XEXP (op0, 0), 1)) == CONST_INT
+	      && GET_CODE (XEXP (XEXP (op0, 0), 0)) == ASHIFT
+	      && XEXP (op0, 1) == XEXP (XEXP (XEXP (op0, 0), 0), 1)
+	      && (tmode = mode_for_size (mode_width - INTVAL (XEXP (op0, 1)),
+					 MODE_INT, 1)) != BLKmode
+	      && (((unsigned HOST_WIDE_INT) const_op
+		   + (GET_MODE_MASK (tmode) >> 1) + 1)
+		  <= GET_MODE_MASK (tmode)))
+	    {
+	      rtx inner = XEXP (XEXP (XEXP (op0, 0), 0), 0);
+	      rtx add_const = XEXP (XEXP (op0, 0), 1);
+	      rtx new_const = gen_binary (ASHIFTRT, GET_MODE (op0), add_const,
+					  XEXP (op0, 1));
+
+	      op0 = gen_binary (PLUS, tmode,
+				gen_lowpart_for_combine (tmode, inner),
+				new_const);
+	      continue;
+	    }
+
+	  /* ... fall through ...  */
+	case LSHIFTRT:
+	  /* If we have (compare (xshiftrt FOO N) (const_int C)) and
+	     the low order N bits of FOO are known to be zero, we can do this
+	     by comparing FOO with C shifted left N bits so long as no
+	     overflow occurs.  */
+	  if (GET_CODE (XEXP (op0, 1)) == CONST_INT
+	      && INTVAL (XEXP (op0, 1)) >= 0
+	      && INTVAL (XEXP (op0, 1)) < HOST_BITS_PER_WIDE_INT
+	      && mode_width <= HOST_BITS_PER_WIDE_INT
+	      && (nonzero_bits (XEXP (op0, 0), mode)
+		  & (((HOST_WIDE_INT) 1 << INTVAL (XEXP (op0, 1))) - 1)) == 0
+	      && (((unsigned HOST_WIDE_INT) const_op
+		   + (GET_CODE (op0) != LSHIFTRT
+		      ? ((GET_MODE_MASK (mode) >> INTVAL (XEXP (op0, 1)) >> 1)
+			 + 1)
+		      : 0))
+		  <= GET_MODE_MASK (mode) >> INTVAL (XEXP (op0, 1))))
+	    {
+	      /* If the shift was logical, then we must make the condition
+		 unsigned.  */
+	      if (GET_CODE (op0) == LSHIFTRT)
+		code = unsigned_condition (code);
+
+	      const_op <<= INTVAL (XEXP (op0, 1));
+	      op1 = GEN_INT (const_op);
+	      op0 = XEXP (op0, 0);
+	      continue;
+	    }
+
+	  /* If we are using this shift to extract just the sign bit, we
+	     can replace this with an LT or GE comparison.  */
+	  if (const_op == 0
+	      && (equality_comparison_p || sign_bit_comparison_p)
+	      && GET_CODE (XEXP (op0, 1)) == CONST_INT
+	      && (unsigned HOST_WIDE_INT) INTVAL (XEXP (op0, 1))
+		 == mode_width - 1)
+	    {
+	      op0 = XEXP (op0, 0);
+	      code = (code == NE || code == GT ? LT : GE);
+	      continue;
+	    }
+	  break;
+
+	default:
+	  break;
+	}
+
+      break;
+    }
+
+  /* Now make any compound operations involved in this comparison.  Then,
+     check for an outmost SUBREG on OP0 that is not doing anything or is
+     paradoxical.  The latter transformation must only be performed when
+     it is known that the "extra" bits will be the same in op0 and op1 or
+     that they don't matter.  There are three cases to consider:
+
+     1. SUBREG_REG (op0) is a register.  In this case the bits are don't
+     care bits and we can assume they have any convenient value.  So
+     making the transformation is safe.
+
+     2. SUBREG_REG (op0) is a memory and LOAD_EXTEND_OP is not defined.
+     In this case the upper bits of op0 are undefined.  We should not make
+     the simplification in that case as we do not know the contents of
+     those bits.
+
+     3. SUBREG_REG (op0) is a memory and LOAD_EXTEND_OP is defined and not
+     NIL.  In that case we know those bits are zeros or ones.  We must
+     also be sure that they are the same as the upper bits of op1.
+
+     We can never remove a SUBREG for a non-equality comparison because
+     the sign bit is in a different place in the underlying object.  */
+
+  op0 = make_compound_operation (op0, op1 == const0_rtx ? COMPARE : SET);
+  op1 = make_compound_operation (op1, SET);
+
+  if (GET_CODE (op0) == SUBREG && subreg_lowpart_p (op0)
+      && GET_MODE_CLASS (GET_MODE (op0)) == MODE_INT
+      && GET_MODE_CLASS (GET_MODE (SUBREG_REG (op0))) == MODE_INT
+      && (code == NE || code == EQ))
+    {
+      if (GET_MODE_SIZE (GET_MODE (op0))
+	  > GET_MODE_SIZE (GET_MODE (SUBREG_REG (op0))))
+	{
+	  /* For paradoxical subregs, allow case 1 as above.  Case 3 isn't
+	     implemented.  */
+          if (GET_CODE (SUBREG_REG (op0)) == REG)
+	    {
+	      op0 = SUBREG_REG (op0);
+	      op1 = gen_lowpart_for_combine (GET_MODE (op0), op1);
+	    }
+	}
+      else if ((GET_MODE_BITSIZE (GET_MODE (SUBREG_REG (op0)))
+		<= HOST_BITS_PER_WIDE_INT)
+	       && (nonzero_bits (SUBREG_REG (op0),
+				 GET_MODE (SUBREG_REG (op0)))
+		   & ~GET_MODE_MASK (GET_MODE (op0))) == 0)
+	{
+	  tem = gen_lowpart_for_combine (GET_MODE (SUBREG_REG (op0)), op1);
+
+	  if ((nonzero_bits (tem, GET_MODE (SUBREG_REG (op0)))
+	       & ~GET_MODE_MASK (GET_MODE (op0))) == 0)
+	    op0 = SUBREG_REG (op0), op1 = tem;
+	}
+    }
+
+  /* We now do the opposite procedure: Some machines don't have compare
+     insns in all modes.  If OP0's mode is an integer mode smaller than a
+     word and we can't do a compare in that mode, see if there is a larger
+     mode for which we can do the compare.  There are a number of cases in
+     which we can use the wider mode.  */
+
+  mode = GET_MODE (op0);
+  if (mode != VOIDmode && GET_MODE_CLASS (mode) == MODE_INT
+      && GET_MODE_SIZE (mode) < UNITS_PER_WORD
+      && ! have_insn_for (COMPARE, mode))
+    for (tmode = GET_MODE_WIDER_MODE (mode);
+	 (tmode != VOIDmode
+	  && GET_MODE_BITSIZE (tmode) <= HOST_BITS_PER_WIDE_INT);
+	 tmode = GET_MODE_WIDER_MODE (tmode))
+      if (have_insn_for (COMPARE, tmode))
+	{
+	  int zero_extended;
+
+	  /* If the only nonzero bits in OP0 and OP1 are those in the
+	     narrower mode and this is an equality or unsigned comparison,
+	     we can use the wider mode.  Similarly for sign-extended
+	     values, in which case it is true for all comparisons.  */
+	  zero_extended = ((code == EQ || code == NE
+			    || code == GEU || code == GTU
+			    || code == LEU || code == LTU)
+			   && (nonzero_bits (op0, tmode)
+			       & ~GET_MODE_MASK (mode)) == 0
+			   && ((GET_CODE (op1) == CONST_INT
+				|| (nonzero_bits (op1, tmode)
+				    & ~GET_MODE_MASK (mode)) == 0)));
+
+	  if (zero_extended
+	      || ((num_sign_bit_copies (op0, tmode)
+		   > (unsigned int) (GET_MODE_BITSIZE (tmode)
+				     - GET_MODE_BITSIZE (mode)))
+		  && (num_sign_bit_copies (op1, tmode)
+		      > (unsigned int) (GET_MODE_BITSIZE (tmode)
+					- GET_MODE_BITSIZE (mode)))))
+	    {
+	      /* If OP0 is an AND and we don't have an AND in MODE either,
+		 make a new AND in the proper mode.  */
+	      if (GET_CODE (op0) == AND
+		  && !have_insn_for (AND, mode))
+		op0 = gen_binary (AND, tmode,
+				  gen_lowpart_for_combine (tmode,
+							   XEXP (op0, 0)),
+				  gen_lowpart_for_combine (tmode,
+							   XEXP (op0, 1)));
+
+	      op0 = gen_lowpart_for_combine (tmode, op0);
+	      if (zero_extended && GET_CODE (op1) == CONST_INT)
+		op1 = GEN_INT (INTVAL (op1) & GET_MODE_MASK (mode));
+	      op1 = gen_lowpart_for_combine (tmode, op1);
+	      break;
+	    }
+
+	  /* If this is a test for negative, we can make an explicit
+	     test of the sign bit.  */
+
+	  if (op1 == const0_rtx && (code == LT || code == GE)
+	      && GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT)
+	    {
+	      op0 = gen_binary (AND, tmode,
+				gen_lowpart_for_combine (tmode, op0),
+				GEN_INT ((HOST_WIDE_INT) 1
+					 << (GET_MODE_BITSIZE (mode) - 1)));
+	      code = (code == LT) ? NE : EQ;
+	      break;
+	    }
+	}
+
+#ifdef CANONICALIZE_COMPARISON
+  /* If this machine only supports a subset of valid comparisons, see if we
+     can convert an unsupported one into a supported one.  */
+  CANONICALIZE_COMPARISON (code, op0, op1);
+#endif
+
+  *pop0 = op0;
+  *pop1 = op1;
+
+  return code;
+}
+
+/* Like jump.c' reversed_comparison_code, but use combine infrastructure for
+   searching backward.  */
+static enum rtx_code
+combine_reversed_comparison_code (rtx exp)
+{
+  enum rtx_code code1 = reversed_comparison_code (exp, NULL);
+  rtx x;
+
+  if (code1 != UNKNOWN
+      || GET_MODE_CLASS (GET_MODE (XEXP (exp, 0))) != MODE_CC)
+    return code1;
+  /* Otherwise try and find where the condition codes were last set and
+     use that.  */
+  x = get_last_value (XEXP (exp, 0));
+  if (!x || GET_CODE (x) != COMPARE)
+    return UNKNOWN;
+  return reversed_comparison_code_parts (GET_CODE (exp),
+					 XEXP (x, 0), XEXP (x, 1), NULL);
+}
+
+/* Return comparison with reversed code of EXP and operands OP0 and OP1.
+   Return NULL_RTX in case we fail to do the reversal.  */
+static rtx
+reversed_comparison (rtx exp, enum machine_mode mode, rtx op0, rtx op1)
+{
+  enum rtx_code reversed_code = combine_reversed_comparison_code (exp);
+  if (reversed_code == UNKNOWN)
+    return NULL_RTX;
+  else
+    return gen_binary (reversed_code, mode, op0, op1);
+}
+
+/* Utility function for record_value_for_reg.  Count number of
+   rtxs in X.  */
+static int
+count_rtxs (rtx x)
+{
+  enum rtx_code code = GET_CODE (x);
+  const char *fmt;
+  int i, ret = 1;
+
+  if (GET_RTX_CLASS (code) == '2'
+      || GET_RTX_CLASS (code) == 'c')
+    {
+      rtx x0 = XEXP (x, 0);
+      rtx x1 = XEXP (x, 1);
+
+      if (x0 == x1)
+	return 1 + 2 * count_rtxs (x0);
+
+      if ((GET_RTX_CLASS (GET_CODE (x1)) == '2'
+	   || GET_RTX_CLASS (GET_CODE (x1)) == 'c')
+	  && (x0 == XEXP (x1, 0) || x0 == XEXP (x1, 1)))
+	return 2 + 2 * count_rtxs (x0)
+	       + count_rtxs (x == XEXP (x1, 0)
+			     ? XEXP (x1, 1) : XEXP (x1, 0));
+
+      if ((GET_RTX_CLASS (GET_CODE (x0)) == '2'
+	   || GET_RTX_CLASS (GET_CODE (x0)) == 'c')
+	  && (x1 == XEXP (x0, 0) || x1 == XEXP (x0, 1)))
+	return 2 + 2 * count_rtxs (x1)
+	       + count_rtxs (x == XEXP (x0, 0)
+			     ? XEXP (x0, 1) : XEXP (x0, 0));
+    }
+
+  fmt = GET_RTX_FORMAT (code);
+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)
+    if (fmt[i] == 'e')
+      ret += count_rtxs (XEXP (x, i));
+
+  return ret;
+}
+
+/* Utility function for following routine.  Called when X is part of a value
+   being stored into reg_last_set_value.  Sets reg_last_set_table_tick
+   for each register mentioned.  Similar to mention_regs in cse.c  */
+
+static void
+update_table_tick (rtx x)
+{
+  enum rtx_code code = GET_CODE (x);
+  const char *fmt = GET_RTX_FORMAT (code);
+  int i;
+
+  if (code == REG)
+    {
+      unsigned int regno = REGNO (x);
+      unsigned int endregno
+	= regno + (regno < FIRST_PSEUDO_REGISTER
+		   ? HARD_REGNO_NREGS (regno, GET_MODE (x)) : 1);
+      unsigned int r;
+
+      for (r = regno; r < endregno; r++)
+	reg_last_set_table_tick[r] = label_tick;
+
+      return;
+    }
+
+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)
+    /* Note that we can't have an "E" in values stored; see
+       get_last_value_validate.  */
+    if (fmt[i] == 'e')
+      {
+	/* Check for identical subexpressions.  If x contains
+	   identical subexpression we only have to traverse one of
+	   them.  */
+	if (i == 0
+	    && (GET_RTX_CLASS (code) == '2'
+		|| GET_RTX_CLASS (code) == 'c'))
+	  {
+	    /* Note that at this point x1 has already been
+	       processed.  */
+	    rtx x0 = XEXP (x, 0);
+	    rtx x1 = XEXP (x, 1);
+
+	    /* If x0 and x1 are identical then there is no need to
+	       process x0.  */
+	    if (x0 == x1)
+	      break;
+
+	    /* If x0 is identical to a subexpression of x1 then while
+	       processing x1, x0 has already been processed.  Thus we
+	       are done with x.  */
+	    if ((GET_RTX_CLASS (GET_CODE (x1)) == '2'
+		 || GET_RTX_CLASS (GET_CODE (x1)) == 'c')
+		&& (x0 == XEXP (x1, 0) || x0 == XEXP (x1, 1)))
+	      break;
+
+	    /* If x1 is identical to a subexpression of x0 then we
+	       still have to process the rest of x0.  */
+	    if ((GET_RTX_CLASS (GET_CODE (x0)) == '2'
+		 || GET_RTX_CLASS (GET_CODE (x0)) == 'c')
+		&& (x1 == XEXP (x0, 0) || x1 == XEXP (x0, 1)))
+	      {
+		update_table_tick (XEXP (x0, x1 == XEXP (x0, 0) ? 1 : 0));
+		break;
+	      }
+	  }
+
+	update_table_tick (XEXP (x, i));
+      }
+}
+
+/* Record that REG is set to VALUE in insn INSN.  If VALUE is zero, we
+   are saying that the register is clobbered and we no longer know its
+   value.  If INSN is zero, don't update reg_last_set; this is only permitted
+   with VALUE also zero and is used to invalidate the register.  */
+
+static void
+record_value_for_reg (rtx reg, rtx insn, rtx value)
+{
+  unsigned int regno = REGNO (reg);
+  unsigned int endregno
+    = regno + (regno < FIRST_PSEUDO_REGISTER
+	       ? HARD_REGNO_NREGS (regno, GET_MODE (reg)) : 1);
+  unsigned int i;
+
+  /* If VALUE contains REG and we have a previous value for REG, substitute
+     the previous value.  */
+  if (value && insn && reg_overlap_mentioned_p (reg, value))
+    {
+      rtx tem;
+
+      /* Set things up so get_last_value is allowed to see anything set up to
+	 our insn.  */
+      subst_low_cuid = INSN_CUID (insn);
+      tem = get_last_value (reg);
+
+      /* If TEM is simply a binary operation with two CLOBBERs as operands,
+	 it isn't going to be useful and will take a lot of time to process,
+	 so just use the CLOBBER.  */
+
+      if (tem)
+	{
+	  if ((GET_RTX_CLASS (GET_CODE (tem)) == '2'
+	       || GET_RTX_CLASS (GET_CODE (tem)) == 'c')
+	      && GET_CODE (XEXP (tem, 0)) == CLOBBER
+	      && GET_CODE (XEXP (tem, 1)) == CLOBBER)
+	    tem = XEXP (tem, 0);
+	  else if (count_occurrences (value, reg, 1) >= 2)
+	    {
+	      /* If there are two or more occurrences of REG in VALUE,
+		 prevent the value from growing too much.  */
+	      if (count_rtxs (tem) > MAX_LAST_VALUE_RTL)
+		tem = gen_rtx_CLOBBER (GET_MODE (tem), const0_rtx);
+	    }
+
+	  value = replace_rtx (copy_rtx (value), reg, tem);
+	}
+    }
+
+  /* For each register modified, show we don't know its value, that
+     we don't know about its bitwise content, that its value has been
+     updated, and that we don't know the location of the death of the
+     register.  */
+  for (i = regno; i < endregno; i++)
+    {
+      if (insn)
+	reg_last_set[i] = insn;
+
+      reg_last_set_value[i] = 0;
+      reg_last_set_mode[i] = 0;
+      reg_last_set_nonzero_bits[i] = 0;
+      reg_last_set_sign_bit_copies[i] = 0;
+      reg_last_death[i] = 0;
+    }
+
+  /* Mark registers that are being referenced in this value.  */
+  if (value)
+    update_table_tick (value);
+
+  /* Now update the status of each register being set.
+     If someone is using this register in this block, set this register
+     to invalid since we will get confused between the two lives in this
+     basic block.  This makes using this register always invalid.  In cse, we
+     scan the table to invalidate all entries using this register, but this
+     is too much work for us.  */
+
+  for (i = regno; i < endregno; i++)
+    {
+      reg_last_set_label[i] = label_tick;
+      if (value && reg_last_set_table_tick[i] == label_tick)
+	reg_last_set_invalid[i] = 1;
+      else
+	reg_last_set_invalid[i] = 0;
+    }
+
+  /* The value being assigned might refer to X (like in "x++;").  In that
+     case, we must replace it with (clobber (const_int 0)) to prevent
+     infinite loops.  */
+  if (value && ! get_last_value_validate (&value, insn,
+					  reg_last_set_label[regno], 0))
+    {
+      value = copy_rtx (value);
+      if (! get_last_value_validate (&value, insn,
+				     reg_last_set_label[regno], 1))
+	value = 0;
+    }
+
+  /* For the main register being modified, update the value, the mode, the
+     nonzero bits, and the number of sign bit copies.  */
+
+  reg_last_set_value[regno] = value;
+
+  if (value)
+    {
+      enum machine_mode mode = GET_MODE (reg);
+      subst_low_cuid = INSN_CUID (insn);
+      reg_last_set_mode[regno] = mode;
+      if (GET_MODE_CLASS (mode) == MODE_INT
+	  && GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT)
+	mode = nonzero_bits_mode;
+      reg_last_set_nonzero_bits[regno] = nonzero_bits (value, mode);
+      reg_last_set_sign_bit_copies[regno]
+	= num_sign_bit_copies (value, GET_MODE (reg));
+    }
+}
+
+/* Called via note_stores from record_dead_and_set_regs to handle one
+   SET or CLOBBER in an insn.  DATA is the instruction in which the
+   set is occurring.  */
+
+static void
+record_dead_and_set_regs_1 (rtx dest, rtx setter, void *data)
+{
+  rtx record_dead_insn = (rtx) data;
+
+  if (GET_CODE (dest) == SUBREG)
+    dest = SUBREG_REG (dest);
+
+  if (GET_CODE (dest) == REG)
+    {
+      /* If we are setting the whole register, we know its value.  Otherwise
+	 show that we don't know the value.  We can handle SUBREG in
+	 some cases.  */
+      if (GET_CODE (setter) == SET && dest == SET_DEST (setter))
+	record_value_for_reg (dest, record_dead_insn, SET_SRC (setter));
+      else if (GET_CODE (setter) == SET
+	       && GET_CODE (SET_DEST (setter)) == SUBREG
+	       && SUBREG_REG (SET_DEST (setter)) == dest
+	       && GET_MODE_BITSIZE (GET_MODE (dest)) <= BITS_PER_WORD
+	       && subreg_lowpart_p (SET_DEST (setter)))
+	record_value_for_reg (dest, record_dead_insn,
+			      gen_lowpart_for_combine (GET_MODE (dest),
+						       SET_SRC (setter)));
+      else
+	record_value_for_reg (dest, record_dead_insn, NULL_RTX);
+    }
+  else if (GET_CODE (dest) == MEM
+	   /* Ignore pushes, they clobber nothing.  */
+	   && ! push_operand (dest, GET_MODE (dest)))
+    mem_last_set = INSN_CUID (record_dead_insn);
+}
+
+/* Update the records of when each REG was most recently set or killed
+   for the things done by INSN.  This is the last thing done in processing
+   INSN in the combiner loop.
+
+   We update reg_last_set, reg_last_set_value, reg_last_set_mode,
+   reg_last_set_nonzero_bits, reg_last_set_sign_bit_copies, reg_last_death,
+   and also the similar information mem_last_set (which insn most recently
+   modified memory) and last_call_cuid (which insn was the most recent
+   subroutine call).  */
+
+static void
+record_dead_and_set_regs (rtx insn)
+{
+  rtx link;
+  unsigned int i;
+
+  for (link = REG_NOTES (insn); link; link = XEXP (link, 1))
+    {
+      if (REG_NOTE_KIND (link) == REG_DEAD
+	  && GET_CODE (XEXP (link, 0)) == REG)
+	{
+	  unsigned int regno = REGNO (XEXP (link, 0));
+	  unsigned int endregno
+	    = regno + (regno < FIRST_PSEUDO_REGISTER
+		       ? HARD_REGNO_NREGS (regno, GET_MODE (XEXP (link, 0)))
+		       : 1);
+
+	  for (i = regno; i < endregno; i++)
+	    reg_last_death[i] = insn;
+	}
+      else if (REG_NOTE_KIND (link) == REG_INC)
+	record_value_for_reg (XEXP (link, 0), insn, NULL_RTX);
+    }
+
+  if (GET_CODE (insn) == CALL_INSN)
+    {
+      for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)
+	if (TEST_HARD_REG_BIT (regs_invalidated_by_call, i))
+	  {
+	    reg_last_set_value[i] = 0;
+	    reg_last_set_mode[i] = 0;
+	    reg_last_set_nonzero_bits[i] = 0;
+	    reg_last_set_sign_bit_copies[i] = 0;
+	    reg_last_death[i] = 0;
+	  }
+
+      last_call_cuid = mem_last_set = INSN_CUID (insn);
+
+      /* Don't bother recording what this insn does.  It might set the
+	 return value register, but we can't combine into a call
+	 pattern anyway, so there's no point trying (and it may cause
+	 a crash, if e.g. we wind up asking for last_set_value of a
+	 SUBREG of the return value register).  */
+      return;
+    }
+
+  note_stores (PATTERN (insn), record_dead_and_set_regs_1, insn);
+}
+
+/* If a SUBREG has the promoted bit set, it is in fact a property of the
+   register present in the SUBREG, so for each such SUBREG go back and
+   adjust nonzero and sign bit information of the registers that are
+   known to have some zero/sign bits set.
+
+   This is needed because when combine blows the SUBREGs away, the
+   information on zero/sign bits is lost and further combines can be
+   missed because of that.  */
+
+static void
+record_promoted_value (rtx insn, rtx subreg)
+{
+  rtx links, set;
+  unsigned int regno = REGNO (SUBREG_REG (subreg));
+  enum machine_mode mode = GET_MODE (subreg);
+
+  if (GET_MODE_BITSIZE (mode) > HOST_BITS_PER_WIDE_INT)
+    return;
+
+  for (links = LOG_LINKS (insn); links;)
+    {
+      insn = XEXP (links, 0);
+      set = single_set (insn);
+
+      if (! set || GET_CODE (SET_DEST (set)) != REG
+	  || REGNO (SET_DEST (set)) != regno
+	  || GET_MODE (SET_DEST (set)) != GET_MODE (SUBREG_REG (subreg)))
+	{
+	  links = XEXP (links, 1);
+	  continue;
+	}
+
+      if (reg_last_set[regno] == insn)
+	{
+	  if (SUBREG_PROMOTED_UNSIGNED_P (subreg) > 0)
+	    reg_last_set_nonzero_bits[regno] &= GET_MODE_MASK (mode);
+	}
+
+      if (GET_CODE (SET_SRC (set)) == REG)
+	{
+	  regno = REGNO (SET_SRC (set));
+	  links = LOG_LINKS (insn);
+	}
+      else
+	break;
+    }
+}
+
+/* Scan X for promoted SUBREGs.  For each one found,
+   note what it implies to the registers used in it.  */
+
+static void
+check_promoted_subreg (rtx insn, rtx x)
+{
+  if (GET_CODE (x) == SUBREG && SUBREG_PROMOTED_VAR_P (x)
+      && GET_CODE (SUBREG_REG (x)) == REG)
+    record_promoted_value (insn, x);
+  else
+    {
+      const char *format = GET_RTX_FORMAT (GET_CODE (x));
+      int i, j;
+
+      for (i = 0; i < GET_RTX_LENGTH (GET_CODE (x)); i++)
+	switch (format[i])
+	  {
+	  case 'e':
+	    check_promoted_subreg (insn, XEXP (x, i));
+	    break;
+	  case 'V':
+	  case 'E':
+	    if (XVEC (x, i) != 0)
+	      for (j = 0; j < XVECLEN (x, i); j++)
+		check_promoted_subreg (insn, XVECEXP (x, i, j));
+	    break;
+	  }
+    }
+}
+
+/* Utility routine for the following function.  Verify that all the registers
+   mentioned in *LOC are valid when *LOC was part of a value set when
+   label_tick == TICK.  Return 0 if some are not.
+
+   If REPLACE is nonzero, replace the invalid reference with
+   (clobber (const_int 0)) and return 1.  This replacement is useful because
+   we often can get useful information about the form of a value (e.g., if
+   it was produced by a shift that always produces -1 or 0) even though
+   we don't know exactly what registers it was produced from.  */
+
+static int
+get_last_value_validate (rtx *loc, rtx insn, int tick, int replace)
+{
+  rtx x = *loc;
+  const char *fmt = GET_RTX_FORMAT (GET_CODE (x));
+  int len = GET_RTX_LENGTH (GET_CODE (x));
+  int i;
+
+  if (GET_CODE (x) == REG)
+    {
+      unsigned int regno = REGNO (x);
+      unsigned int endregno
+	= regno + (regno < FIRST_PSEUDO_REGISTER
+		   ? HARD_REGNO_NREGS (regno, GET_MODE (x)) : 1);
+      unsigned int j;
+
+      for (j = regno; j < endregno; j++)
+	if (reg_last_set_invalid[j]
+	    /* If this is a pseudo-register that was only set once and not
+	       live at the beginning of the function, it is always valid.  */
+	    || (! (regno >= FIRST_PSEUDO_REGISTER
+		   && REG_N_SETS (regno) == 1
+		   && (! REGNO_REG_SET_P
+		       (ENTRY_BLOCK_PTR->next_bb->global_live_at_start, regno)))
+		&& reg_last_set_label[j] > tick))
+	  {
+	    if (replace)
+	      *loc = gen_rtx_CLOBBER (GET_MODE (x), const0_rtx);
+	    return replace;
+	  }
+
+      return 1;
+    }
+  /* If this is a memory reference, make sure that there were
+     no stores after it that might have clobbered the value.  We don't
+     have alias info, so we assume any store invalidates it.  */
+  else if (GET_CODE (x) == MEM && ! RTX_UNCHANGING_P (x)
+	   && INSN_CUID (insn) <= mem_last_set)
+    {
+      if (replace)
+	*loc = gen_rtx_CLOBBER (GET_MODE (x), const0_rtx);
+      return replace;
+    }
+
+  for (i = 0; i < len; i++)
+    {
+      if (fmt[i] == 'e')
+	{
+	  /* Check for identical subexpressions.  If x contains
+	     identical subexpression we only have to traverse one of
+	     them.  */
+	  if (i == 1
+	      && (GET_RTX_CLASS (GET_CODE (x)) == '2'
+		  || GET_RTX_CLASS (GET_CODE (x)) == 'c'))
+	    {
+	      /* Note that at this point x0 has already been checked
+		 and found valid.  */
+	      rtx x0 = XEXP (x, 0);
+	      rtx x1 = XEXP (x, 1);
+
+	      /* If x0 and x1 are identical then x is also valid.  */
+	      if (x0 == x1)
+		return 1;
+
+	      /* If x1 is identical to a subexpression of x0 then
+		 while checking x0, x1 has already been checked.  Thus
+		 it is valid and so as x.  */
+	      if ((GET_RTX_CLASS (GET_CODE (x0)) == '2'
+		   || GET_RTX_CLASS (GET_CODE (x0)) == 'c')
+		  && (x1 == XEXP (x0, 0) || x1 == XEXP (x0, 1)))
+		return 1;
+
+	      /* If x0 is identical to a subexpression of x1 then x is
+		 valid iff the rest of x1 is valid.  */
+	      if ((GET_RTX_CLASS (GET_CODE (x1)) == '2'
+		   || GET_RTX_CLASS (GET_CODE (x1)) == 'c')
+		  && (x0 == XEXP (x1, 0) || x0 == XEXP (x1, 1)))
+		return
+		  get_last_value_validate (&XEXP (x1,
+						  x0 == XEXP (x1, 0) ? 1 : 0),
+					   insn, tick, replace);
+	    }
+
+	  if (get_last_value_validate (&XEXP (x, i), insn, tick,
+				       replace) == 0)
+	    return 0;
+	}
+      /* Don't bother with these.  They shouldn't occur anyway.  */
+      else if (fmt[i] == 'E')
+	return 0;
+    }
+
+  /* If we haven't found a reason for it to be invalid, it is valid.  */
+  return 1;
+}
+
+/* Get the last value assigned to X, if known.  Some registers
+   in the value may be replaced with (clobber (const_int 0)) if their value
+   is known longer known reliably.  */
+
+static rtx
+get_last_value (rtx x)
+{
+  unsigned int regno;
+  rtx value;
+
+  /* If this is a non-paradoxical SUBREG, get the value of its operand and
+     then convert it to the desired mode.  If this is a paradoxical SUBREG,
+     we cannot predict what values the "extra" bits might have.  */
+  if (GET_CODE (x) == SUBREG
+      && subreg_lowpart_p (x)
+      && (GET_MODE_SIZE (GET_MODE (x))
+	  <= GET_MODE_SIZE (GET_MODE (SUBREG_REG (x))))
+      && (value = get_last_value (SUBREG_REG (x))) != 0)
+    return gen_lowpart_for_combine (GET_MODE (x), value);
+
+  if (GET_CODE (x) != REG)
+    return 0;
+
+  regno = REGNO (x);
+  value = reg_last_set_value[regno];
+
+  /* If we don't have a value, or if it isn't for this basic block and
+     it's either a hard register, set more than once, or it's a live
+     at the beginning of the function, return 0.
+
+     Because if it's not live at the beginning of the function then the reg
+     is always set before being used (is never used without being set).
+     And, if it's set only once, and it's always set before use, then all
+     uses must have the same last value, even if it's not from this basic
+     block.  */
+
+  if (value == 0
+      || (reg_last_set_label[regno] != label_tick
+	  && (regno < FIRST_PSEUDO_REGISTER
+	      || REG_N_SETS (regno) != 1
+	      || (REGNO_REG_SET_P
+		  (ENTRY_BLOCK_PTR->next_bb->global_live_at_start, regno)))))
+    return 0;
+
+  /* If the value was set in a later insn than the ones we are processing,
+     we can't use it even if the register was only set once.  */
+  if (INSN_CUID (reg_last_set[regno]) >= subst_low_cuid)
+    return 0;
+
+  /* If the value has all its registers valid, return it.  */
+  if (get_last_value_validate (&value, reg_last_set[regno],
+			       reg_last_set_label[regno], 0))
+    return value;
+
+  /* Otherwise, make a copy and replace any invalid register with
+     (clobber (const_int 0)).  If that fails for some reason, return 0.  */
+
+  value = copy_rtx (value);
+  if (get_last_value_validate (&value, reg_last_set[regno],
+			       reg_last_set_label[regno], 1))
+    return value;
+
+  return 0;
+}
+
+/* Return nonzero if expression X refers to a REG or to memory
+   that is set in an instruction more recent than FROM_CUID.  */
+
+static int
+use_crosses_set_p (rtx x, int from_cuid)
+{
+  const char *fmt;
+  int i;
+  enum rtx_code code = GET_CODE (x);
+
+  if (code == REG)
+    {
+      unsigned int regno = REGNO (x);
+      unsigned endreg = regno + (regno < FIRST_PSEUDO_REGISTER
+				 ? HARD_REGNO_NREGS (regno, GET_MODE (x)) : 1);
+
+#ifdef PUSH_ROUNDING
+      /* Don't allow uses of the stack pointer to be moved,
+	 because we don't know whether the move crosses a push insn.  */
+      if (regno == STACK_POINTER_REGNUM && PUSH_ARGS)
+	return 1;
+#endif
+      for (; regno < endreg; regno++)
+	if (reg_last_set[regno]
+	    && INSN_CUID (reg_last_set[regno]) > from_cuid)
+	  return 1;
+      return 0;
+    }
+
+  if (code == MEM && mem_last_set > from_cuid)
+    return 1;
+
+  fmt = GET_RTX_FORMAT (code);
+
+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)
+    {
+      if (fmt[i] == 'E')
+	{
+	  int j;
+	  for (j = XVECLEN (x, i) - 1; j >= 0; j--)
+	    if (use_crosses_set_p (XVECEXP (x, i, j), from_cuid))
+	      return 1;
+	}
+      else if (fmt[i] == 'e'
+	       && use_crosses_set_p (XEXP (x, i), from_cuid))
+	return 1;
+    }
+  return 0;
+}
+
+/* Define three variables used for communication between the following
+   routines.  */
+
+static unsigned int reg_dead_regno, reg_dead_endregno;
+static int reg_dead_flag;
+
+/* Function called via note_stores from reg_dead_at_p.
+
+   If DEST is within [reg_dead_regno, reg_dead_endregno), set
+   reg_dead_flag to 1 if X is a CLOBBER and to -1 it is a SET.  */
+
+static void
+reg_dead_at_p_1 (rtx dest, rtx x, void *data ATTRIBUTE_UNUSED)
+{
+  unsigned int regno, endregno;
+
+  if (GET_CODE (dest) != REG)
+    return;
+
+  regno = REGNO (dest);
+  endregno = regno + (regno < FIRST_PSEUDO_REGISTER
+		      ? HARD_REGNO_NREGS (regno, GET_MODE (dest)) : 1);
+
+  if (reg_dead_endregno > regno && reg_dead_regno < endregno)
+    reg_dead_flag = (GET_CODE (x) == CLOBBER) ? 1 : -1;
+}
+
+/* Return nonzero if REG is known to be dead at INSN.
+
+   We scan backwards from INSN.  If we hit a REG_DEAD note or a CLOBBER
+   referencing REG, it is dead.  If we hit a SET referencing REG, it is
+   live.  Otherwise, see if it is live or dead at the start of the basic
+   block we are in.  Hard regs marked as being live in NEWPAT_USED_REGS
+   must be assumed to be always live.  */
+
+static int
+reg_dead_at_p (rtx reg, rtx insn)
+{
+  basic_block block;
+  unsigned int i;
+
+  /* Set variables for reg_dead_at_p_1.  */
+  reg_dead_regno = REGNO (reg);
+  reg_dead_endregno = reg_dead_regno + (reg_dead_regno < FIRST_PSEUDO_REGISTER
+					? HARD_REGNO_NREGS (reg_dead_regno,
+							    GET_MODE (reg))
+					: 1);
+
+  reg_dead_flag = 0;
+
+  /* Check that reg isn't mentioned in NEWPAT_USED_REGS.  */
+  if (reg_dead_regno < FIRST_PSEUDO_REGISTER)
+    {
+      for (i = reg_dead_regno; i < reg_dead_endregno; i++)
+	if (TEST_HARD_REG_BIT (newpat_used_regs, i))
+	  return 0;
+    }
+
+  /* Scan backwards until we find a REG_DEAD note, SET, CLOBBER, label, or
+     beginning of function.  */
+  for (; insn && GET_CODE (insn) != CODE_LABEL && GET_CODE (insn) != BARRIER;
+       insn = prev_nonnote_insn (insn))
+    {
+      note_stores (PATTERN (insn), reg_dead_at_p_1, NULL);
+      if (reg_dead_flag)
+	return reg_dead_flag == 1 ? 1 : 0;
+
+      if (find_regno_note (insn, REG_DEAD, reg_dead_regno))
+	return 1;
+    }
+
+  /* Get the basic block that we were in.  */
+  if (insn == 0)
+    block = ENTRY_BLOCK_PTR->next_bb;
+  else
+    {
+      FOR_EACH_BB (block)
+	if (insn == BB_HEAD (block))
+	  break;
+
+      if (block == EXIT_BLOCK_PTR)
+	return 0;
+    }
+
+  for (i = reg_dead_regno; i < reg_dead_endregno; i++)
+    if (REGNO_REG_SET_P (block->global_live_at_start, i))
+      return 0;
+
+  return 1;
+}
+
+/* Note hard registers in X that are used.  This code is similar to
+   that in flow.c, but much simpler since we don't care about pseudos.  */
+
+static void
+mark_used_regs_combine (rtx x)
+{
+  RTX_CODE code = GET_CODE (x);
+  unsigned int regno;
+  int i;
+
+  switch (code)
+    {
+    case LABEL_REF:
+    case SYMBOL_REF:
+    case CONST_INT:
+    case CONST:
+    case CONST_DOUBLE:
+    case CONST_VECTOR:
+    case PC:
+    case ADDR_VEC:
+    case ADDR_DIFF_VEC:
+    case ASM_INPUT:
+#ifdef HAVE_cc0
+    /* CC0 must die in the insn after it is set, so we don't need to take
+       special note of it here.  */
+    case CC0:
+#endif
+      return;
+
+    case CLOBBER:
+      /* If we are clobbering a MEM, mark any hard registers inside the
+	 address as used.  */
+      if (GET_CODE (XEXP (x, 0)) == MEM)
+	mark_used_regs_combine (XEXP (XEXP (x, 0), 0));
+      return;
+
+    case REG:
+      regno = REGNO (x);
+      /* A hard reg in a wide mode may really be multiple registers.
+	 If so, mark all of them just like the first.  */
+      if (regno < FIRST_PSEUDO_REGISTER)
+	{
+	  unsigned int endregno, r;
+
+	  /* None of this applies to the stack, frame or arg pointers.  */
+	  if (regno == STACK_POINTER_REGNUM
+#if FRAME_POINTER_REGNUM != HARD_FRAME_POINTER_REGNUM
+	      || regno == HARD_FRAME_POINTER_REGNUM
+#endif
+#if FRAME_POINTER_REGNUM != ARG_POINTER_REGNUM
+	      || (regno == ARG_POINTER_REGNUM && fixed_regs[regno])
+#endif
+	      || regno == FRAME_POINTER_REGNUM)
+	    return;
+
+	  endregno = regno + HARD_REGNO_NREGS (regno, GET_MODE (x));
+	  for (r = regno; r < endregno; r++)
+	    SET_HARD_REG_BIT (newpat_used_regs, r);
+	}
+      return;
+
+    case SET:
+      {
+	/* If setting a MEM, or a SUBREG of a MEM, then note any hard regs in
+	   the address.  */
+	rtx testreg = SET_DEST (x);
+
+	while (GET_CODE (testreg) == SUBREG
+	       || GET_CODE (testreg) == ZERO_EXTRACT
+	       || GET_CODE (testreg) == SIGN_EXTRACT
+	       || GET_CODE (testreg) == STRICT_LOW_PART)
+	  testreg = XEXP (testreg, 0);
+
+	if (GET_CODE (testreg) == MEM)
+	  mark_used_regs_combine (XEXP (testreg, 0));
+
+	mark_used_regs_combine (SET_SRC (x));
+      }
+      return;
+
+    default:
+      break;
+    }
+
+  /* Recursively scan the operands of this expression.  */
+
+  {
+    const char *fmt = GET_RTX_FORMAT (code);
+
+    for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)
+      {
+	if (fmt[i] == 'e')
+	  mark_used_regs_combine (XEXP (x, i));
+	else if (fmt[i] == 'E')
+	  {
+	    int j;
+
+	    for (j = 0; j < XVECLEN (x, i); j++)
+	      mark_used_regs_combine (XVECEXP (x, i, j));
+	  }
+      }
+  }
+}
+
+/* Remove register number REGNO from the dead registers list of INSN.
+
+   Return the note used to record the death, if there was one.  */
+
+rtx
+remove_death (unsigned int regno, rtx insn)
+{
+  rtx note = find_regno_note (insn, REG_DEAD, regno);
+
+  if (note)
+    {
+      REG_N_DEATHS (regno)--;
+      remove_note (insn, note);
+    }
+
+  return note;
+}
+
+/* For each register (hardware or pseudo) used within expression X, if its
+   death is in an instruction with cuid between FROM_CUID (inclusive) and
+   TO_INSN (exclusive), put a REG_DEAD note for that register in the
+   list headed by PNOTES.
+
+   That said, don't move registers killed by maybe_kill_insn.
+
+   This is done when X is being merged by combination into TO_INSN.  These
+   notes will then be distributed as needed.  */
+
+static void
+move_deaths (rtx x, rtx maybe_kill_insn, int from_cuid, rtx to_insn,
+	     rtx *pnotes)
+{
+  const char *fmt;
+  int len, i;
+  enum rtx_code code = GET_CODE (x);
+
+  if (code == REG)
+    {
+      unsigned int regno = REGNO (x);
+      rtx where_dead = reg_last_death[regno];
+      rtx before_dead, after_dead;
+
+      /* Don't move the register if it gets killed in between from and to.  */
+      if (maybe_kill_insn && reg_set_p (x, maybe_kill_insn)
+	  && ! reg_referenced_p (x, maybe_kill_insn))
+	return;
+
+      /* WHERE_DEAD could be a USE insn made by combine, so first we
+	 make sure that we have insns with valid INSN_CUID values.  */
+      before_dead = where_dead;
+      while (before_dead && INSN_UID (before_dead) > max_uid_cuid)
+	before_dead = PREV_INSN (before_dead);
+
+      after_dead = where_dead;
+      while (after_dead && INSN_UID (after_dead) > max_uid_cuid)
+	after_dead = NEXT_INSN (after_dead);
+
+      if (before_dead && after_dead
+	  && INSN_CUID (before_dead) >= from_cuid
+	  && (INSN_CUID (after_dead) < INSN_CUID (to_insn)
+	      || (where_dead != after_dead
+		  && INSN_CUID (after_dead) == INSN_CUID (to_insn))))
+	{
+	  rtx note = remove_death (regno, where_dead);
+
+	  /* It is possible for the call above to return 0.  This can occur
+	     when reg_last_death points to I2 or I1 that we combined with.
+	     In that case make a new note.
+
+	     We must also check for the case where X is a hard register
+	     and NOTE is a death note for a range of hard registers
+	     including X.  In that case, we must put REG_DEAD notes for
+	     the remaining registers in place of NOTE.  */
+
+	  if (note != 0 && regno < FIRST_PSEUDO_REGISTER
+	      && (GET_MODE_SIZE (GET_MODE (XEXP (note, 0)))
+		  > GET_MODE_SIZE (GET_MODE (x))))
+	    {
+	      unsigned int deadregno = REGNO (XEXP (note, 0));
+	      unsigned int deadend
+		= (deadregno + HARD_REGNO_NREGS (deadregno,
+						 GET_MODE (XEXP (note, 0))));
+	      unsigned int ourend
+		= regno + HARD_REGNO_NREGS (regno, GET_MODE (x));
+	      unsigned int i;
+
+	      for (i = deadregno; i < deadend; i++)
+		if (i < regno || i >= ourend)
+		  REG_NOTES (where_dead)
+		    = gen_rtx_EXPR_LIST (REG_DEAD,
+					 regno_reg_rtx[i],
+					 REG_NOTES (where_dead));
+	    }
+
+	  /* If we didn't find any note, or if we found a REG_DEAD note that
+	     covers only part of the given reg, and we have a multi-reg hard
+	     register, then to be safe we must check for REG_DEAD notes
+	     for each register other than the first.  They could have
+	     their own REG_DEAD notes lying around.  */
+	  else if ((note == 0
+		    || (note != 0
+			&& (GET_MODE_SIZE (GET_MODE (XEXP (note, 0)))
+			    < GET_MODE_SIZE (GET_MODE (x)))))
+		   && regno < FIRST_PSEUDO_REGISTER
+		   && HARD_REGNO_NREGS (regno, GET_MODE (x)) > 1)
+	    {
+	      unsigned int ourend
+		= regno + HARD_REGNO_NREGS (regno, GET_MODE (x));
+	      unsigned int i, offset;
+	      rtx oldnotes = 0;
+
+	      if (note)
+		offset = HARD_REGNO_NREGS (regno, GET_MODE (XEXP (note, 0)));
+	      else
+		offset = 1;
+
+	      for (i = regno + offset; i < ourend; i++)
+		move_deaths (regno_reg_rtx[i],
+			     maybe_kill_insn, from_cuid, to_insn, &oldnotes);
+	    }
+
+	  if (note != 0 && GET_MODE (XEXP (note, 0)) == GET_MODE (x))
+	    {
+	      XEXP (note, 1) = *pnotes;
+	      *pnotes = note;
+	    }
+	  else
+	    *pnotes = gen_rtx_EXPR_LIST (REG_DEAD, x, *pnotes);
+
+	  REG_N_DEATHS (regno)++;
+	}
+
+      return;
+    }
+
+  else if (GET_CODE (x) == SET)
+    {
+      rtx dest = SET_DEST (x);
+
+      move_deaths (SET_SRC (x), maybe_kill_insn, from_cuid, to_insn, pnotes);
+
+      /* In the case of a ZERO_EXTRACT, a STRICT_LOW_PART, or a SUBREG
+	 that accesses one word of a multi-word item, some
+	 piece of everything register in the expression is used by
+	 this insn, so remove any old death.  */
+      /* ??? So why do we test for equality of the sizes?  */
+
+      if (GET_CODE (dest) == ZERO_EXTRACT
+	  || GET_CODE (dest) == STRICT_LOW_PART
+	  || (GET_CODE (dest) == SUBREG
+	      && (((GET_MODE_SIZE (GET_MODE (dest))
+		    + UNITS_PER_WORD - 1) / UNITS_PER_WORD)
+		  == ((GET_MODE_SIZE (GET_MODE (SUBREG_REG (dest)))
+		       + UNITS_PER_WORD - 1) / UNITS_PER_WORD))))
+	{
+	  move_deaths (dest, maybe_kill_insn, from_cuid, to_insn, pnotes);
+	  return;
+	}
+
+      /* If this is some other SUBREG, we know it replaces the entire
+	 value, so use that as the destination.  */
+      if (GET_CODE (dest) == SUBREG)
+	dest = SUBREG_REG (dest);
+
+      /* If this is a MEM, adjust deaths of anything used in the address.
+	 For a REG (the only other possibility), the entire value is
+	 being replaced so the old value is not used in this insn.  */
+
+      if (GET_CODE (dest) == MEM)
+	move_deaths (XEXP (dest, 0), maybe_kill_insn, from_cuid,
+		     to_insn, pnotes);
+      return;
+    }
+
+  else if (GET_CODE (x) == CLOBBER)
+    return;
+
+  len = GET_RTX_LENGTH (code);
+  fmt = GET_RTX_FORMAT (code);
+
+  for (i = 0; i < len; i++)
+    {
+      if (fmt[i] == 'E')
+	{
+	  int j;
+	  for (j = XVECLEN (x, i) - 1; j >= 0; j--)
+	    move_deaths (XVECEXP (x, i, j), maybe_kill_insn, from_cuid,
+			 to_insn, pnotes);
+	}
+      else if (fmt[i] == 'e')
+	move_deaths (XEXP (x, i), maybe_kill_insn, from_cuid, to_insn, pnotes);
+    }
+}
+
+/* Return 1 if X is the target of a bit-field assignment in BODY, the
+   pattern of an insn.  X must be a REG.  */
+
+static int
+reg_bitfield_target_p (rtx x, rtx body)
+{
+  int i;
+
+  if (GET_CODE (body) == SET)
+    {
+      rtx dest = SET_DEST (body);
+      rtx target;
+      unsigned int regno, tregno, endregno, endtregno;
+
+      if (GET_CODE (dest) == ZERO_EXTRACT)
+	target = XEXP (dest, 0);
+      else if (GET_CODE (dest) == STRICT_LOW_PART)
+	target = SUBREG_REG (XEXP (dest, 0));
+      else
+	return 0;
+
+      if (GET_CODE (target) == SUBREG)
+	target = SUBREG_REG (target);
+
+      if (GET_CODE (target) != REG)
+	return 0;
+
+      tregno = REGNO (target), regno = REGNO (x);
+      if (tregno >= FIRST_PSEUDO_REGISTER || regno >= FIRST_PSEUDO_REGISTER)
+	return target == x;
+
+      endtregno = tregno + HARD_REGNO_NREGS (tregno, GET_MODE (target));
+      endregno = regno + HARD_REGNO_NREGS (regno, GET_MODE (x));
+
+      return endregno > tregno && regno < endtregno;
+    }
+
+  else if (GET_CODE (body) == PARALLEL)
+    for (i = XVECLEN (body, 0) - 1; i >= 0; i--)
+      if (reg_bitfield_target_p (x, XVECEXP (body, 0, i)))
+	return 1;
+
+  return 0;
+}
+
+/* Given a chain of REG_NOTES originally from FROM_INSN, try to place them
+   as appropriate.  I3 and I2 are the insns resulting from the combination
+   insns including FROM (I2 may be zero).
+
+   Each note in the list is either ignored or placed on some insns, depending
+   on the type of note.  */
+
+static void
+distribute_notes (rtx notes, rtx from_insn, rtx i3, rtx i2)
+{
+  rtx note, next_note;
+  rtx tem;
+
+  for (note = notes; note; note = next_note)
+    {
+      rtx place = 0, place2 = 0;
+
+      /* If this NOTE references a pseudo register, ensure it references
+	 the latest copy of that register.  */
+      if (XEXP (note, 0) && GET_CODE (XEXP (note, 0)) == REG
+	  && REGNO (XEXP (note, 0)) >= FIRST_PSEUDO_REGISTER)
+	XEXP (note, 0) = regno_reg_rtx[REGNO (XEXP (note, 0))];
+
+      next_note = XEXP (note, 1);
+      switch (REG_NOTE_KIND (note))
+	{
+	case REG_BR_PROB:
+	case REG_BR_PRED:
+	  /* Doesn't matter much where we put this, as long as it's somewhere.
+	     It is preferable to keep these notes on branches, which is most
+	     likely to be i3.  */
+	  place = i3;
+	  break;
+
+	case REG_VALUE_PROFILE:
+	  /* Just get rid of this note, as it is unused later anyway.  */
+	  break;
+
+	case REG_VTABLE_REF:
+	  /* ??? Should remain with *a particular* memory load.  Given the
+	     nature of vtable data, the last insn seems relatively safe.  */
+	  place = i3;
+	  break;
+
+	case REG_NON_LOCAL_GOTO:
+	  if (GET_CODE (i3) == JUMP_INSN)
+	    place = i3;
+	  else if (i2 && GET_CODE (i2) == JUMP_INSN)
+	    place = i2;
+	  else
+	    abort ();
+	  break;
+
+	case REG_EH_REGION:
+	  /* These notes must remain with the call or trapping instruction.  */
+	  if (GET_CODE (i3) == CALL_INSN)
+	    place = i3;
+	  else if (i2 && GET_CODE (i2) == CALL_INSN)
+	    place = i2;
+	  else if (flag_non_call_exceptions)
+	    {
+	      if (may_trap_p (i3))
+		place = i3;
+	      else if (i2 && may_trap_p (i2))
+		place = i2;
+	      /* ??? Otherwise assume we've combined things such that we
+		 can now prove that the instructions can't trap.  Drop the
+		 note in this case.  */
+	    }
+	  else
+	    abort ();
+	  break;
+
+	case REG_ALWAYS_RETURN:
+	case REG_NORETURN:
+	case REG_SETJMP:
+	  /* These notes must remain with the call.  It should not be
+	     possible for both I2 and I3 to be a call.  */
+	  if (GET_CODE (i3) == CALL_INSN)
+	    place = i3;
+	  else if (i2 && GET_CODE (i2) == CALL_INSN)
+	    place = i2;
+	  else
+	    abort ();
+	  break;
+
+	case REG_UNUSED:
+	  /* Any clobbers for i3 may still exist, and so we must process
+	     REG_UNUSED notes from that insn.
+
+	     Any clobbers from i2 or i1 can only exist if they were added by
+	     recog_for_combine.  In that case, recog_for_combine created the
+	     necessary REG_UNUSED notes.  Trying to keep any original
+	     REG_UNUSED notes from these insns can cause incorrect output
+	     if it is for the same register as the original i3 dest.
+	     In that case, we will notice that the register is set in i3,
+	     and then add a REG_UNUSED note for the destination of i3, which
+	     is wrong.  However, it is possible to have REG_UNUSED notes from
+	     i2 or i1 for register which were both used and clobbered, so
+	     we keep notes from i2 or i1 if they will turn into REG_DEAD
+	     notes.  */
+
+	  /* If this register is set or clobbered in I3, put the note there
+	     unless there is one already.  */
+	  if (reg_set_p (XEXP (note, 0), PATTERN (i3)))
+	    {
+	      if (from_insn != i3)
+		break;
+
+	      if (! (GET_CODE (XEXP (note, 0)) == REG
+		     ? find_regno_note (i3, REG_UNUSED, REGNO (XEXP (note, 0)))
+		     : find_reg_note (i3, REG_UNUSED, XEXP (note, 0))))
+		place = i3;
+	    }
+	  /* Otherwise, if this register is used by I3, then this register
+	     now dies here, so we must put a REG_DEAD note here unless there
+	     is one already.  */
+	  else if (reg_referenced_p (XEXP (note, 0), PATTERN (i3))
+		   && ! (GET_CODE (XEXP (note, 0)) == REG
+			 ? find_regno_note (i3, REG_DEAD,
+					    REGNO (XEXP (note, 0)))
+			 : find_reg_note (i3, REG_DEAD, XEXP (note, 0))))
+	    {
+	      PUT_REG_NOTE_KIND (note, REG_DEAD);
+	      place = i3;
+	    }
+	  break;
+
+	case REG_EQUAL:
+	case REG_EQUIV:
+	case REG_NOALIAS:
+	  /* These notes say something about results of an insn.  We can
+	     only support them if they used to be on I3 in which case they
+	     remain on I3.  Otherwise they are ignored.
+
+	     If the note refers to an expression that is not a constant, we
+	     must also ignore the note since we cannot tell whether the
+	     equivalence is still true.  It might be possible to do
+	     slightly better than this (we only have a problem if I2DEST
+	     or I1DEST is present in the expression), but it doesn't
+	     seem worth the trouble.  */
+
+	  if (from_insn == i3
+	      && (XEXP (note, 0) == 0 || CONSTANT_P (XEXP (note, 0))))
+	    place = i3;
+	  break;
+
+	case REG_INC:
+	case REG_NO_CONFLICT:
+	  /* These notes say something about how a register is used.  They must
+	     be present on any use of the register in I2 or I3.  */
+	  if (reg_mentioned_p (XEXP (note, 0), PATTERN (i3)))
+	    place = i3;
+
+	  if (i2 && reg_mentioned_p (XEXP (note, 0), PATTERN (i2)))
+	    {
+	      if (place)
+		place2 = i2;
+	      else
+		place = i2;
+	    }
+	  break;
+
+	case REG_LABEL:
+	  /* This can show up in several ways -- either directly in the
+	     pattern, or hidden off in the constant pool with (or without?)
+	     a REG_EQUAL note.  */
+	  /* ??? Ignore the without-reg_equal-note problem for now.  */
+	  if (reg_mentioned_p (XEXP (note, 0), PATTERN (i3))
+	      || ((tem = find_reg_note (i3, REG_EQUAL, NULL_RTX))
+		  && GET_CODE (XEXP (tem, 0)) == LABEL_REF
+		  && XEXP (XEXP (tem, 0), 0) == XEXP (note, 0)))
+	    place = i3;
+
+	  if (i2
+	      && (reg_mentioned_p (XEXP (note, 0), PATTERN (i2))
+		  || ((tem = find_reg_note (i2, REG_EQUAL, NULL_RTX))
+		      && GET_CODE (XEXP (tem, 0)) == LABEL_REF
+		      && XEXP (XEXP (tem, 0), 0) == XEXP (note, 0))))
+	    {
+	      if (place)
+		place2 = i2;
+	      else
+		place = i2;
+	    }
+
+	  /* Don't attach REG_LABEL note to a JUMP_INSN which has
+	     JUMP_LABEL already.  Instead, decrement LABEL_NUSES.  */
+	  if (place && GET_CODE (place) == JUMP_INSN && JUMP_LABEL (place))
+	    {
+	      if (JUMP_LABEL (place) != XEXP (note, 0))
+		abort ();
+	      if (GET_CODE (JUMP_LABEL (place)) == CODE_LABEL)
+		LABEL_NUSES (JUMP_LABEL (place))--;
+	      place = 0;
+	    }
+	  if (place2 && GET_CODE (place2) == JUMP_INSN && JUMP_LABEL (place2))
+	    {
+	      if (JUMP_LABEL (place2) != XEXP (note, 0))
+		abort ();
+	      if (GET_CODE (JUMP_LABEL (place2)) == CODE_LABEL)
+		LABEL_NUSES (JUMP_LABEL (place2))--;
+	      place2 = 0;
+	    }
+	  break;
+
+	case REG_NONNEG:
+	  /* This note says something about the value of a register prior
+	     to the execution of an insn.  It is too much trouble to see
+	     if the note is still correct in all situations.  It is better
+	     to simply delete it.  */
+	  break;
+
+	case REG_RETVAL:
+	  /* If the insn previously containing this note still exists,
+	     put it back where it was.  Otherwise move it to the previous
+	     insn.  Adjust the corresponding REG_LIBCALL note.  */
+	  if (GET_CODE (from_insn) != NOTE)
+	    place = from_insn;
+	  else
+	    {
+	      tem = find_reg_note (XEXP (note, 0), REG_LIBCALL, NULL_RTX);
+	      place = prev_real_insn (from_insn);
+	      if (tem && place)
+		XEXP (tem, 0) = place;
+	      /* If we're deleting the last remaining instruction of a
+		 libcall sequence, don't add the notes.  */
+	      else if (XEXP (note, 0) == from_insn)
+		tem = place = 0;
+	      /* Don't add the dangling REG_RETVAL note.  */
+	      else if (! tem)
+		place = 0;
+	    }
+	  break;
+
+	case REG_LIBCALL:
+	  /* This is handled similarly to REG_RETVAL.  */
+	  if (GET_CODE (from_insn) != NOTE)
+	    place = from_insn;
+	  else
+	    {
+	      tem = find_reg_note (XEXP (note, 0), REG_RETVAL, NULL_RTX);
+	      place = next_real_insn (from_insn);
+	      if (tem && place)
+		XEXP (tem, 0) = place;
+	      /* If we're deleting the last remaining instruction of a
+		 libcall sequence, don't add the notes.  */
+	      else if (XEXP (note, 0) == from_insn)
+		tem = place = 0;
+	      /* Don't add the dangling REG_LIBCALL note.  */
+	      else if (! tem)
+		place = 0;
+	    }
+	  break;
+
+	case REG_DEAD:
+	  /* If the register is used as an input in I3, it dies there.
+	     Similarly for I2, if it is nonzero and adjacent to I3.
+
+	     If the register is not used as an input in either I3 or I2
+	     and it is not one of the registers we were supposed to eliminate,
+	     there are two possibilities.  We might have a non-adjacent I2
+	     or we might have somehow eliminated an additional register
+	     from a computation.  For example, we might have had A & B where
+	     we discover that B will always be zero.  In this case we will
+	     eliminate the reference to A.
+
+	     In both cases, we must search to see if we can find a previous
+	     use of A and put the death note there.  */
+
+	  if (from_insn
+	      && GET_CODE (from_insn) == CALL_INSN
+	      && find_reg_fusage (from_insn, USE, XEXP (note, 0)))
+	    place = from_insn;
+	  else if (reg_referenced_p (XEXP (note, 0), PATTERN (i3)))
+	    place = i3;
+	  else if (i2 != 0 && next_nonnote_insn (i2) == i3
+		   && reg_referenced_p (XEXP (note, 0), PATTERN (i2)))
+	    place = i2;
+
+	  if (place == 0)
+	    {
+	      basic_block bb = this_basic_block;
+
+	      for (tem = PREV_INSN (i3); place == 0; tem = PREV_INSN (tem))
+		{
+		  if (! INSN_P (tem))
+		    {
+		      if (tem == BB_HEAD (bb))
+			break;
+		      continue;
+		    }
+
+		  /* If the register is being set at TEM, see if that is all
+		     TEM is doing.  If so, delete TEM.  Otherwise, make this
+		     into a REG_UNUSED note instead.  Don't delete sets to
+		     global register vars.  */
+		  if ((REGNO (XEXP (note, 0)) >= FIRST_PSEUDO_REGISTER
+		       || !global_regs[REGNO (XEXP (note, 0))])
+		      && reg_set_p (XEXP (note, 0), PATTERN (tem)))
+		    {
+		      rtx set = single_set (tem);
+		      rtx inner_dest = 0;
+#ifdef HAVE_cc0
+		      rtx cc0_setter = NULL_RTX;
+#endif
+
+		      if (set != 0)
+			for (inner_dest = SET_DEST (set);
+			     (GET_CODE (inner_dest) == STRICT_LOW_PART
+			      || GET_CODE (inner_dest) == SUBREG
+			      || GET_CODE (inner_dest) == ZERO_EXTRACT);
+			     inner_dest = XEXP (inner_dest, 0))
+			  ;
+
+		      /* Verify that it was the set, and not a clobber that
+			 modified the register.
+
+			 CC0 targets must be careful to maintain setter/user
+			 pairs.  If we cannot delete the setter due to side
+			 effects, mark the user with an UNUSED note instead
+			 of deleting it.  */
+
+		      if (set != 0 && ! side_effects_p (SET_SRC (set))
+			  && rtx_equal_p (XEXP (note, 0), inner_dest)
+#ifdef HAVE_cc0
+			  && (! reg_mentioned_p (cc0_rtx, SET_SRC (set))
+			      || ((cc0_setter = prev_cc0_setter (tem)) != NULL
+				  && sets_cc0_p (PATTERN (cc0_setter)) > 0))
+#endif
+			  )
+			{
+			  /* Move the notes and links of TEM elsewhere.
+			     This might delete other dead insns recursively.
+			     First set the pattern to something that won't use
+			     any register.  */
+			  rtx old_notes = REG_NOTES (tem);
+
+			  PATTERN (tem) = pc_rtx;
+			  REG_NOTES (tem) = NULL;
+
+			  distribute_notes (old_notes, tem, tem, NULL_RTX);
+			  distribute_links (LOG_LINKS (tem));
+
+			  PUT_CODE (tem, NOTE);
+			  NOTE_LINE_NUMBER (tem) = NOTE_INSN_DELETED;
+			  NOTE_SOURCE_FILE (tem) = 0;
+
+#ifdef HAVE_cc0
+			  /* Delete the setter too.  */
+			  if (cc0_setter)
+			    {
+			      PATTERN (cc0_setter) = pc_rtx;
+			      old_notes = REG_NOTES (cc0_setter);
+			      REG_NOTES (cc0_setter) = NULL;
+
+			      distribute_notes (old_notes, cc0_setter,
+						cc0_setter, NULL_RTX);
+			      distribute_links (LOG_LINKS (cc0_setter));
+
+			      PUT_CODE (cc0_setter, NOTE);
+			      NOTE_LINE_NUMBER (cc0_setter)
+				= NOTE_INSN_DELETED;
+			      NOTE_SOURCE_FILE (cc0_setter) = 0;
+			    }
+#endif
+			}
+		      /* If the register is both set and used here, put the
+			 REG_DEAD note here, but place a REG_UNUSED note
+			 here too unless there already is one.  */
+		      else if (reg_referenced_p (XEXP (note, 0),
+						 PATTERN (tem)))
+			{
+			  place = tem;
+
+			  if (! find_regno_note (tem, REG_UNUSED,
+						 REGNO (XEXP (note, 0))))
+			    REG_NOTES (tem)
+			      = gen_rtx_EXPR_LIST (REG_UNUSED, XEXP (note, 0),
+						   REG_NOTES (tem));
+			}
+		      else
+			{
+			  PUT_REG_NOTE_KIND (note, REG_UNUSED);
+
+			  /*  If there isn't already a REG_UNUSED note, put one
+			      here.  */
+			  if (! find_regno_note (tem, REG_UNUSED,
+						 REGNO (XEXP (note, 0))))
+			    place = tem;
+			  break;
+			}
+		    }
+		  else if (reg_referenced_p (XEXP (note, 0), PATTERN (tem))
+			   || (GET_CODE (tem) == CALL_INSN
+			       && find_reg_fusage (tem, USE, XEXP (note, 0))))
+		    {
+		      place = tem;
+
+		      /* If we are doing a 3->2 combination, and we have a
+			 register which formerly died in i3 and was not used
+			 by i2, which now no longer dies in i3 and is used in
+			 i2 but does not die in i2, and place is between i2
+			 and i3, then we may need to move a link from place to
+			 i2.  */
+		      if (i2 && INSN_UID (place) <= max_uid_cuid
+			  && INSN_CUID (place) > INSN_CUID (i2)
+			  && from_insn
+			  && INSN_CUID (from_insn) > INSN_CUID (i2)
+			  && reg_referenced_p (XEXP (note, 0), PATTERN (i2)))
+			{
+			  rtx links = LOG_LINKS (place);
+			  LOG_LINKS (place) = 0;
+			  distribute_links (links);
+			}
+		      break;
+		    }
+
+		  if (tem == BB_HEAD (bb))
+		    break;
+		}
+
+	      /* We haven't found an insn for the death note and it
+		 is still a REG_DEAD note, but we have hit the beginning
+		 of the block.  If the existing life info says the reg
+		 was dead, there's nothing left to do.  Otherwise, we'll
+		 need to do a global life update after combine.  */
+	      if (REG_NOTE_KIND (note) == REG_DEAD && place == 0
+		  && REGNO_REG_SET_P (bb->global_live_at_start,
+				      REGNO (XEXP (note, 0))))
+		SET_BIT (refresh_blocks, this_basic_block->index);
+	    }
+
+	  /* If the register is set or already dead at PLACE, we needn't do
+	     anything with this note if it is still a REG_DEAD note.
+	     We can here if it is set at all, not if is it totally replace,
+	     which is what `dead_or_set_p' checks, so also check for it being
+	     set partially.  */
+
+	  if (place && REG_NOTE_KIND (note) == REG_DEAD)
+	    {
+	      unsigned int regno = REGNO (XEXP (note, 0));
+
+	      /* Similarly, if the instruction on which we want to place
+		 the note is a noop, we'll need do a global live update
+		 after we remove them in delete_noop_moves.  */
+	      if (noop_move_p (place))
+		SET_BIT (refresh_blocks, this_basic_block->index);
+
+	      if (dead_or_set_p (place, XEXP (note, 0))
+		  || reg_bitfield_target_p (XEXP (note, 0), PATTERN (place)))
+		{
+		  /* Unless the register previously died in PLACE, clear
+		     reg_last_death.  [I no longer understand why this is
+		     being done.] */
+		  if (reg_last_death[regno] != place)
+		    reg_last_death[regno] = 0;
+		  place = 0;
+		}
+	      else
+		reg_last_death[regno] = place;
+
+	      /* If this is a death note for a hard reg that is occupying
+		 multiple registers, ensure that we are still using all
+		 parts of the object.  If we find a piece of the object
+		 that is unused, we must arrange for an appropriate REG_DEAD
+		 note to be added for it.  However, we can't just emit a USE
+		 and tag the note to it, since the register might actually
+		 be dead; so we recourse, and the recursive call then finds
+		 the previous insn that used this register.  */
+
+	      if (place && regno < FIRST_PSEUDO_REGISTER
+		  && HARD_REGNO_NREGS (regno, GET_MODE (XEXP (note, 0))) > 1)
+		{
+		  unsigned int endregno
+		    = regno + HARD_REGNO_NREGS (regno,
+						GET_MODE (XEXP (note, 0)));
+		  int all_used = 1;
+		  unsigned int i;
+
+		  for (i = regno; i < endregno; i++)
+		    if ((! refers_to_regno_p (i, i + 1, PATTERN (place), 0)
+			 && ! find_regno_fusage (place, USE, i))
+			|| dead_or_set_regno_p (place, i))
+		      all_used = 0;
+
+		  if (! all_used)
+		    {
+		      /* Put only REG_DEAD notes for pieces that are
+			 not already dead or set.  */
+
+		      for (i = regno; i < endregno;
+			   i += HARD_REGNO_NREGS (i, reg_raw_mode[i]))
+			{
+			  rtx piece = regno_reg_rtx[i];
+			  basic_block bb = this_basic_block;
+
+			  if (! dead_or_set_p (place, piece)
+			      && ! reg_bitfield_target_p (piece,
+							  PATTERN (place)))
+			    {
+			      rtx new_note
+				= gen_rtx_EXPR_LIST (REG_DEAD, piece, NULL_RTX);
+
+			      distribute_notes (new_note, place, place,
+						NULL_RTX);
+			    }
+			  else if (! refers_to_regno_p (i, i + 1,
+							PATTERN (place), 0)
+				   && ! find_regno_fusage (place, USE, i))
+			    for (tem = PREV_INSN (place); ;
+				 tem = PREV_INSN (tem))
+			      {
+				if (! INSN_P (tem))
+				  {
+				    if (tem == BB_HEAD (bb))
+				      {
+					SET_BIT (refresh_blocks,
+						 this_basic_block->index);
+					break;
+				      }
+				    continue;
+				  }
+				if (dead_or_set_p (tem, piece)
+				    || reg_bitfield_target_p (piece,
+							      PATTERN (tem)))
+				  {
+				    REG_NOTES (tem)
+				      = gen_rtx_EXPR_LIST (REG_UNUSED, piece,
+							   REG_NOTES (tem));
+				    break;
+				  }
+			      }
+
+			}
+
+		      place = 0;
+		    }
+		}
+	    }
+	  break;
+
+	default:
+	  /* Any other notes should not be present at this point in the
+	     compilation.  */
+	  abort ();
+	}
+
+      if (place)
+	{
+	  XEXP (note, 1) = REG_NOTES (place);
+	  REG_NOTES (place) = note;
+	}
+      else if ((REG_NOTE_KIND (note) == REG_DEAD
+		|| REG_NOTE_KIND (note) == REG_UNUSED)
+	       && GET_CODE (XEXP (note, 0)) == REG)
+	REG_N_DEATHS (REGNO (XEXP (note, 0)))--;
+
+      if (place2)
+	{
+	  if ((REG_NOTE_KIND (note) == REG_DEAD
+	       || REG_NOTE_KIND (note) == REG_UNUSED)
+	      && GET_CODE (XEXP (note, 0)) == REG)
+	    REG_N_DEATHS (REGNO (XEXP (note, 0)))++;
+
+	  REG_NOTES (place2) = gen_rtx_fmt_ee (GET_CODE (note),
+					       REG_NOTE_KIND (note),
+					       XEXP (note, 0),
+					       REG_NOTES (place2));
+	}
+    }
+}
+
+/* Similarly to above, distribute the LOG_LINKS that used to be present on
+   I3, I2, and I1 to new locations.  This is also called to add a link
+   pointing at I3 when I3's destination is changed.  */
+
+static void
+distribute_links (rtx links)
+{
+  rtx link, next_link;
+
+  for (link = links; link; link = next_link)
+    {
+      rtx place = 0;
+      rtx insn;
+      rtx set, reg;
+
+      next_link = XEXP (link, 1);
+
+      /* If the insn that this link points to is a NOTE or isn't a single
+	 set, ignore it.  In the latter case, it isn't clear what we
+	 can do other than ignore the link, since we can't tell which
+	 register it was for.  Such links wouldn't be used by combine
+	 anyway.
+
+	 It is not possible for the destination of the target of the link to
+	 have been changed by combine.  The only potential of this is if we
+	 replace I3, I2, and I1 by I3 and I2.  But in that case the
+	 destination of I2 also remains unchanged.  */
+
+      if (GET_CODE (XEXP (link, 0)) == NOTE
+	  || (set = single_set (XEXP (link, 0))) == 0)
+	continue;
+
+      reg = SET_DEST (set);
+      while (GET_CODE (reg) == SUBREG || GET_CODE (reg) == ZERO_EXTRACT
+	     || GET_CODE (reg) == SIGN_EXTRACT
+	     || GET_CODE (reg) == STRICT_LOW_PART)
+	reg = XEXP (reg, 0);
+
+      /* A LOG_LINK is defined as being placed on the first insn that uses
+	 a register and points to the insn that sets the register.  Start
+	 searching at the next insn after the target of the link and stop
+	 when we reach a set of the register or the end of the basic block.
+
+	 Note that this correctly handles the link that used to point from
+	 I3 to I2.  Also note that not much searching is typically done here
+	 since most links don't point very far away.  */
+
+      for (insn = NEXT_INSN (XEXP (link, 0));
+	   (insn && (this_basic_block->next_bb == EXIT_BLOCK_PTR
+		     || BB_HEAD (this_basic_block->next_bb) != insn));
+	   insn = NEXT_INSN (insn))
+	if (INSN_P (insn) && reg_overlap_mentioned_p (reg, PATTERN (insn)))
+	  {
+	    if (reg_referenced_p (reg, PATTERN (insn)))
+	      place = insn;
+	    break;
+	  }
+	else if (GET_CODE (insn) == CALL_INSN
+		 && find_reg_fusage (insn, USE, reg))
+	  {
+	    place = insn;
+	    break;
+	  }
+	else if (INSN_P (insn) && reg_set_p (reg, insn))
+	  break;
+
+      /* If we found a place to put the link, place it there unless there
+	 is already a link to the same insn as LINK at that point.  */
+
+      if (place)
+	{
+	  rtx link2;
+
+	  for (link2 = LOG_LINKS (place); link2; link2 = XEXP (link2, 1))
+	    if (XEXP (link2, 0) == XEXP (link, 0))
+	      break;
+
+	  if (link2 == 0)
+	    {
+	      XEXP (link, 1) = LOG_LINKS (place);
+	      LOG_LINKS (place) = link;
+
+	      /* Set added_links_insn to the earliest insn we added a
+		 link to.  */
+	      if (added_links_insn == 0
+		  || INSN_CUID (added_links_insn) > INSN_CUID (place))
+		added_links_insn = place;
+	    }
+	}
+    }
+}
+
+/* Compute INSN_CUID for INSN, which is an insn made by combine.  */
+
+static int
+insn_cuid (rtx insn)
+{
+  while (insn != 0 && INSN_UID (insn) > max_uid_cuid
+	 && GET_CODE (insn) == INSN && GET_CODE (PATTERN (insn)) == USE)
+    insn = NEXT_INSN (insn);
+
+  if (INSN_UID (insn) > max_uid_cuid)
+    abort ();
+
+  return INSN_CUID (insn);
+}
+
+void
+dump_combine_stats (FILE *file)
+{
+  fnotice
+    (file,
+     ";; Combiner statistics: %d attempts, %d substitutions (%d requiring new space),\n;; %d successes.\n\n",
+     combine_attempts, combine_merges, combine_extras, combine_successes);
+}
+
+void
+dump_combine_total_stats (FILE *file)
+{
+  fnotice
+    (file,
+     "\n;; Combiner totals: %d attempts, %d substitutions (%d requiring new space),\n;; %d successes.\n",
+     total_attempts, total_merges, total_extras, total_successes);
+}
diff -Naur gcc-3.4.4/gcc/common.opt gcc-3.4.4-ssp/gcc/common.opt
--- gcc-3.4.4/gcc/common.opt	2004-10-28 06:43:09.000000000 +0300
+++ gcc-3.4.4-ssp/gcc/common.opt	2005-05-25 14:03:21.000000000 +0300
@@ -152,6 +152,10 @@
 Common
 Warn when a variable is unused
 
+Wstack-protector
+Common
+Warn when not issuing stack smashing protection for some reason
+
 aux-info
 Common Separate
 -aux-info <file>	Emit declaration information into <file>
@@ -738,6 +742,14 @@
 Common
 Put zero initialized data in the bss section
 
+fstack-protector
+Common
+Enables stack protection
+
+fstack-protector-all
+Common
+Enables stack protection of every function
+
 g
 Common JoinedOrMissing
 Generate debug information in default format
diff -Naur gcc-3.4.4/gcc/config/arm/arm.md gcc-3.4.4-ssp/gcc/config/arm/arm.md
--- gcc-3.4.4/gcc/config/arm/arm.md	2005-01-25 14:50:34.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/config/arm/arm.md	2005-05-25 14:03:22.000000000 +0300
@@ -3840,7 +3840,13 @@
 	(match_operand:DI 1 "general_operand" ""))]
   "TARGET_EITHER"
   "
-  if (TARGET_THUMB)
+  if (TARGET_ARM)
+    {
+      /* Everything except mem = const or mem = mem can be done easily */
+      if (GET_CODE (operands[0]) == MEM)
+        operands[1] = force_reg (DImode, operands[1]);
+    }
+  else /* TARGET_THUMB.... */
     {
       if (!no_new_pseudos)
         {
diff -Naur gcc-3.4.4/gcc/config/t-linux gcc-3.4.4-ssp/gcc/config/t-linux
--- gcc-3.4.4/gcc/config/t-linux	2003-09-23 21:55:57.000000000 +0300
+++ gcc-3.4.4-ssp/gcc/config/t-linux	2005-05-25 14:03:22.000000000 +0300
@@ -1,7 +1,7 @@
 # Compile crtbeginS.o and crtendS.o with pic.
 CRTSTUFF_T_CFLAGS_S = $(CRTSTUFF_T_CFLAGS) -fPIC
 # Compile libgcc2.a with pic.
-TARGET_LIBGCC2_CFLAGS = -fPIC
+TARGET_LIBGCC2_CFLAGS = -fPIC -DHAVE_SYSLOG
 
 # Override t-slibgcc-elf-ver to export some libgcc symbols with
 # the symbol versions that glibc used.
diff -Naur gcc-3.4.4/gcc/configure gcc-3.4.4-ssp/gcc/configure
--- gcc-3.4.4/gcc/configure	2005-01-08 03:20:19.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/configure	2005-05-25 14:09:54.000000000 +0300
@@ -878,6 +878,7 @@
   --enable-initfini-array	use .init_array/.fini_array sections
   --enable-sjlj-exceptions
                           arrange to use setjmp/longjmp exception handling
+  --enable-stack-protector    force use stack protection as defaults
   --disable-win32-registry
                           disable lookup of installation paths in the
                           Registry on Windows hosts
@@ -4809,6 +4810,15 @@
 fi;
 
 
+# Determine whether or not stack protection is enabled.
+# Check whether --enable-stack-protector or --disable-stack-protector was given.
+if test "${enable_stack_protector+set}" = set; then
+  ENABLESSP="-DSTACK_PROTECTOR"
+
+else
+  ENABLESSP=""
+fi;
+
 # -------------------------
 # Checks for other programs
 # -------------------------
@@ -13098,6 +13108,7 @@
 s,@TARGET_SYSTEM_ROOT_DEFINE@,$TARGET_SYSTEM_ROOT_DEFINE,;t t
 s,@CROSS_SYSTEM_HEADER_DIR@,$CROSS_SYSTEM_HEADER_DIR,;t t
 s,@onestep@,$onestep,;t t
+s,@ENABLESSP@,$ENABLESSP,;t t
 s,@SET_MAKE@,$SET_MAKE,;t t
 s,@AWK@,$AWK,;t t
 s,@LN@,$LN,;t t
diff -Naur gcc-3.4.4/gcc/cse.c gcc-3.4.4-ssp/gcc/cse.c
--- gcc-3.4.4/gcc/cse.c	2004-10-26 21:05:42.000000000 +0300
+++ gcc-3.4.4-ssp/gcc/cse.c	2005-05-25 14:03:22.000000000 +0300
@@ -4212,7 +4212,14 @@
 
 	      if (new_const == 0)
 		break;
-
+#ifndef FRAME_GROWS_DOWNWARD
+	      if (flag_propolice_protection
+		  && GET_CODE (y) == PLUS
+		  && XEXP (y, 0) == frame_pointer_rtx
+		  && INTVAL (inner_const) > 0
+		  && INTVAL (new_const) <= 0)
+		break;
+#endif
 	      /* If we are associating shift operations, don't let this
 		 produce a shift of the size of the object or larger.
 		 This could occur when we follow a sign-extend by a right
@@ -4744,6 +4751,14 @@
       if (SET_DEST (x) == pc_rtx
 	  && GET_CODE (SET_SRC (x)) == LABEL_REF)
 	;
+      /* cut the reg propagation of stack-protected argument.  */
+      else if (SET_VOLATILE_P (x)) {
+	rtx x1 = SET_DEST (x);
+	if (GET_CODE (x1) == SUBREG && GET_CODE (SUBREG_REG (x1)) == REG)
+	  x1 = SUBREG_REG (x1);
+	if (! REGNO_QTY_VALID_P(REGNO (x1)))
+	  make_new_qty (REGNO (x1), GET_MODE (x1));
+      }
 
       /* Don't count call-insns, (set (reg 0) (call ...)), as a set.
 	 The hard function value register is used only once, to copy to
diff -Naur gcc-3.4.4/gcc/cse.c~ gcc-3.4.4-ssp/gcc/cse.c~
--- gcc-3.4.4/gcc/cse.c~	1970-01-01 02:00:00.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/cse.c~	2004-10-26 21:05:42.000000000 +0300
@@ -0,0 +1,8027 @@
+/* Common subexpression elimination for GNU compiler.
+   Copyright (C) 1987, 1988, 1989, 1992, 1993, 1994, 1995, 1996, 1997, 1998
+   1999, 2000, 2001, 2002, 2003, 2004 Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 2, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+02111-1307, USA.  */
+
+#include "config.h"
+/* stdio.h must precede rtl.h for FFS.  */
+#include "system.h"
+#include "coretypes.h"
+#include "tm.h"
+
+#include "rtl.h"
+#include "tm_p.h"
+#include "regs.h"
+#include "hard-reg-set.h"
+#include "basic-block.h"
+#include "flags.h"
+#include "real.h"
+#include "insn-config.h"
+#include "recog.h"
+#include "function.h"
+#include "expr.h"
+#include "toplev.h"
+#include "output.h"
+#include "ggc.h"
+#include "timevar.h"
+#include "except.h"
+#include "target.h"
+#include "params.h"
+
+/* The basic idea of common subexpression elimination is to go
+   through the code, keeping a record of expressions that would
+   have the same value at the current scan point, and replacing
+   expressions encountered with the cheapest equivalent expression.
+
+   It is too complicated to keep track of the different possibilities
+   when control paths merge in this code; so, at each label, we forget all
+   that is known and start fresh.  This can be described as processing each
+   extended basic block separately.  We have a separate pass to perform
+   global CSE.
+
+   Note CSE can turn a conditional or computed jump into a nop or
+   an unconditional jump.  When this occurs we arrange to run the jump
+   optimizer after CSE to delete the unreachable code.
+
+   We use two data structures to record the equivalent expressions:
+   a hash table for most expressions, and a vector of "quantity
+   numbers" to record equivalent (pseudo) registers.
+
+   The use of the special data structure for registers is desirable
+   because it is faster.  It is possible because registers references
+   contain a fairly small number, the register number, taken from
+   a contiguously allocated series, and two register references are
+   identical if they have the same number.  General expressions
+   do not have any such thing, so the only way to retrieve the
+   information recorded on an expression other than a register
+   is to keep it in a hash table.
+
+Registers and "quantity numbers":
+
+   At the start of each basic block, all of the (hardware and pseudo)
+   registers used in the function are given distinct quantity
+   numbers to indicate their contents.  During scan, when the code
+   copies one register into another, we copy the quantity number.
+   When a register is loaded in any other way, we allocate a new
+   quantity number to describe the value generated by this operation.
+   `reg_qty' records what quantity a register is currently thought
+   of as containing.
+
+   All real quantity numbers are greater than or equal to zero.
+   If register N has not been assigned a quantity, reg_qty[N] will
+   equal -N - 1, which is always negative.
+
+   Quantity numbers below zero do not exist and none of the `qty_table'
+   entries should be referenced with a negative index.
+
+   We also maintain a bidirectional chain of registers for each
+   quantity number.  The `qty_table` members `first_reg' and `last_reg',
+   and `reg_eqv_table' members `next' and `prev' hold these chains.
+
+   The first register in a chain is the one whose lifespan is least local.
+   Among equals, it is the one that was seen first.
+   We replace any equivalent register with that one.
+
+   If two registers have the same quantity number, it must be true that
+   REG expressions with qty_table `mode' must be in the hash table for both
+   registers and must be in the same class.
+
+   The converse is not true.  Since hard registers may be referenced in
+   any mode, two REG expressions might be equivalent in the hash table
+   but not have the same quantity number if the quantity number of one
+   of the registers is not the same mode as those expressions.
+
+Constants and quantity numbers
+
+   When a quantity has a known constant value, that value is stored
+   in the appropriate qty_table `const_rtx'.  This is in addition to
+   putting the constant in the hash table as is usual for non-regs.
+
+   Whether a reg or a constant is preferred is determined by the configuration
+   macro CONST_COSTS and will often depend on the constant value.  In any
+   event, expressions containing constants can be simplified, by fold_rtx.
+
+   When a quantity has a known nearly constant value (such as an address
+   of a stack slot), that value is stored in the appropriate qty_table
+   `const_rtx'.
+
+   Integer constants don't have a machine mode.  However, cse
+   determines the intended machine mode from the destination
+   of the instruction that moves the constant.  The machine mode
+   is recorded in the hash table along with the actual RTL
+   constant expression so that different modes are kept separate.
+
+Other expressions:
+
+   To record known equivalences among expressions in general
+   we use a hash table called `table'.  It has a fixed number of buckets
+   that contain chains of `struct table_elt' elements for expressions.
+   These chains connect the elements whose expressions have the same
+   hash codes.
+
+   Other chains through the same elements connect the elements which
+   currently have equivalent values.
+
+   Register references in an expression are canonicalized before hashing
+   the expression.  This is done using `reg_qty' and qty_table `first_reg'.
+   The hash code of a register reference is computed using the quantity
+   number, not the register number.
+
+   When the value of an expression changes, it is necessary to remove from the
+   hash table not just that expression but all expressions whose values
+   could be different as a result.
+
+     1. If the value changing is in memory, except in special cases
+     ANYTHING referring to memory could be changed.  That is because
+     nobody knows where a pointer does not point.
+     The function `invalidate_memory' removes what is necessary.
+
+     The special cases are when the address is constant or is
+     a constant plus a fixed register such as the frame pointer
+     or a static chain pointer.  When such addresses are stored in,
+     we can tell exactly which other such addresses must be invalidated
+     due to overlap.  `invalidate' does this.
+     All expressions that refer to non-constant
+     memory addresses are also invalidated.  `invalidate_memory' does this.
+
+     2. If the value changing is a register, all expressions
+     containing references to that register, and only those,
+     must be removed.
+
+   Because searching the entire hash table for expressions that contain
+   a register is very slow, we try to figure out when it isn't necessary.
+   Precisely, this is necessary only when expressions have been
+   entered in the hash table using this register, and then the value has
+   changed, and then another expression wants to be added to refer to
+   the register's new value.  This sequence of circumstances is rare
+   within any one basic block.
+
+   The vectors `reg_tick' and `reg_in_table' are used to detect this case.
+   reg_tick[i] is incremented whenever a value is stored in register i.
+   reg_in_table[i] holds -1 if no references to register i have been
+   entered in the table; otherwise, it contains the value reg_tick[i] had
+   when the references were entered.  If we want to enter a reference
+   and reg_in_table[i] != reg_tick[i], we must scan and remove old references.
+   Until we want to enter a new entry, the mere fact that the two vectors
+   don't match makes the entries be ignored if anyone tries to match them.
+
+   Registers themselves are entered in the hash table as well as in
+   the equivalent-register chains.  However, the vectors `reg_tick'
+   and `reg_in_table' do not apply to expressions which are simple
+   register references.  These expressions are removed from the table
+   immediately when they become invalid, and this can be done even if
+   we do not immediately search for all the expressions that refer to
+   the register.
+
+   A CLOBBER rtx in an instruction invalidates its operand for further
+   reuse.  A CLOBBER or SET rtx whose operand is a MEM:BLK
+   invalidates everything that resides in memory.
+
+Related expressions:
+
+   Constant expressions that differ only by an additive integer
+   are called related.  When a constant expression is put in
+   the table, the related expression with no constant term
+   is also entered.  These are made to point at each other
+   so that it is possible to find out if there exists any
+   register equivalent to an expression related to a given expression.  */
+
+/* One plus largest register number used in this function.  */
+
+static int max_reg;
+
+/* One plus largest instruction UID used in this function at time of
+   cse_main call.  */
+
+static int max_insn_uid;
+
+/* Length of qty_table vector.  We know in advance we will not need
+   a quantity number this big.  */
+
+static int max_qty;
+
+/* Next quantity number to be allocated.
+   This is 1 + the largest number needed so far.  */
+
+static int next_qty;
+
+/* Per-qty information tracking.
+
+   `first_reg' and `last_reg' track the head and tail of the
+   chain of registers which currently contain this quantity.
+
+   `mode' contains the machine mode of this quantity.
+
+   `const_rtx' holds the rtx of the constant value of this
+   quantity, if known.  A summations of the frame/arg pointer
+   and a constant can also be entered here.  When this holds
+   a known value, `const_insn' is the insn which stored the
+   constant value.
+
+   `comparison_{code,const,qty}' are used to track when a
+   comparison between a quantity and some constant or register has
+   been passed.  In such a case, we know the results of the comparison
+   in case we see it again.  These members record a comparison that
+   is known to be true.  `comparison_code' holds the rtx code of such
+   a comparison, else it is set to UNKNOWN and the other two
+   comparison members are undefined.  `comparison_const' holds
+   the constant being compared against, or zero if the comparison
+   is not against a constant.  `comparison_qty' holds the quantity
+   being compared against when the result is known.  If the comparison
+   is not with a register, `comparison_qty' is -1.  */
+
+struct qty_table_elem
+{
+  rtx const_rtx;
+  rtx const_insn;
+  rtx comparison_const;
+  int comparison_qty;
+  unsigned int first_reg, last_reg;
+  /* The sizes of these fields should match the sizes of the
+     code and mode fields of struct rtx_def (see rtl.h).  */
+  ENUM_BITFIELD(rtx_code) comparison_code : 16;
+  ENUM_BITFIELD(machine_mode) mode : 8;
+};
+
+/* The table of all qtys, indexed by qty number.  */
+static struct qty_table_elem *qty_table;
+
+#ifdef HAVE_cc0
+/* For machines that have a CC0, we do not record its value in the hash
+   table since its use is guaranteed to be the insn immediately following
+   its definition and any other insn is presumed to invalidate it.
+
+   Instead, we store below the value last assigned to CC0.  If it should
+   happen to be a constant, it is stored in preference to the actual
+   assigned value.  In case it is a constant, we store the mode in which
+   the constant should be interpreted.  */
+
+static rtx prev_insn_cc0;
+static enum machine_mode prev_insn_cc0_mode;
+
+/* Previous actual insn.  0 if at first insn of basic block.  */
+
+static rtx prev_insn;
+#endif
+
+/* Insn being scanned.  */
+
+static rtx this_insn;
+
+/* Index by register number, gives the number of the next (or
+   previous) register in the chain of registers sharing the same
+   value.
+
+   Or -1 if this register is at the end of the chain.
+
+   If reg_qty[N] == N, reg_eqv_table[N].next is undefined.  */
+
+/* Per-register equivalence chain.  */
+struct reg_eqv_elem
+{
+  int next, prev;
+};
+
+/* The table of all register equivalence chains.  */
+static struct reg_eqv_elem *reg_eqv_table;
+
+struct cse_reg_info
+{
+  /* Next in hash chain.  */
+  struct cse_reg_info *hash_next;
+
+  /* The next cse_reg_info structure in the free or used list.  */
+  struct cse_reg_info *next;
+
+  /* Search key */
+  unsigned int regno;
+
+  /* The quantity number of the register's current contents.  */
+  int reg_qty;
+
+  /* The number of times the register has been altered in the current
+     basic block.  */
+  int reg_tick;
+
+  /* The REG_TICK value at which rtx's containing this register are
+     valid in the hash table.  If this does not equal the current
+     reg_tick value, such expressions existing in the hash table are
+     invalid.  */
+  int reg_in_table;
+
+  /* The SUBREG that was set when REG_TICK was last incremented.  Set
+     to -1 if the last store was to the whole register, not a subreg.  */
+  unsigned int subreg_ticked;
+};
+
+/* A free list of cse_reg_info entries.  */
+static struct cse_reg_info *cse_reg_info_free_list;
+
+/* A used list of cse_reg_info entries.  */
+static struct cse_reg_info *cse_reg_info_used_list;
+static struct cse_reg_info *cse_reg_info_used_list_end;
+
+/* A mapping from registers to cse_reg_info data structures.  */
+#define REGHASH_SHIFT	7
+#define REGHASH_SIZE	(1 << REGHASH_SHIFT)
+#define REGHASH_MASK	(REGHASH_SIZE - 1)
+static struct cse_reg_info *reg_hash[REGHASH_SIZE];
+
+#define REGHASH_FN(REGNO)	\
+	(((REGNO) ^ ((REGNO) >> REGHASH_SHIFT)) & REGHASH_MASK)
+
+/* The last lookup we did into the cse_reg_info_tree.  This allows us
+   to cache repeated lookups.  */
+static unsigned int cached_regno;
+static struct cse_reg_info *cached_cse_reg_info;
+
+/* A HARD_REG_SET containing all the hard registers for which there is
+   currently a REG expression in the hash table.  Note the difference
+   from the above variables, which indicate if the REG is mentioned in some
+   expression in the table.  */
+
+static HARD_REG_SET hard_regs_in_table;
+
+/* CUID of insn that starts the basic block currently being cse-processed.  */
+
+static int cse_basic_block_start;
+
+/* CUID of insn that ends the basic block currently being cse-processed.  */
+
+static int cse_basic_block_end;
+
+/* Vector mapping INSN_UIDs to cuids.
+   The cuids are like uids but increase monotonically always.
+   We use them to see whether a reg is used outside a given basic block.  */
+
+static int *uid_cuid;
+
+/* Highest UID in UID_CUID.  */
+static int max_uid;
+
+/* Get the cuid of an insn.  */
+
+#define INSN_CUID(INSN) (uid_cuid[INSN_UID (INSN)])
+
+/* Nonzero if this pass has made changes, and therefore it's
+   worthwhile to run the garbage collector.  */
+
+static int cse_altered;
+
+/* Nonzero if cse has altered conditional jump insns
+   in such a way that jump optimization should be redone.  */
+
+static int cse_jumps_altered;
+
+/* Nonzero if we put a LABEL_REF into the hash table for an INSN without a
+   REG_LABEL, we have to rerun jump after CSE to put in the note.  */
+static int recorded_label_ref;
+
+/* canon_hash stores 1 in do_not_record
+   if it notices a reference to CC0, PC, or some other volatile
+   subexpression.  */
+
+static int do_not_record;
+
+#ifdef LOAD_EXTEND_OP
+
+/* Scratch rtl used when looking for load-extended copy of a MEM.  */
+static rtx memory_extend_rtx;
+#endif
+
+/* canon_hash stores 1 in hash_arg_in_memory
+   if it notices a reference to memory within the expression being hashed.  */
+
+static int hash_arg_in_memory;
+
+/* The hash table contains buckets which are chains of `struct table_elt's,
+   each recording one expression's information.
+   That expression is in the `exp' field.
+
+   The canon_exp field contains a canonical (from the point of view of
+   alias analysis) version of the `exp' field.
+
+   Those elements with the same hash code are chained in both directions
+   through the `next_same_hash' and `prev_same_hash' fields.
+
+   Each set of expressions with equivalent values
+   are on a two-way chain through the `next_same_value'
+   and `prev_same_value' fields, and all point with
+   the `first_same_value' field at the first element in
+   that chain.  The chain is in order of increasing cost.
+   Each element's cost value is in its `cost' field.
+
+   The `in_memory' field is nonzero for elements that
+   involve any reference to memory.  These elements are removed
+   whenever a write is done to an unidentified location in memory.
+   To be safe, we assume that a memory address is unidentified unless
+   the address is either a symbol constant or a constant plus
+   the frame pointer or argument pointer.
+
+   The `related_value' field is used to connect related expressions
+   (that differ by adding an integer).
+   The related expressions are chained in a circular fashion.
+   `related_value' is zero for expressions for which this
+   chain is not useful.
+
+   The `cost' field stores the cost of this element's expression.
+   The `regcost' field stores the value returned by approx_reg_cost for
+   this element's expression.
+
+   The `is_const' flag is set if the element is a constant (including
+   a fixed address).
+
+   The `flag' field is used as a temporary during some search routines.
+
+   The `mode' field is usually the same as GET_MODE (`exp'), but
+   if `exp' is a CONST_INT and has no machine mode then the `mode'
+   field is the mode it was being used as.  Each constant is
+   recorded separately for each mode it is used with.  */
+
+struct table_elt
+{
+  rtx exp;
+  rtx canon_exp;
+  struct table_elt *next_same_hash;
+  struct table_elt *prev_same_hash;
+  struct table_elt *next_same_value;
+  struct table_elt *prev_same_value;
+  struct table_elt *first_same_value;
+  struct table_elt *related_value;
+  int cost;
+  int regcost;
+  /* The size of this field should match the size
+     of the mode field of struct rtx_def (see rtl.h).  */
+  ENUM_BITFIELD(machine_mode) mode : 8;
+  char in_memory;
+  char is_const;
+  char flag;
+};
+
+/* We don't want a lot of buckets, because we rarely have very many
+   things stored in the hash table, and a lot of buckets slows
+   down a lot of loops that happen frequently.  */
+#define HASH_SHIFT	5
+#define HASH_SIZE	(1 << HASH_SHIFT)
+#define HASH_MASK	(HASH_SIZE - 1)
+
+/* Compute hash code of X in mode M.  Special-case case where X is a pseudo
+   register (hard registers may require `do_not_record' to be set).  */
+
+#define HASH(X, M)	\
+ ((GET_CODE (X) == REG && REGNO (X) >= FIRST_PSEUDO_REGISTER	\
+  ? (((unsigned) REG << 7) + (unsigned) REG_QTY (REGNO (X)))	\
+  : canon_hash (X, M)) & HASH_MASK)
+
+/* Determine whether register number N is considered a fixed register for the
+   purpose of approximating register costs.
+   It is desirable to replace other regs with fixed regs, to reduce need for
+   non-fixed hard regs.
+   A reg wins if it is either the frame pointer or designated as fixed.  */
+#define FIXED_REGNO_P(N)  \
+  ((N) == FRAME_POINTER_REGNUM || (N) == HARD_FRAME_POINTER_REGNUM \
+   || fixed_regs[N] || global_regs[N])
+
+/* Compute cost of X, as stored in the `cost' field of a table_elt.  Fixed
+   hard registers and pointers into the frame are the cheapest with a cost
+   of 0.  Next come pseudos with a cost of one and other hard registers with
+   a cost of 2.  Aside from these special cases, call `rtx_cost'.  */
+
+#define CHEAP_REGNO(N) \
+  ((N) == FRAME_POINTER_REGNUM || (N) == HARD_FRAME_POINTER_REGNUM	\
+   || (N) == STACK_POINTER_REGNUM || (N) == ARG_POINTER_REGNUM		\
+   || ((N) >= FIRST_VIRTUAL_REGISTER && (N) <= LAST_VIRTUAL_REGISTER)	\
+   || ((N) < FIRST_PSEUDO_REGISTER					\
+       && FIXED_REGNO_P (N) && REGNO_REG_CLASS (N) != NO_REGS))
+
+#define COST(X) (GET_CODE (X) == REG ? 0 : notreg_cost (X, SET))
+#define COST_IN(X,OUTER) (GET_CODE (X) == REG ? 0 : notreg_cost (X, OUTER))
+
+/* Get the info associated with register N.  */
+
+#define GET_CSE_REG_INFO(N)			\
+  (((N) == cached_regno && cached_cse_reg_info)	\
+   ? cached_cse_reg_info : get_cse_reg_info ((N)))
+
+/* Get the number of times this register has been updated in this
+   basic block.  */
+
+#define REG_TICK(N) ((GET_CSE_REG_INFO (N))->reg_tick)
+
+/* Get the point at which REG was recorded in the table.  */
+
+#define REG_IN_TABLE(N) ((GET_CSE_REG_INFO (N))->reg_in_table)
+
+/* Get the SUBREG set at the last increment to REG_TICK (-1 if not a
+   SUBREG).  */
+
+#define SUBREG_TICKED(N) ((GET_CSE_REG_INFO (N))->subreg_ticked)
+
+/* Get the quantity number for REG.  */
+
+#define REG_QTY(N) ((GET_CSE_REG_INFO (N))->reg_qty)
+
+/* Determine if the quantity number for register X represents a valid index
+   into the qty_table.  */
+
+#define REGNO_QTY_VALID_P(N) (REG_QTY (N) >= 0)
+
+static struct table_elt *table[HASH_SIZE];
+
+/* Chain of `struct table_elt's made so far for this function
+   but currently removed from the table.  */
+
+static struct table_elt *free_element_chain;
+
+/* Number of `struct table_elt' structures made so far for this function.  */
+
+static int n_elements_made;
+
+/* Maximum value `n_elements_made' has had so far in this compilation
+   for functions previously processed.  */
+
+static int max_elements_made;
+
+/* Surviving equivalence class when two equivalence classes are merged
+   by recording the effects of a jump in the last insn.  Zero if the
+   last insn was not a conditional jump.  */
+
+static struct table_elt *last_jump_equiv_class;
+
+/* Set to the cost of a constant pool reference if one was found for a
+   symbolic constant.  If this was found, it means we should try to
+   convert constants into constant pool entries if they don't fit in
+   the insn.  */
+
+static int constant_pool_entries_cost;
+static int constant_pool_entries_regcost;
+
+/* This data describes a block that will be processed by cse_basic_block.  */
+
+struct cse_basic_block_data
+{
+  /* Lowest CUID value of insns in block.  */
+  int low_cuid;
+  /* Highest CUID value of insns in block.  */
+  int high_cuid;
+  /* Total number of SETs in block.  */
+  int nsets;
+  /* Last insn in the block.  */
+  rtx last;
+  /* Size of current branch path, if any.  */
+  int path_size;
+  /* Current branch path, indicating which branches will be taken.  */
+  struct branch_path
+    {
+      /* The branch insn.  */
+      rtx branch;
+      /* Whether it should be taken or not.  AROUND is the same as taken
+	 except that it is used when the destination label is not preceded
+       by a BARRIER.  */
+      enum taken {TAKEN, NOT_TAKEN, AROUND} status;
+    } *path;
+};
+
+static bool fixed_base_plus_p (rtx x);
+static int notreg_cost (rtx, enum rtx_code);
+static int approx_reg_cost_1 (rtx *, void *);
+static int approx_reg_cost (rtx);
+static int preferrable (int, int, int, int);
+static void new_basic_block (void);
+static void make_new_qty (unsigned int, enum machine_mode);
+static void make_regs_eqv (unsigned int, unsigned int);
+static void delete_reg_equiv (unsigned int);
+static int mention_regs (rtx);
+static int insert_regs (rtx, struct table_elt *, int);
+static void remove_from_table (struct table_elt *, unsigned);
+static struct table_elt *lookup	(rtx, unsigned, enum machine_mode);
+static struct table_elt *lookup_for_remove (rtx, unsigned, enum machine_mode);
+static rtx lookup_as_function (rtx, enum rtx_code);
+static struct table_elt *insert (rtx, struct table_elt *, unsigned,
+				 enum machine_mode);
+static void merge_equiv_classes (struct table_elt *, struct table_elt *);
+static void invalidate (rtx, enum machine_mode);
+static int cse_rtx_varies_p (rtx, int);
+static void remove_invalid_refs (unsigned int);
+static void remove_invalid_subreg_refs (unsigned int, unsigned int,
+					enum machine_mode);
+static void rehash_using_reg (rtx);
+static void invalidate_memory (void);
+static void invalidate_for_call (void);
+static rtx use_related_value (rtx, struct table_elt *);
+static unsigned canon_hash (rtx, enum machine_mode);
+static unsigned canon_hash_string (const char *);
+static unsigned safe_hash (rtx, enum machine_mode);
+static int exp_equiv_p (rtx, rtx, int, int);
+static rtx canon_reg (rtx, rtx);
+static void find_best_addr (rtx, rtx *, enum machine_mode);
+static enum rtx_code find_comparison_args (enum rtx_code, rtx *, rtx *,
+					   enum machine_mode *,
+					   enum machine_mode *);
+static rtx fold_rtx (rtx, rtx);
+static rtx equiv_constant (rtx);
+static void record_jump_equiv (rtx, int);
+static void record_jump_cond (enum rtx_code, enum machine_mode, rtx, rtx,
+			      int);
+static void cse_insn (rtx, rtx);
+static int addr_affects_sp_p (rtx);
+static void invalidate_from_clobbers (rtx);
+static rtx cse_process_notes (rtx, rtx);
+static void cse_around_loop (rtx);
+static void invalidate_skipped_set (rtx, rtx, void *);
+static void invalidate_skipped_block (rtx);
+static void cse_check_loop_start (rtx, rtx, void *);
+static void cse_set_around_loop (rtx, rtx, rtx);
+static rtx cse_basic_block (rtx, rtx, struct branch_path *, int);
+static void count_reg_usage (rtx, int *, int);
+static int check_for_label_ref (rtx *, void *);
+extern void dump_class (struct table_elt*);
+static struct cse_reg_info * get_cse_reg_info (unsigned int);
+static int check_dependence (rtx *, void *);
+
+static void flush_hash_table (void);
+static bool insn_live_p (rtx, int *);
+static bool set_live_p (rtx, rtx, int *);
+static bool dead_libcall_p (rtx, int *);
+static int cse_change_cc_mode (rtx *, void *);
+static void cse_change_cc_mode_insns (rtx, rtx, rtx);
+static enum machine_mode cse_cc_succs (basic_block, rtx, rtx, bool);
+
+/* Nonzero if X has the form (PLUS frame-pointer integer).  We check for
+   virtual regs here because the simplify_*_operation routines are called
+   by integrate.c, which is called before virtual register instantiation.  */
+
+static bool
+fixed_base_plus_p (rtx x)
+{
+  switch (GET_CODE (x))
+    {
+    case REG:
+      if (x == frame_pointer_rtx || x == hard_frame_pointer_rtx)
+	return true;
+      if (x == arg_pointer_rtx && fixed_regs[ARG_POINTER_REGNUM])
+	return true;
+      if (REGNO (x) >= FIRST_VIRTUAL_REGISTER
+	  && REGNO (x) <= LAST_VIRTUAL_REGISTER)
+	return true;
+      return false;
+
+    case PLUS:
+      if (GET_CODE (XEXP (x, 1)) != CONST_INT)
+	return false;
+      return fixed_base_plus_p (XEXP (x, 0));
+
+    case ADDRESSOF:
+      return true;
+
+    default:
+      return false;
+    }
+}
+
+/* Dump the expressions in the equivalence class indicated by CLASSP.
+   This function is used only for debugging.  */
+void
+dump_class (struct table_elt *classp)
+{
+  struct table_elt *elt;
+
+  fprintf (stderr, "Equivalence chain for ");
+  print_rtl (stderr, classp->exp);
+  fprintf (stderr, ": \n");
+
+  for (elt = classp->first_same_value; elt; elt = elt->next_same_value)
+    {
+      print_rtl (stderr, elt->exp);
+      fprintf (stderr, "\n");
+    }
+}
+
+/* Subroutine of approx_reg_cost; called through for_each_rtx.  */
+
+static int
+approx_reg_cost_1 (rtx *xp, void *data)
+{
+  rtx x = *xp;
+  int *cost_p = data;
+
+  if (x && GET_CODE (x) == REG)
+    {
+      unsigned int regno = REGNO (x);
+
+      if (! CHEAP_REGNO (regno))
+	{
+	  if (regno < FIRST_PSEUDO_REGISTER)
+	    {
+	      if (SMALL_REGISTER_CLASSES)
+		return 1;
+	      *cost_p += 2;
+	    }
+	  else
+	    *cost_p += 1;
+	}
+    }
+
+  return 0;
+}
+
+/* Return an estimate of the cost of the registers used in an rtx.
+   This is mostly the number of different REG expressions in the rtx;
+   however for some exceptions like fixed registers we use a cost of
+   0.  If any other hard register reference occurs, return MAX_COST.  */
+
+static int
+approx_reg_cost (rtx x)
+{
+  int cost = 0;
+
+  if (for_each_rtx (&x, approx_reg_cost_1, (void *) &cost))
+    return MAX_COST;
+
+  return cost;
+}
+
+/* Return a negative value if an rtx A, whose costs are given by COST_A
+   and REGCOST_A, is more desirable than an rtx B.
+   Return a positive value if A is less desirable, or 0 if the two are
+   equally good.  */
+static int
+preferrable (int cost_a, int regcost_a, int cost_b, int regcost_b)
+{
+  /* First, get rid of cases involving expressions that are entirely
+     unwanted.  */
+  if (cost_a != cost_b)
+    {
+      if (cost_a == MAX_COST)
+	return 1;
+      if (cost_b == MAX_COST)
+	return -1;
+    }
+
+  /* Avoid extending lifetimes of hardregs.  */
+  if (regcost_a != regcost_b)
+    {
+      if (regcost_a == MAX_COST)
+	return 1;
+      if (regcost_b == MAX_COST)
+	return -1;
+    }
+
+  /* Normal operation costs take precedence.  */
+  if (cost_a != cost_b)
+    return cost_a - cost_b;
+  /* Only if these are identical consider effects on register pressure.  */
+  if (regcost_a != regcost_b)
+    return regcost_a - regcost_b;
+  return 0;
+}
+
+/* Internal function, to compute cost when X is not a register; called
+   from COST macro to keep it simple.  */
+
+static int
+notreg_cost (rtx x, enum rtx_code outer)
+{
+  return ((GET_CODE (x) == SUBREG
+	   && GET_CODE (SUBREG_REG (x)) == REG
+	   && GET_MODE_CLASS (GET_MODE (x)) == MODE_INT
+	   && GET_MODE_CLASS (GET_MODE (SUBREG_REG (x))) == MODE_INT
+	   && (GET_MODE_SIZE (GET_MODE (x))
+	       < GET_MODE_SIZE (GET_MODE (SUBREG_REG (x))))
+	   && subreg_lowpart_p (x)
+	   && TRULY_NOOP_TRUNCATION (GET_MODE_BITSIZE (GET_MODE (x)),
+				     GET_MODE_BITSIZE (GET_MODE (SUBREG_REG (x)))))
+	  ? 0
+	  : rtx_cost (x, outer) * 2);
+}
+
+/* Return an estimate of the cost of computing rtx X.
+   One use is in cse, to decide which expression to keep in the hash table.
+   Another is in rtl generation, to pick the cheapest way to multiply.
+   Other uses like the latter are expected in the future.  */
+
+int
+rtx_cost (rtx x, enum rtx_code outer_code ATTRIBUTE_UNUSED)
+{
+  int i, j;
+  enum rtx_code code;
+  const char *fmt;
+  int total;
+
+  if (x == 0)
+    return 0;
+
+  /* Compute the default costs of certain things.
+     Note that targetm.rtx_costs can override the defaults.  */
+
+  code = GET_CODE (x);
+  switch (code)
+    {
+    case MULT:
+      total = COSTS_N_INSNS (5);
+      break;
+    case DIV:
+    case UDIV:
+    case MOD:
+    case UMOD:
+      total = COSTS_N_INSNS (7);
+      break;
+    case USE:
+      /* Used in loop.c and combine.c as a marker.  */
+      total = 0;
+      break;
+    default:
+      total = COSTS_N_INSNS (1);
+    }
+
+  switch (code)
+    {
+    case REG:
+      return 0;
+
+    case SUBREG:
+      /* If we can't tie these modes, make this expensive.  The larger
+	 the mode, the more expensive it is.  */
+      if (! MODES_TIEABLE_P (GET_MODE (x), GET_MODE (SUBREG_REG (x))))
+	return COSTS_N_INSNS (2
+			      + GET_MODE_SIZE (GET_MODE (x)) / UNITS_PER_WORD);
+      break;
+
+    default:
+      if ((*targetm.rtx_costs) (x, code, outer_code, &total))
+	return total;
+      break;
+    }
+
+  /* Sum the costs of the sub-rtx's, plus cost of this operation,
+     which is already in total.  */
+
+  fmt = GET_RTX_FORMAT (code);
+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)
+    if (fmt[i] == 'e')
+      total += rtx_cost (XEXP (x, i), code);
+    else if (fmt[i] == 'E')
+      for (j = 0; j < XVECLEN (x, i); j++)
+	total += rtx_cost (XVECEXP (x, i, j), code);
+
+  return total;
+}
+
+/* Return cost of address expression X.
+   Expect that X is properly formed address reference.  */
+
+int
+address_cost (rtx x, enum machine_mode mode)
+{
+  /* The address_cost target hook does not deal with ADDRESSOF nodes.  But,
+     during CSE, such nodes are present.  Using an ADDRESSOF node which
+     refers to the address of a REG is a good thing because we can then
+     turn (MEM (ADDRESSSOF (REG))) into just plain REG.  */
+
+  if (GET_CODE (x) == ADDRESSOF && REG_P (XEXP ((x), 0)))
+    return -1;
+
+  /* We may be asked for cost of various unusual addresses, such as operands
+     of push instruction.  It is not worthwhile to complicate writing
+     of the target hook by such cases.  */
+
+  if (!memory_address_p (mode, x))
+    return 1000;
+
+  return (*targetm.address_cost) (x);
+}
+
+/* If the target doesn't override, compute the cost as with arithmetic.  */
+
+int
+default_address_cost (rtx x)
+{
+  return rtx_cost (x, MEM);
+}
+
+static struct cse_reg_info *
+get_cse_reg_info (unsigned int regno)
+{
+  struct cse_reg_info **hash_head = &reg_hash[REGHASH_FN (regno)];
+  struct cse_reg_info *p;
+
+  for (p = *hash_head; p != NULL; p = p->hash_next)
+    if (p->regno == regno)
+      break;
+
+  if (p == NULL)
+    {
+      /* Get a new cse_reg_info structure.  */
+      if (cse_reg_info_free_list)
+	{
+	  p = cse_reg_info_free_list;
+	  cse_reg_info_free_list = p->next;
+	}
+      else
+	p = xmalloc (sizeof (struct cse_reg_info));
+
+      /* Insert into hash table.  */
+      p->hash_next = *hash_head;
+      *hash_head = p;
+
+      /* Initialize it.  */
+      p->reg_tick = 1;
+      p->reg_in_table = -1;
+      p->subreg_ticked = -1;
+      p->reg_qty = -regno - 1;
+      p->regno = regno;
+      p->next = cse_reg_info_used_list;
+      cse_reg_info_used_list = p;
+      if (!cse_reg_info_used_list_end)
+	cse_reg_info_used_list_end = p;
+    }
+
+  /* Cache this lookup; we tend to be looking up information about the
+     same register several times in a row.  */
+  cached_regno = regno;
+  cached_cse_reg_info = p;
+
+  return p;
+}
+
+/* Clear the hash table and initialize each register with its own quantity,
+   for a new basic block.  */
+
+static void
+new_basic_block (void)
+{
+  int i;
+
+  next_qty = 0;
+
+  /* Clear out hash table state for this pass.  */
+
+  memset (reg_hash, 0, sizeof reg_hash);
+
+  if (cse_reg_info_used_list)
+    {
+      cse_reg_info_used_list_end->next = cse_reg_info_free_list;
+      cse_reg_info_free_list = cse_reg_info_used_list;
+      cse_reg_info_used_list = cse_reg_info_used_list_end = 0;
+    }
+  cached_cse_reg_info = 0;
+
+  CLEAR_HARD_REG_SET (hard_regs_in_table);
+
+  /* The per-quantity values used to be initialized here, but it is
+     much faster to initialize each as it is made in `make_new_qty'.  */
+
+  for (i = 0; i < HASH_SIZE; i++)
+    {
+      struct table_elt *first;
+
+      first = table[i];
+      if (first != NULL)
+	{
+	  struct table_elt *last = first;
+
+	  table[i] = NULL;
+
+	  while (last->next_same_hash != NULL)
+	    last = last->next_same_hash;
+
+	  /* Now relink this hash entire chain into
+	     the free element list.  */
+
+	  last->next_same_hash = free_element_chain;
+	  free_element_chain = first;
+	}
+    }
+
+#ifdef HAVE_cc0
+  prev_insn = 0;
+  prev_insn_cc0 = 0;
+#endif
+}
+
+/* Say that register REG contains a quantity in mode MODE not in any
+   register before and initialize that quantity.  */
+
+static void
+make_new_qty (unsigned int reg, enum machine_mode mode)
+{
+  int q;
+  struct qty_table_elem *ent;
+  struct reg_eqv_elem *eqv;
+
+  if (next_qty >= max_qty)
+    abort ();
+
+  q = REG_QTY (reg) = next_qty++;
+  ent = &qty_table[q];
+  ent->first_reg = reg;
+  ent->last_reg = reg;
+  ent->mode = mode;
+  ent->const_rtx = ent->const_insn = NULL_RTX;
+  ent->comparison_code = UNKNOWN;
+
+  eqv = &reg_eqv_table[reg];
+  eqv->next = eqv->prev = -1;
+}
+
+/* Make reg NEW equivalent to reg OLD.
+   OLD is not changing; NEW is.  */
+
+static void
+make_regs_eqv (unsigned int new, unsigned int old)
+{
+  unsigned int lastr, firstr;
+  int q = REG_QTY (old);
+  struct qty_table_elem *ent;
+
+  ent = &qty_table[q];
+
+  /* Nothing should become eqv until it has a "non-invalid" qty number.  */
+  if (! REGNO_QTY_VALID_P (old))
+    abort ();
+
+  REG_QTY (new) = q;
+  firstr = ent->first_reg;
+  lastr = ent->last_reg;
+
+  /* Prefer fixed hard registers to anything.  Prefer pseudo regs to other
+     hard regs.  Among pseudos, if NEW will live longer than any other reg
+     of the same qty, and that is beyond the current basic block,
+     make it the new canonical replacement for this qty.  */
+  if (! (firstr < FIRST_PSEUDO_REGISTER && FIXED_REGNO_P (firstr))
+      /* Certain fixed registers might be of the class NO_REGS.  This means
+	 that not only can they not be allocated by the compiler, but
+	 they cannot be used in substitutions or canonicalizations
+	 either.  */
+      && (new >= FIRST_PSEUDO_REGISTER || REGNO_REG_CLASS (new) != NO_REGS)
+      && ((new < FIRST_PSEUDO_REGISTER && FIXED_REGNO_P (new))
+	  || (new >= FIRST_PSEUDO_REGISTER
+	      && (firstr < FIRST_PSEUDO_REGISTER
+		  || ((uid_cuid[REGNO_LAST_UID (new)] > cse_basic_block_end
+		       || (uid_cuid[REGNO_FIRST_UID (new)]
+			   < cse_basic_block_start))
+		      && (uid_cuid[REGNO_LAST_UID (new)]
+			  > uid_cuid[REGNO_LAST_UID (firstr)]))))))
+    {
+      reg_eqv_table[firstr].prev = new;
+      reg_eqv_table[new].next = firstr;
+      reg_eqv_table[new].prev = -1;
+      ent->first_reg = new;
+    }
+  else
+    {
+      /* If NEW is a hard reg (known to be non-fixed), insert at end.
+	 Otherwise, insert before any non-fixed hard regs that are at the
+	 end.  Registers of class NO_REGS cannot be used as an
+	 equivalent for anything.  */
+      while (lastr < FIRST_PSEUDO_REGISTER && reg_eqv_table[lastr].prev >= 0
+	     && (REGNO_REG_CLASS (lastr) == NO_REGS || ! FIXED_REGNO_P (lastr))
+	     && new >= FIRST_PSEUDO_REGISTER)
+	lastr = reg_eqv_table[lastr].prev;
+      reg_eqv_table[new].next = reg_eqv_table[lastr].next;
+      if (reg_eqv_table[lastr].next >= 0)
+	reg_eqv_table[reg_eqv_table[lastr].next].prev = new;
+      else
+	qty_table[q].last_reg = new;
+      reg_eqv_table[lastr].next = new;
+      reg_eqv_table[new].prev = lastr;
+    }
+}
+
+/* Remove REG from its equivalence class.  */
+
+static void
+delete_reg_equiv (unsigned int reg)
+{
+  struct qty_table_elem *ent;
+  int q = REG_QTY (reg);
+  int p, n;
+
+  /* If invalid, do nothing.  */
+  if (! REGNO_QTY_VALID_P (reg))
+    return;
+
+  ent = &qty_table[q];
+
+  p = reg_eqv_table[reg].prev;
+  n = reg_eqv_table[reg].next;
+
+  if (n != -1)
+    reg_eqv_table[n].prev = p;
+  else
+    ent->last_reg = p;
+  if (p != -1)
+    reg_eqv_table[p].next = n;
+  else
+    ent->first_reg = n;
+
+  REG_QTY (reg) = -reg - 1;
+}
+
+/* Remove any invalid expressions from the hash table
+   that refer to any of the registers contained in expression X.
+
+   Make sure that newly inserted references to those registers
+   as subexpressions will be considered valid.
+
+   mention_regs is not called when a register itself
+   is being stored in the table.
+
+   Return 1 if we have done something that may have changed the hash code
+   of X.  */
+
+static int
+mention_regs (rtx x)
+{
+  enum rtx_code code;
+  int i, j;
+  const char *fmt;
+  int changed = 0;
+
+  if (x == 0)
+    return 0;
+
+  code = GET_CODE (x);
+  if (code == REG)
+    {
+      unsigned int regno = REGNO (x);
+      unsigned int endregno
+	= regno + (regno >= FIRST_PSEUDO_REGISTER ? 1
+		   : HARD_REGNO_NREGS (regno, GET_MODE (x)));
+      unsigned int i;
+
+      for (i = regno; i < endregno; i++)
+	{
+	  if (REG_IN_TABLE (i) >= 0 && REG_IN_TABLE (i) != REG_TICK (i))
+	    remove_invalid_refs (i);
+
+	  REG_IN_TABLE (i) = REG_TICK (i);
+	  SUBREG_TICKED (i) = -1;
+	}
+
+      return 0;
+    }
+
+  /* If this is a SUBREG, we don't want to discard other SUBREGs of the same
+     pseudo if they don't use overlapping words.  We handle only pseudos
+     here for simplicity.  */
+  if (code == SUBREG && GET_CODE (SUBREG_REG (x)) == REG
+      && REGNO (SUBREG_REG (x)) >= FIRST_PSEUDO_REGISTER)
+    {
+      unsigned int i = REGNO (SUBREG_REG (x));
+
+      if (REG_IN_TABLE (i) >= 0 && REG_IN_TABLE (i) != REG_TICK (i))
+	{
+	  /* If REG_IN_TABLE (i) differs from REG_TICK (i) by one, and
+	     the last store to this register really stored into this
+	     subreg, then remove the memory of this subreg.
+	     Otherwise, remove any memory of the entire register and
+	     all its subregs from the table.  */
+	  if (REG_TICK (i) - REG_IN_TABLE (i) > 1
+	      || SUBREG_TICKED (i) != REGNO (SUBREG_REG (x)))
+	    remove_invalid_refs (i);
+	  else
+	    remove_invalid_subreg_refs (i, SUBREG_BYTE (x), GET_MODE (x));
+	}
+
+      REG_IN_TABLE (i) = REG_TICK (i);
+      SUBREG_TICKED (i) = REGNO (SUBREG_REG (x));
+      return 0;
+    }
+
+  /* If X is a comparison or a COMPARE and either operand is a register
+     that does not have a quantity, give it one.  This is so that a later
+     call to record_jump_equiv won't cause X to be assigned a different
+     hash code and not found in the table after that call.
+
+     It is not necessary to do this here, since rehash_using_reg can
+     fix up the table later, but doing this here eliminates the need to
+     call that expensive function in the most common case where the only
+     use of the register is in the comparison.  */
+
+  if (code == COMPARE || GET_RTX_CLASS (code) == '<')
+    {
+      if (GET_CODE (XEXP (x, 0)) == REG
+	  && ! REGNO_QTY_VALID_P (REGNO (XEXP (x, 0))))
+	if (insert_regs (XEXP (x, 0), NULL, 0))
+	  {
+	    rehash_using_reg (XEXP (x, 0));
+	    changed = 1;
+	  }
+
+      if (GET_CODE (XEXP (x, 1)) == REG
+	  && ! REGNO_QTY_VALID_P (REGNO (XEXP (x, 1))))
+	if (insert_regs (XEXP (x, 1), NULL, 0))
+	  {
+	    rehash_using_reg (XEXP (x, 1));
+	    changed = 1;
+	  }
+    }
+
+  fmt = GET_RTX_FORMAT (code);
+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)
+    if (fmt[i] == 'e')
+      changed |= mention_regs (XEXP (x, i));
+    else if (fmt[i] == 'E')
+      for (j = 0; j < XVECLEN (x, i); j++)
+	changed |= mention_regs (XVECEXP (x, i, j));
+
+  return changed;
+}
+
+/* Update the register quantities for inserting X into the hash table
+   with a value equivalent to CLASSP.
+   (If the class does not contain a REG, it is irrelevant.)
+   If MODIFIED is nonzero, X is a destination; it is being modified.
+   Note that delete_reg_equiv should be called on a register
+   before insert_regs is done on that register with MODIFIED != 0.
+
+   Nonzero value means that elements of reg_qty have changed
+   so X's hash code may be different.  */
+
+static int
+insert_regs (rtx x, struct table_elt *classp, int modified)
+{
+  if (GET_CODE (x) == REG)
+    {
+      unsigned int regno = REGNO (x);
+      int qty_valid;
+
+      /* If REGNO is in the equivalence table already but is of the
+	 wrong mode for that equivalence, don't do anything here.  */
+
+      qty_valid = REGNO_QTY_VALID_P (regno);
+      if (qty_valid)
+	{
+	  struct qty_table_elem *ent = &qty_table[REG_QTY (regno)];
+
+	  if (ent->mode != GET_MODE (x))
+	    return 0;
+	}
+
+      if (modified || ! qty_valid)
+	{
+	  if (classp)
+	    for (classp = classp->first_same_value;
+		 classp != 0;
+		 classp = classp->next_same_value)
+	      if (GET_CODE (classp->exp) == REG
+		  && GET_MODE (classp->exp) == GET_MODE (x))
+		{
+		  make_regs_eqv (regno, REGNO (classp->exp));
+		  return 1;
+		}
+
+	  /* Mention_regs for a SUBREG checks if REG_TICK is exactly one larger
+	     than REG_IN_TABLE to find out if there was only a single preceding
+	     invalidation - for the SUBREG - or another one, which would be
+	     for the full register.  However, if we find here that REG_TICK
+	     indicates that the register is invalid, it means that it has
+	     been invalidated in a separate operation.  The SUBREG might be used
+	     now (then this is a recursive call), or we might use the full REG
+	     now and a SUBREG of it later.  So bump up REG_TICK so that
+	     mention_regs will do the right thing.  */
+	  if (! modified
+	      && REG_IN_TABLE (regno) >= 0
+	      && REG_TICK (regno) == REG_IN_TABLE (regno) + 1)
+	    REG_TICK (regno)++;
+	  make_new_qty (regno, GET_MODE (x));
+	  return 1;
+	}
+
+      return 0;
+    }
+
+  /* If X is a SUBREG, we will likely be inserting the inner register in the
+     table.  If that register doesn't have an assigned quantity number at
+     this point but does later, the insertion that we will be doing now will
+     not be accessible because its hash code will have changed.  So assign
+     a quantity number now.  */
+
+  else if (GET_CODE (x) == SUBREG && GET_CODE (SUBREG_REG (x)) == REG
+	   && ! REGNO_QTY_VALID_P (REGNO (SUBREG_REG (x))))
+    {
+      insert_regs (SUBREG_REG (x), NULL, 0);
+      mention_regs (x);
+      return 1;
+    }
+  else
+    return mention_regs (x);
+}
+
+/* Look in or update the hash table.  */
+
+/* Remove table element ELT from use in the table.
+   HASH is its hash code, made using the HASH macro.
+   It's an argument because often that is known in advance
+   and we save much time not recomputing it.  */
+
+static void
+remove_from_table (struct table_elt *elt, unsigned int hash)
+{
+  if (elt == 0)
+    return;
+
+  /* Mark this element as removed.  See cse_insn.  */
+  elt->first_same_value = 0;
+
+  /* Remove the table element from its equivalence class.  */
+
+  {
+    struct table_elt *prev = elt->prev_same_value;
+    struct table_elt *next = elt->next_same_value;
+
+    if (next)
+      next->prev_same_value = prev;
+
+    if (prev)
+      prev->next_same_value = next;
+    else
+      {
+	struct table_elt *newfirst = next;
+	while (next)
+	  {
+	    next->first_same_value = newfirst;
+	    next = next->next_same_value;
+	  }
+      }
+  }
+
+  /* Remove the table element from its hash bucket.  */
+
+  {
+    struct table_elt *prev = elt->prev_same_hash;
+    struct table_elt *next = elt->next_same_hash;
+
+    if (next)
+      next->prev_same_hash = prev;
+
+    if (prev)
+      prev->next_same_hash = next;
+    else if (table[hash] == elt)
+      table[hash] = next;
+    else
+      {
+	/* This entry is not in the proper hash bucket.  This can happen
+	   when two classes were merged by `merge_equiv_classes'.  Search
+	   for the hash bucket that it heads.  This happens only very
+	   rarely, so the cost is acceptable.  */
+	for (hash = 0; hash < HASH_SIZE; hash++)
+	  if (table[hash] == elt)
+	    table[hash] = next;
+      }
+  }
+
+  /* Remove the table element from its related-value circular chain.  */
+
+  if (elt->related_value != 0 && elt->related_value != elt)
+    {
+      struct table_elt *p = elt->related_value;
+
+      while (p->related_value != elt)
+	p = p->related_value;
+      p->related_value = elt->related_value;
+      if (p->related_value == p)
+	p->related_value = 0;
+    }
+
+  /* Now add it to the free element chain.  */
+  elt->next_same_hash = free_element_chain;
+  free_element_chain = elt;
+}
+
+/* Look up X in the hash table and return its table element,
+   or 0 if X is not in the table.
+
+   MODE is the machine-mode of X, or if X is an integer constant
+   with VOIDmode then MODE is the mode with which X will be used.
+
+   Here we are satisfied to find an expression whose tree structure
+   looks like X.  */
+
+static struct table_elt *
+lookup (rtx x, unsigned int hash, enum machine_mode mode)
+{
+  struct table_elt *p;
+
+  for (p = table[hash]; p; p = p->next_same_hash)
+    if (mode == p->mode && ((x == p->exp && GET_CODE (x) == REG)
+			    || exp_equiv_p (x, p->exp, GET_CODE (x) != REG, 0)))
+      return p;
+
+  return 0;
+}
+
+/* Like `lookup' but don't care whether the table element uses invalid regs.
+   Also ignore discrepancies in the machine mode of a register.  */
+
+static struct table_elt *
+lookup_for_remove (rtx x, unsigned int hash, enum machine_mode mode)
+{
+  struct table_elt *p;
+
+  if (GET_CODE (x) == REG)
+    {
+      unsigned int regno = REGNO (x);
+
+      /* Don't check the machine mode when comparing registers;
+	 invalidating (REG:SI 0) also invalidates (REG:DF 0).  */
+      for (p = table[hash]; p; p = p->next_same_hash)
+	if (GET_CODE (p->exp) == REG
+	    && REGNO (p->exp) == regno)
+	  return p;
+    }
+  else
+    {
+      for (p = table[hash]; p; p = p->next_same_hash)
+	if (mode == p->mode && (x == p->exp || exp_equiv_p (x, p->exp, 0, 0)))
+	  return p;
+    }
+
+  return 0;
+}
+
+/* Look for an expression equivalent to X and with code CODE.
+   If one is found, return that expression.  */
+
+static rtx
+lookup_as_function (rtx x, enum rtx_code code)
+{
+  struct table_elt *p
+    = lookup (x, safe_hash (x, VOIDmode) & HASH_MASK, GET_MODE (x));
+
+  /* If we are looking for a CONST_INT, the mode doesn't really matter, as
+     long as we are narrowing.  So if we looked in vain for a mode narrower
+     than word_mode before, look for word_mode now.  */
+  if (p == 0 && code == CONST_INT
+      && GET_MODE_SIZE (GET_MODE (x)) < GET_MODE_SIZE (word_mode))
+    {
+      x = copy_rtx (x);
+      PUT_MODE (x, word_mode);
+      p = lookup (x, safe_hash (x, VOIDmode) & HASH_MASK, word_mode);
+    }
+
+  if (p == 0)
+    return 0;
+
+  for (p = p->first_same_value; p; p = p->next_same_value)
+    if (GET_CODE (p->exp) == code
+	/* Make sure this is a valid entry in the table.  */
+	&& exp_equiv_p (p->exp, p->exp, 1, 0))
+      return p->exp;
+
+  return 0;
+}
+
+/* Insert X in the hash table, assuming HASH is its hash code
+   and CLASSP is an element of the class it should go in
+   (or 0 if a new class should be made).
+   It is inserted at the proper position to keep the class in
+   the order cheapest first.
+
+   MODE is the machine-mode of X, or if X is an integer constant
+   with VOIDmode then MODE is the mode with which X will be used.
+
+   For elements of equal cheapness, the most recent one
+   goes in front, except that the first element in the list
+   remains first unless a cheaper element is added.  The order of
+   pseudo-registers does not matter, as canon_reg will be called to
+   find the cheapest when a register is retrieved from the table.
+
+   The in_memory field in the hash table element is set to 0.
+   The caller must set it nonzero if appropriate.
+
+   You should call insert_regs (X, CLASSP, MODIFY) before calling here,
+   and if insert_regs returns a nonzero value
+   you must then recompute its hash code before calling here.
+
+   If necessary, update table showing constant values of quantities.  */
+
+#define CHEAPER(X, Y) \
+ (preferrable ((X)->cost, (X)->regcost, (Y)->cost, (Y)->regcost) < 0)
+
+static struct table_elt *
+insert (rtx x, struct table_elt *classp, unsigned int hash, enum machine_mode mode)
+{
+  struct table_elt *elt;
+
+  /* If X is a register and we haven't made a quantity for it,
+     something is wrong.  */
+  if (GET_CODE (x) == REG && ! REGNO_QTY_VALID_P (REGNO (x)))
+    abort ();
+
+  /* If X is a hard register, show it is being put in the table.  */
+  if (GET_CODE (x) == REG && REGNO (x) < FIRST_PSEUDO_REGISTER)
+    {
+      unsigned int regno = REGNO (x);
+      unsigned int endregno = regno + HARD_REGNO_NREGS (regno, GET_MODE (x));
+      unsigned int i;
+
+      for (i = regno; i < endregno; i++)
+	SET_HARD_REG_BIT (hard_regs_in_table, i);
+    }
+
+  /* Put an element for X into the right hash bucket.  */
+
+  elt = free_element_chain;
+  if (elt)
+    free_element_chain = elt->next_same_hash;
+  else
+    {
+      n_elements_made++;
+      elt = xmalloc (sizeof (struct table_elt));
+    }
+
+  elt->exp = x;
+  elt->canon_exp = NULL_RTX;
+  elt->cost = COST (x);
+  elt->regcost = approx_reg_cost (x);
+  elt->next_same_value = 0;
+  elt->prev_same_value = 0;
+  elt->next_same_hash = table[hash];
+  elt->prev_same_hash = 0;
+  elt->related_value = 0;
+  elt->in_memory = 0;
+  elt->mode = mode;
+  elt->is_const = (CONSTANT_P (x)
+		   /* GNU C++ takes advantage of this for `this'
+		      (and other const values).  */
+		   || (GET_CODE (x) == REG
+		       && RTX_UNCHANGING_P (x)
+		       && REGNO (x) >= FIRST_PSEUDO_REGISTER)
+		   || fixed_base_plus_p (x));
+
+  if (table[hash])
+    table[hash]->prev_same_hash = elt;
+  table[hash] = elt;
+
+  /* Put it into the proper value-class.  */
+  if (classp)
+    {
+      classp = classp->first_same_value;
+      if (CHEAPER (elt, classp))
+	/* Insert at the head of the class.  */
+	{
+	  struct table_elt *p;
+	  elt->next_same_value = classp;
+	  classp->prev_same_value = elt;
+	  elt->first_same_value = elt;
+
+	  for (p = classp; p; p = p->next_same_value)
+	    p->first_same_value = elt;
+	}
+      else
+	{
+	  /* Insert not at head of the class.  */
+	  /* Put it after the last element cheaper than X.  */
+	  struct table_elt *p, *next;
+
+	  for (p = classp; (next = p->next_same_value) && CHEAPER (next, elt);
+	       p = next);
+
+	  /* Put it after P and before NEXT.  */
+	  elt->next_same_value = next;
+	  if (next)
+	    next->prev_same_value = elt;
+
+	  elt->prev_same_value = p;
+	  p->next_same_value = elt;
+	  elt->first_same_value = classp;
+	}
+    }
+  else
+    elt->first_same_value = elt;
+
+  /* If this is a constant being set equivalent to a register or a register
+     being set equivalent to a constant, note the constant equivalence.
+
+     If this is a constant, it cannot be equivalent to a different constant,
+     and a constant is the only thing that can be cheaper than a register.  So
+     we know the register is the head of the class (before the constant was
+     inserted).
+
+     If this is a register that is not already known equivalent to a
+     constant, we must check the entire class.
+
+     If this is a register that is already known equivalent to an insn,
+     update the qtys `const_insn' to show that `this_insn' is the latest
+     insn making that quantity equivalent to the constant.  */
+
+  if (elt->is_const && classp && GET_CODE (classp->exp) == REG
+      && GET_CODE (x) != REG)
+    {
+      int exp_q = REG_QTY (REGNO (classp->exp));
+      struct qty_table_elem *exp_ent = &qty_table[exp_q];
+
+      exp_ent->const_rtx = gen_lowpart_if_possible (exp_ent->mode, x);
+      exp_ent->const_insn = this_insn;
+    }
+
+  else if (GET_CODE (x) == REG
+	   && classp
+	   && ! qty_table[REG_QTY (REGNO (x))].const_rtx
+	   && ! elt->is_const)
+    {
+      struct table_elt *p;
+
+      for (p = classp; p != 0; p = p->next_same_value)
+	{
+	  if (p->is_const && GET_CODE (p->exp) != REG)
+	    {
+	      int x_q = REG_QTY (REGNO (x));
+	      struct qty_table_elem *x_ent = &qty_table[x_q];
+
+	      x_ent->const_rtx
+		= gen_lowpart_if_possible (GET_MODE (x), p->exp);
+	      x_ent->const_insn = this_insn;
+	      break;
+	    }
+	}
+    }
+
+  else if (GET_CODE (x) == REG
+	   && qty_table[REG_QTY (REGNO (x))].const_rtx
+	   && GET_MODE (x) == qty_table[REG_QTY (REGNO (x))].mode)
+    qty_table[REG_QTY (REGNO (x))].const_insn = this_insn;
+
+  /* If this is a constant with symbolic value,
+     and it has a term with an explicit integer value,
+     link it up with related expressions.  */
+  if (GET_CODE (x) == CONST)
+    {
+      rtx subexp = get_related_value (x);
+      unsigned subhash;
+      struct table_elt *subelt, *subelt_prev;
+
+      if (subexp != 0)
+	{
+	  /* Get the integer-free subexpression in the hash table.  */
+	  subhash = safe_hash (subexp, mode) & HASH_MASK;
+	  subelt = lookup (subexp, subhash, mode);
+	  if (subelt == 0)
+	    subelt = insert (subexp, NULL, subhash, mode);
+	  /* Initialize SUBELT's circular chain if it has none.  */
+	  if (subelt->related_value == 0)
+	    subelt->related_value = subelt;
+	  /* Find the element in the circular chain that precedes SUBELT.  */
+	  subelt_prev = subelt;
+	  while (subelt_prev->related_value != subelt)
+	    subelt_prev = subelt_prev->related_value;
+	  /* Put new ELT into SUBELT's circular chain just before SUBELT.
+	     This way the element that follows SUBELT is the oldest one.  */
+	  elt->related_value = subelt_prev->related_value;
+	  subelt_prev->related_value = elt;
+	}
+    }
+
+  return elt;
+}
+
+/* Given two equivalence classes, CLASS1 and CLASS2, put all the entries from
+   CLASS2 into CLASS1.  This is done when we have reached an insn which makes
+   the two classes equivalent.
+
+   CLASS1 will be the surviving class; CLASS2 should not be used after this
+   call.
+
+   Any invalid entries in CLASS2 will not be copied.  */
+
+static void
+merge_equiv_classes (struct table_elt *class1, struct table_elt *class2)
+{
+  struct table_elt *elt, *next, *new;
+
+  /* Ensure we start with the head of the classes.  */
+  class1 = class1->first_same_value;
+  class2 = class2->first_same_value;
+
+  /* If they were already equal, forget it.  */
+  if (class1 == class2)
+    return;
+
+  for (elt = class2; elt; elt = next)
+    {
+      unsigned int hash;
+      rtx exp = elt->exp;
+      enum machine_mode mode = elt->mode;
+
+      next = elt->next_same_value;
+
+      /* Remove old entry, make a new one in CLASS1's class.
+	 Don't do this for invalid entries as we cannot find their
+	 hash code (it also isn't necessary).  */
+      if (GET_CODE (exp) == REG || exp_equiv_p (exp, exp, 1, 0))
+	{
+	  bool need_rehash = false;
+
+	  hash_arg_in_memory = 0;
+	  hash = HASH (exp, mode);
+
+	  if (GET_CODE (exp) == REG)
+	    {
+	      need_rehash = REGNO_QTY_VALID_P (REGNO (exp));
+	      delete_reg_equiv (REGNO (exp));
+	    }
+
+	  remove_from_table (elt, hash);
+
+	  if (insert_regs (exp, class1, 0) || need_rehash)
+	    {
+	      rehash_using_reg (exp);
+	      hash = HASH (exp, mode);
+	    }
+	  new = insert (exp, class1, hash, mode);
+	  new->in_memory = hash_arg_in_memory;
+	}
+    }
+}
+
+/* Flush the entire hash table.  */
+
+static void
+flush_hash_table (void)
+{
+  int i;
+  struct table_elt *p;
+
+  for (i = 0; i < HASH_SIZE; i++)
+    for (p = table[i]; p; p = table[i])
+      {
+	/* Note that invalidate can remove elements
+	   after P in the current hash chain.  */
+	if (GET_CODE (p->exp) == REG)
+	  invalidate (p->exp, p->mode);
+	else
+	  remove_from_table (p, i);
+      }
+}
+
+/* Function called for each rtx to check whether true dependence exist.  */
+struct check_dependence_data
+{
+  enum machine_mode mode;
+  rtx exp;
+  rtx addr;
+};
+
+static int
+check_dependence (rtx *x, void *data)
+{
+  struct check_dependence_data *d = (struct check_dependence_data *) data;
+  if (*x && GET_CODE (*x) == MEM)
+    return canon_true_dependence (d->exp, d->mode, d->addr, *x,
+		    		  cse_rtx_varies_p);
+  else
+    return 0;
+}
+
+/* Remove from the hash table, or mark as invalid, all expressions whose
+   values could be altered by storing in X.  X is a register, a subreg, or
+   a memory reference with nonvarying address (because, when a memory
+   reference with a varying address is stored in, all memory references are
+   removed by invalidate_memory so specific invalidation is superfluous).
+   FULL_MODE, if not VOIDmode, indicates that this much should be
+   invalidated instead of just the amount indicated by the mode of X.  This
+   is only used for bitfield stores into memory.
+
+   A nonvarying address may be just a register or just a symbol reference,
+   or it may be either of those plus a numeric offset.  */
+
+static void
+invalidate (rtx x, enum machine_mode full_mode)
+{
+  int i;
+  struct table_elt *p;
+  rtx addr;
+
+  switch (GET_CODE (x))
+    {
+    case REG:
+      {
+	/* If X is a register, dependencies on its contents are recorded
+	   through the qty number mechanism.  Just change the qty number of
+	   the register, mark it as invalid for expressions that refer to it,
+	   and remove it itself.  */
+	unsigned int regno = REGNO (x);
+	unsigned int hash = HASH (x, GET_MODE (x));
+
+	/* Remove REGNO from any quantity list it might be on and indicate
+	   that its value might have changed.  If it is a pseudo, remove its
+	   entry from the hash table.
+
+	   For a hard register, we do the first two actions above for any
+	   additional hard registers corresponding to X.  Then, if any of these
+	   registers are in the table, we must remove any REG entries that
+	   overlap these registers.  */
+
+	delete_reg_equiv (regno);
+	REG_TICK (regno)++;
+	SUBREG_TICKED (regno) = -1;
+
+	if (regno >= FIRST_PSEUDO_REGISTER)
+	  {
+	    /* Because a register can be referenced in more than one mode,
+	       we might have to remove more than one table entry.  */
+	    struct table_elt *elt;
+
+	    while ((elt = lookup_for_remove (x, hash, GET_MODE (x))))
+	      remove_from_table (elt, hash);
+	  }
+	else
+	  {
+	    HOST_WIDE_INT in_table
+	      = TEST_HARD_REG_BIT (hard_regs_in_table, regno);
+	    unsigned int endregno
+	      = regno + HARD_REGNO_NREGS (regno, GET_MODE (x));
+	    unsigned int tregno, tendregno, rn;
+	    struct table_elt *p, *next;
+
+	    CLEAR_HARD_REG_BIT (hard_regs_in_table, regno);
+
+	    for (rn = regno + 1; rn < endregno; rn++)
+	      {
+		in_table |= TEST_HARD_REG_BIT (hard_regs_in_table, rn);
+		CLEAR_HARD_REG_BIT (hard_regs_in_table, rn);
+		delete_reg_equiv (rn);
+		REG_TICK (rn)++;
+		SUBREG_TICKED (rn) = -1;
+	      }
+
+	    if (in_table)
+	      for (hash = 0; hash < HASH_SIZE; hash++)
+		for (p = table[hash]; p; p = next)
+		  {
+		    next = p->next_same_hash;
+
+		    if (GET_CODE (p->exp) != REG
+			|| REGNO (p->exp) >= FIRST_PSEUDO_REGISTER)
+		      continue;
+
+		    tregno = REGNO (p->exp);
+		    tendregno
+		      = tregno + HARD_REGNO_NREGS (tregno, GET_MODE (p->exp));
+		    if (tendregno > regno && tregno < endregno)
+		      remove_from_table (p, hash);
+		  }
+	  }
+      }
+      return;
+
+    case SUBREG:
+      invalidate (SUBREG_REG (x), VOIDmode);
+      return;
+
+    case PARALLEL:
+      for (i = XVECLEN (x, 0) - 1; i >= 0; --i)
+	invalidate (XVECEXP (x, 0, i), VOIDmode);
+      return;
+
+    case EXPR_LIST:
+      /* This is part of a disjoint return value; extract the location in
+	 question ignoring the offset.  */
+      invalidate (XEXP (x, 0), VOIDmode);
+      return;
+
+    case MEM:
+      addr = canon_rtx (get_addr (XEXP (x, 0)));
+      /* Calculate the canonical version of X here so that
+	 true_dependence doesn't generate new RTL for X on each call.  */
+      x = canon_rtx (x);
+
+      /* Remove all hash table elements that refer to overlapping pieces of
+	 memory.  */
+      if (full_mode == VOIDmode)
+	full_mode = GET_MODE (x);
+
+      for (i = 0; i < HASH_SIZE; i++)
+	{
+	  struct table_elt *next;
+
+	  for (p = table[i]; p; p = next)
+	    {
+	      next = p->next_same_hash;
+	      if (p->in_memory)
+		{
+		  struct check_dependence_data d;
+
+		  /* Just canonicalize the expression once;
+		     otherwise each time we call invalidate
+		     true_dependence will canonicalize the
+		     expression again.  */
+		  if (!p->canon_exp)
+		    p->canon_exp = canon_rtx (p->exp);
+		  d.exp = x;
+		  d.addr = addr;
+		  d.mode = full_mode;
+		  if (for_each_rtx (&p->canon_exp, check_dependence, &d))
+		    remove_from_table (p, i);
+		}
+	    }
+	}
+      return;
+
+    default:
+      abort ();
+    }
+}
+
+/* Remove all expressions that refer to register REGNO,
+   since they are already invalid, and we are about to
+   mark that register valid again and don't want the old
+   expressions to reappear as valid.  */
+
+static void
+remove_invalid_refs (unsigned int regno)
+{
+  unsigned int i;
+  struct table_elt *p, *next;
+
+  for (i = 0; i < HASH_SIZE; i++)
+    for (p = table[i]; p; p = next)
+      {
+	next = p->next_same_hash;
+	if (GET_CODE (p->exp) != REG
+	    && refers_to_regno_p (regno, regno + 1, p->exp, (rtx *) 0))
+	  remove_from_table (p, i);
+      }
+}
+
+/* Likewise for a subreg with subreg_reg REGNO, subreg_byte OFFSET,
+   and mode MODE.  */
+static void
+remove_invalid_subreg_refs (unsigned int regno, unsigned int offset,
+			    enum machine_mode mode)
+{
+  unsigned int i;
+  struct table_elt *p, *next;
+  unsigned int end = offset + (GET_MODE_SIZE (mode) - 1);
+
+  for (i = 0; i < HASH_SIZE; i++)
+    for (p = table[i]; p; p = next)
+      {
+	rtx exp = p->exp;
+	next = p->next_same_hash;
+
+	if (GET_CODE (exp) != REG
+	    && (GET_CODE (exp) != SUBREG
+		|| GET_CODE (SUBREG_REG (exp)) != REG
+		|| REGNO (SUBREG_REG (exp)) != regno
+		|| (((SUBREG_BYTE (exp)
+		      + (GET_MODE_SIZE (GET_MODE (exp)) - 1)) >= offset)
+		    && SUBREG_BYTE (exp) <= end))
+	    && refers_to_regno_p (regno, regno + 1, p->exp, (rtx *) 0))
+	  remove_from_table (p, i);
+      }
+}
+
+/* Recompute the hash codes of any valid entries in the hash table that
+   reference X, if X is a register, or SUBREG_REG (X) if X is a SUBREG.
+
+   This is called when we make a jump equivalence.  */
+
+static void
+rehash_using_reg (rtx x)
+{
+  unsigned int i;
+  struct table_elt *p, *next;
+  unsigned hash;
+
+  if (GET_CODE (x) == SUBREG)
+    x = SUBREG_REG (x);
+
+  /* If X is not a register or if the register is known not to be in any
+     valid entries in the table, we have no work to do.  */
+
+  if (GET_CODE (x) != REG
+      || REG_IN_TABLE (REGNO (x)) < 0
+      || REG_IN_TABLE (REGNO (x)) != REG_TICK (REGNO (x)))
+    return;
+
+  /* Scan all hash chains looking for valid entries that mention X.
+     If we find one and it is in the wrong hash chain, move it.  */
+
+  for (i = 0; i < HASH_SIZE; i++)
+    for (p = table[i]; p; p = next)
+      {
+	next = p->next_same_hash;
+	if (reg_mentioned_p (x, p->exp)
+	    && exp_equiv_p (p->exp, p->exp, 1, 0)
+	    && i != (hash = safe_hash (p->exp, p->mode) & HASH_MASK))
+	  {
+	    if (p->next_same_hash)
+	      p->next_same_hash->prev_same_hash = p->prev_same_hash;
+
+	    if (p->prev_same_hash)
+	      p->prev_same_hash->next_same_hash = p->next_same_hash;
+	    else
+	      table[i] = p->next_same_hash;
+
+	    p->next_same_hash = table[hash];
+	    p->prev_same_hash = 0;
+	    if (table[hash])
+	      table[hash]->prev_same_hash = p;
+	    table[hash] = p;
+	  }
+      }
+}
+
+/* Remove from the hash table any expression that is a call-clobbered
+   register.  Also update their TICK values.  */
+
+static void
+invalidate_for_call (void)
+{
+  unsigned int regno, endregno;
+  unsigned int i;
+  unsigned hash;
+  struct table_elt *p, *next;
+  int in_table = 0;
+
+  /* Go through all the hard registers.  For each that is clobbered in
+     a CALL_INSN, remove the register from quantity chains and update
+     reg_tick if defined.  Also see if any of these registers is currently
+     in the table.  */
+
+  for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)
+    if (TEST_HARD_REG_BIT (regs_invalidated_by_call, regno))
+      {
+	delete_reg_equiv (regno);
+	if (REG_TICK (regno) >= 0)
+	  {
+	    REG_TICK (regno)++;
+	    SUBREG_TICKED (regno) = -1;
+	  }
+
+	in_table |= (TEST_HARD_REG_BIT (hard_regs_in_table, regno) != 0);
+      }
+
+  /* In the case where we have no call-clobbered hard registers in the
+     table, we are done.  Otherwise, scan the table and remove any
+     entry that overlaps a call-clobbered register.  */
+
+  if (in_table)
+    for (hash = 0; hash < HASH_SIZE; hash++)
+      for (p = table[hash]; p; p = next)
+	{
+	  next = p->next_same_hash;
+
+	  if (GET_CODE (p->exp) != REG
+	      || REGNO (p->exp) >= FIRST_PSEUDO_REGISTER)
+	    continue;
+
+	  regno = REGNO (p->exp);
+	  endregno = regno + HARD_REGNO_NREGS (regno, GET_MODE (p->exp));
+
+	  for (i = regno; i < endregno; i++)
+	    if (TEST_HARD_REG_BIT (regs_invalidated_by_call, i))
+	      {
+		remove_from_table (p, hash);
+		break;
+	      }
+	}
+}
+
+/* Given an expression X of type CONST,
+   and ELT which is its table entry (or 0 if it
+   is not in the hash table),
+   return an alternate expression for X as a register plus integer.
+   If none can be found, return 0.  */
+
+static rtx
+use_related_value (rtx x, struct table_elt *elt)
+{
+  struct table_elt *relt = 0;
+  struct table_elt *p, *q;
+  HOST_WIDE_INT offset;
+
+  /* First, is there anything related known?
+     If we have a table element, we can tell from that.
+     Otherwise, must look it up.  */
+
+  if (elt != 0 && elt->related_value != 0)
+    relt = elt;
+  else if (elt == 0 && GET_CODE (x) == CONST)
+    {
+      rtx subexp = get_related_value (x);
+      if (subexp != 0)
+	relt = lookup (subexp,
+		       safe_hash (subexp, GET_MODE (subexp)) & HASH_MASK,
+		       GET_MODE (subexp));
+    }
+
+  if (relt == 0)
+    return 0;
+
+  /* Search all related table entries for one that has an
+     equivalent register.  */
+
+  p = relt;
+  while (1)
+    {
+      /* This loop is strange in that it is executed in two different cases.
+	 The first is when X is already in the table.  Then it is searching
+	 the RELATED_VALUE list of X's class (RELT).  The second case is when
+	 X is not in the table.  Then RELT points to a class for the related
+	 value.
+
+	 Ensure that, whatever case we are in, that we ignore classes that have
+	 the same value as X.  */
+
+      if (rtx_equal_p (x, p->exp))
+	q = 0;
+      else
+	for (q = p->first_same_value; q; q = q->next_same_value)
+	  if (GET_CODE (q->exp) == REG)
+	    break;
+
+      if (q)
+	break;
+
+      p = p->related_value;
+
+      /* We went all the way around, so there is nothing to be found.
+	 Alternatively, perhaps RELT was in the table for some other reason
+	 and it has no related values recorded.  */
+      if (p == relt || p == 0)
+	break;
+    }
+
+  if (q == 0)
+    return 0;
+
+  offset = (get_integer_term (x) - get_integer_term (p->exp));
+  /* Note: OFFSET may be 0 if P->xexp and X are related by commutativity.  */
+  return plus_constant (q->exp, offset);
+}
+
+/* Hash a string.  Just add its bytes up.  */
+static inline unsigned
+canon_hash_string (const char *ps)
+{
+  unsigned hash = 0;
+  const unsigned char *p = (const unsigned char *) ps;
+
+  if (p)
+    while (*p)
+      hash += *p++;
+
+  return hash;
+}
+
+/* Hash an rtx.  We are careful to make sure the value is never negative.
+   Equivalent registers hash identically.
+   MODE is used in hashing for CONST_INTs only;
+   otherwise the mode of X is used.
+
+   Store 1 in do_not_record if any subexpression is volatile.
+
+   Store 1 in hash_arg_in_memory if X contains a MEM rtx
+   which does not have the RTX_UNCHANGING_P bit set.
+
+   Note that cse_insn knows that the hash code of a MEM expression
+   is just (int) MEM plus the hash code of the address.  */
+
+static unsigned
+canon_hash (rtx x, enum machine_mode mode)
+{
+  int i, j;
+  unsigned hash = 0;
+  enum rtx_code code;
+  const char *fmt;
+
+  /* repeat is used to turn tail-recursion into iteration.  */
+ repeat:
+  if (x == 0)
+    return hash;
+
+  code = GET_CODE (x);
+  switch (code)
+    {
+    case REG:
+      {
+	unsigned int regno = REGNO (x);
+	bool record;
+
+	/* On some machines, we can't record any non-fixed hard register,
+	   because extending its life will cause reload problems.  We
+	   consider ap, fp, sp, gp to be fixed for this purpose.
+
+	   We also consider CCmode registers to be fixed for this purpose;
+	   failure to do so leads to failure to simplify 0<100 type of
+	   conditionals.
+
+	   On all machines, we can't record any global registers.
+	   Nor should we record any register that is in a small
+	   class, as defined by CLASS_LIKELY_SPILLED_P.  */
+
+	if (regno >= FIRST_PSEUDO_REGISTER)
+	  record = true;
+	else if (x == frame_pointer_rtx
+		 || x == hard_frame_pointer_rtx
+		 || x == arg_pointer_rtx
+		 || x == stack_pointer_rtx
+		 || x == pic_offset_table_rtx)
+	  record = true;
+	else if (global_regs[regno])
+	  record = false;
+	else if (fixed_regs[regno])
+	  record = true;
+	else if (GET_MODE_CLASS (GET_MODE (x)) == MODE_CC)
+	  record = true;
+	else if (SMALL_REGISTER_CLASSES)
+	  record = false;
+	else if (CLASS_LIKELY_SPILLED_P (REGNO_REG_CLASS (regno)))
+	  record = false;
+	else
+	  record = true;
+
+	if (!record)
+	  {
+	    do_not_record = 1;
+	    return 0;
+	  }
+
+	hash += ((unsigned) REG << 7) + (unsigned) REG_QTY (regno);
+	return hash;
+      }
+
+    /* We handle SUBREG of a REG specially because the underlying
+       reg changes its hash value with every value change; we don't
+       want to have to forget unrelated subregs when one subreg changes.  */
+    case SUBREG:
+      {
+	if (GET_CODE (SUBREG_REG (x)) == REG)
+	  {
+	    hash += (((unsigned) SUBREG << 7)
+		     + REGNO (SUBREG_REG (x))
+		     + (SUBREG_BYTE (x) / UNITS_PER_WORD));
+	    return hash;
+	  }
+	break;
+      }
+
+    case CONST_INT:
+      {
+	unsigned HOST_WIDE_INT tem = INTVAL (x);
+	hash += ((unsigned) CONST_INT << 7) + (unsigned) mode + tem;
+	return hash;
+      }
+
+    case CONST_DOUBLE:
+      /* This is like the general case, except that it only counts
+	 the integers representing the constant.  */
+      hash += (unsigned) code + (unsigned) GET_MODE (x);
+      if (GET_MODE (x) != VOIDmode)
+	hash += real_hash (CONST_DOUBLE_REAL_VALUE (x));
+      else
+	hash += ((unsigned) CONST_DOUBLE_LOW (x)
+		 + (unsigned) CONST_DOUBLE_HIGH (x));
+      return hash;
+
+    case CONST_VECTOR:
+      {
+	int units;
+	rtx elt;
+
+	units = CONST_VECTOR_NUNITS (x);
+
+	for (i = 0; i < units; ++i)
+	  {
+	    elt = CONST_VECTOR_ELT (x, i);
+	    hash += canon_hash (elt, GET_MODE (elt));
+	  }
+
+	return hash;
+      }
+
+      /* Assume there is only one rtx object for any given label.  */
+    case LABEL_REF:
+      hash += ((unsigned) LABEL_REF << 7) + (unsigned long) XEXP (x, 0);
+      return hash;
+
+    case SYMBOL_REF:
+      hash += ((unsigned) SYMBOL_REF << 7) + (unsigned long) XSTR (x, 0);
+      return hash;
+
+    case MEM:
+      /* We don't record if marked volatile or if BLKmode since we don't
+	 know the size of the move.  */
+      if (MEM_VOLATILE_P (x) || GET_MODE (x) == BLKmode)
+	{
+	  do_not_record = 1;
+	  return 0;
+	}
+      if (! RTX_UNCHANGING_P (x) || fixed_base_plus_p (XEXP (x, 0)))
+	hash_arg_in_memory = 1;
+
+      /* Now that we have already found this special case,
+	 might as well speed it up as much as possible.  */
+      hash += (unsigned) MEM;
+      x = XEXP (x, 0);
+      goto repeat;
+
+    case USE:
+      /* A USE that mentions non-volatile memory needs special
+	 handling since the MEM may be BLKmode which normally
+	 prevents an entry from being made.  Pure calls are
+	 marked by a USE which mentions BLKmode memory.  */
+      if (GET_CODE (XEXP (x, 0)) == MEM
+	  && ! MEM_VOLATILE_P (XEXP (x, 0)))
+	{
+	  hash += (unsigned) USE;
+	  x = XEXP (x, 0);
+
+	  if (! RTX_UNCHANGING_P (x) || fixed_base_plus_p (XEXP (x, 0)))
+	    hash_arg_in_memory = 1;
+
+	  /* Now that we have already found this special case,
+	     might as well speed it up as much as possible.  */
+	  hash += (unsigned) MEM;
+	  x = XEXP (x, 0);
+	  goto repeat;
+	}
+      break;
+
+    case PRE_DEC:
+    case PRE_INC:
+    case POST_DEC:
+    case POST_INC:
+    case PRE_MODIFY:
+    case POST_MODIFY:
+    case PC:
+    case CC0:
+    case CALL:
+    case UNSPEC_VOLATILE:
+      do_not_record = 1;
+      return 0;
+
+    case ASM_OPERANDS:
+      if (MEM_VOLATILE_P (x))
+	{
+	  do_not_record = 1;
+	  return 0;
+	}
+      else
+	{
+	  /* We don't want to take the filename and line into account.  */
+	  hash += (unsigned) code + (unsigned) GET_MODE (x)
+	    + canon_hash_string (ASM_OPERANDS_TEMPLATE (x))
+	    + canon_hash_string (ASM_OPERANDS_OUTPUT_CONSTRAINT (x))
+	    + (unsigned) ASM_OPERANDS_OUTPUT_IDX (x);
+
+	  if (ASM_OPERANDS_INPUT_LENGTH (x))
+	    {
+	      for (i = 1; i < ASM_OPERANDS_INPUT_LENGTH (x); i++)
+		{
+		  hash += (canon_hash (ASM_OPERANDS_INPUT (x, i),
+				       GET_MODE (ASM_OPERANDS_INPUT (x, i)))
+			   + canon_hash_string (ASM_OPERANDS_INPUT_CONSTRAINT
+						(x, i)));
+		}
+
+	      hash += canon_hash_string (ASM_OPERANDS_INPUT_CONSTRAINT (x, 0));
+	      x = ASM_OPERANDS_INPUT (x, 0);
+	      mode = GET_MODE (x);
+	      goto repeat;
+	    }
+
+	  return hash;
+	}
+      break;
+
+    default:
+      break;
+    }
+
+  i = GET_RTX_LENGTH (code) - 1;
+  hash += (unsigned) code + (unsigned) GET_MODE (x);
+  fmt = GET_RTX_FORMAT (code);
+  for (; i >= 0; i--)
+    {
+      if (fmt[i] == 'e')
+	{
+	  rtx tem = XEXP (x, i);
+
+	  /* If we are about to do the last recursive call
+	     needed at this level, change it into iteration.
+	     This function  is called enough to be worth it.  */
+	  if (i == 0)
+	    {
+	      x = tem;
+	      goto repeat;
+	    }
+	  hash += canon_hash (tem, 0);
+	}
+      else if (fmt[i] == 'E')
+	for (j = 0; j < XVECLEN (x, i); j++)
+	  hash += canon_hash (XVECEXP (x, i, j), 0);
+      else if (fmt[i] == 's')
+	hash += canon_hash_string (XSTR (x, i));
+      else if (fmt[i] == 'i')
+	{
+	  unsigned tem = XINT (x, i);
+	  hash += tem;
+	}
+      else if (fmt[i] == '0' || fmt[i] == 't')
+	/* Unused.  */
+	;
+      else
+	abort ();
+    }
+  return hash;
+}
+
+/* Like canon_hash but with no side effects.  */
+
+static unsigned
+safe_hash (rtx x, enum machine_mode mode)
+{
+  int save_do_not_record = do_not_record;
+  int save_hash_arg_in_memory = hash_arg_in_memory;
+  unsigned hash = canon_hash (x, mode);
+  hash_arg_in_memory = save_hash_arg_in_memory;
+  do_not_record = save_do_not_record;
+  return hash;
+}
+
+/* Return 1 iff X and Y would canonicalize into the same thing,
+   without actually constructing the canonicalization of either one.
+   If VALIDATE is nonzero,
+   we assume X is an expression being processed from the rtl
+   and Y was found in the hash table.  We check register refs
+   in Y for being marked as valid.
+
+   If EQUAL_VALUES is nonzero, we allow a register to match a constant value
+   that is known to be in the register.  Ordinarily, we don't allow them
+   to match, because letting them match would cause unpredictable results
+   in all the places that search a hash table chain for an equivalent
+   for a given value.  A possible equivalent that has different structure
+   has its hash code computed from different data.  Whether the hash code
+   is the same as that of the given value is pure luck.  */
+
+static int
+exp_equiv_p (rtx x, rtx y, int validate, int equal_values)
+{
+  int i, j;
+  enum rtx_code code;
+  const char *fmt;
+
+  /* Note: it is incorrect to assume an expression is equivalent to itself
+     if VALIDATE is nonzero.  */
+  if (x == y && !validate)
+    return 1;
+  if (x == 0 || y == 0)
+    return x == y;
+
+  code = GET_CODE (x);
+  if (code != GET_CODE (y))
+    {
+      if (!equal_values)
+	return 0;
+
+      /* If X is a constant and Y is a register or vice versa, they may be
+	 equivalent.  We only have to validate if Y is a register.  */
+      if (CONSTANT_P (x) && GET_CODE (y) == REG
+	  && REGNO_QTY_VALID_P (REGNO (y)))
+	{
+	  int y_q = REG_QTY (REGNO (y));
+	  struct qty_table_elem *y_ent = &qty_table[y_q];
+
+	  if (GET_MODE (y) == y_ent->mode
+	      && rtx_equal_p (x, y_ent->const_rtx)
+	      && (! validate || REG_IN_TABLE (REGNO (y)) == REG_TICK (REGNO (y))))
+	    return 1;
+	}
+
+      if (CONSTANT_P (y) && code == REG
+	  && REGNO_QTY_VALID_P (REGNO (x)))
+	{
+	  int x_q = REG_QTY (REGNO (x));
+	  struct qty_table_elem *x_ent = &qty_table[x_q];
+
+	  if (GET_MODE (x) == x_ent->mode
+	      && rtx_equal_p (y, x_ent->const_rtx))
+	    return 1;
+	}
+
+      return 0;
+    }
+
+  /* (MULT:SI x y) and (MULT:HI x y) are NOT equivalent.  */
+  if (GET_MODE (x) != GET_MODE (y))
+    return 0;
+
+  switch (code)
+    {
+    case PC:
+    case CC0:
+    case CONST_INT:
+      return x == y;
+
+    case LABEL_REF:
+      return XEXP (x, 0) == XEXP (y, 0);
+
+    case SYMBOL_REF:
+      return XSTR (x, 0) == XSTR (y, 0);
+
+    case REG:
+      {
+	unsigned int regno = REGNO (y);
+	unsigned int endregno
+	  = regno + (regno >= FIRST_PSEUDO_REGISTER ? 1
+		     : HARD_REGNO_NREGS (regno, GET_MODE (y)));
+	unsigned int i;
+
+	/* If the quantities are not the same, the expressions are not
+	   equivalent.  If there are and we are not to validate, they
+	   are equivalent.  Otherwise, ensure all regs are up-to-date.  */
+
+	if (REG_QTY (REGNO (x)) != REG_QTY (regno))
+	  return 0;
+
+	if (! validate)
+	  return 1;
+
+	for (i = regno; i < endregno; i++)
+	  if (REG_IN_TABLE (i) != REG_TICK (i))
+	    return 0;
+
+	return 1;
+      }
+
+    /*  For commutative operations, check both orders.  */
+    case PLUS:
+    case MULT:
+    case AND:
+    case IOR:
+    case XOR:
+    case NE:
+    case EQ:
+      return ((exp_equiv_p (XEXP (x, 0), XEXP (y, 0), validate, equal_values)
+	       && exp_equiv_p (XEXP (x, 1), XEXP (y, 1),
+			       validate, equal_values))
+	      || (exp_equiv_p (XEXP (x, 0), XEXP (y, 1),
+			       validate, equal_values)
+		  && exp_equiv_p (XEXP (x, 1), XEXP (y, 0),
+				  validate, equal_values)));
+
+    case ASM_OPERANDS:
+      /* We don't use the generic code below because we want to
+	 disregard filename and line numbers.  */
+
+      /* A volatile asm isn't equivalent to any other.  */
+      if (MEM_VOLATILE_P (x) || MEM_VOLATILE_P (y))
+	return 0;
+
+      if (GET_MODE (x) != GET_MODE (y)
+	  || strcmp (ASM_OPERANDS_TEMPLATE (x), ASM_OPERANDS_TEMPLATE (y))
+	  || strcmp (ASM_OPERANDS_OUTPUT_CONSTRAINT (x),
+		     ASM_OPERANDS_OUTPUT_CONSTRAINT (y))
+	  || ASM_OPERANDS_OUTPUT_IDX (x) != ASM_OPERANDS_OUTPUT_IDX (y)
+	  || ASM_OPERANDS_INPUT_LENGTH (x) != ASM_OPERANDS_INPUT_LENGTH (y))
+	return 0;
+
+      if (ASM_OPERANDS_INPUT_LENGTH (x))
+	{
+	  for (i = ASM_OPERANDS_INPUT_LENGTH (x) - 1; i >= 0; i--)
+	    if (! exp_equiv_p (ASM_OPERANDS_INPUT (x, i),
+			       ASM_OPERANDS_INPUT (y, i),
+			       validate, equal_values)
+		|| strcmp (ASM_OPERANDS_INPUT_CONSTRAINT (x, i),
+			   ASM_OPERANDS_INPUT_CONSTRAINT (y, i)))
+	      return 0;
+	}
+
+      return 1;
+
+    default:
+      break;
+    }
+
+  /* Compare the elements.  If any pair of corresponding elements
+     fail to match, return 0 for the whole things.  */
+
+  fmt = GET_RTX_FORMAT (code);
+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)
+    {
+      switch (fmt[i])
+	{
+	case 'e':
+	  if (! exp_equiv_p (XEXP (x, i), XEXP (y, i), validate, equal_values))
+	    return 0;
+	  break;
+
+	case 'E':
+	  if (XVECLEN (x, i) != XVECLEN (y, i))
+	    return 0;
+	  for (j = 0; j < XVECLEN (x, i); j++)
+	    if (! exp_equiv_p (XVECEXP (x, i, j), XVECEXP (y, i, j),
+			       validate, equal_values))
+	      return 0;
+	  break;
+
+	case 's':
+	  if (strcmp (XSTR (x, i), XSTR (y, i)))
+	    return 0;
+	  break;
+
+	case 'i':
+	  if (XINT (x, i) != XINT (y, i))
+	    return 0;
+	  break;
+
+	case 'w':
+	  if (XWINT (x, i) != XWINT (y, i))
+	    return 0;
+	  break;
+
+	case '0':
+	case 't':
+	  break;
+
+	default:
+	  abort ();
+	}
+    }
+
+  return 1;
+}
+
+/* Return 1 if X has a value that can vary even between two
+   executions of the program.  0 means X can be compared reliably
+   against certain constants or near-constants.  */
+
+static int
+cse_rtx_varies_p (rtx x, int from_alias)
+{
+  /* We need not check for X and the equivalence class being of the same
+     mode because if X is equivalent to a constant in some mode, it
+     doesn't vary in any mode.  */
+
+  if (GET_CODE (x) == REG
+      && REGNO_QTY_VALID_P (REGNO (x)))
+    {
+      int x_q = REG_QTY (REGNO (x));
+      struct qty_table_elem *x_ent = &qty_table[x_q];
+
+      if (GET_MODE (x) == x_ent->mode
+	  && x_ent->const_rtx != NULL_RTX)
+	return 0;
+    }
+
+  if (GET_CODE (x) == PLUS
+      && GET_CODE (XEXP (x, 1)) == CONST_INT
+      && GET_CODE (XEXP (x, 0)) == REG
+      && REGNO_QTY_VALID_P (REGNO (XEXP (x, 0))))
+    {
+      int x0_q = REG_QTY (REGNO (XEXP (x, 0)));
+      struct qty_table_elem *x0_ent = &qty_table[x0_q];
+
+      if ((GET_MODE (XEXP (x, 0)) == x0_ent->mode)
+	  && x0_ent->const_rtx != NULL_RTX)
+	return 0;
+    }
+
+  /* This can happen as the result of virtual register instantiation, if
+     the initial constant is too large to be a valid address.  This gives
+     us a three instruction sequence, load large offset into a register,
+     load fp minus a constant into a register, then a MEM which is the
+     sum of the two `constant' registers.  */
+  if (GET_CODE (x) == PLUS
+      && GET_CODE (XEXP (x, 0)) == REG
+      && GET_CODE (XEXP (x, 1)) == REG
+      && REGNO_QTY_VALID_P (REGNO (XEXP (x, 0)))
+      && REGNO_QTY_VALID_P (REGNO (XEXP (x, 1))))
+    {
+      int x0_q = REG_QTY (REGNO (XEXP (x, 0)));
+      int x1_q = REG_QTY (REGNO (XEXP (x, 1)));
+      struct qty_table_elem *x0_ent = &qty_table[x0_q];
+      struct qty_table_elem *x1_ent = &qty_table[x1_q];
+
+      if ((GET_MODE (XEXP (x, 0)) == x0_ent->mode)
+	  && x0_ent->const_rtx != NULL_RTX
+	  && (GET_MODE (XEXP (x, 1)) == x1_ent->mode)
+	  && x1_ent->const_rtx != NULL_RTX)
+	return 0;
+    }
+
+  return rtx_varies_p (x, from_alias);
+}
+
+/* Canonicalize an expression:
+   replace each register reference inside it
+   with the "oldest" equivalent register.
+
+   If INSN is nonzero and we are replacing a pseudo with a hard register
+   or vice versa, validate_change is used to ensure that INSN remains valid
+   after we make our substitution.  The calls are made with IN_GROUP nonzero
+   so apply_change_group must be called upon the outermost return from this
+   function (unless INSN is zero).  The result of apply_change_group can
+   generally be discarded since the changes we are making are optional.  */
+
+static rtx
+canon_reg (rtx x, rtx insn)
+{
+  int i;
+  enum rtx_code code;
+  const char *fmt;
+
+  if (x == 0)
+    return x;
+
+  code = GET_CODE (x);
+  switch (code)
+    {
+    case PC:
+    case CC0:
+    case CONST:
+    case CONST_INT:
+    case CONST_DOUBLE:
+    case CONST_VECTOR:
+    case SYMBOL_REF:
+    case LABEL_REF:
+    case ADDR_VEC:
+    case ADDR_DIFF_VEC:
+      return x;
+
+    case REG:
+      {
+	int first;
+	int q;
+	struct qty_table_elem *ent;
+
+	/* Never replace a hard reg, because hard regs can appear
+	   in more than one machine mode, and we must preserve the mode
+	   of each occurrence.  Also, some hard regs appear in
+	   MEMs that are shared and mustn't be altered.  Don't try to
+	   replace any reg that maps to a reg of class NO_REGS.  */
+	if (REGNO (x) < FIRST_PSEUDO_REGISTER
+	    || ! REGNO_QTY_VALID_P (REGNO (x)))
+	  return x;
+
+	q = REG_QTY (REGNO (x));
+	ent = &qty_table[q];
+	first = ent->first_reg;
+	return (first >= FIRST_PSEUDO_REGISTER ? regno_reg_rtx[first]
+		: REGNO_REG_CLASS (first) == NO_REGS ? x
+		: gen_rtx_REG (ent->mode, first));
+      }
+
+    default:
+      break;
+    }
+
+  fmt = GET_RTX_FORMAT (code);
+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)
+    {
+      int j;
+
+      if (fmt[i] == 'e')
+	{
+	  rtx new = canon_reg (XEXP (x, i), insn);
+	  int insn_code;
+
+	  /* If replacing pseudo with hard reg or vice versa, ensure the
+	     insn remains valid.  Likewise if the insn has MATCH_DUPs.  */
+	  if (insn != 0 && new != 0
+	      && GET_CODE (new) == REG && GET_CODE (XEXP (x, i)) == REG
+	      && (((REGNO (new) < FIRST_PSEUDO_REGISTER)
+		   != (REGNO (XEXP (x, i)) < FIRST_PSEUDO_REGISTER))
+		  || (insn_code = recog_memoized (insn)) < 0
+		  || insn_data[insn_code].n_dups > 0))
+	    validate_change (insn, &XEXP (x, i), new, 1);
+	  else
+	    XEXP (x, i) = new;
+	}
+      else if (fmt[i] == 'E')
+	for (j = 0; j < XVECLEN (x, i); j++)
+	  XVECEXP (x, i, j) = canon_reg (XVECEXP (x, i, j), insn);
+    }
+
+  return x;
+}
+
+/* LOC is a location within INSN that is an operand address (the contents of
+   a MEM).  Find the best equivalent address to use that is valid for this
+   insn.
+
+   On most CISC machines, complicated address modes are costly, and rtx_cost
+   is a good approximation for that cost.  However, most RISC machines have
+   only a few (usually only one) memory reference formats.  If an address is
+   valid at all, it is often just as cheap as any other address.  Hence, for
+   RISC machines, we use `address_cost' to compare the costs of various
+   addresses.  For two addresses of equal cost, choose the one with the
+   highest `rtx_cost' value as that has the potential of eliminating the
+   most insns.  For equal costs, we choose the first in the equivalence
+   class.  Note that we ignore the fact that pseudo registers are cheaper than
+   hard registers here because we would also prefer the pseudo registers.  */
+
+static void
+find_best_addr (rtx insn, rtx *loc, enum machine_mode mode)
+{
+  struct table_elt *elt;
+  rtx addr = *loc;
+  struct table_elt *p;
+  int found_better = 1;
+  int save_do_not_record = do_not_record;
+  int save_hash_arg_in_memory = hash_arg_in_memory;
+  int addr_volatile;
+  int regno;
+  unsigned hash;
+
+  /* Do not try to replace constant addresses or addresses of local and
+     argument slots.  These MEM expressions are made only once and inserted
+     in many instructions, as well as being used to control symbol table
+     output.  It is not safe to clobber them.
+
+     There are some uncommon cases where the address is already in a register
+     for some reason, but we cannot take advantage of that because we have
+     no easy way to unshare the MEM.  In addition, looking up all stack
+     addresses is costly.  */
+  if ((GET_CODE (addr) == PLUS
+       && GET_CODE (XEXP (addr, 0)) == REG
+       && GET_CODE (XEXP (addr, 1)) == CONST_INT
+       && (regno = REGNO (XEXP (addr, 0)),
+	   regno == FRAME_POINTER_REGNUM || regno == HARD_FRAME_POINTER_REGNUM
+	   || regno == ARG_POINTER_REGNUM))
+      || (GET_CODE (addr) == REG
+	  && (regno = REGNO (addr), regno == FRAME_POINTER_REGNUM
+	      || regno == HARD_FRAME_POINTER_REGNUM
+	      || regno == ARG_POINTER_REGNUM))
+      || GET_CODE (addr) == ADDRESSOF
+      || CONSTANT_ADDRESS_P (addr))
+    return;
+
+  /* If this address is not simply a register, try to fold it.  This will
+     sometimes simplify the expression.  Many simplifications
+     will not be valid, but some, usually applying the associative rule, will
+     be valid and produce better code.  */
+  if (GET_CODE (addr) != REG)
+    {
+      rtx folded = fold_rtx (copy_rtx (addr), NULL_RTX);
+      int addr_folded_cost = address_cost (folded, mode);
+      int addr_cost = address_cost (addr, mode);
+
+      if ((addr_folded_cost < addr_cost
+	   || (addr_folded_cost == addr_cost
+	       /* ??? The rtx_cost comparison is left over from an older
+		  version of this code.  It is probably no longer helpful.  */
+	       && (rtx_cost (folded, MEM) > rtx_cost (addr, MEM)
+		   || approx_reg_cost (folded) < approx_reg_cost (addr))))
+	  && validate_change (insn, loc, folded, 0))
+	addr = folded;
+    }
+
+  /* If this address is not in the hash table, we can't look for equivalences
+     of the whole address.  Also, ignore if volatile.  */
+
+  do_not_record = 0;
+  hash = HASH (addr, Pmode);
+  addr_volatile = do_not_record;
+  do_not_record = save_do_not_record;
+  hash_arg_in_memory = save_hash_arg_in_memory;
+
+  if (addr_volatile)
+    return;
+
+  elt = lookup (addr, hash, Pmode);
+
+  if (elt)
+    {
+      /* We need to find the best (under the criteria documented above) entry
+	 in the class that is valid.  We use the `flag' field to indicate
+	 choices that were invalid and iterate until we can't find a better
+	 one that hasn't already been tried.  */
+
+      for (p = elt->first_same_value; p; p = p->next_same_value)
+	p->flag = 0;
+
+      while (found_better)
+	{
+	  int best_addr_cost = address_cost (*loc, mode);
+	  int best_rtx_cost = (elt->cost + 1) >> 1;
+	  int exp_cost;
+	  struct table_elt *best_elt = elt;
+
+	  found_better = 0;
+	  for (p = elt->first_same_value; p; p = p->next_same_value)
+	    if (! p->flag)
+	      {
+		if ((GET_CODE (p->exp) == REG
+		     || exp_equiv_p (p->exp, p->exp, 1, 0))
+		    && ((exp_cost = address_cost (p->exp, mode)) < best_addr_cost
+			|| (exp_cost == best_addr_cost
+			    && ((p->cost + 1) >> 1) > best_rtx_cost)))
+		  {
+		    found_better = 1;
+		    best_addr_cost = exp_cost;
+		    best_rtx_cost = (p->cost + 1) >> 1;
+		    best_elt = p;
+		  }
+	      }
+
+	  if (found_better)
+	    {
+	      if (validate_change (insn, loc,
+				   canon_reg (copy_rtx (best_elt->exp),
+					      NULL_RTX), 0))
+		return;
+	      else
+		best_elt->flag = 1;
+	    }
+	}
+    }
+
+  /* If the address is a binary operation with the first operand a register
+     and the second a constant, do the same as above, but looking for
+     equivalences of the register.  Then try to simplify before checking for
+     the best address to use.  This catches a few cases:  First is when we
+     have REG+const and the register is another REG+const.  We can often merge
+     the constants and eliminate one insn and one register.  It may also be
+     that a machine has a cheap REG+REG+const.  Finally, this improves the
+     code on the Alpha for unaligned byte stores.  */
+
+  if (flag_expensive_optimizations
+      && (GET_RTX_CLASS (GET_CODE (*loc)) == '2'
+	  || GET_RTX_CLASS (GET_CODE (*loc)) == 'c')
+      && GET_CODE (XEXP (*loc, 0)) == REG)
+    {
+      rtx op1 = XEXP (*loc, 1);
+
+      do_not_record = 0;
+      hash = HASH (XEXP (*loc, 0), Pmode);
+      do_not_record = save_do_not_record;
+      hash_arg_in_memory = save_hash_arg_in_memory;
+
+      elt = lookup (XEXP (*loc, 0), hash, Pmode);
+      if (elt == 0)
+	return;
+
+      /* We need to find the best (under the criteria documented above) entry
+	 in the class that is valid.  We use the `flag' field to indicate
+	 choices that were invalid and iterate until we can't find a better
+	 one that hasn't already been tried.  */
+
+      for (p = elt->first_same_value; p; p = p->next_same_value)
+	p->flag = 0;
+
+      while (found_better)
+	{
+	  int best_addr_cost = address_cost (*loc, mode);
+	  int best_rtx_cost = (COST (*loc) + 1) >> 1;
+	  struct table_elt *best_elt = elt;
+	  rtx best_rtx = *loc;
+	  int count;
+
+	  /* This is at worst case an O(n^2) algorithm, so limit our search
+	     to the first 32 elements on the list.  This avoids trouble
+	     compiling code with very long basic blocks that can easily
+	     call simplify_gen_binary so many times that we run out of
+	     memory.  */
+
+	  found_better = 0;
+	  for (p = elt->first_same_value, count = 0;
+	       p && count < 32;
+	       p = p->next_same_value, count++)
+	    if (! p->flag
+		&& (GET_CODE (p->exp) == REG
+		    || exp_equiv_p (p->exp, p->exp, 1, 0)))
+	      {
+		rtx new = simplify_gen_binary (GET_CODE (*loc), Pmode,
+					       p->exp, op1);
+		int new_cost;
+		new_cost = address_cost (new, mode);
+
+		if (new_cost < best_addr_cost
+		    || (new_cost == best_addr_cost
+			&& (COST (new) + 1) >> 1 > best_rtx_cost))
+		  {
+		    found_better = 1;
+		    best_addr_cost = new_cost;
+		    best_rtx_cost = (COST (new) + 1) >> 1;
+		    best_elt = p;
+		    best_rtx = new;
+		  }
+	      }
+
+	  if (found_better)
+	    {
+	      if (validate_change (insn, loc,
+				   canon_reg (copy_rtx (best_rtx),
+					      NULL_RTX), 0))
+		return;
+	      else
+		best_elt->flag = 1;
+	    }
+	}
+    }
+}
+
+/* Given an operation (CODE, *PARG1, *PARG2), where code is a comparison
+   operation (EQ, NE, GT, etc.), follow it back through the hash table and
+   what values are being compared.
+
+   *PARG1 and *PARG2 are updated to contain the rtx representing the values
+   actually being compared.  For example, if *PARG1 was (cc0) and *PARG2
+   was (const_int 0), *PARG1 and *PARG2 will be set to the objects that were
+   compared to produce cc0.
+
+   The return value is the comparison operator and is either the code of
+   A or the code corresponding to the inverse of the comparison.  */
+
+static enum rtx_code
+find_comparison_args (enum rtx_code code, rtx *parg1, rtx *parg2,
+		      enum machine_mode *pmode1, enum machine_mode *pmode2)
+{
+  rtx arg1, arg2;
+
+  arg1 = *parg1, arg2 = *parg2;
+
+  /* If ARG2 is const0_rtx, see what ARG1 is equivalent to.  */
+
+  while (arg2 == CONST0_RTX (GET_MODE (arg1)))
+    {
+      /* Set nonzero when we find something of interest.  */
+      rtx x = 0;
+      int reverse_code = 0;
+      struct table_elt *p = 0;
+
+      /* If arg1 is a COMPARE, extract the comparison arguments from it.
+	 On machines with CC0, this is the only case that can occur, since
+	 fold_rtx will return the COMPARE or item being compared with zero
+	 when given CC0.  */
+
+      if (GET_CODE (arg1) == COMPARE && arg2 == const0_rtx)
+	x = arg1;
+
+      /* If ARG1 is a comparison operator and CODE is testing for
+	 STORE_FLAG_VALUE, get the inner arguments.  */
+
+      else if (GET_RTX_CLASS (GET_CODE (arg1)) == '<')
+	{
+#ifdef FLOAT_STORE_FLAG_VALUE
+	  REAL_VALUE_TYPE fsfv;
+#endif
+
+	  if (code == NE
+	      || (GET_MODE_CLASS (GET_MODE (arg1)) == MODE_INT
+		  && code == LT && STORE_FLAG_VALUE == -1)
+#ifdef FLOAT_STORE_FLAG_VALUE
+	      || (GET_MODE_CLASS (GET_MODE (arg1)) == MODE_FLOAT
+		  && (fsfv = FLOAT_STORE_FLAG_VALUE (GET_MODE (arg1)),
+		      REAL_VALUE_NEGATIVE (fsfv)))
+#endif
+	      )
+	    x = arg1;
+	  else if (code == EQ
+		   || (GET_MODE_CLASS (GET_MODE (arg1)) == MODE_INT
+		       && code == GE && STORE_FLAG_VALUE == -1)
+#ifdef FLOAT_STORE_FLAG_VALUE
+		   || (GET_MODE_CLASS (GET_MODE (arg1)) == MODE_FLOAT
+		       && (fsfv = FLOAT_STORE_FLAG_VALUE (GET_MODE (arg1)),
+			   REAL_VALUE_NEGATIVE (fsfv)))
+#endif
+		   )
+	    x = arg1, reverse_code = 1;
+	}
+
+      /* ??? We could also check for
+
+	 (ne (and (eq (...) (const_int 1))) (const_int 0))
+
+	 and related forms, but let's wait until we see them occurring.  */
+
+      if (x == 0)
+	/* Look up ARG1 in the hash table and see if it has an equivalence
+	   that lets us see what is being compared.  */
+	p = lookup (arg1, safe_hash (arg1, GET_MODE (arg1)) & HASH_MASK,
+		    GET_MODE (arg1));
+      if (p)
+	{
+	  p = p->first_same_value;
+
+	  /* If what we compare is already known to be constant, that is as
+	     good as it gets.
+	     We need to break the loop in this case, because otherwise we
+	     can have an infinite loop when looking at a reg that is known
+	     to be a constant which is the same as a comparison of a reg
+	     against zero which appears later in the insn stream, which in
+	     turn is constant and the same as the comparison of the first reg
+	     against zero...  */
+	  if (p->is_const)
+	    break;
+	}
+
+      for (; p; p = p->next_same_value)
+	{
+	  enum machine_mode inner_mode = GET_MODE (p->exp);
+#ifdef FLOAT_STORE_FLAG_VALUE
+	  REAL_VALUE_TYPE fsfv;
+#endif
+
+	  /* If the entry isn't valid, skip it.  */
+	  if (! exp_equiv_p (p->exp, p->exp, 1, 0))
+	    continue;
+
+	  if (GET_CODE (p->exp) == COMPARE
+	      /* Another possibility is that this machine has a compare insn
+		 that includes the comparison code.  In that case, ARG1 would
+		 be equivalent to a comparison operation that would set ARG1 to
+		 either STORE_FLAG_VALUE or zero.  If this is an NE operation,
+		 ORIG_CODE is the actual comparison being done; if it is an EQ,
+		 we must reverse ORIG_CODE.  On machine with a negative value
+		 for STORE_FLAG_VALUE, also look at LT and GE operations.  */
+	      || ((code == NE
+		   || (code == LT
+		       && GET_MODE_CLASS (inner_mode) == MODE_INT
+		       && (GET_MODE_BITSIZE (inner_mode)
+			   <= HOST_BITS_PER_WIDE_INT)
+		       && (STORE_FLAG_VALUE
+			   & ((HOST_WIDE_INT) 1
+			      << (GET_MODE_BITSIZE (inner_mode) - 1))))
+#ifdef FLOAT_STORE_FLAG_VALUE
+		   || (code == LT
+		       && GET_MODE_CLASS (inner_mode) == MODE_FLOAT
+		       && (fsfv = FLOAT_STORE_FLAG_VALUE (GET_MODE (arg1)),
+			   REAL_VALUE_NEGATIVE (fsfv)))
+#endif
+		   )
+		  && GET_RTX_CLASS (GET_CODE (p->exp)) == '<'))
+	    {
+	      x = p->exp;
+	      break;
+	    }
+	  else if ((code == EQ
+		    || (code == GE
+			&& GET_MODE_CLASS (inner_mode) == MODE_INT
+			&& (GET_MODE_BITSIZE (inner_mode)
+			    <= HOST_BITS_PER_WIDE_INT)
+			&& (STORE_FLAG_VALUE
+			    & ((HOST_WIDE_INT) 1
+			       << (GET_MODE_BITSIZE (inner_mode) - 1))))
+#ifdef FLOAT_STORE_FLAG_VALUE
+		    || (code == GE
+			&& GET_MODE_CLASS (inner_mode) == MODE_FLOAT
+			&& (fsfv = FLOAT_STORE_FLAG_VALUE (GET_MODE (arg1)),
+			    REAL_VALUE_NEGATIVE (fsfv)))
+#endif
+		    )
+		   && GET_RTX_CLASS (GET_CODE (p->exp)) == '<')
+	    {
+	      reverse_code = 1;
+	      x = p->exp;
+	      break;
+	    }
+
+	  /* If this non-trapping address, e.g. fp + constant, the
+	     equivalent is a better operand since it may let us predict
+	     the value of the comparison.  */
+	  else if (!rtx_addr_can_trap_p (p->exp))
+	    {
+	      arg1 = p->exp;
+	      continue;
+	    }
+	}
+
+      /* If we didn't find a useful equivalence for ARG1, we are done.
+	 Otherwise, set up for the next iteration.  */
+      if (x == 0)
+	break;
+
+      /* If we need to reverse the comparison, make sure that that is
+	 possible -- we can't necessarily infer the value of GE from LT
+	 with floating-point operands.  */
+      if (reverse_code)
+	{
+	  enum rtx_code reversed = reversed_comparison_code (x, NULL_RTX);
+	  if (reversed == UNKNOWN)
+	    break;
+	  else
+	    code = reversed;
+	}
+      else if (GET_RTX_CLASS (GET_CODE (x)) == '<')
+	code = GET_CODE (x);
+      arg1 = XEXP (x, 0), arg2 = XEXP (x, 1);
+    }
+
+  /* Return our results.  Return the modes from before fold_rtx
+     because fold_rtx might produce const_int, and then it's too late.  */
+  *pmode1 = GET_MODE (arg1), *pmode2 = GET_MODE (arg2);
+  *parg1 = fold_rtx (arg1, 0), *parg2 = fold_rtx (arg2, 0);
+
+  return code;
+}
+
+/* If X is a nontrivial arithmetic operation on an argument
+   for which a constant value can be determined, return
+   the result of operating on that value, as a constant.
+   Otherwise, return X, possibly with one or more operands
+   modified by recursive calls to this function.
+
+   If X is a register whose contents are known, we do NOT
+   return those contents here.  equiv_constant is called to
+   perform that task.
+
+   INSN is the insn that we may be modifying.  If it is 0, make a copy
+   of X before modifying it.  */
+
+static rtx
+fold_rtx (rtx x, rtx insn)
+{
+  enum rtx_code code;
+  enum machine_mode mode;
+  const char *fmt;
+  int i;
+  rtx new = 0;
+  int copied = 0;
+  int must_swap = 0;
+
+  /* Folded equivalents of first two operands of X.  */
+  rtx folded_arg0;
+  rtx folded_arg1;
+
+  /* Constant equivalents of first three operands of X;
+     0 when no such equivalent is known.  */
+  rtx const_arg0;
+  rtx const_arg1;
+  rtx const_arg2;
+
+  /* The mode of the first operand of X.  We need this for sign and zero
+     extends.  */
+  enum machine_mode mode_arg0;
+
+  if (x == 0)
+    return x;
+
+  mode = GET_MODE (x);
+  code = GET_CODE (x);
+  switch (code)
+    {
+    case CONST:
+    case CONST_INT:
+    case CONST_DOUBLE:
+    case CONST_VECTOR:
+    case SYMBOL_REF:
+    case LABEL_REF:
+    case REG:
+      /* No use simplifying an EXPR_LIST
+	 since they are used only for lists of args
+	 in a function call's REG_EQUAL note.  */
+    case EXPR_LIST:
+      /* Changing anything inside an ADDRESSOF is incorrect; we don't
+	 want to (e.g.,) make (addressof (const_int 0)) just because
+	 the location is known to be zero.  */
+    case ADDRESSOF:
+      return x;
+
+#ifdef HAVE_cc0
+    case CC0:
+      return prev_insn_cc0;
+#endif
+
+    case PC:
+      /* If the next insn is a CODE_LABEL followed by a jump table,
+	 PC's value is a LABEL_REF pointing to that label.  That
+	 lets us fold switch statements on the VAX.  */
+      {
+	rtx next;
+	if (insn && tablejump_p (insn, &next, NULL))
+	  return gen_rtx_LABEL_REF (Pmode, next);
+      }
+      break;
+
+    case SUBREG:
+      /* See if we previously assigned a constant value to this SUBREG.  */
+      if ((new = lookup_as_function (x, CONST_INT)) != 0
+	  || (new = lookup_as_function (x, CONST_DOUBLE)) != 0)
+	return new;
+
+      /* If this is a paradoxical SUBREG, we have no idea what value the
+	 extra bits would have.  However, if the operand is equivalent
+	 to a SUBREG whose operand is the same as our mode, and all the
+	 modes are within a word, we can just use the inner operand
+	 because these SUBREGs just say how to treat the register.
+
+	 Similarly if we find an integer constant.  */
+
+      if (GET_MODE_SIZE (mode) > GET_MODE_SIZE (GET_MODE (SUBREG_REG (x))))
+	{
+	  enum machine_mode imode = GET_MODE (SUBREG_REG (x));
+	  struct table_elt *elt;
+
+	  if (GET_MODE_SIZE (mode) <= UNITS_PER_WORD
+	      && GET_MODE_SIZE (imode) <= UNITS_PER_WORD
+	      && (elt = lookup (SUBREG_REG (x), HASH (SUBREG_REG (x), imode),
+				imode)) != 0)
+	    for (elt = elt->first_same_value; elt; elt = elt->next_same_value)
+	      {
+		if (CONSTANT_P (elt->exp)
+		    && GET_MODE (elt->exp) == VOIDmode)
+		  return elt->exp;
+
+		if (GET_CODE (elt->exp) == SUBREG
+		    && GET_MODE (SUBREG_REG (elt->exp)) == mode
+		    && exp_equiv_p (elt->exp, elt->exp, 1, 0))
+		  return copy_rtx (SUBREG_REG (elt->exp));
+	      }
+
+	  return x;
+	}
+
+      /* Fold SUBREG_REG.  If it changed, see if we can simplify the SUBREG.
+	 We might be able to if the SUBREG is extracting a single word in an
+	 integral mode or extracting the low part.  */
+
+      folded_arg0 = fold_rtx (SUBREG_REG (x), insn);
+      const_arg0 = equiv_constant (folded_arg0);
+      if (const_arg0)
+	folded_arg0 = const_arg0;
+
+      if (folded_arg0 != SUBREG_REG (x))
+	{
+	  new = simplify_subreg (mode, folded_arg0,
+				 GET_MODE (SUBREG_REG (x)), SUBREG_BYTE (x));
+	  if (new)
+	    return new;
+	}
+
+      /* If this is a narrowing SUBREG and our operand is a REG, see if
+	 we can find an equivalence for REG that is an arithmetic operation
+	 in a wider mode where both operands are paradoxical SUBREGs
+	 from objects of our result mode.  In that case, we couldn't report
+	 an equivalent value for that operation, since we don't know what the
+	 extra bits will be.  But we can find an equivalence for this SUBREG
+	 by folding that operation is the narrow mode.  This allows us to
+	 fold arithmetic in narrow modes when the machine only supports
+	 word-sized arithmetic.
+
+	 Also look for a case where we have a SUBREG whose operand is the
+	 same as our result.  If both modes are smaller than a word, we
+	 are simply interpreting a register in different modes and we
+	 can use the inner value.  */
+
+      if (GET_CODE (folded_arg0) == REG
+	  && GET_MODE_SIZE (mode) < GET_MODE_SIZE (GET_MODE (folded_arg0))
+	  && subreg_lowpart_p (x))
+	{
+	  struct table_elt *elt;
+
+	  /* We can use HASH here since we know that canon_hash won't be
+	     called.  */
+	  elt = lookup (folded_arg0,
+			HASH (folded_arg0, GET_MODE (folded_arg0)),
+			GET_MODE (folded_arg0));
+
+	  if (elt)
+	    elt = elt->first_same_value;
+
+	  for (; elt; elt = elt->next_same_value)
+	    {
+	      enum rtx_code eltcode = GET_CODE (elt->exp);
+
+	      /* Just check for unary and binary operations.  */
+	      if (GET_RTX_CLASS (GET_CODE (elt->exp)) == '1'
+		  && GET_CODE (elt->exp) != SIGN_EXTEND
+		  && GET_CODE (elt->exp) != ZERO_EXTEND
+		  && GET_CODE (XEXP (elt->exp, 0)) == SUBREG
+		  && GET_MODE (SUBREG_REG (XEXP (elt->exp, 0))) == mode
+		  && (GET_MODE_CLASS (mode)
+		      == GET_MODE_CLASS (GET_MODE (XEXP (elt->exp, 0)))))
+		{
+		  rtx op0 = SUBREG_REG (XEXP (elt->exp, 0));
+
+		  if (GET_CODE (op0) != REG && ! CONSTANT_P (op0))
+		    op0 = fold_rtx (op0, NULL_RTX);
+
+		  op0 = equiv_constant (op0);
+		  if (op0)
+		    new = simplify_unary_operation (GET_CODE (elt->exp), mode,
+						    op0, mode);
+		}
+	      else if ((GET_RTX_CLASS (GET_CODE (elt->exp)) == '2'
+			|| GET_RTX_CLASS (GET_CODE (elt->exp)) == 'c')
+		       && eltcode != DIV && eltcode != MOD
+		       && eltcode != UDIV && eltcode != UMOD
+		       && eltcode != ASHIFTRT && eltcode != LSHIFTRT
+		       && eltcode != ROTATE && eltcode != ROTATERT
+		       && ((GET_CODE (XEXP (elt->exp, 0)) == SUBREG
+			    && (GET_MODE (SUBREG_REG (XEXP (elt->exp, 0)))
+				== mode))
+			   || CONSTANT_P (XEXP (elt->exp, 0)))
+		       && ((GET_CODE (XEXP (elt->exp, 1)) == SUBREG
+			    && (GET_MODE (SUBREG_REG (XEXP (elt->exp, 1)))
+				== mode))
+			   || CONSTANT_P (XEXP (elt->exp, 1))))
+		{
+		  rtx op0 = gen_lowpart_common (mode, XEXP (elt->exp, 0));
+		  rtx op1 = gen_lowpart_common (mode, XEXP (elt->exp, 1));
+
+		  if (op0 && GET_CODE (op0) != REG && ! CONSTANT_P (op0))
+		    op0 = fold_rtx (op0, NULL_RTX);
+
+		  if (op0)
+		    op0 = equiv_constant (op0);
+
+		  if (op1 && GET_CODE (op1) != REG && ! CONSTANT_P (op1))
+		    op1 = fold_rtx (op1, NULL_RTX);
+
+		  if (op1)
+		    op1 = equiv_constant (op1);
+
+		  /* If we are looking for the low SImode part of
+		     (ashift:DI c (const_int 32)), it doesn't work
+		     to compute that in SImode, because a 32-bit shift
+		     in SImode is unpredictable.  We know the value is 0.  */
+		  if (op0 && op1
+		      && GET_CODE (elt->exp) == ASHIFT
+		      && GET_CODE (op1) == CONST_INT
+		      && INTVAL (op1) >= GET_MODE_BITSIZE (mode))
+		    {
+		      if (INTVAL (op1) < GET_MODE_BITSIZE (GET_MODE (elt->exp)))
+
+			/* If the count fits in the inner mode's width,
+			   but exceeds the outer mode's width,
+			   the value will get truncated to 0
+			   by the subreg.  */
+			new = const0_rtx;
+		      else
+			/* If the count exceeds even the inner mode's width,
+			   don't fold this expression.  */
+			new = 0;
+		    }
+		  else if (op0 && op1)
+		    new = simplify_binary_operation (GET_CODE (elt->exp), mode,
+						     op0, op1);
+		}
+
+	      else if (GET_CODE (elt->exp) == SUBREG
+		       && GET_MODE (SUBREG_REG (elt->exp)) == mode
+		       && (GET_MODE_SIZE (GET_MODE (folded_arg0))
+			   <= UNITS_PER_WORD)
+		       && exp_equiv_p (elt->exp, elt->exp, 1, 0))
+		new = copy_rtx (SUBREG_REG (elt->exp));
+
+	      if (new)
+		return new;
+	    }
+	}
+
+      return x;
+
+    case NOT:
+    case NEG:
+      /* If we have (NOT Y), see if Y is known to be (NOT Z).
+	 If so, (NOT Y) simplifies to Z.  Similarly for NEG.  */
+      new = lookup_as_function (XEXP (x, 0), code);
+      if (new)
+	return fold_rtx (copy_rtx (XEXP (new, 0)), insn);
+      break;
+
+    case MEM:
+      /* If we are not actually processing an insn, don't try to find the
+	 best address.  Not only don't we care, but we could modify the
+	 MEM in an invalid way since we have no insn to validate against.  */
+      if (insn != 0)
+	find_best_addr (insn, &XEXP (x, 0), GET_MODE (x));
+
+      {
+	/* Even if we don't fold in the insn itself,
+	   we can safely do so here, in hopes of getting a constant.  */
+	rtx addr = fold_rtx (XEXP (x, 0), NULL_RTX);
+	rtx base = 0;
+	HOST_WIDE_INT offset = 0;
+
+	if (GET_CODE (addr) == REG
+	    && REGNO_QTY_VALID_P (REGNO (addr)))
+	  {
+	    int addr_q = REG_QTY (REGNO (addr));
+	    struct qty_table_elem *addr_ent = &qty_table[addr_q];
+
+	    if (GET_MODE (addr) == addr_ent->mode
+		&& addr_ent->const_rtx != NULL_RTX)
+	      addr = addr_ent->const_rtx;
+	  }
+
+	/* If address is constant, split it into a base and integer offset.  */
+	if (GET_CODE (addr) == SYMBOL_REF || GET_CODE (addr) == LABEL_REF)
+	  base = addr;
+	else if (GET_CODE (addr) == CONST && GET_CODE (XEXP (addr, 0)) == PLUS
+		 && GET_CODE (XEXP (XEXP (addr, 0), 1)) == CONST_INT)
+	  {
+	    base = XEXP (XEXP (addr, 0), 0);
+	    offset = INTVAL (XEXP (XEXP (addr, 0), 1));
+	  }
+	else if (GET_CODE (addr) == LO_SUM
+		 && GET_CODE (XEXP (addr, 1)) == SYMBOL_REF)
+	  base = XEXP (addr, 1);
+	else if (GET_CODE (addr) == ADDRESSOF)
+	  return change_address (x, VOIDmode, addr);
+
+	/* If this is a constant pool reference, we can fold it into its
+	   constant to allow better value tracking.  */
+	if (base && GET_CODE (base) == SYMBOL_REF
+	    && CONSTANT_POOL_ADDRESS_P (base))
+	  {
+	    rtx constant = get_pool_constant (base);
+	    enum machine_mode const_mode = get_pool_mode (base);
+	    rtx new;
+
+	    if (CONSTANT_P (constant) && GET_CODE (constant) != CONST_INT)
+	      {
+		constant_pool_entries_cost = COST (constant);
+		constant_pool_entries_regcost = approx_reg_cost (constant);
+	      }
+
+	    /* If we are loading the full constant, we have an equivalence.  */
+	    if (offset == 0 && mode == const_mode)
+	      return constant;
+
+	    /* If this actually isn't a constant (weird!), we can't do
+	       anything.  Otherwise, handle the two most common cases:
+	       extracting a word from a multi-word constant, and extracting
+	       the low-order bits.  Other cases don't seem common enough to
+	       worry about.  */
+	    if (! CONSTANT_P (constant))
+	      return x;
+
+	    if (GET_MODE_CLASS (mode) == MODE_INT
+		&& GET_MODE_SIZE (mode) == UNITS_PER_WORD
+		&& offset % UNITS_PER_WORD == 0
+		&& (new = operand_subword (constant,
+					   offset / UNITS_PER_WORD,
+					   0, const_mode)) != 0)
+	      return new;
+
+	    if (((BYTES_BIG_ENDIAN
+		  && offset == GET_MODE_SIZE (GET_MODE (constant)) - 1)
+		 || (! BYTES_BIG_ENDIAN && offset == 0))
+		&& (new = gen_lowpart_if_possible (mode, constant)) != 0)
+	      return new;
+	  }
+
+	/* If this is a reference to a label at a known position in a jump
+	   table, we also know its value.  */
+	if (base && GET_CODE (base) == LABEL_REF)
+	  {
+	    rtx label = XEXP (base, 0);
+	    rtx table_insn = NEXT_INSN (label);
+
+	    if (table_insn && GET_CODE (table_insn) == JUMP_INSN
+		&& GET_CODE (PATTERN (table_insn)) == ADDR_VEC)
+	      {
+		rtx table = PATTERN (table_insn);
+
+		if (offset >= 0
+		    && (offset / GET_MODE_SIZE (GET_MODE (table))
+			< XVECLEN (table, 0)))
+		  return XVECEXP (table, 0,
+				  offset / GET_MODE_SIZE (GET_MODE (table)));
+	      }
+	    if (table_insn && GET_CODE (table_insn) == JUMP_INSN
+		&& GET_CODE (PATTERN (table_insn)) == ADDR_DIFF_VEC)
+	      {
+		rtx table = PATTERN (table_insn);
+
+		if (offset >= 0
+		    && (offset / GET_MODE_SIZE (GET_MODE (table))
+			< XVECLEN (table, 1)))
+		  {
+		    offset /= GET_MODE_SIZE (GET_MODE (table));
+		    new = gen_rtx_MINUS (Pmode, XVECEXP (table, 1, offset),
+					 XEXP (table, 0));
+
+		    if (GET_MODE (table) != Pmode)
+		      new = gen_rtx_TRUNCATE (GET_MODE (table), new);
+
+		    /* Indicate this is a constant.  This isn't a
+		       valid form of CONST, but it will only be used
+		       to fold the next insns and then discarded, so
+		       it should be safe.
+
+		       Note this expression must be explicitly discarded,
+		       by cse_insn, else it may end up in a REG_EQUAL note
+		       and "escape" to cause problems elsewhere.  */
+		    return gen_rtx_CONST (GET_MODE (new), new);
+		  }
+	      }
+	  }
+
+	return x;
+      }
+
+#ifdef NO_FUNCTION_CSE
+    case CALL:
+      if (CONSTANT_P (XEXP (XEXP (x, 0), 0)))
+	return x;
+      break;
+#endif
+
+    case ASM_OPERANDS:
+      for (i = ASM_OPERANDS_INPUT_LENGTH (x) - 1; i >= 0; i--)
+	validate_change (insn, &ASM_OPERANDS_INPUT (x, i),
+			 fold_rtx (ASM_OPERANDS_INPUT (x, i), insn), 0);
+      break;
+
+    default:
+      break;
+    }
+
+  const_arg0 = 0;
+  const_arg1 = 0;
+  const_arg2 = 0;
+  mode_arg0 = VOIDmode;
+
+  /* Try folding our operands.
+     Then see which ones have constant values known.  */
+
+  fmt = GET_RTX_FORMAT (code);
+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)
+    if (fmt[i] == 'e')
+      {
+	rtx arg = XEXP (x, i);
+	rtx folded_arg = arg, const_arg = 0;
+	enum machine_mode mode_arg = GET_MODE (arg);
+	rtx cheap_arg, expensive_arg;
+	rtx replacements[2];
+	int j;
+	int old_cost = COST_IN (XEXP (x, i), code);
+
+	/* Most arguments are cheap, so handle them specially.  */
+	switch (GET_CODE (arg))
+	  {
+	  case REG:
+	    /* This is the same as calling equiv_constant; it is duplicated
+	       here for speed.  */
+	    if (REGNO_QTY_VALID_P (REGNO (arg)))
+	      {
+		int arg_q = REG_QTY (REGNO (arg));
+		struct qty_table_elem *arg_ent = &qty_table[arg_q];
+
+		if (arg_ent->const_rtx != NULL_RTX
+		    && GET_CODE (arg_ent->const_rtx) != REG
+		    && GET_CODE (arg_ent->const_rtx) != PLUS)
+		  const_arg
+		    = gen_lowpart_if_possible (GET_MODE (arg),
+					       arg_ent->const_rtx);
+	      }
+	    break;
+
+	  case CONST:
+	  case CONST_INT:
+	  case SYMBOL_REF:
+	  case LABEL_REF:
+	  case CONST_DOUBLE:
+	  case CONST_VECTOR:
+	    const_arg = arg;
+	    break;
+
+#ifdef HAVE_cc0
+	  case CC0:
+	    folded_arg = prev_insn_cc0;
+	    mode_arg = prev_insn_cc0_mode;
+	    const_arg = equiv_constant (folded_arg);
+	    break;
+#endif
+
+	  default:
+	    folded_arg = fold_rtx (arg, insn);
+	    const_arg = equiv_constant (folded_arg);
+	  }
+
+	/* For the first three operands, see if the operand
+	   is constant or equivalent to a constant.  */
+	switch (i)
+	  {
+	  case 0:
+	    folded_arg0 = folded_arg;
+	    const_arg0 = const_arg;
+	    mode_arg0 = mode_arg;
+	    break;
+	  case 1:
+	    folded_arg1 = folded_arg;
+	    const_arg1 = const_arg;
+	    break;
+	  case 2:
+	    const_arg2 = const_arg;
+	    break;
+	  }
+
+	/* Pick the least expensive of the folded argument and an
+	   equivalent constant argument.  */
+	if (const_arg == 0 || const_arg == folded_arg
+	    || COST_IN (const_arg, code) > COST_IN (folded_arg, code))
+	  cheap_arg = folded_arg, expensive_arg = const_arg;
+	else
+	  cheap_arg = const_arg, expensive_arg = folded_arg;
+
+	/* Try to replace the operand with the cheapest of the two
+	   possibilities.  If it doesn't work and this is either of the first
+	   two operands of a commutative operation, try swapping them.
+	   If THAT fails, try the more expensive, provided it is cheaper
+	   than what is already there.  */
+
+	if (cheap_arg == XEXP (x, i))
+	  continue;
+
+	if (insn == 0 && ! copied)
+	  {
+	    x = copy_rtx (x);
+	    copied = 1;
+	  }
+
+	/* Order the replacements from cheapest to most expensive.  */
+	replacements[0] = cheap_arg;
+	replacements[1] = expensive_arg;
+
+	for (j = 0; j < 2 && replacements[j]; j++)
+	  {
+	    int new_cost = COST_IN (replacements[j], code);
+
+	    /* Stop if what existed before was cheaper.  Prefer constants
+	       in the case of a tie.  */
+	    if (new_cost > old_cost
+		|| (new_cost == old_cost && CONSTANT_P (XEXP (x, i))))
+	      break;
+
+	    /* It's not safe to substitute the operand of a conversion
+	       operator with a constant, as the conversion's identity
+	       depends upon the mode of it's operand.  This optimization
+	       is handled by the call to simplify_unary_operation.  */
+	    if (GET_RTX_CLASS (code) == '1'
+		&& GET_MODE (replacements[j]) != mode_arg0
+		&& (code == ZERO_EXTEND
+		    || code == SIGN_EXTEND
+		    || code == TRUNCATE
+		    || code == FLOAT_TRUNCATE
+		    || code == FLOAT_EXTEND
+		    || code == FLOAT
+		    || code == FIX
+		    || code == UNSIGNED_FLOAT
+		    || code == UNSIGNED_FIX))
+	      continue;
+
+	    if (validate_change (insn, &XEXP (x, i), replacements[j], 0))
+	      break;
+
+	    if (code == NE || code == EQ || GET_RTX_CLASS (code) == 'c'
+		|| code == LTGT || code == UNEQ || code == ORDERED
+		|| code == UNORDERED)
+	      {
+		validate_change (insn, &XEXP (x, i), XEXP (x, 1 - i), 1);
+		validate_change (insn, &XEXP (x, 1 - i), replacements[j], 1);
+
+		if (apply_change_group ())
+		  {
+		    /* Swap them back to be invalid so that this loop can
+		       continue and flag them to be swapped back later.  */
+		    rtx tem;
+
+		    tem = XEXP (x, 0); XEXP (x, 0) = XEXP (x, 1);
+				       XEXP (x, 1) = tem;
+		    must_swap = 1;
+		    break;
+		  }
+	      }
+	  }
+      }
+
+    else
+      {
+	if (fmt[i] == 'E')
+	  /* Don't try to fold inside of a vector of expressions.
+	     Doing nothing is harmless.  */
+	  {;}
+      }
+
+  /* If a commutative operation, place a constant integer as the second
+     operand unless the first operand is also a constant integer.  Otherwise,
+     place any constant second unless the first operand is also a constant.  */
+
+  if (code == EQ || code == NE || GET_RTX_CLASS (code) == 'c'
+      || code == LTGT || code == UNEQ || code == ORDERED
+      || code == UNORDERED)
+    {
+      if (must_swap
+	  || swap_commutative_operands_p (const_arg0 ? const_arg0
+						     : XEXP (x, 0),
+					  const_arg1 ? const_arg1
+						     : XEXP (x, 1)))
+	{
+	  rtx tem = XEXP (x, 0);
+
+	  if (insn == 0 && ! copied)
+	    {
+	      x = copy_rtx (x);
+	      copied = 1;
+	    }
+
+	  validate_change (insn, &XEXP (x, 0), XEXP (x, 1), 1);
+	  validate_change (insn, &XEXP (x, 1), tem, 1);
+	  if (apply_change_group ())
+	    {
+	      tem = const_arg0, const_arg0 = const_arg1, const_arg1 = tem;
+	      tem = folded_arg0, folded_arg0 = folded_arg1, folded_arg1 = tem;
+	    }
+	}
+    }
+
+  /* If X is an arithmetic operation, see if we can simplify it.  */
+
+  switch (GET_RTX_CLASS (code))
+    {
+    case '1':
+      {
+	int is_const = 0;
+
+	/* We can't simplify extension ops unless we know the
+	   original mode.  */
+	if ((code == ZERO_EXTEND || code == SIGN_EXTEND)
+	    && mode_arg0 == VOIDmode)
+	  break;
+
+	/* If we had a CONST, strip it off and put it back later if we
+	   fold.  */
+	if (const_arg0 != 0 && GET_CODE (const_arg0) == CONST)
+	  is_const = 1, const_arg0 = XEXP (const_arg0, 0);
+
+	new = simplify_unary_operation (code, mode,
+					const_arg0 ? const_arg0 : folded_arg0,
+					mode_arg0);
+	if (new != 0 && is_const)
+	  new = gen_rtx_CONST (mode, new);
+      }
+      break;
+
+    case '<':
+      /* Don't perform any simplifications of vector mode comparisons.  */
+      if (VECTOR_MODE_P (mode))
+	break;
+
+      /* See what items are actually being compared and set FOLDED_ARG[01]
+	 to those values and CODE to the actual comparison code.  If any are
+	 constant, set CONST_ARG0 and CONST_ARG1 appropriately.  We needn't
+	 do anything if both operands are already known to be constant.  */
+
+      if (const_arg0 == 0 || const_arg1 == 0)
+	{
+	  struct table_elt *p0, *p1;
+	  rtx true_rtx = const_true_rtx, false_rtx = const0_rtx;
+	  enum machine_mode mode_arg1;
+
+#ifdef FLOAT_STORE_FLAG_VALUE
+	  if (GET_MODE_CLASS (mode) == MODE_FLOAT)
+	    {
+	      true_rtx = (CONST_DOUBLE_FROM_REAL_VALUE
+			  (FLOAT_STORE_FLAG_VALUE (mode), mode));
+	      false_rtx = CONST0_RTX (mode);
+	    }
+#endif
+
+	  code = find_comparison_args (code, &folded_arg0, &folded_arg1,
+				       &mode_arg0, &mode_arg1);
+	  const_arg0 = equiv_constant (folded_arg0);
+	  const_arg1 = equiv_constant (folded_arg1);
+
+	  /* If the mode is VOIDmode or a MODE_CC mode, we don't know
+	     what kinds of things are being compared, so we can't do
+	     anything with this comparison.  */
+
+	  if (mode_arg0 == VOIDmode || GET_MODE_CLASS (mode_arg0) == MODE_CC)
+	    break;
+
+	  /* If we do not now have two constants being compared, see
+	     if we can nevertheless deduce some things about the
+	     comparison.  */
+	  if (const_arg0 == 0 || const_arg1 == 0)
+	    {
+	      /* Some addresses are known to be nonzero.  We don't know
+		 their sign, but equality comparisons are known.  */
+	      if (const_arg1 == const0_rtx
+		  && nonzero_address_p (folded_arg0))
+		{
+		  if (code == EQ)
+		    return false_rtx;
+		  else if (code == NE)
+		    return true_rtx;
+		}
+
+	      /* See if the two operands are the same.  */
+
+	      if (folded_arg0 == folded_arg1
+		  || (GET_CODE (folded_arg0) == REG
+		      && GET_CODE (folded_arg1) == REG
+		      && (REG_QTY (REGNO (folded_arg0))
+			  == REG_QTY (REGNO (folded_arg1))))
+		  || ((p0 = lookup (folded_arg0,
+				    (safe_hash (folded_arg0, mode_arg0)
+				     & HASH_MASK), mode_arg0))
+		      && (p1 = lookup (folded_arg1,
+				       (safe_hash (folded_arg1, mode_arg0)
+					& HASH_MASK), mode_arg0))
+		      && p0->first_same_value == p1->first_same_value))
+		{
+		  /* Sadly two equal NaNs are not equivalent.  */
+		  if (!HONOR_NANS (mode_arg0))
+		    return ((code == EQ || code == LE || code == GE
+			     || code == LEU || code == GEU || code == UNEQ
+			     || code == UNLE || code == UNGE
+			     || code == ORDERED)
+			    ? true_rtx : false_rtx);
+		  /* Take care for the FP compares we can resolve.  */
+		  if (code == UNEQ || code == UNLE || code == UNGE)
+		    return true_rtx;
+		  if (code == LTGT || code == LT || code == GT)
+		    return false_rtx;
+		}
+
+	      /* If FOLDED_ARG0 is a register, see if the comparison we are
+		 doing now is either the same as we did before or the reverse
+		 (we only check the reverse if not floating-point).  */
+	      else if (GET_CODE (folded_arg0) == REG)
+		{
+		  int qty = REG_QTY (REGNO (folded_arg0));
+
+		  if (REGNO_QTY_VALID_P (REGNO (folded_arg0)))
+		    {
+		      struct qty_table_elem *ent = &qty_table[qty];
+
+		      if ((comparison_dominates_p (ent->comparison_code, code)
+			   || (! FLOAT_MODE_P (mode_arg0)
+			       && comparison_dominates_p (ent->comparison_code,
+						          reverse_condition (code))))
+			  && (rtx_equal_p (ent->comparison_const, folded_arg1)
+			      || (const_arg1
+				  && rtx_equal_p (ent->comparison_const,
+						  const_arg1))
+			      || (GET_CODE (folded_arg1) == REG
+				  && (REG_QTY (REGNO (folded_arg1)) == ent->comparison_qty))))
+			return (comparison_dominates_p (ent->comparison_code, code)
+				? true_rtx : false_rtx);
+		    }
+		}
+	    }
+	}
+
+      /* If we are comparing against zero, see if the first operand is
+	 equivalent to an IOR with a constant.  If so, we may be able to
+	 determine the result of this comparison.  */
+
+      if (const_arg1 == const0_rtx)
+	{
+	  rtx y = lookup_as_function (folded_arg0, IOR);
+	  rtx inner_const;
+
+	  if (y != 0
+	      && (inner_const = equiv_constant (XEXP (y, 1))) != 0
+	      && GET_CODE (inner_const) == CONST_INT
+	      && INTVAL (inner_const) != 0)
+	    {
+	      int sign_bitnum = GET_MODE_BITSIZE (mode_arg0) - 1;
+	      int has_sign = (HOST_BITS_PER_WIDE_INT >= sign_bitnum
+			      && (INTVAL (inner_const)
+				  & ((HOST_WIDE_INT) 1 << sign_bitnum)));
+	      rtx true_rtx = const_true_rtx, false_rtx = const0_rtx;
+
+#ifdef FLOAT_STORE_FLAG_VALUE
+	      if (GET_MODE_CLASS (mode) == MODE_FLOAT)
+		{
+		  true_rtx = (CONST_DOUBLE_FROM_REAL_VALUE
+			  (FLOAT_STORE_FLAG_VALUE (mode), mode));
+		  false_rtx = CONST0_RTX (mode);
+		}
+#endif
+
+	      switch (code)
+		{
+		case EQ:
+		  return false_rtx;
+		case NE:
+		  return true_rtx;
+		case LT:  case LE:
+		  if (has_sign)
+		    return true_rtx;
+		  break;
+		case GT:  case GE:
+		  if (has_sign)
+		    return false_rtx;
+		  break;
+		default:
+		  break;
+		}
+	    }
+	}
+
+      new = simplify_relational_operation (code,
+					   (mode_arg0 != VOIDmode
+					    ? mode_arg0
+					    : (GET_MODE (const_arg0
+							 ? const_arg0
+							 : folded_arg0)
+					       != VOIDmode)
+					    ? GET_MODE (const_arg0
+							? const_arg0
+							: folded_arg0)
+					    : GET_MODE (const_arg1
+							? const_arg1
+							: folded_arg1)),
+					   const_arg0 ? const_arg0 : folded_arg0,
+					   const_arg1 ? const_arg1 : folded_arg1);
+#ifdef FLOAT_STORE_FLAG_VALUE
+      if (new != 0 && GET_MODE_CLASS (mode) == MODE_FLOAT)
+	{
+	  if (new == const0_rtx)
+	    new = CONST0_RTX (mode);
+	  else
+	    new = (CONST_DOUBLE_FROM_REAL_VALUE
+		   (FLOAT_STORE_FLAG_VALUE (mode), mode));
+	}
+#endif
+      break;
+
+    case '2':
+    case 'c':
+      switch (code)
+	{
+	case PLUS:
+	  /* If the second operand is a LABEL_REF, see if the first is a MINUS
+	     with that LABEL_REF as its second operand.  If so, the result is
+	     the first operand of that MINUS.  This handles switches with an
+	     ADDR_DIFF_VEC table.  */
+	  if (const_arg1 && GET_CODE (const_arg1) == LABEL_REF)
+	    {
+	      rtx y
+		= GET_CODE (folded_arg0) == MINUS ? folded_arg0
+		: lookup_as_function (folded_arg0, MINUS);
+
+	      if (y != 0 && GET_CODE (XEXP (y, 1)) == LABEL_REF
+		  && XEXP (XEXP (y, 1), 0) == XEXP (const_arg1, 0))
+		return XEXP (y, 0);
+
+	      /* Now try for a CONST of a MINUS like the above.  */
+	      if ((y = (GET_CODE (folded_arg0) == CONST ? folded_arg0
+			: lookup_as_function (folded_arg0, CONST))) != 0
+		  && GET_CODE (XEXP (y, 0)) == MINUS
+		  && GET_CODE (XEXP (XEXP (y, 0), 1)) == LABEL_REF
+		  && XEXP (XEXP (XEXP (y, 0), 1), 0) == XEXP (const_arg1, 0))
+		return XEXP (XEXP (y, 0), 0);
+	    }
+
+	  /* Likewise if the operands are in the other order.  */
+	  if (const_arg0 && GET_CODE (const_arg0) == LABEL_REF)
+	    {
+	      rtx y
+		= GET_CODE (folded_arg1) == MINUS ? folded_arg1
+		: lookup_as_function (folded_arg1, MINUS);
+
+	      if (y != 0 && GET_CODE (XEXP (y, 1)) == LABEL_REF
+		  && XEXP (XEXP (y, 1), 0) == XEXP (const_arg0, 0))
+		return XEXP (y, 0);
+
+	      /* Now try for a CONST of a MINUS like the above.  */
+	      if ((y = (GET_CODE (folded_arg1) == CONST ? folded_arg1
+			: lookup_as_function (folded_arg1, CONST))) != 0
+		  && GET_CODE (XEXP (y, 0)) == MINUS
+		  && GET_CODE (XEXP (XEXP (y, 0), 1)) == LABEL_REF
+		  && XEXP (XEXP (XEXP (y, 0), 1), 0) == XEXP (const_arg0, 0))
+		return XEXP (XEXP (y, 0), 0);
+	    }
+
+	  /* If second operand is a register equivalent to a negative
+	     CONST_INT, see if we can find a register equivalent to the
+	     positive constant.  Make a MINUS if so.  Don't do this for
+	     a non-negative constant since we might then alternate between
+	     choosing positive and negative constants.  Having the positive
+	     constant previously-used is the more common case.  Be sure
+	     the resulting constant is non-negative; if const_arg1 were
+	     the smallest negative number this would overflow: depending
+	     on the mode, this would either just be the same value (and
+	     hence not save anything) or be incorrect.  */
+	  if (const_arg1 != 0 && GET_CODE (const_arg1) == CONST_INT
+	      && INTVAL (const_arg1) < 0
+	      /* This used to test
+
+	         -INTVAL (const_arg1) >= 0
+
+		 But The Sun V5.0 compilers mis-compiled that test.  So
+		 instead we test for the problematic value in a more direct
+		 manner and hope the Sun compilers get it correct.  */
+	      && INTVAL (const_arg1) !=
+	        ((HOST_WIDE_INT) 1 << (HOST_BITS_PER_WIDE_INT - 1))
+	      && GET_CODE (folded_arg1) == REG)
+	    {
+	      rtx new_const = GEN_INT (-INTVAL (const_arg1));
+	      struct table_elt *p
+		= lookup (new_const, safe_hash (new_const, mode) & HASH_MASK,
+			  mode);
+
+	      if (p)
+		for (p = p->first_same_value; p; p = p->next_same_value)
+		  if (GET_CODE (p->exp) == REG)
+		    return simplify_gen_binary (MINUS, mode, folded_arg0,
+						canon_reg (p->exp, NULL_RTX));
+	    }
+	  goto from_plus;
+
+	case MINUS:
+	  /* If we have (MINUS Y C), see if Y is known to be (PLUS Z C2).
+	     If so, produce (PLUS Z C2-C).  */
+	  if (const_arg1 != 0 && GET_CODE (const_arg1) == CONST_INT)
+	    {
+	      rtx y = lookup_as_function (XEXP (x, 0), PLUS);
+	      if (y && GET_CODE (XEXP (y, 1)) == CONST_INT)
+		return fold_rtx (plus_constant (copy_rtx (y),
+						-INTVAL (const_arg1)),
+				 NULL_RTX);
+	    }
+
+	  /* Fall through.  */
+
+	from_plus:
+	case SMIN:    case SMAX:      case UMIN:    case UMAX:
+	case IOR:     case AND:       case XOR:
+	case MULT:
+	case ASHIFT:  case LSHIFTRT:  case ASHIFTRT:
+	  /* If we have (<op> <reg> <const_int>) for an associative OP and REG
+	     is known to be of similar form, we may be able to replace the
+	     operation with a combined operation.  This may eliminate the
+	     intermediate operation if every use is simplified in this way.
+	     Note that the similar optimization done by combine.c only works
+	     if the intermediate operation's result has only one reference.  */
+
+	  if (GET_CODE (folded_arg0) == REG
+	      && const_arg1 && GET_CODE (const_arg1) == CONST_INT)
+	    {
+	      int is_shift
+		= (code == ASHIFT || code == ASHIFTRT || code == LSHIFTRT);
+	      rtx y = lookup_as_function (folded_arg0, code);
+	      rtx inner_const;
+	      enum rtx_code associate_code;
+	      rtx new_const;
+
+	      if (y == 0
+		  || 0 == (inner_const
+			   = equiv_constant (fold_rtx (XEXP (y, 1), 0)))
+		  || GET_CODE (inner_const) != CONST_INT
+		  /* If we have compiled a statement like
+		     "if (x == (x & mask1))", and now are looking at
+		     "x & mask2", we will have a case where the first operand
+		     of Y is the same as our first operand.  Unless we detect
+		     this case, an infinite loop will result.  */
+		  || XEXP (y, 0) == folded_arg0)
+		break;
+
+	      /* Don't associate these operations if they are a PLUS with the
+		 same constant and it is a power of two.  These might be doable
+		 with a pre- or post-increment.  Similarly for two subtracts of
+		 identical powers of two with post decrement.  */
+
+	      if (code == PLUS && const_arg1 == inner_const
+		  && ((HAVE_PRE_INCREMENT
+			  && exact_log2 (INTVAL (const_arg1)) >= 0)
+		      || (HAVE_POST_INCREMENT
+			  && exact_log2 (INTVAL (const_arg1)) >= 0)
+		      || (HAVE_PRE_DECREMENT
+			  && exact_log2 (- INTVAL (const_arg1)) >= 0)
+		      || (HAVE_POST_DECREMENT
+			  && exact_log2 (- INTVAL (const_arg1)) >= 0)))
+		break;
+
+	      /* Compute the code used to compose the constants.  For example,
+		 A-C1-C2 is A-(C1 + C2), so if CODE == MINUS, we want PLUS.  */
+
+	      associate_code = (is_shift || code == MINUS ? PLUS : code);
+
+	      new_const = simplify_binary_operation (associate_code, mode,
+						     const_arg1, inner_const);
+
+	      if (new_const == 0)
+		break;
+
+	      /* If we are associating shift operations, don't let this
+		 produce a shift of the size of the object or larger.
+		 This could occur when we follow a sign-extend by a right
+		 shift on a machine that does a sign-extend as a pair
+		 of shifts.  */
+
+	      if (is_shift && GET_CODE (new_const) == CONST_INT
+		  && INTVAL (new_const) >= GET_MODE_BITSIZE (mode))
+		{
+		  /* As an exception, we can turn an ASHIFTRT of this
+		     form into a shift of the number of bits - 1.  */
+		  if (code == ASHIFTRT)
+		    new_const = GEN_INT (GET_MODE_BITSIZE (mode) - 1);
+		  else
+		    break;
+		}
+
+	      y = copy_rtx (XEXP (y, 0));
+
+	      /* If Y contains our first operand (the most common way this
+		 can happen is if Y is a MEM), we would do into an infinite
+		 loop if we tried to fold it.  So don't in that case.  */
+
+	      if (! reg_mentioned_p (folded_arg0, y))
+		y = fold_rtx (y, insn);
+
+	      return simplify_gen_binary (code, mode, y, new_const);
+	    }
+	  break;
+
+	case DIV:       case UDIV:
+	  /* ??? The associative optimization performed immediately above is
+	     also possible for DIV and UDIV using associate_code of MULT.
+	     However, we would need extra code to verify that the
+	     multiplication does not overflow, that is, there is no overflow
+	     in the calculation of new_const.  */
+	  break;
+
+	default:
+	  break;
+	}
+
+      new = simplify_binary_operation (code, mode,
+				       const_arg0 ? const_arg0 : folded_arg0,
+				       const_arg1 ? const_arg1 : folded_arg1);
+      break;
+
+    case 'o':
+      /* (lo_sum (high X) X) is simply X.  */
+      if (code == LO_SUM && const_arg0 != 0
+	  && GET_CODE (const_arg0) == HIGH
+	  && rtx_equal_p (XEXP (const_arg0, 0), const_arg1))
+	return const_arg1;
+      break;
+
+    case '3':
+    case 'b':
+      new = simplify_ternary_operation (code, mode, mode_arg0,
+					const_arg0 ? const_arg0 : folded_arg0,
+					const_arg1 ? const_arg1 : folded_arg1,
+					const_arg2 ? const_arg2 : XEXP (x, 2));
+      break;
+
+    case 'x':
+      /* Eliminate CONSTANT_P_RTX if its constant.  */
+      if (code == CONSTANT_P_RTX)
+	{
+	  if (const_arg0)
+	    return const1_rtx;
+	  if (optimize == 0 || !flag_gcse)
+	    return const0_rtx;
+	}
+      break;
+    }
+
+  return new ? new : x;
+}
+
+/* Return a constant value currently equivalent to X.
+   Return 0 if we don't know one.  */
+
+static rtx
+equiv_constant (rtx x)
+{
+  if (GET_CODE (x) == REG
+      && REGNO_QTY_VALID_P (REGNO (x)))
+    {
+      int x_q = REG_QTY (REGNO (x));
+      struct qty_table_elem *x_ent = &qty_table[x_q];
+
+      if (x_ent->const_rtx)
+	x = gen_lowpart_if_possible (GET_MODE (x), x_ent->const_rtx);
+    }
+
+  if (x == 0 || CONSTANT_P (x))
+    return x;
+
+  /* If X is a MEM, try to fold it outside the context of any insn to see if
+     it might be equivalent to a constant.  That handles the case where it
+     is a constant-pool reference.  Then try to look it up in the hash table
+     in case it is something whose value we have seen before.  */
+
+  if (GET_CODE (x) == MEM)
+    {
+      struct table_elt *elt;
+
+      x = fold_rtx (x, NULL_RTX);
+      if (CONSTANT_P (x))
+	return x;
+
+      elt = lookup (x, safe_hash (x, GET_MODE (x)) & HASH_MASK, GET_MODE (x));
+      if (elt == 0)
+	return 0;
+
+      for (elt = elt->first_same_value; elt; elt = elt->next_same_value)
+	if (elt->is_const && CONSTANT_P (elt->exp))
+	  return elt->exp;
+    }
+
+  return 0;
+}
+
+/* Assuming that X is an rtx (e.g., MEM, REG or SUBREG) for a fixed-point
+   number, return an rtx (MEM, SUBREG, or CONST_INT) that refers to the
+   least-significant part of X.
+   MODE specifies how big a part of X to return.
+
+   If the requested operation cannot be done, 0 is returned.
+
+   This is similar to gen_lowpart in emit-rtl.c.  */
+
+rtx
+gen_lowpart_if_possible (enum machine_mode mode, rtx x)
+{
+  rtx result = gen_lowpart_common (mode, x);
+
+  if (result)
+    return result;
+  else if (GET_CODE (x) == MEM)
+    {
+      /* This is the only other case we handle.  */
+      int offset = 0;
+      rtx new;
+
+      if (WORDS_BIG_ENDIAN)
+	offset = (MAX (GET_MODE_SIZE (GET_MODE (x)), UNITS_PER_WORD)
+		  - MAX (GET_MODE_SIZE (mode), UNITS_PER_WORD));
+      if (BYTES_BIG_ENDIAN)
+	/* Adjust the address so that the address-after-the-data is
+	   unchanged.  */
+	offset -= (MIN (UNITS_PER_WORD, GET_MODE_SIZE (mode))
+		   - MIN (UNITS_PER_WORD, GET_MODE_SIZE (GET_MODE (x))));
+
+      new = adjust_address_nv (x, mode, offset);
+      if (! memory_address_p (mode, XEXP (new, 0)))
+	return 0;
+
+      return new;
+    }
+  else
+    return 0;
+}
+
+/* Given INSN, a jump insn, TAKEN indicates if we are following the "taken"
+   branch.  It will be zero if not.
+
+   In certain cases, this can cause us to add an equivalence.  For example,
+   if we are following the taken case of
+	if (i == 2)
+   we can add the fact that `i' and '2' are now equivalent.
+
+   In any case, we can record that this comparison was passed.  If the same
+   comparison is seen later, we will know its value.  */
+
+static void
+record_jump_equiv (rtx insn, int taken)
+{
+  int cond_known_true;
+  rtx op0, op1;
+  rtx set;
+  enum machine_mode mode, mode0, mode1;
+  int reversed_nonequality = 0;
+  enum rtx_code code;
+
+  /* Ensure this is the right kind of insn.  */
+  if (! any_condjump_p (insn))
+    return;
+  set = pc_set (insn);
+
+  /* See if this jump condition is known true or false.  */
+  if (taken)
+    cond_known_true = (XEXP (SET_SRC (set), 2) == pc_rtx);
+  else
+    cond_known_true = (XEXP (SET_SRC (set), 1) == pc_rtx);
+
+  /* Get the type of comparison being done and the operands being compared.
+     If we had to reverse a non-equality condition, record that fact so we
+     know that it isn't valid for floating-point.  */
+  code = GET_CODE (XEXP (SET_SRC (set), 0));
+  op0 = fold_rtx (XEXP (XEXP (SET_SRC (set), 0), 0), insn);
+  op1 = fold_rtx (XEXP (XEXP (SET_SRC (set), 0), 1), insn);
+
+  code = find_comparison_args (code, &op0, &op1, &mode0, &mode1);
+  if (! cond_known_true)
+    {
+      code = reversed_comparison_code_parts (code, op0, op1, insn);
+
+      /* Don't remember if we can't find the inverse.  */
+      if (code == UNKNOWN)
+	return;
+    }
+
+  /* The mode is the mode of the non-constant.  */
+  mode = mode0;
+  if (mode1 != VOIDmode)
+    mode = mode1;
+
+  record_jump_cond (code, mode, op0, op1, reversed_nonequality);
+}
+
+/* We know that comparison CODE applied to OP0 and OP1 in MODE is true.
+   REVERSED_NONEQUALITY is nonzero if CODE had to be swapped.
+   Make any useful entries we can with that information.  Called from
+   above function and called recursively.  */
+
+static void
+record_jump_cond (enum rtx_code code, enum machine_mode mode, rtx op0,
+		  rtx op1, int reversed_nonequality)
+{
+  unsigned op0_hash, op1_hash;
+  int op0_in_memory, op1_in_memory;
+  struct table_elt *op0_elt, *op1_elt;
+
+  /* If OP0 and OP1 are known equal, and either is a paradoxical SUBREG,
+     we know that they are also equal in the smaller mode (this is also
+     true for all smaller modes whether or not there is a SUBREG, but
+     is not worth testing for with no SUBREG).  */
+
+  /* Note that GET_MODE (op0) may not equal MODE.  */
+  if (code == EQ && GET_CODE (op0) == SUBREG
+      && (GET_MODE_SIZE (GET_MODE (op0))
+	  > GET_MODE_SIZE (GET_MODE (SUBREG_REG (op0)))))
+    {
+      enum machine_mode inner_mode = GET_MODE (SUBREG_REG (op0));
+      rtx tem = gen_lowpart_if_possible (inner_mode, op1);
+
+      record_jump_cond (code, mode, SUBREG_REG (op0),
+			tem ? tem : gen_rtx_SUBREG (inner_mode, op1, 0),
+			reversed_nonequality);
+    }
+
+  if (code == EQ && GET_CODE (op1) == SUBREG
+      && (GET_MODE_SIZE (GET_MODE (op1))
+	  > GET_MODE_SIZE (GET_MODE (SUBREG_REG (op1)))))
+    {
+      enum machine_mode inner_mode = GET_MODE (SUBREG_REG (op1));
+      rtx tem = gen_lowpart_if_possible (inner_mode, op0);
+
+      record_jump_cond (code, mode, SUBREG_REG (op1),
+			tem ? tem : gen_rtx_SUBREG (inner_mode, op0, 0),
+			reversed_nonequality);
+    }
+
+  /* Similarly, if this is an NE comparison, and either is a SUBREG
+     making a smaller mode, we know the whole thing is also NE.  */
+
+  /* Note that GET_MODE (op0) may not equal MODE;
+     if we test MODE instead, we can get an infinite recursion
+     alternating between two modes each wider than MODE.  */
+
+  if (code == NE && GET_CODE (op0) == SUBREG
+      && subreg_lowpart_p (op0)
+      && (GET_MODE_SIZE (GET_MODE (op0))
+	  < GET_MODE_SIZE (GET_MODE (SUBREG_REG (op0)))))
+    {
+      enum machine_mode inner_mode = GET_MODE (SUBREG_REG (op0));
+      rtx tem = gen_lowpart_if_possible (inner_mode, op1);
+
+      record_jump_cond (code, mode, SUBREG_REG (op0),
+			tem ? tem : gen_rtx_SUBREG (inner_mode, op1, 0),
+			reversed_nonequality);
+    }
+
+  if (code == NE && GET_CODE (op1) == SUBREG
+      && subreg_lowpart_p (op1)
+      && (GET_MODE_SIZE (GET_MODE (op1))
+	  < GET_MODE_SIZE (GET_MODE (SUBREG_REG (op1)))))
+    {
+      enum machine_mode inner_mode = GET_MODE (SUBREG_REG (op1));
+      rtx tem = gen_lowpart_if_possible (inner_mode, op0);
+
+      record_jump_cond (code, mode, SUBREG_REG (op1),
+			tem ? tem : gen_rtx_SUBREG (inner_mode, op0, 0),
+			reversed_nonequality);
+    }
+
+  /* Hash both operands.  */
+
+  do_not_record = 0;
+  hash_arg_in_memory = 0;
+  op0_hash = HASH (op0, mode);
+  op0_in_memory = hash_arg_in_memory;
+
+  if (do_not_record)
+    return;
+
+  do_not_record = 0;
+  hash_arg_in_memory = 0;
+  op1_hash = HASH (op1, mode);
+  op1_in_memory = hash_arg_in_memory;
+
+  if (do_not_record)
+    return;
+
+  /* Look up both operands.  */
+  op0_elt = lookup (op0, op0_hash, mode);
+  op1_elt = lookup (op1, op1_hash, mode);
+
+  /* If both operands are already equivalent or if they are not in the
+     table but are identical, do nothing.  */
+  if ((op0_elt != 0 && op1_elt != 0
+       && op0_elt->first_same_value == op1_elt->first_same_value)
+      || op0 == op1 || rtx_equal_p (op0, op1))
+    return;
+
+  /* If we aren't setting two things equal all we can do is save this
+     comparison.   Similarly if this is floating-point.  In the latter
+     case, OP1 might be zero and both -0.0 and 0.0 are equal to it.
+     If we record the equality, we might inadvertently delete code
+     whose intent was to change -0 to +0.  */
+
+  if (code != EQ || FLOAT_MODE_P (GET_MODE (op0)))
+    {
+      struct qty_table_elem *ent;
+      int qty;
+
+      /* If we reversed a floating-point comparison, if OP0 is not a
+	 register, or if OP1 is neither a register or constant, we can't
+	 do anything.  */
+
+      if (GET_CODE (op1) != REG)
+	op1 = equiv_constant (op1);
+
+      if ((reversed_nonequality && FLOAT_MODE_P (mode))
+	  || GET_CODE (op0) != REG || op1 == 0)
+	return;
+
+      /* Put OP0 in the hash table if it isn't already.  This gives it a
+	 new quantity number.  */
+      if (op0_elt == 0)
+	{
+	  if (insert_regs (op0, NULL, 0))
+	    {
+	      rehash_using_reg (op0);
+	      op0_hash = HASH (op0, mode);
+
+	      /* If OP0 is contained in OP1, this changes its hash code
+		 as well.  Faster to rehash than to check, except
+		 for the simple case of a constant.  */
+	      if (! CONSTANT_P (op1))
+		op1_hash = HASH (op1,mode);
+	    }
+
+	  op0_elt = insert (op0, NULL, op0_hash, mode);
+	  op0_elt->in_memory = op0_in_memory;
+	}
+
+      qty = REG_QTY (REGNO (op0));
+      ent = &qty_table[qty];
+
+      ent->comparison_code = code;
+      if (GET_CODE (op1) == REG)
+	{
+	  /* Look it up again--in case op0 and op1 are the same.  */
+	  op1_elt = lookup (op1, op1_hash, mode);
+
+	  /* Put OP1 in the hash table so it gets a new quantity number.  */
+	  if (op1_elt == 0)
+	    {
+	      if (insert_regs (op1, NULL, 0))
+		{
+		  rehash_using_reg (op1);
+		  op1_hash = HASH (op1, mode);
+		}
+
+	      op1_elt = insert (op1, NULL, op1_hash, mode);
+	      op1_elt->in_memory = op1_in_memory;
+	    }
+
+	  ent->comparison_const = NULL_RTX;
+	  ent->comparison_qty = REG_QTY (REGNO (op1));
+	}
+      else
+	{
+	  ent->comparison_const = op1;
+	  ent->comparison_qty = -1;
+	}
+
+      return;
+    }
+
+  /* If either side is still missing an equivalence, make it now,
+     then merge the equivalences.  */
+
+  if (op0_elt == 0)
+    {
+      if (insert_regs (op0, NULL, 0))
+	{
+	  rehash_using_reg (op0);
+	  op0_hash = HASH (op0, mode);
+	}
+
+      op0_elt = insert (op0, NULL, op0_hash, mode);
+      op0_elt->in_memory = op0_in_memory;
+    }
+
+  if (op1_elt == 0)
+    {
+      if (insert_regs (op1, NULL, 0))
+	{
+	  rehash_using_reg (op1);
+	  op1_hash = HASH (op1, mode);
+	}
+
+      op1_elt = insert (op1, NULL, op1_hash, mode);
+      op1_elt->in_memory = op1_in_memory;
+    }
+
+  merge_equiv_classes (op0_elt, op1_elt);
+  last_jump_equiv_class = op0_elt;
+}
+
+/* CSE processing for one instruction.
+   First simplify sources and addresses of all assignments
+   in the instruction, using previously-computed equivalents values.
+   Then install the new sources and destinations in the table
+   of available values.
+
+   If LIBCALL_INSN is nonzero, don't record any equivalence made in
+   the insn.  It means that INSN is inside libcall block.  In this
+   case LIBCALL_INSN is the corresponding insn with REG_LIBCALL.  */
+
+/* Data on one SET contained in the instruction.  */
+
+struct set
+{
+  /* The SET rtx itself.  */
+  rtx rtl;
+  /* The SET_SRC of the rtx (the original value, if it is changing).  */
+  rtx src;
+  /* The hash-table element for the SET_SRC of the SET.  */
+  struct table_elt *src_elt;
+  /* Hash value for the SET_SRC.  */
+  unsigned src_hash;
+  /* Hash value for the SET_DEST.  */
+  unsigned dest_hash;
+  /* The SET_DEST, with SUBREG, etc., stripped.  */
+  rtx inner_dest;
+  /* Nonzero if the SET_SRC is in memory.  */
+  char src_in_memory;
+  /* Nonzero if the SET_SRC contains something
+     whose value cannot be predicted and understood.  */
+  char src_volatile;
+  /* Original machine mode, in case it becomes a CONST_INT.
+     The size of this field should match the size of the mode
+     field of struct rtx_def (see rtl.h).  */
+  ENUM_BITFIELD(machine_mode) mode : 8;
+  /* A constant equivalent for SET_SRC, if any.  */
+  rtx src_const;
+  /* Original SET_SRC value used for libcall notes.  */
+  rtx orig_src;
+  /* Hash value of constant equivalent for SET_SRC.  */
+  unsigned src_const_hash;
+  /* Table entry for constant equivalent for SET_SRC, if any.  */
+  struct table_elt *src_const_elt;
+};
+
+static void
+cse_insn (rtx insn, rtx libcall_insn)
+{
+  rtx x = PATTERN (insn);
+  int i;
+  rtx tem;
+  int n_sets = 0;
+
+#ifdef HAVE_cc0
+  /* Records what this insn does to set CC0.  */
+  rtx this_insn_cc0 = 0;
+  enum machine_mode this_insn_cc0_mode = VOIDmode;
+#endif
+
+  rtx src_eqv = 0;
+  struct table_elt *src_eqv_elt = 0;
+  int src_eqv_volatile = 0;
+  int src_eqv_in_memory = 0;
+  unsigned src_eqv_hash = 0;
+
+  struct set *sets = (struct set *) 0;
+
+  this_insn = insn;
+
+  /* Find all the SETs and CLOBBERs in this instruction.
+     Record all the SETs in the array `set' and count them.
+     Also determine whether there is a CLOBBER that invalidates
+     all memory references, or all references at varying addresses.  */
+
+  if (GET_CODE (insn) == CALL_INSN)
+    {
+      for (tem = CALL_INSN_FUNCTION_USAGE (insn); tem; tem = XEXP (tem, 1))
+	{
+	  if (GET_CODE (XEXP (tem, 0)) == CLOBBER)
+	    invalidate (SET_DEST (XEXP (tem, 0)), VOIDmode);
+	  XEXP (tem, 0) = canon_reg (XEXP (tem, 0), insn);
+	}
+    }
+
+  if (GET_CODE (x) == SET)
+    {
+      sets = alloca (sizeof (struct set));
+      sets[0].rtl = x;
+
+      /* Ignore SETs that are unconditional jumps.
+	 They never need cse processing, so this does not hurt.
+	 The reason is not efficiency but rather
+	 so that we can test at the end for instructions
+	 that have been simplified to unconditional jumps
+	 and not be misled by unchanged instructions
+	 that were unconditional jumps to begin with.  */
+      if (SET_DEST (x) == pc_rtx
+	  && GET_CODE (SET_SRC (x)) == LABEL_REF)
+	;
+
+      /* Don't count call-insns, (set (reg 0) (call ...)), as a set.
+	 The hard function value register is used only once, to copy to
+	 someplace else, so it isn't worth cse'ing (and on 80386 is unsafe)!
+	 Ensure we invalidate the destination register.  On the 80386 no
+	 other code would invalidate it since it is a fixed_reg.
+	 We need not check the return of apply_change_group; see canon_reg.  */
+
+      else if (GET_CODE (SET_SRC (x)) == CALL)
+	{
+	  canon_reg (SET_SRC (x), insn);
+	  apply_change_group ();
+	  fold_rtx (SET_SRC (x), insn);
+	  invalidate (SET_DEST (x), VOIDmode);
+	}
+      else
+	n_sets = 1;
+    }
+  else if (GET_CODE (x) == PARALLEL)
+    {
+      int lim = XVECLEN (x, 0);
+
+      sets = alloca (lim * sizeof (struct set));
+
+      /* Find all regs explicitly clobbered in this insn,
+	 and ensure they are not replaced with any other regs
+	 elsewhere in this insn.
+	 When a reg that is clobbered is also used for input,
+	 we should presume that that is for a reason,
+	 and we should not substitute some other register
+	 which is not supposed to be clobbered.
+	 Therefore, this loop cannot be merged into the one below
+	 because a CALL may precede a CLOBBER and refer to the
+	 value clobbered.  We must not let a canonicalization do
+	 anything in that case.  */
+      for (i = 0; i < lim; i++)
+	{
+	  rtx y = XVECEXP (x, 0, i);
+	  if (GET_CODE (y) == CLOBBER)
+	    {
+	      rtx clobbered = XEXP (y, 0);
+
+	      if (GET_CODE (clobbered) == REG
+		  || GET_CODE (clobbered) == SUBREG)
+		invalidate (clobbered, VOIDmode);
+	      else if (GET_CODE (clobbered) == STRICT_LOW_PART
+		       || GET_CODE (clobbered) == ZERO_EXTRACT)
+		invalidate (XEXP (clobbered, 0), GET_MODE (clobbered));
+	    }
+	}
+
+      for (i = 0; i < lim; i++)
+	{
+	  rtx y = XVECEXP (x, 0, i);
+	  if (GET_CODE (y) == SET)
+	    {
+	      /* As above, we ignore unconditional jumps and call-insns and
+		 ignore the result of apply_change_group.  */
+	      if (GET_CODE (SET_SRC (y)) == CALL)
+		{
+		  canon_reg (SET_SRC (y), insn);
+		  apply_change_group ();
+		  fold_rtx (SET_SRC (y), insn);
+		  invalidate (SET_DEST (y), VOIDmode);
+		}
+	      else if (SET_DEST (y) == pc_rtx
+		       && GET_CODE (SET_SRC (y)) == LABEL_REF)
+		;
+	      else
+		sets[n_sets++].rtl = y;
+	    }
+	  else if (GET_CODE (y) == CLOBBER)
+	    {
+	      /* If we clobber memory, canon the address.
+		 This does nothing when a register is clobbered
+		 because we have already invalidated the reg.  */
+	      if (GET_CODE (XEXP (y, 0)) == MEM)
+		canon_reg (XEXP (y, 0), NULL_RTX);
+	    }
+	  else if (GET_CODE (y) == USE
+		   && ! (GET_CODE (XEXP (y, 0)) == REG
+			 && REGNO (XEXP (y, 0)) < FIRST_PSEUDO_REGISTER))
+	    canon_reg (y, NULL_RTX);
+	  else if (GET_CODE (y) == CALL)
+	    {
+	      /* The result of apply_change_group can be ignored; see
+		 canon_reg.  */
+	      canon_reg (y, insn);
+	      apply_change_group ();
+	      fold_rtx (y, insn);
+	    }
+	}
+    }
+  else if (GET_CODE (x) == CLOBBER)
+    {
+      if (GET_CODE (XEXP (x, 0)) == MEM)
+	canon_reg (XEXP (x, 0), NULL_RTX);
+    }
+
+  /* Canonicalize a USE of a pseudo register or memory location.  */
+  else if (GET_CODE (x) == USE
+	   && ! (GET_CODE (XEXP (x, 0)) == REG
+		 && REGNO (XEXP (x, 0)) < FIRST_PSEUDO_REGISTER))
+    canon_reg (XEXP (x, 0), NULL_RTX);
+  else if (GET_CODE (x) == CALL)
+    {
+      /* The result of apply_change_group can be ignored; see canon_reg.  */
+      canon_reg (x, insn);
+      apply_change_group ();
+      fold_rtx (x, insn);
+    }
+
+  /* Store the equivalent value in SRC_EQV, if different, or if the DEST
+     is a STRICT_LOW_PART.  The latter condition is necessary because SRC_EQV
+     is handled specially for this case, and if it isn't set, then there will
+     be no equivalence for the destination.  */
+  if (n_sets == 1 && REG_NOTES (insn) != 0
+      && (tem = find_reg_note (insn, REG_EQUAL, NULL_RTX)) != 0
+      && (! rtx_equal_p (XEXP (tem, 0), SET_SRC (sets[0].rtl))
+	  || GET_CODE (SET_DEST (sets[0].rtl)) == STRICT_LOW_PART))
+    {
+      src_eqv = fold_rtx (canon_reg (XEXP (tem, 0), NULL_RTX), insn);
+      XEXP (tem, 0) = src_eqv;
+    }
+
+  /* Canonicalize sources and addresses of destinations.
+     We do this in a separate pass to avoid problems when a MATCH_DUP is
+     present in the insn pattern.  In that case, we want to ensure that
+     we don't break the duplicate nature of the pattern.  So we will replace
+     both operands at the same time.  Otherwise, we would fail to find an
+     equivalent substitution in the loop calling validate_change below.
+
+     We used to suppress canonicalization of DEST if it appears in SRC,
+     but we don't do this any more.  */
+
+  for (i = 0; i < n_sets; i++)
+    {
+      rtx dest = SET_DEST (sets[i].rtl);
+      rtx src = SET_SRC (sets[i].rtl);
+      rtx new = canon_reg (src, insn);
+      int insn_code;
+
+      sets[i].orig_src = src;
+      if ((GET_CODE (new) == REG && GET_CODE (src) == REG
+	   && ((REGNO (new) < FIRST_PSEUDO_REGISTER)
+	       != (REGNO (src) < FIRST_PSEUDO_REGISTER)))
+	  || (insn_code = recog_memoized (insn)) < 0
+	  || insn_data[insn_code].n_dups > 0)
+	validate_change (insn, &SET_SRC (sets[i].rtl), new, 1);
+      else
+	SET_SRC (sets[i].rtl) = new;
+
+      if (GET_CODE (dest) == ZERO_EXTRACT || GET_CODE (dest) == SIGN_EXTRACT)
+	{
+	  validate_change (insn, &XEXP (dest, 1),
+			   canon_reg (XEXP (dest, 1), insn), 1);
+	  validate_change (insn, &XEXP (dest, 2),
+			   canon_reg (XEXP (dest, 2), insn), 1);
+	}
+
+      while (GET_CODE (dest) == SUBREG || GET_CODE (dest) == STRICT_LOW_PART
+	     || GET_CODE (dest) == ZERO_EXTRACT
+	     || GET_CODE (dest) == SIGN_EXTRACT)
+	dest = XEXP (dest, 0);
+
+      if (GET_CODE (dest) == MEM)
+	canon_reg (dest, insn);
+    }
+
+  /* Now that we have done all the replacements, we can apply the change
+     group and see if they all work.  Note that this will cause some
+     canonicalizations that would have worked individually not to be applied
+     because some other canonicalization didn't work, but this should not
+     occur often.
+
+     The result of apply_change_group can be ignored; see canon_reg.  */
+
+  apply_change_group ();
+
+  /* Set sets[i].src_elt to the class each source belongs to.
+     Detect assignments from or to volatile things
+     and set set[i] to zero so they will be ignored
+     in the rest of this function.
+
+     Nothing in this loop changes the hash table or the register chains.  */
+
+  for (i = 0; i < n_sets; i++)
+    {
+      rtx src, dest;
+      rtx src_folded;
+      struct table_elt *elt = 0, *p;
+      enum machine_mode mode;
+      rtx src_eqv_here;
+      rtx src_const = 0;
+      rtx src_related = 0;
+      struct table_elt *src_const_elt = 0;
+      int src_cost = MAX_COST;
+      int src_eqv_cost = MAX_COST;
+      int src_folded_cost = MAX_COST;
+      int src_related_cost = MAX_COST;
+      int src_elt_cost = MAX_COST;
+      int src_regcost = MAX_COST;
+      int src_eqv_regcost = MAX_COST;
+      int src_folded_regcost = MAX_COST;
+      int src_related_regcost = MAX_COST;
+      int src_elt_regcost = MAX_COST;
+      /* Set nonzero if we need to call force_const_mem on with the
+	 contents of src_folded before using it.  */
+      int src_folded_force_flag = 0;
+
+      dest = SET_DEST (sets[i].rtl);
+      src = SET_SRC (sets[i].rtl);
+
+      /* If SRC is a constant that has no machine mode,
+	 hash it with the destination's machine mode.
+	 This way we can keep different modes separate.  */
+
+      mode = GET_MODE (src) == VOIDmode ? GET_MODE (dest) : GET_MODE (src);
+      sets[i].mode = mode;
+
+      if (src_eqv)
+	{
+	  enum machine_mode eqvmode = mode;
+	  if (GET_CODE (dest) == STRICT_LOW_PART)
+	    eqvmode = GET_MODE (SUBREG_REG (XEXP (dest, 0)));
+	  do_not_record = 0;
+	  hash_arg_in_memory = 0;
+	  src_eqv_hash = HASH (src_eqv, eqvmode);
+
+	  /* Find the equivalence class for the equivalent expression.  */
+
+	  if (!do_not_record)
+	    src_eqv_elt = lookup (src_eqv, src_eqv_hash, eqvmode);
+
+	  src_eqv_volatile = do_not_record;
+	  src_eqv_in_memory = hash_arg_in_memory;
+	}
+
+      /* If this is a STRICT_LOW_PART assignment, src_eqv corresponds to the
+	 value of the INNER register, not the destination.  So it is not
+	 a valid substitution for the source.  But save it for later.  */
+      if (GET_CODE (dest) == STRICT_LOW_PART)
+	src_eqv_here = 0;
+      else
+	src_eqv_here = src_eqv;
+
+      /* Simplify and foldable subexpressions in SRC.  Then get the fully-
+	 simplified result, which may not necessarily be valid.  */
+      src_folded = fold_rtx (src, insn);
+
+#if 0
+      /* ??? This caused bad code to be generated for the m68k port with -O2.
+	 Suppose src is (CONST_INT -1), and that after truncation src_folded
+	 is (CONST_INT 3).  Suppose src_folded is then used for src_const.
+	 At the end we will add src and src_const to the same equivalence
+	 class.  We now have 3 and -1 on the same equivalence class.  This
+	 causes later instructions to be mis-optimized.  */
+      /* If storing a constant in a bitfield, pre-truncate the constant
+	 so we will be able to record it later.  */
+      if (GET_CODE (SET_DEST (sets[i].rtl)) == ZERO_EXTRACT
+	  || GET_CODE (SET_DEST (sets[i].rtl)) == SIGN_EXTRACT)
+	{
+	  rtx width = XEXP (SET_DEST (sets[i].rtl), 1);
+
+	  if (GET_CODE (src) == CONST_INT
+	      && GET_CODE (width) == CONST_INT
+	      && INTVAL (width) < HOST_BITS_PER_WIDE_INT
+	      && (INTVAL (src) & ((HOST_WIDE_INT) (-1) << INTVAL (width))))
+	    src_folded
+	      = GEN_INT (INTVAL (src) & (((HOST_WIDE_INT) 1
+					  << INTVAL (width)) - 1));
+	}
+#endif
+
+      /* Compute SRC's hash code, and also notice if it
+	 should not be recorded at all.  In that case,
+	 prevent any further processing of this assignment.  */
+      do_not_record = 0;
+      hash_arg_in_memory = 0;
+
+      sets[i].src = src;
+      sets[i].src_hash = HASH (src, mode);
+      sets[i].src_volatile = do_not_record;
+      sets[i].src_in_memory = hash_arg_in_memory;
+
+      /* If SRC is a MEM, there is a REG_EQUIV note for SRC, and DEST is
+	 a pseudo, do not record SRC.  Using SRC as a replacement for
+	 anything else will be incorrect in that situation.  Note that
+	 this usually occurs only for stack slots, in which case all the
+	 RTL would be referring to SRC, so we don't lose any optimization
+	 opportunities by not having SRC in the hash table.  */
+
+      if (GET_CODE (src) == MEM
+	  && find_reg_note (insn, REG_EQUIV, NULL_RTX) != 0
+	  && GET_CODE (dest) == REG
+	  && REGNO (dest) >= FIRST_PSEUDO_REGISTER)
+	sets[i].src_volatile = 1;
+
+#if 0
+      /* It is no longer clear why we used to do this, but it doesn't
+	 appear to still be needed.  So let's try without it since this
+	 code hurts cse'ing widened ops.  */
+      /* If source is a perverse subreg (such as QI treated as an SI),
+	 treat it as volatile.  It may do the work of an SI in one context
+	 where the extra bits are not being used, but cannot replace an SI
+	 in general.  */
+      if (GET_CODE (src) == SUBREG
+	  && (GET_MODE_SIZE (GET_MODE (src))
+	      > GET_MODE_SIZE (GET_MODE (SUBREG_REG (src)))))
+	sets[i].src_volatile = 1;
+#endif
+
+      /* Locate all possible equivalent forms for SRC.  Try to replace
+         SRC in the insn with each cheaper equivalent.
+
+         We have the following types of equivalents: SRC itself, a folded
+         version, a value given in a REG_EQUAL note, or a value related
+	 to a constant.
+
+         Each of these equivalents may be part of an additional class
+         of equivalents (if more than one is in the table, they must be in
+         the same class; we check for this).
+
+	 If the source is volatile, we don't do any table lookups.
+
+         We note any constant equivalent for possible later use in a
+         REG_NOTE.  */
+
+      if (!sets[i].src_volatile)
+	elt = lookup (src, sets[i].src_hash, mode);
+
+      sets[i].src_elt = elt;
+
+      if (elt && src_eqv_here && src_eqv_elt)
+	{
+	  if (elt->first_same_value != src_eqv_elt->first_same_value)
+	    {
+	      /* The REG_EQUAL is indicating that two formerly distinct
+		 classes are now equivalent.  So merge them.  */
+	      merge_equiv_classes (elt, src_eqv_elt);
+	      src_eqv_hash = HASH (src_eqv, elt->mode);
+	      src_eqv_elt = lookup (src_eqv, src_eqv_hash, elt->mode);
+	    }
+
+	  src_eqv_here = 0;
+	}
+
+      else if (src_eqv_elt)
+	elt = src_eqv_elt;
+
+      /* Try to find a constant somewhere and record it in `src_const'.
+	 Record its table element, if any, in `src_const_elt'.  Look in
+	 any known equivalences first.  (If the constant is not in the
+	 table, also set `sets[i].src_const_hash').  */
+      if (elt)
+	for (p = elt->first_same_value; p; p = p->next_same_value)
+	  if (p->is_const)
+	    {
+	      src_const = p->exp;
+	      src_const_elt = elt;
+	      break;
+	    }
+
+      if (src_const == 0
+	  && (CONSTANT_P (src_folded)
+	      /* Consider (minus (label_ref L1) (label_ref L2)) as
+		 "constant" here so we will record it. This allows us
+		 to fold switch statements when an ADDR_DIFF_VEC is used.  */
+	      || (GET_CODE (src_folded) == MINUS
+		  && GET_CODE (XEXP (src_folded, 0)) == LABEL_REF
+		  && GET_CODE (XEXP (src_folded, 1)) == LABEL_REF)))
+	src_const = src_folded, src_const_elt = elt;
+      else if (src_const == 0 && src_eqv_here && CONSTANT_P (src_eqv_here))
+	src_const = src_eqv_here, src_const_elt = src_eqv_elt;
+
+      /* If we don't know if the constant is in the table, get its
+	 hash code and look it up.  */
+      if (src_const && src_const_elt == 0)
+	{
+	  sets[i].src_const_hash = HASH (src_const, mode);
+	  src_const_elt = lookup (src_const, sets[i].src_const_hash, mode);
+	}
+
+      sets[i].src_const = src_const;
+      sets[i].src_const_elt = src_const_elt;
+
+      /* If the constant and our source are both in the table, mark them as
+	 equivalent.  Otherwise, if a constant is in the table but the source
+	 isn't, set ELT to it.  */
+      if (src_const_elt && elt
+	  && src_const_elt->first_same_value != elt->first_same_value)
+	merge_equiv_classes (elt, src_const_elt);
+      else if (src_const_elt && elt == 0)
+	elt = src_const_elt;
+
+      /* See if there is a register linearly related to a constant
+         equivalent of SRC.  */
+      if (src_const
+	  && (GET_CODE (src_const) == CONST
+	      || (src_const_elt && src_const_elt->related_value != 0)))
+	{
+	  src_related = use_related_value (src_const, src_const_elt);
+	  if (src_related)
+	    {
+	      struct table_elt *src_related_elt
+		= lookup (src_related, HASH (src_related, mode), mode);
+	      if (src_related_elt && elt)
+		{
+		  if (elt->first_same_value
+		      != src_related_elt->first_same_value)
+		    /* This can occur when we previously saw a CONST
+		       involving a SYMBOL_REF and then see the SYMBOL_REF
+		       twice.  Merge the involved classes.  */
+		    merge_equiv_classes (elt, src_related_elt);
+
+		  src_related = 0;
+		  src_related_elt = 0;
+		}
+	      else if (src_related_elt && elt == 0)
+		elt = src_related_elt;
+	    }
+	}
+
+      /* See if we have a CONST_INT that is already in a register in a
+	 wider mode.  */
+
+      if (src_const && src_related == 0 && GET_CODE (src_const) == CONST_INT
+	  && GET_MODE_CLASS (mode) == MODE_INT
+	  && GET_MODE_BITSIZE (mode) < BITS_PER_WORD)
+	{
+	  enum machine_mode wider_mode;
+
+	  for (wider_mode = GET_MODE_WIDER_MODE (mode);
+	       GET_MODE_BITSIZE (wider_mode) <= BITS_PER_WORD
+	       && src_related == 0;
+	       wider_mode = GET_MODE_WIDER_MODE (wider_mode))
+	    {
+	      struct table_elt *const_elt
+		= lookup (src_const, HASH (src_const, wider_mode), wider_mode);
+
+	      if (const_elt == 0)
+		continue;
+
+	      for (const_elt = const_elt->first_same_value;
+		   const_elt; const_elt = const_elt->next_same_value)
+		if (GET_CODE (const_elt->exp) == REG)
+		  {
+		    src_related = gen_lowpart_if_possible (mode,
+							   const_elt->exp);
+		    break;
+		  }
+	    }
+	}
+
+      /* Another possibility is that we have an AND with a constant in
+	 a mode narrower than a word.  If so, it might have been generated
+	 as part of an "if" which would narrow the AND.  If we already
+	 have done the AND in a wider mode, we can use a SUBREG of that
+	 value.  */
+
+      if (flag_expensive_optimizations && ! src_related
+	  && GET_CODE (src) == AND && GET_CODE (XEXP (src, 1)) == CONST_INT
+	  && GET_MODE_SIZE (mode) < UNITS_PER_WORD)
+	{
+	  enum machine_mode tmode;
+	  rtx new_and = gen_rtx_AND (VOIDmode, NULL_RTX, XEXP (src, 1));
+
+	  for (tmode = GET_MODE_WIDER_MODE (mode);
+	       GET_MODE_SIZE (tmode) <= UNITS_PER_WORD;
+	       tmode = GET_MODE_WIDER_MODE (tmode))
+	    {
+	      rtx inner = gen_lowpart_if_possible (tmode, XEXP (src, 0));
+	      struct table_elt *larger_elt;
+
+	      if (inner)
+		{
+		  PUT_MODE (new_and, tmode);
+		  XEXP (new_and, 0) = inner;
+		  larger_elt = lookup (new_and, HASH (new_and, tmode), tmode);
+		  if (larger_elt == 0)
+		    continue;
+
+		  for (larger_elt = larger_elt->first_same_value;
+		       larger_elt; larger_elt = larger_elt->next_same_value)
+		    if (GET_CODE (larger_elt->exp) == REG)
+		      {
+			src_related
+			  = gen_lowpart_if_possible (mode, larger_elt->exp);
+			break;
+		      }
+
+		  if (src_related)
+		    break;
+		}
+	    }
+	}
+
+#ifdef LOAD_EXTEND_OP
+      /* See if a MEM has already been loaded with a widening operation;
+	 if it has, we can use a subreg of that.  Many CISC machines
+	 also have such operations, but this is only likely to be
+	 beneficial these machines.  */
+
+      if (flag_expensive_optimizations && src_related == 0
+	  && (GET_MODE_SIZE (mode) < UNITS_PER_WORD)
+	  && GET_MODE_CLASS (mode) == MODE_INT
+	  && GET_CODE (src) == MEM && ! do_not_record
+	  && LOAD_EXTEND_OP (mode) != NIL)
+	{
+	  enum machine_mode tmode;
+
+	  /* Set what we are trying to extend and the operation it might
+	     have been extended with.  */
+	  PUT_CODE (memory_extend_rtx, LOAD_EXTEND_OP (mode));
+	  XEXP (memory_extend_rtx, 0) = src;
+
+	  for (tmode = GET_MODE_WIDER_MODE (mode);
+	       GET_MODE_SIZE (tmode) <= UNITS_PER_WORD;
+	       tmode = GET_MODE_WIDER_MODE (tmode))
+	    {
+	      struct table_elt *larger_elt;
+
+	      PUT_MODE (memory_extend_rtx, tmode);
+	      larger_elt = lookup (memory_extend_rtx,
+				   HASH (memory_extend_rtx, tmode), tmode);
+	      if (larger_elt == 0)
+		continue;
+
+	      for (larger_elt = larger_elt->first_same_value;
+		   larger_elt; larger_elt = larger_elt->next_same_value)
+		if (GET_CODE (larger_elt->exp) == REG)
+		  {
+		    src_related = gen_lowpart_if_possible (mode,
+							   larger_elt->exp);
+		    break;
+		  }
+
+	      if (src_related)
+		break;
+	    }
+	}
+#endif /* LOAD_EXTEND_OP */
+
+      if (src == src_folded)
+	src_folded = 0;
+
+      /* At this point, ELT, if nonzero, points to a class of expressions
+         equivalent to the source of this SET and SRC, SRC_EQV, SRC_FOLDED,
+	 and SRC_RELATED, if nonzero, each contain additional equivalent
+	 expressions.  Prune these latter expressions by deleting expressions
+	 already in the equivalence class.
+
+	 Check for an equivalent identical to the destination.  If found,
+	 this is the preferred equivalent since it will likely lead to
+	 elimination of the insn.  Indicate this by placing it in
+	 `src_related'.  */
+
+      if (elt)
+	elt = elt->first_same_value;
+      for (p = elt; p; p = p->next_same_value)
+	{
+	  enum rtx_code code = GET_CODE (p->exp);
+
+	  /* If the expression is not valid, ignore it.  Then we do not
+	     have to check for validity below.  In most cases, we can use
+	     `rtx_equal_p', since canonicalization has already been done.  */
+	  if (code != REG && ! exp_equiv_p (p->exp, p->exp, 1, 0))
+	    continue;
+
+	  /* Also skip paradoxical subregs, unless that's what we're
+	     looking for.  */
+	  if (code == SUBREG
+	      && (GET_MODE_SIZE (GET_MODE (p->exp))
+		  > GET_MODE_SIZE (GET_MODE (SUBREG_REG (p->exp))))
+	      && ! (src != 0
+		    && GET_CODE (src) == SUBREG
+		    && GET_MODE (src) == GET_MODE (p->exp)
+		    && (GET_MODE_SIZE (GET_MODE (SUBREG_REG (src)))
+			< GET_MODE_SIZE (GET_MODE (SUBREG_REG (p->exp))))))
+	    continue;
+
+	  if (src && GET_CODE (src) == code && rtx_equal_p (src, p->exp))
+	    src = 0;
+	  else if (src_folded && GET_CODE (src_folded) == code
+		   && rtx_equal_p (src_folded, p->exp))
+	    src_folded = 0;
+	  else if (src_eqv_here && GET_CODE (src_eqv_here) == code
+		   && rtx_equal_p (src_eqv_here, p->exp))
+	    src_eqv_here = 0;
+	  else if (src_related && GET_CODE (src_related) == code
+		   && rtx_equal_p (src_related, p->exp))
+	    src_related = 0;
+
+	  /* This is the same as the destination of the insns, we want
+	     to prefer it.  Copy it to src_related.  The code below will
+	     then give it a negative cost.  */
+	  if (GET_CODE (dest) == code && rtx_equal_p (p->exp, dest))
+	    src_related = dest;
+	}
+
+      /* Find the cheapest valid equivalent, trying all the available
+         possibilities.  Prefer items not in the hash table to ones
+         that are when they are equal cost.  Note that we can never
+         worsen an insn as the current contents will also succeed.
+	 If we find an equivalent identical to the destination, use it as best,
+	 since this insn will probably be eliminated in that case.  */
+      if (src)
+	{
+	  if (rtx_equal_p (src, dest))
+	    src_cost = src_regcost = -1;
+	  else
+	    {
+	      src_cost = COST (src);
+	      src_regcost = approx_reg_cost (src);
+	    }
+	}
+
+      if (src_eqv_here)
+	{
+	  if (rtx_equal_p (src_eqv_here, dest))
+	    src_eqv_cost = src_eqv_regcost = -1;
+	  else
+	    {
+	      src_eqv_cost = COST (src_eqv_here);
+	      src_eqv_regcost = approx_reg_cost (src_eqv_here);
+	    }
+	}
+
+      if (src_folded)
+	{
+	  if (rtx_equal_p (src_folded, dest))
+	    src_folded_cost = src_folded_regcost = -1;
+	  else
+	    {
+	      src_folded_cost = COST (src_folded);
+	      src_folded_regcost = approx_reg_cost (src_folded);
+	    }
+	}
+
+      if (src_related)
+	{
+	  if (rtx_equal_p (src_related, dest))
+	    src_related_cost = src_related_regcost = -1;
+	  else
+	    {
+	      src_related_cost = COST (src_related);
+	      src_related_regcost = approx_reg_cost (src_related);
+	    }
+	}
+
+      /* If this was an indirect jump insn, a known label will really be
+	 cheaper even though it looks more expensive.  */
+      if (dest == pc_rtx && src_const && GET_CODE (src_const) == LABEL_REF)
+	src_folded = src_const, src_folded_cost = src_folded_regcost = -1;
+
+      /* Terminate loop when replacement made.  This must terminate since
+         the current contents will be tested and will always be valid.  */
+      while (1)
+	{
+	  rtx trial;
+
+	  /* Skip invalid entries.  */
+	  while (elt && GET_CODE (elt->exp) != REG
+		 && ! exp_equiv_p (elt->exp, elt->exp, 1, 0))
+	    elt = elt->next_same_value;
+
+	  /* A paradoxical subreg would be bad here: it'll be the right
+	     size, but later may be adjusted so that the upper bits aren't
+	     what we want.  So reject it.  */
+	  if (elt != 0
+	      && GET_CODE (elt->exp) == SUBREG
+	      && (GET_MODE_SIZE (GET_MODE (elt->exp))
+		  > GET_MODE_SIZE (GET_MODE (SUBREG_REG (elt->exp))))
+	      /* It is okay, though, if the rtx we're trying to match
+		 will ignore any of the bits we can't predict.  */
+	      && ! (src != 0
+		    && GET_CODE (src) == SUBREG
+		    && GET_MODE (src) == GET_MODE (elt->exp)
+		    && (GET_MODE_SIZE (GET_MODE (SUBREG_REG (src)))
+			< GET_MODE_SIZE (GET_MODE (SUBREG_REG (elt->exp))))))
+	    {
+	      elt = elt->next_same_value;
+	      continue;
+	    }
+
+	  if (elt)
+	    {
+	      src_elt_cost = elt->cost;
+	      src_elt_regcost = elt->regcost;
+	    }
+
+	  /* Find cheapest and skip it for the next time.   For items
+	     of equal cost, use this order:
+	     src_folded, src, src_eqv, src_related and hash table entry.  */
+	  if (src_folded
+	      && preferrable (src_folded_cost, src_folded_regcost,
+			      src_cost, src_regcost) <= 0
+	      && preferrable (src_folded_cost, src_folded_regcost,
+			      src_eqv_cost, src_eqv_regcost) <= 0
+	      && preferrable (src_folded_cost, src_folded_regcost,
+			      src_related_cost, src_related_regcost) <= 0
+	      && preferrable (src_folded_cost, src_folded_regcost,
+			      src_elt_cost, src_elt_regcost) <= 0)
+	    {
+	      trial = src_folded, src_folded_cost = MAX_COST;
+	      if (src_folded_force_flag)
+		{
+		  rtx forced = force_const_mem (mode, trial);
+		  if (forced)
+		    trial = forced;
+		}
+	    }
+	  else if (src
+		   && preferrable (src_cost, src_regcost,
+				   src_eqv_cost, src_eqv_regcost) <= 0
+		   && preferrable (src_cost, src_regcost,
+				   src_related_cost, src_related_regcost) <= 0
+		   && preferrable (src_cost, src_regcost,
+				   src_elt_cost, src_elt_regcost) <= 0)
+	    trial = src, src_cost = MAX_COST;
+	  else if (src_eqv_here
+		   && preferrable (src_eqv_cost, src_eqv_regcost,
+				   src_related_cost, src_related_regcost) <= 0
+		   && preferrable (src_eqv_cost, src_eqv_regcost,
+				   src_elt_cost, src_elt_regcost) <= 0)
+	    trial = copy_rtx (src_eqv_here), src_eqv_cost = MAX_COST;
+	  else if (src_related
+		   && preferrable (src_related_cost, src_related_regcost,
+				   src_elt_cost, src_elt_regcost) <= 0)
+	    trial = copy_rtx (src_related), src_related_cost = MAX_COST;
+	  else
+	    {
+	      trial = copy_rtx (elt->exp);
+	      elt = elt->next_same_value;
+	      src_elt_cost = MAX_COST;
+	    }
+
+	  /* We don't normally have an insn matching (set (pc) (pc)), so
+	     check for this separately here.  We will delete such an
+	     insn below.
+
+	     For other cases such as a table jump or conditional jump
+	     where we know the ultimate target, go ahead and replace the
+	     operand.  While that may not make a valid insn, we will
+	     reemit the jump below (and also insert any necessary
+	     barriers).  */
+	  if (n_sets == 1 && dest == pc_rtx
+	      && (trial == pc_rtx
+		  || (GET_CODE (trial) == LABEL_REF
+		      && ! condjump_p (insn))))
+	    {
+	      SET_SRC (sets[i].rtl) = trial;
+	      cse_jumps_altered = 1;
+	      break;
+	    }
+
+	  /* Look for a substitution that makes a valid insn.  */
+	  else if (validate_change (insn, &SET_SRC (sets[i].rtl), trial, 0))
+	    {
+	      rtx new = canon_reg (SET_SRC (sets[i].rtl), insn);
+
+	      /* If we just made a substitution inside a libcall, then we
+		 need to make the same substitution in any notes attached
+		 to the RETVAL insn.  */
+	      if (libcall_insn
+		  && (GET_CODE (sets[i].orig_src) == REG
+		      || GET_CODE (sets[i].orig_src) == SUBREG
+		      || GET_CODE (sets[i].orig_src) == MEM))
+		simplify_replace_rtx (REG_NOTES (libcall_insn),
+				      sets[i].orig_src, copy_rtx (new));
+
+	      /* The result of apply_change_group can be ignored; see
+		 canon_reg.  */
+
+	      validate_change (insn, &SET_SRC (sets[i].rtl), new, 1);
+	      apply_change_group ();
+	      break;
+	    }
+
+	  /* If we previously found constant pool entries for
+	     constants and this is a constant, try making a
+	     pool entry.  Put it in src_folded unless we already have done
+	     this since that is where it likely came from.  */
+
+	  else if (constant_pool_entries_cost
+		   && CONSTANT_P (trial)
+		   /* Reject cases that will abort in decode_rtx_const.
+		      On the alpha when simplifying a switch, we get
+		      (const (truncate (minus (label_ref) (label_ref)))).  */
+		   && ! (GET_CODE (trial) == CONST
+			 && GET_CODE (XEXP (trial, 0)) == TRUNCATE)
+		   /* Likewise on IA-64, except without the truncate.  */
+		   && ! (GET_CODE (trial) == CONST
+			 && GET_CODE (XEXP (trial, 0)) == MINUS
+			 && GET_CODE (XEXP (XEXP (trial, 0), 0)) == LABEL_REF
+			 && GET_CODE (XEXP (XEXP (trial, 0), 1)) == LABEL_REF)
+		   && (src_folded == 0
+		       || (GET_CODE (src_folded) != MEM
+			   && ! src_folded_force_flag))
+		   && GET_MODE_CLASS (mode) != MODE_CC
+		   && mode != VOIDmode)
+	    {
+	      src_folded_force_flag = 1;
+	      src_folded = trial;
+	      src_folded_cost = constant_pool_entries_cost;
+	      src_folded_regcost = constant_pool_entries_regcost;
+	    }
+	}
+
+      src = SET_SRC (sets[i].rtl);
+
+      /* In general, it is good to have a SET with SET_SRC == SET_DEST.
+	 However, there is an important exception:  If both are registers
+	 that are not the head of their equivalence class, replace SET_SRC
+	 with the head of the class.  If we do not do this, we will have
+	 both registers live over a portion of the basic block.  This way,
+	 their lifetimes will likely abut instead of overlapping.  */
+      if (GET_CODE (dest) == REG
+	  && REGNO_QTY_VALID_P (REGNO (dest)))
+	{
+	  int dest_q = REG_QTY (REGNO (dest));
+	  struct qty_table_elem *dest_ent = &qty_table[dest_q];
+
+	  if (dest_ent->mode == GET_MODE (dest)
+	      && dest_ent->first_reg != REGNO (dest)
+	      && GET_CODE (src) == REG && REGNO (src) == REGNO (dest)
+	      /* Don't do this if the original insn had a hard reg as
+		 SET_SRC or SET_DEST.  */
+	      && (GET_CODE (sets[i].src) != REG
+		  || REGNO (sets[i].src) >= FIRST_PSEUDO_REGISTER)
+	      && (GET_CODE (dest) != REG || REGNO (dest) >= FIRST_PSEUDO_REGISTER))
+	    /* We can't call canon_reg here because it won't do anything if
+	       SRC is a hard register.  */
+	    {
+	      int src_q = REG_QTY (REGNO (src));
+	      struct qty_table_elem *src_ent = &qty_table[src_q];
+	      int first = src_ent->first_reg;
+	      rtx new_src
+		= (first >= FIRST_PSEUDO_REGISTER
+		   ? regno_reg_rtx[first] : gen_rtx_REG (GET_MODE (src), first));
+
+	      /* We must use validate-change even for this, because this
+		 might be a special no-op instruction, suitable only to
+		 tag notes onto.  */
+	      if (validate_change (insn, &SET_SRC (sets[i].rtl), new_src, 0))
+		{
+		  src = new_src;
+		  /* If we had a constant that is cheaper than what we are now
+		     setting SRC to, use that constant.  We ignored it when we
+		     thought we could make this into a no-op.  */
+		  if (src_const && COST (src_const) < COST (src)
+		      && validate_change (insn, &SET_SRC (sets[i].rtl),
+					  src_const, 0))
+		    src = src_const;
+		}
+	    }
+	}
+
+      /* If we made a change, recompute SRC values.  */
+      if (src != sets[i].src)
+	{
+	  cse_altered = 1;
+	  do_not_record = 0;
+	  hash_arg_in_memory = 0;
+	  sets[i].src = src;
+	  sets[i].src_hash = HASH (src, mode);
+	  sets[i].src_volatile = do_not_record;
+	  sets[i].src_in_memory = hash_arg_in_memory;
+	  sets[i].src_elt = lookup (src, sets[i].src_hash, mode);
+	}
+
+      /* If this is a single SET, we are setting a register, and we have an
+	 equivalent constant, we want to add a REG_NOTE.   We don't want
+	 to write a REG_EQUAL note for a constant pseudo since verifying that
+	 that pseudo hasn't been eliminated is a pain.  Such a note also
+	 won't help anything.
+
+	 Avoid a REG_EQUAL note for (CONST (MINUS (LABEL_REF) (LABEL_REF)))
+	 which can be created for a reference to a compile time computable
+	 entry in a jump table.  */
+
+      if (n_sets == 1 && src_const && GET_CODE (dest) == REG
+	  && GET_CODE (src_const) != REG
+	  && ! (GET_CODE (src_const) == CONST
+		&& GET_CODE (XEXP (src_const, 0)) == MINUS
+		&& GET_CODE (XEXP (XEXP (src_const, 0), 0)) == LABEL_REF
+		&& GET_CODE (XEXP (XEXP (src_const, 0), 1)) == LABEL_REF))
+	{
+	  /* We only want a REG_EQUAL note if src_const != src.  */
+	  if (! rtx_equal_p (src, src_const))
+	    {
+	      /* Make sure that the rtx is not shared.  */
+	      src_const = copy_rtx (src_const);
+
+	      /* Record the actual constant value in a REG_EQUAL note,
+		 making a new one if one does not already exist.  */
+	      set_unique_reg_note (insn, REG_EQUAL, src_const);
+	    }
+	}
+
+      /* Now deal with the destination.  */
+      do_not_record = 0;
+
+      /* Look within any SIGN_EXTRACT or ZERO_EXTRACT
+	 to the MEM or REG within it.  */
+      while (GET_CODE (dest) == SIGN_EXTRACT
+	     || GET_CODE (dest) == ZERO_EXTRACT
+	     || GET_CODE (dest) == SUBREG
+	     || GET_CODE (dest) == STRICT_LOW_PART)
+	dest = XEXP (dest, 0);
+
+      sets[i].inner_dest = dest;
+
+      if (GET_CODE (dest) == MEM)
+	{
+#ifdef PUSH_ROUNDING
+	  /* Stack pushes invalidate the stack pointer.  */
+	  rtx addr = XEXP (dest, 0);
+	  if (GET_RTX_CLASS (GET_CODE (addr)) == 'a'
+	      && XEXP (addr, 0) == stack_pointer_rtx)
+	    invalidate (stack_pointer_rtx, Pmode);
+#endif
+	  dest = fold_rtx (dest, insn);
+	}
+
+      /* Compute the hash code of the destination now,
+	 before the effects of this instruction are recorded,
+	 since the register values used in the address computation
+	 are those before this instruction.  */
+      sets[i].dest_hash = HASH (dest, mode);
+
+      /* Don't enter a bit-field in the hash table
+	 because the value in it after the store
+	 may not equal what was stored, due to truncation.  */
+
+      if (GET_CODE (SET_DEST (sets[i].rtl)) == ZERO_EXTRACT
+	  || GET_CODE (SET_DEST (sets[i].rtl)) == SIGN_EXTRACT)
+	{
+	  rtx width = XEXP (SET_DEST (sets[i].rtl), 1);
+
+	  if (src_const != 0 && GET_CODE (src_const) == CONST_INT
+	      && GET_CODE (width) == CONST_INT
+	      && INTVAL (width) < HOST_BITS_PER_WIDE_INT
+	      && ! (INTVAL (src_const)
+		    & ((HOST_WIDE_INT) (-1) << INTVAL (width))))
+	    /* Exception: if the value is constant,
+	       and it won't be truncated, record it.  */
+	    ;
+	  else
+	    {
+	      /* This is chosen so that the destination will be invalidated
+		 but no new value will be recorded.
+		 We must invalidate because sometimes constant
+		 values can be recorded for bitfields.  */
+	      sets[i].src_elt = 0;
+	      sets[i].src_volatile = 1;
+	      src_eqv = 0;
+	      src_eqv_elt = 0;
+	    }
+	}
+
+      /* If only one set in a JUMP_INSN and it is now a no-op, we can delete
+	 the insn.  */
+      else if (n_sets == 1 && dest == pc_rtx && src == pc_rtx)
+	{
+	  /* One less use of the label this insn used to jump to.  */
+	  delete_insn (insn);
+	  cse_jumps_altered = 1;
+	  /* No more processing for this set.  */
+	  sets[i].rtl = 0;
+	}
+
+      /* If this SET is now setting PC to a label, we know it used to
+	 be a conditional or computed branch.  */
+      else if (dest == pc_rtx && GET_CODE (src) == LABEL_REF)
+	{
+	  /* Now emit a BARRIER after the unconditional jump.  */
+	  if (NEXT_INSN (insn) == 0
+	      || GET_CODE (NEXT_INSN (insn)) != BARRIER)
+	    emit_barrier_after (insn);
+
+	  /* We reemit the jump in as many cases as possible just in
+	     case the form of an unconditional jump is significantly
+	     different than a computed jump or conditional jump.
+
+	     If this insn has multiple sets, then reemitting the
+	     jump is nontrivial.  So instead we just force rerecognition
+	     and hope for the best.  */
+	  if (n_sets == 1)
+	    {
+	      rtx new = emit_jump_insn_after (gen_jump (XEXP (src, 0)), insn);
+
+	      JUMP_LABEL (new) = XEXP (src, 0);
+	      LABEL_NUSES (XEXP (src, 0))++;
+	      delete_insn (insn);
+	      insn = new;
+
+	      /* Now emit a BARRIER after the unconditional jump.  */
+	      if (NEXT_INSN (insn) == 0
+		  || GET_CODE (NEXT_INSN (insn)) != BARRIER)
+		emit_barrier_after (insn);
+	    }
+	  else
+	    INSN_CODE (insn) = -1;
+
+	  never_reached_warning (insn, NULL);
+
+	  /* Do not bother deleting any unreachable code,
+	     let jump/flow do that.  */
+
+	  cse_jumps_altered = 1;
+	  sets[i].rtl = 0;
+	}
+
+      /* If destination is volatile, invalidate it and then do no further
+	 processing for this assignment.  */
+
+      else if (do_not_record)
+	{
+	  if (GET_CODE (dest) == REG || GET_CODE (dest) == SUBREG)
+	    invalidate (dest, VOIDmode);
+	  else if (GET_CODE (dest) == MEM)
+	    {
+	      /* Outgoing arguments for a libcall don't
+		 affect any recorded expressions.  */
+	      if (! libcall_insn || insn == libcall_insn)
+		invalidate (dest, VOIDmode);
+	    }
+	  else if (GET_CODE (dest) == STRICT_LOW_PART
+		   || GET_CODE (dest) == ZERO_EXTRACT)
+	    invalidate (XEXP (dest, 0), GET_MODE (dest));
+	  sets[i].rtl = 0;
+	}
+
+      if (sets[i].rtl != 0 && dest != SET_DEST (sets[i].rtl))
+	sets[i].dest_hash = HASH (SET_DEST (sets[i].rtl), mode);
+
+#ifdef HAVE_cc0
+      /* If setting CC0, record what it was set to, or a constant, if it
+	 is equivalent to a constant.  If it is being set to a floating-point
+	 value, make a COMPARE with the appropriate constant of 0.  If we
+	 don't do this, later code can interpret this as a test against
+	 const0_rtx, which can cause problems if we try to put it into an
+	 insn as a floating-point operand.  */
+      if (dest == cc0_rtx)
+	{
+	  this_insn_cc0 = src_const && mode != VOIDmode ? src_const : src;
+	  this_insn_cc0_mode = mode;
+	  if (FLOAT_MODE_P (mode))
+	    this_insn_cc0 = gen_rtx_COMPARE (VOIDmode, this_insn_cc0,
+					     CONST0_RTX (mode));
+	}
+#endif
+    }
+
+  /* Now enter all non-volatile source expressions in the hash table
+     if they are not already present.
+     Record their equivalence classes in src_elt.
+     This way we can insert the corresponding destinations into
+     the same classes even if the actual sources are no longer in them
+     (having been invalidated).  */
+
+  if (src_eqv && src_eqv_elt == 0 && sets[0].rtl != 0 && ! src_eqv_volatile
+      && ! rtx_equal_p (src_eqv, SET_DEST (sets[0].rtl)))
+    {
+      struct table_elt *elt;
+      struct table_elt *classp = sets[0].src_elt;
+      rtx dest = SET_DEST (sets[0].rtl);
+      enum machine_mode eqvmode = GET_MODE (dest);
+
+      if (GET_CODE (dest) == STRICT_LOW_PART)
+	{
+	  eqvmode = GET_MODE (SUBREG_REG (XEXP (dest, 0)));
+	  classp = 0;
+	}
+      if (insert_regs (src_eqv, classp, 0))
+	{
+	  rehash_using_reg (src_eqv);
+	  src_eqv_hash = HASH (src_eqv, eqvmode);
+	}
+      elt = insert (src_eqv, classp, src_eqv_hash, eqvmode);
+      elt->in_memory = src_eqv_in_memory;
+      src_eqv_elt = elt;
+
+      /* Check to see if src_eqv_elt is the same as a set source which
+	 does not yet have an elt, and if so set the elt of the set source
+	 to src_eqv_elt.  */
+      for (i = 0; i < n_sets; i++)
+	if (sets[i].rtl && sets[i].src_elt == 0
+	    && rtx_equal_p (SET_SRC (sets[i].rtl), src_eqv))
+	  sets[i].src_elt = src_eqv_elt;
+    }
+
+  for (i = 0; i < n_sets; i++)
+    if (sets[i].rtl && ! sets[i].src_volatile
+	&& ! rtx_equal_p (SET_SRC (sets[i].rtl), SET_DEST (sets[i].rtl)))
+      {
+	if (GET_CODE (SET_DEST (sets[i].rtl)) == STRICT_LOW_PART)
+	  {
+	    /* REG_EQUAL in setting a STRICT_LOW_PART
+	       gives an equivalent for the entire destination register,
+	       not just for the subreg being stored in now.
+	       This is a more interesting equivalence, so we arrange later
+	       to treat the entire reg as the destination.  */
+	    sets[i].src_elt = src_eqv_elt;
+	    sets[i].src_hash = src_eqv_hash;
+	  }
+	else
+	  {
+	    /* Insert source and constant equivalent into hash table, if not
+	       already present.  */
+	    struct table_elt *classp = src_eqv_elt;
+	    rtx src = sets[i].src;
+	    rtx dest = SET_DEST (sets[i].rtl);
+	    enum machine_mode mode
+	      = GET_MODE (src) == VOIDmode ? GET_MODE (dest) : GET_MODE (src);
+
+	    /* It's possible that we have a source value known to be
+	       constant but don't have a REG_EQUAL note on the insn.
+	       Lack of a note will mean src_eqv_elt will be NULL.  This
+	       can happen where we've generated a SUBREG to access a
+	       CONST_INT that is already in a register in a wider mode.
+	       Ensure that the source expression is put in the proper
+	       constant class.  */
+	    if (!classp)
+	      classp = sets[i].src_const_elt;
+
+	    if (sets[i].src_elt == 0)
+	      {
+		/* Don't put a hard register source into the table if this is
+		   the last insn of a libcall.  In this case, we only need
+		   to put src_eqv_elt in src_elt.  */
+		if (! find_reg_note (insn, REG_RETVAL, NULL_RTX))
+		  {
+		    struct table_elt *elt;
+
+		    /* Note that these insert_regs calls cannot remove
+		       any of the src_elt's, because they would have failed to
+		       match if not still valid.  */
+		    if (insert_regs (src, classp, 0))
+		      {
+			rehash_using_reg (src);
+			sets[i].src_hash = HASH (src, mode);
+		      }
+		    elt = insert (src, classp, sets[i].src_hash, mode);
+		    elt->in_memory = sets[i].src_in_memory;
+		    sets[i].src_elt = classp = elt;
+		  }
+		else
+		  sets[i].src_elt = classp;
+	      }
+	    if (sets[i].src_const && sets[i].src_const_elt == 0
+		&& src != sets[i].src_const
+		&& ! rtx_equal_p (sets[i].src_const, src))
+	      sets[i].src_elt = insert (sets[i].src_const, classp,
+					sets[i].src_const_hash, mode);
+	  }
+      }
+    else if (sets[i].src_elt == 0)
+      /* If we did not insert the source into the hash table (e.g., it was
+	 volatile), note the equivalence class for the REG_EQUAL value, if any,
+	 so that the destination goes into that class.  */
+      sets[i].src_elt = src_eqv_elt;
+
+  invalidate_from_clobbers (x);
+
+  /* Some registers are invalidated by subroutine calls.  Memory is
+     invalidated by non-constant calls.  */
+
+  if (GET_CODE (insn) == CALL_INSN)
+    {
+      if (! CONST_OR_PURE_CALL_P (insn))
+	invalidate_memory ();
+      invalidate_for_call ();
+    }
+
+  /* Now invalidate everything set by this instruction.
+     If a SUBREG or other funny destination is being set,
+     sets[i].rtl is still nonzero, so here we invalidate the reg
+     a part of which is being set.  */
+
+  for (i = 0; i < n_sets; i++)
+    if (sets[i].rtl)
+      {
+	/* We can't use the inner dest, because the mode associated with
+	   a ZERO_EXTRACT is significant.  */
+	rtx dest = SET_DEST (sets[i].rtl);
+
+	/* Needed for registers to remove the register from its
+	   previous quantity's chain.
+	   Needed for memory if this is a nonvarying address, unless
+	   we have just done an invalidate_memory that covers even those.  */
+	if (GET_CODE (dest) == REG || GET_CODE (dest) == SUBREG)
+	  invalidate (dest, VOIDmode);
+	else if (GET_CODE (dest) == MEM)
+	  {
+	    /* Outgoing arguments for a libcall don't
+	       affect any recorded expressions.  */
+	    if (! libcall_insn || insn == libcall_insn)
+	      invalidate (dest, VOIDmode);
+	  }
+	else if (GET_CODE (dest) == STRICT_LOW_PART
+		 || GET_CODE (dest) == ZERO_EXTRACT)
+	  invalidate (XEXP (dest, 0), GET_MODE (dest));
+      }
+
+  /* A volatile ASM invalidates everything.  */
+  if (GET_CODE (insn) == INSN
+      && GET_CODE (PATTERN (insn)) == ASM_OPERANDS
+      && MEM_VOLATILE_P (PATTERN (insn)))
+    flush_hash_table ();
+
+  /* Make sure registers mentioned in destinations
+     are safe for use in an expression to be inserted.
+     This removes from the hash table
+     any invalid entry that refers to one of these registers.
+
+     We don't care about the return value from mention_regs because
+     we are going to hash the SET_DEST values unconditionally.  */
+
+  for (i = 0; i < n_sets; i++)
+    {
+      if (sets[i].rtl)
+	{
+	  rtx x = SET_DEST (sets[i].rtl);
+
+	  if (GET_CODE (x) != REG)
+	    mention_regs (x);
+	  else
+	    {
+	      /* We used to rely on all references to a register becoming
+		 inaccessible when a register changes to a new quantity,
+		 since that changes the hash code.  However, that is not
+		 safe, since after HASH_SIZE new quantities we get a
+		 hash 'collision' of a register with its own invalid
+		 entries.  And since SUBREGs have been changed not to
+		 change their hash code with the hash code of the register,
+		 it wouldn't work any longer at all.  So we have to check
+		 for any invalid references lying around now.
+		 This code is similar to the REG case in mention_regs,
+		 but it knows that reg_tick has been incremented, and
+		 it leaves reg_in_table as -1 .  */
+	      unsigned int regno = REGNO (x);
+	      unsigned int endregno
+		= regno + (regno >= FIRST_PSEUDO_REGISTER ? 1
+			   : HARD_REGNO_NREGS (regno, GET_MODE (x)));
+	      unsigned int i;
+
+	      for (i = regno; i < endregno; i++)
+		{
+		  if (REG_IN_TABLE (i) >= 0)
+		    {
+		      remove_invalid_refs (i);
+		      REG_IN_TABLE (i) = -1;
+		    }
+		}
+	    }
+	}
+    }
+
+  /* We may have just removed some of the src_elt's from the hash table.
+     So replace each one with the current head of the same class.  */
+
+  for (i = 0; i < n_sets; i++)
+    if (sets[i].rtl)
+      {
+	if (sets[i].src_elt && sets[i].src_elt->first_same_value == 0)
+	  /* If elt was removed, find current head of same class,
+	     or 0 if nothing remains of that class.  */
+	  {
+	    struct table_elt *elt = sets[i].src_elt;
+
+	    while (elt && elt->prev_same_value)
+	      elt = elt->prev_same_value;
+
+	    while (elt && elt->first_same_value == 0)
+	      elt = elt->next_same_value;
+	    sets[i].src_elt = elt ? elt->first_same_value : 0;
+	  }
+      }
+
+  /* Now insert the destinations into their equivalence classes.  */
+
+  for (i = 0; i < n_sets; i++)
+    if (sets[i].rtl)
+      {
+	rtx dest = SET_DEST (sets[i].rtl);
+	rtx inner_dest = sets[i].inner_dest;
+	struct table_elt *elt;
+
+	/* Don't record value if we are not supposed to risk allocating
+	   floating-point values in registers that might be wider than
+	   memory.  */
+	if ((flag_float_store
+	     && GET_CODE (dest) == MEM
+	     && FLOAT_MODE_P (GET_MODE (dest)))
+	    /* Don't record BLKmode values, because we don't know the
+	       size of it, and can't be sure that other BLKmode values
+	       have the same or smaller size.  */
+	    || GET_MODE (dest) == BLKmode
+	    /* Don't record values of destinations set inside a libcall block
+	       since we might delete the libcall.  Things should have been set
+	       up so we won't want to reuse such a value, but we play it safe
+	       here.  */
+	    || libcall_insn
+	    /* If we didn't put a REG_EQUAL value or a source into the hash
+	       table, there is no point is recording DEST.  */
+	    || sets[i].src_elt == 0
+	    /* If DEST is a paradoxical SUBREG and SRC is a ZERO_EXTEND
+	       or SIGN_EXTEND, don't record DEST since it can cause
+	       some tracking to be wrong.
+
+	       ??? Think about this more later.  */
+	    || (GET_CODE (dest) == SUBREG
+		&& (GET_MODE_SIZE (GET_MODE (dest))
+		    > GET_MODE_SIZE (GET_MODE (SUBREG_REG (dest))))
+		&& (GET_CODE (sets[i].src) == SIGN_EXTEND
+		    || GET_CODE (sets[i].src) == ZERO_EXTEND)))
+	  continue;
+
+	/* STRICT_LOW_PART isn't part of the value BEING set,
+	   and neither is the SUBREG inside it.
+	   Note that in this case SETS[I].SRC_ELT is really SRC_EQV_ELT.  */
+	if (GET_CODE (dest) == STRICT_LOW_PART)
+	  dest = SUBREG_REG (XEXP (dest, 0));
+
+	if (GET_CODE (dest) == REG || GET_CODE (dest) == SUBREG)
+	  /* Registers must also be inserted into chains for quantities.  */
+	  if (insert_regs (dest, sets[i].src_elt, 1))
+	    {
+	      /* If `insert_regs' changes something, the hash code must be
+		 recalculated.  */
+	      rehash_using_reg (dest);
+	      sets[i].dest_hash = HASH (dest, GET_MODE (dest));
+	    }
+
+	if (GET_CODE (inner_dest) == MEM
+	    && GET_CODE (XEXP (inner_dest, 0)) == ADDRESSOF)
+	  /* Given (SET (MEM (ADDRESSOF (X))) Y) we don't want to say
+	     that (MEM (ADDRESSOF (X))) is equivalent to Y.
+	     Consider the case in which the address of the MEM is
+	     passed to a function, which alters the MEM.  Then, if we
+	     later use Y instead of the MEM we'll miss the update.  */
+	  elt = insert (dest, 0, sets[i].dest_hash, GET_MODE (dest));
+	else
+	  elt = insert (dest, sets[i].src_elt,
+			sets[i].dest_hash, GET_MODE (dest));
+
+	elt->in_memory = (GET_CODE (sets[i].inner_dest) == MEM
+			  && (! RTX_UNCHANGING_P (sets[i].inner_dest)
+			      || fixed_base_plus_p (XEXP (sets[i].inner_dest,
+							  0))));
+
+	/* If we have (set (subreg:m1 (reg:m2 foo) 0) (bar:m1)), M1 is no
+	   narrower than M2, and both M1 and M2 are the same number of words,
+	   we are also doing (set (reg:m2 foo) (subreg:m2 (bar:m1) 0)) so
+	   make that equivalence as well.
+
+	   However, BAR may have equivalences for which gen_lowpart_if_possible
+	   will produce a simpler value than gen_lowpart_if_possible applied to
+	   BAR (e.g., if BAR was ZERO_EXTENDed from M2), so we will scan all
+	   BAR's equivalences.  If we don't get a simplified form, make
+	   the SUBREG.  It will not be used in an equivalence, but will
+	   cause two similar assignments to be detected.
+
+	   Note the loop below will find SUBREG_REG (DEST) since we have
+	   already entered SRC and DEST of the SET in the table.  */
+
+	if (GET_CODE (dest) == SUBREG
+	    && (((GET_MODE_SIZE (GET_MODE (SUBREG_REG (dest))) - 1)
+		 / UNITS_PER_WORD)
+		== (GET_MODE_SIZE (GET_MODE (dest)) - 1) / UNITS_PER_WORD)
+	    && (GET_MODE_SIZE (GET_MODE (dest))
+		>= GET_MODE_SIZE (GET_MODE (SUBREG_REG (dest))))
+	    && sets[i].src_elt != 0)
+	  {
+	    enum machine_mode new_mode = GET_MODE (SUBREG_REG (dest));
+	    struct table_elt *elt, *classp = 0;
+
+	    for (elt = sets[i].src_elt->first_same_value; elt;
+		 elt = elt->next_same_value)
+	      {
+		rtx new_src = 0;
+		unsigned src_hash;
+		struct table_elt *src_elt;
+		int byte = 0;
+
+		/* Ignore invalid entries.  */
+		if (GET_CODE (elt->exp) != REG
+		    && ! exp_equiv_p (elt->exp, elt->exp, 1, 0))
+		  continue;
+
+		/* We may have already been playing subreg games.  If the
+		   mode is already correct for the destination, use it.  */
+		if (GET_MODE (elt->exp) == new_mode)
+		  new_src = elt->exp;
+		else
+		  {
+		    /* Calculate big endian correction for the SUBREG_BYTE.
+		       We have already checked that M1 (GET_MODE (dest))
+		       is not narrower than M2 (new_mode).  */
+		    if (BYTES_BIG_ENDIAN)
+		      byte = (GET_MODE_SIZE (GET_MODE (dest))
+			      - GET_MODE_SIZE (new_mode));
+
+		    new_src = simplify_gen_subreg (new_mode, elt->exp,
+					           GET_MODE (dest), byte);
+		  }
+
+		/* The call to simplify_gen_subreg fails if the value
+		   is VOIDmode, yet we can't do any simplification, e.g.
+		   for EXPR_LISTs denoting function call results.
+		   It is invalid to construct a SUBREG with a VOIDmode
+		   SUBREG_REG, hence a zero new_src means we can't do
+		   this substitution.  */
+		if (! new_src)
+		  continue;
+
+		src_hash = HASH (new_src, new_mode);
+		src_elt = lookup (new_src, src_hash, new_mode);
+
+		/* Put the new source in the hash table is if isn't
+		   already.  */
+		if (src_elt == 0)
+		  {
+		    if (insert_regs (new_src, classp, 0))
+		      {
+			rehash_using_reg (new_src);
+			src_hash = HASH (new_src, new_mode);
+		      }
+		    src_elt = insert (new_src, classp, src_hash, new_mode);
+		    src_elt->in_memory = elt->in_memory;
+		  }
+		else if (classp && classp != src_elt->first_same_value)
+		  /* Show that two things that we've seen before are
+		     actually the same.  */
+		  merge_equiv_classes (src_elt, classp);
+
+		classp = src_elt->first_same_value;
+		/* Ignore invalid entries.  */
+		while (classp
+		       && GET_CODE (classp->exp) != REG
+		       && ! exp_equiv_p (classp->exp, classp->exp, 1, 0))
+		  classp = classp->next_same_value;
+	      }
+	  }
+      }
+
+  /* Special handling for (set REG0 REG1) where REG0 is the
+     "cheapest", cheaper than REG1.  After cse, REG1 will probably not
+     be used in the sequel, so (if easily done) change this insn to
+     (set REG1 REG0) and replace REG1 with REG0 in the previous insn
+     that computed their value.  Then REG1 will become a dead store
+     and won't cloud the situation for later optimizations.
+
+     Do not make this change if REG1 is a hard register, because it will
+     then be used in the sequel and we may be changing a two-operand insn
+     into a three-operand insn.
+
+     Also do not do this if we are operating on a copy of INSN.
+
+     Also don't do this if INSN ends a libcall; this would cause an unrelated
+     register to be set in the middle of a libcall, and we then get bad code
+     if the libcall is deleted.  */
+
+  if (n_sets == 1 && sets[0].rtl && GET_CODE (SET_DEST (sets[0].rtl)) == REG
+      && NEXT_INSN (PREV_INSN (insn)) == insn
+      && GET_CODE (SET_SRC (sets[0].rtl)) == REG
+      && REGNO (SET_SRC (sets[0].rtl)) >= FIRST_PSEUDO_REGISTER
+      && REGNO_QTY_VALID_P (REGNO (SET_SRC (sets[0].rtl))))
+    {
+      int src_q = REG_QTY (REGNO (SET_SRC (sets[0].rtl)));
+      struct qty_table_elem *src_ent = &qty_table[src_q];
+
+      if ((src_ent->first_reg == REGNO (SET_DEST (sets[0].rtl)))
+	  && ! find_reg_note (insn, REG_RETVAL, NULL_RTX))
+	{
+	  rtx prev = insn;
+	  /* Scan for the previous nonnote insn, but stop at a basic
+	     block boundary.  */
+	  do
+	    {
+	      prev = PREV_INSN (prev);
+	    }
+	  while (prev && GET_CODE (prev) == NOTE
+		 && NOTE_LINE_NUMBER (prev) != NOTE_INSN_BASIC_BLOCK);
+
+	  /* Do not swap the registers around if the previous instruction
+	     attaches a REG_EQUIV note to REG1.
+
+	     ??? It's not entirely clear whether we can transfer a REG_EQUIV
+	     from the pseudo that originally shadowed an incoming argument
+	     to another register.  Some uses of REG_EQUIV might rely on it
+	     being attached to REG1 rather than REG2.
+
+	     This section previously turned the REG_EQUIV into a REG_EQUAL
+	     note.  We cannot do that because REG_EQUIV may provide an
+	     uninitialized stack slot when REG_PARM_STACK_SPACE is used.  */
+
+	  if (prev != 0 && GET_CODE (prev) == INSN
+	      && GET_CODE (PATTERN (prev)) == SET
+	      && SET_DEST (PATTERN (prev)) == SET_SRC (sets[0].rtl)
+	      && ! find_reg_note (prev, REG_EQUIV, NULL_RTX))
+	    {
+	      rtx dest = SET_DEST (sets[0].rtl);
+	      rtx src = SET_SRC (sets[0].rtl);
+	      rtx note;
+
+	      validate_change (prev, &SET_DEST (PATTERN (prev)), dest, 1);
+	      validate_change (insn, &SET_DEST (sets[0].rtl), src, 1);
+	      validate_change (insn, &SET_SRC (sets[0].rtl), dest, 1);
+	      apply_change_group ();
+
+	      /* If INSN has a REG_EQUAL note, and this note mentions
+		 REG0, then we must delete it, because the value in
+		 REG0 has changed.  If the note's value is REG1, we must
+		 also delete it because that is now this insn's dest.  */
+	      note = find_reg_note (insn, REG_EQUAL, NULL_RTX);
+	      if (note != 0
+		  && (reg_mentioned_p (dest, XEXP (note, 0))
+		      || rtx_equal_p (src, XEXP (note, 0))))
+		remove_note (insn, note);
+	    }
+	}
+    }
+
+  /* If this is a conditional jump insn, record any known equivalences due to
+     the condition being tested.  */
+
+  last_jump_equiv_class = 0;
+  if (GET_CODE (insn) == JUMP_INSN
+      && n_sets == 1 && GET_CODE (x) == SET
+      && GET_CODE (SET_SRC (x)) == IF_THEN_ELSE)
+    record_jump_equiv (insn, 0);
+
+#ifdef HAVE_cc0
+  /* If the previous insn set CC0 and this insn no longer references CC0,
+     delete the previous insn.  Here we use the fact that nothing expects CC0
+     to be valid over an insn, which is true until the final pass.  */
+  if (prev_insn && GET_CODE (prev_insn) == INSN
+      && (tem = single_set (prev_insn)) != 0
+      && SET_DEST (tem) == cc0_rtx
+      && ! reg_mentioned_p (cc0_rtx, x))
+    delete_insn (prev_insn);
+
+  prev_insn_cc0 = this_insn_cc0;
+  prev_insn_cc0_mode = this_insn_cc0_mode;
+  prev_insn = insn;
+#endif
+}
+
+/* Remove from the hash table all expressions that reference memory.  */
+
+static void
+invalidate_memory (void)
+{
+  int i;
+  struct table_elt *p, *next;
+
+  for (i = 0; i < HASH_SIZE; i++)
+    for (p = table[i]; p; p = next)
+      {
+	next = p->next_same_hash;
+	if (p->in_memory)
+	  remove_from_table (p, i);
+      }
+}
+
+/* If ADDR is an address that implicitly affects the stack pointer, return
+   1 and update the register tables to show the effect.  Else, return 0.  */
+
+static int
+addr_affects_sp_p (rtx addr)
+{
+  if (GET_RTX_CLASS (GET_CODE (addr)) == 'a'
+      && GET_CODE (XEXP (addr, 0)) == REG
+      && REGNO (XEXP (addr, 0)) == STACK_POINTER_REGNUM)
+    {
+      if (REG_TICK (STACK_POINTER_REGNUM) >= 0)
+	{
+	  REG_TICK (STACK_POINTER_REGNUM)++;
+	  /* Is it possible to use a subreg of SP?  */
+	  SUBREG_TICKED (STACK_POINTER_REGNUM) = -1;
+	}
+
+      /* This should be *very* rare.  */
+      if (TEST_HARD_REG_BIT (hard_regs_in_table, STACK_POINTER_REGNUM))
+	invalidate (stack_pointer_rtx, VOIDmode);
+
+      return 1;
+    }
+
+  return 0;
+}
+
+/* Perform invalidation on the basis of everything about an insn
+   except for invalidating the actual places that are SET in it.
+   This includes the places CLOBBERed, and anything that might
+   alias with something that is SET or CLOBBERed.
+
+   X is the pattern of the insn.  */
+
+static void
+invalidate_from_clobbers (rtx x)
+{
+  if (GET_CODE (x) == CLOBBER)
+    {
+      rtx ref = XEXP (x, 0);
+      if (ref)
+	{
+	  if (GET_CODE (ref) == REG || GET_CODE (ref) == SUBREG
+	      || GET_CODE (ref) == MEM)
+	    invalidate (ref, VOIDmode);
+	  else if (GET_CODE (ref) == STRICT_LOW_PART
+		   || GET_CODE (ref) == ZERO_EXTRACT)
+	    invalidate (XEXP (ref, 0), GET_MODE (ref));
+	}
+    }
+  else if (GET_CODE (x) == PARALLEL)
+    {
+      int i;
+      for (i = XVECLEN (x, 0) - 1; i >= 0; i--)
+	{
+	  rtx y = XVECEXP (x, 0, i);
+	  if (GET_CODE (y) == CLOBBER)
+	    {
+	      rtx ref = XEXP (y, 0);
+	      if (GET_CODE (ref) == REG || GET_CODE (ref) == SUBREG
+		  || GET_CODE (ref) == MEM)
+		invalidate (ref, VOIDmode);
+	      else if (GET_CODE (ref) == STRICT_LOW_PART
+		       || GET_CODE (ref) == ZERO_EXTRACT)
+		invalidate (XEXP (ref, 0), GET_MODE (ref));
+	    }
+	}
+    }
+}
+
+/* Process X, part of the REG_NOTES of an insn.  Look at any REG_EQUAL notes
+   and replace any registers in them with either an equivalent constant
+   or the canonical form of the register.  If we are inside an address,
+   only do this if the address remains valid.
+
+   OBJECT is 0 except when within a MEM in which case it is the MEM.
+
+   Return the replacement for X.  */
+
+static rtx
+cse_process_notes (rtx x, rtx object)
+{
+  enum rtx_code code = GET_CODE (x);
+  const char *fmt = GET_RTX_FORMAT (code);
+  int i;
+
+  switch (code)
+    {
+    case CONST_INT:
+    case CONST:
+    case SYMBOL_REF:
+    case LABEL_REF:
+    case CONST_DOUBLE:
+    case CONST_VECTOR:
+    case PC:
+    case CC0:
+    case LO_SUM:
+      return x;
+
+    case MEM:
+      validate_change (x, &XEXP (x, 0),
+		       cse_process_notes (XEXP (x, 0), x), 0);
+      return x;
+
+    case EXPR_LIST:
+    case INSN_LIST:
+      if (REG_NOTE_KIND (x) == REG_EQUAL)
+	XEXP (x, 0) = cse_process_notes (XEXP (x, 0), NULL_RTX);
+      if (XEXP (x, 1))
+	XEXP (x, 1) = cse_process_notes (XEXP (x, 1), NULL_RTX);
+      return x;
+
+    case SIGN_EXTEND:
+    case ZERO_EXTEND:
+    case SUBREG:
+      {
+	rtx new = cse_process_notes (XEXP (x, 0), object);
+	/* We don't substitute VOIDmode constants into these rtx,
+	   since they would impede folding.  */
+	if (GET_MODE (new) != VOIDmode)
+	  validate_change (object, &XEXP (x, 0), new, 0);
+	return x;
+      }
+
+    case REG:
+      i = REG_QTY (REGNO (x));
+
+      /* Return a constant or a constant register.  */
+      if (REGNO_QTY_VALID_P (REGNO (x)))
+	{
+	  struct qty_table_elem *ent = &qty_table[i];
+
+	  if (ent->const_rtx != NULL_RTX
+	      && (CONSTANT_P (ent->const_rtx)
+		  || GET_CODE (ent->const_rtx) == REG))
+	    {
+	      rtx new = gen_lowpart_if_possible (GET_MODE (x), ent->const_rtx);
+	      if (new)
+		return new;
+	    }
+	}
+
+      /* Otherwise, canonicalize this register.  */
+      return canon_reg (x, NULL_RTX);
+
+    default:
+      break;
+    }
+
+  for (i = 0; i < GET_RTX_LENGTH (code); i++)
+    if (fmt[i] == 'e')
+      validate_change (object, &XEXP (x, i),
+		       cse_process_notes (XEXP (x, i), object), 0);
+
+  return x;
+}
+
+/* Find common subexpressions between the end test of a loop and the beginning
+   of the loop.  LOOP_START is the CODE_LABEL at the start of a loop.
+
+   Often we have a loop where an expression in the exit test is used
+   in the body of the loop.  For example "while (*p) *q++ = *p++;".
+   Because of the way we duplicate the loop exit test in front of the loop,
+   however, we don't detect that common subexpression.  This will be caught
+   when global cse is implemented, but this is a quite common case.
+
+   This function handles the most common cases of these common expressions.
+   It is called after we have processed the basic block ending with the
+   NOTE_INSN_LOOP_END note that ends a loop and the previous JUMP_INSN
+   jumps to a label used only once.  */
+
+static void
+cse_around_loop (rtx loop_start)
+{
+  rtx insn;
+  int i;
+  struct table_elt *p;
+
+  /* If the jump at the end of the loop doesn't go to the start, we don't
+     do anything.  */
+  for (insn = PREV_INSN (loop_start);
+       insn && (GET_CODE (insn) == NOTE && NOTE_LINE_NUMBER (insn) >= 0);
+       insn = PREV_INSN (insn))
+    ;
+
+  if (insn == 0
+      || GET_CODE (insn) != NOTE
+      || NOTE_LINE_NUMBER (insn) != NOTE_INSN_LOOP_BEG)
+    return;
+
+  /* If the last insn of the loop (the end test) was an NE comparison,
+     we will interpret it as an EQ comparison, since we fell through
+     the loop.  Any equivalences resulting from that comparison are
+     therefore not valid and must be invalidated.  */
+  if (last_jump_equiv_class)
+    for (p = last_jump_equiv_class->first_same_value; p;
+	 p = p->next_same_value)
+      {
+	if (GET_CODE (p->exp) == MEM || GET_CODE (p->exp) == REG
+	    || (GET_CODE (p->exp) == SUBREG
+		&& GET_CODE (SUBREG_REG (p->exp)) == REG))
+	  invalidate (p->exp, VOIDmode);
+	else if (GET_CODE (p->exp) == STRICT_LOW_PART
+		 || GET_CODE (p->exp) == ZERO_EXTRACT)
+	  invalidate (XEXP (p->exp, 0), GET_MODE (p->exp));
+      }
+
+  /* Process insns starting after LOOP_START until we hit a CALL_INSN or
+     a CODE_LABEL (we could handle a CALL_INSN, but it isn't worth it).
+
+     The only thing we do with SET_DEST is invalidate entries, so we
+     can safely process each SET in order.  It is slightly less efficient
+     to do so, but we only want to handle the most common cases.
+
+     The gen_move_insn call in cse_set_around_loop may create new pseudos.
+     These pseudos won't have valid entries in any of the tables indexed
+     by register number, such as reg_qty.  We avoid out-of-range array
+     accesses by not processing any instructions created after cse started.  */
+
+  for (insn = NEXT_INSN (loop_start);
+       GET_CODE (insn) != CALL_INSN && GET_CODE (insn) != CODE_LABEL
+       && INSN_UID (insn) < max_insn_uid
+       && ! (GET_CODE (insn) == NOTE
+	     && NOTE_LINE_NUMBER (insn) == NOTE_INSN_LOOP_END);
+       insn = NEXT_INSN (insn))
+    {
+      if (INSN_P (insn)
+	  && (GET_CODE (PATTERN (insn)) == SET
+	      || GET_CODE (PATTERN (insn)) == CLOBBER))
+	cse_set_around_loop (PATTERN (insn), insn, loop_start);
+      else if (INSN_P (insn) && GET_CODE (PATTERN (insn)) == PARALLEL)
+	for (i = XVECLEN (PATTERN (insn), 0) - 1; i >= 0; i--)
+	  if (GET_CODE (XVECEXP (PATTERN (insn), 0, i)) == SET
+	      || GET_CODE (XVECEXP (PATTERN (insn), 0, i)) == CLOBBER)
+	    cse_set_around_loop (XVECEXP (PATTERN (insn), 0, i), insn,
+				 loop_start);
+    }
+}
+
+/* Process one SET of an insn that was skipped.  We ignore CLOBBERs
+   since they are done elsewhere.  This function is called via note_stores.  */
+
+static void
+invalidate_skipped_set (rtx dest, rtx set, void *data ATTRIBUTE_UNUSED)
+{
+  enum rtx_code code = GET_CODE (dest);
+
+  if (code == MEM
+      && ! addr_affects_sp_p (dest)	/* If this is not a stack push ...  */
+      /* There are times when an address can appear varying and be a PLUS
+	 during this scan when it would be a fixed address were we to know
+	 the proper equivalences.  So invalidate all memory if there is
+	 a BLKmode or nonscalar memory reference or a reference to a
+	 variable address.  */
+      && (MEM_IN_STRUCT_P (dest) || GET_MODE (dest) == BLKmode
+	  || cse_rtx_varies_p (XEXP (dest, 0), 0)))
+    {
+      invalidate_memory ();
+      return;
+    }
+
+  if (GET_CODE (set) == CLOBBER
+      || CC0_P (dest)
+      || dest == pc_rtx)
+    return;
+
+  if (code == STRICT_LOW_PART || code == ZERO_EXTRACT)
+    invalidate (XEXP (dest, 0), GET_MODE (dest));
+  else if (code == REG || code == SUBREG || code == MEM)
+    invalidate (dest, VOIDmode);
+}
+
+/* Invalidate all insns from START up to the end of the function or the
+   next label.  This called when we wish to CSE around a block that is
+   conditionally executed.  */
+
+static void
+invalidate_skipped_block (rtx start)
+{
+  rtx insn;
+
+  for (insn = start; insn && GET_CODE (insn) != CODE_LABEL;
+       insn = NEXT_INSN (insn))
+    {
+      if (! INSN_P (insn))
+	continue;
+
+      if (GET_CODE (insn) == CALL_INSN)
+	{
+	  if (! CONST_OR_PURE_CALL_P (insn))
+	    invalidate_memory ();
+	  invalidate_for_call ();
+	}
+
+      invalidate_from_clobbers (PATTERN (insn));
+      note_stores (PATTERN (insn), invalidate_skipped_set, NULL);
+    }
+}
+
+/* If modifying X will modify the value in *DATA (which is really an
+   `rtx *'), indicate that fact by setting the pointed to value to
+   NULL_RTX.  */
+
+static void
+cse_check_loop_start (rtx x, rtx set ATTRIBUTE_UNUSED, void *data)
+{
+  rtx *cse_check_loop_start_value = (rtx *) data;
+
+  if (*cse_check_loop_start_value == NULL_RTX
+      || GET_CODE (x) == CC0 || GET_CODE (x) == PC)
+    return;
+
+  if ((GET_CODE (x) == MEM && GET_CODE (*cse_check_loop_start_value) == MEM)
+      || reg_overlap_mentioned_p (x, *cse_check_loop_start_value))
+    *cse_check_loop_start_value = NULL_RTX;
+}
+
+/* X is a SET or CLOBBER contained in INSN that was found near the start of
+   a loop that starts with the label at LOOP_START.
+
+   If X is a SET, we see if its SET_SRC is currently in our hash table.
+   If so, we see if it has a value equal to some register used only in the
+   loop exit code (as marked by jump.c).
+
+   If those two conditions are true, we search backwards from the start of
+   the loop to see if that same value was loaded into a register that still
+   retains its value at the start of the loop.
+
+   If so, we insert an insn after the load to copy the destination of that
+   load into the equivalent register and (try to) replace our SET_SRC with that
+   register.
+
+   In any event, we invalidate whatever this SET or CLOBBER modifies.  */
+
+static void
+cse_set_around_loop (rtx x, rtx insn, rtx loop_start)
+{
+  struct table_elt *src_elt;
+
+  /* If this is a SET, see if we can replace SET_SRC, but ignore SETs that
+     are setting PC or CC0 or whose SET_SRC is already a register.  */
+  if (GET_CODE (x) == SET
+      && GET_CODE (SET_DEST (x)) != PC && GET_CODE (SET_DEST (x)) != CC0
+      && GET_CODE (SET_SRC (x)) != REG)
+    {
+      src_elt = lookup (SET_SRC (x),
+			HASH (SET_SRC (x), GET_MODE (SET_DEST (x))),
+			GET_MODE (SET_DEST (x)));
+
+      if (src_elt)
+	for (src_elt = src_elt->first_same_value; src_elt;
+	     src_elt = src_elt->next_same_value)
+	  if (GET_CODE (src_elt->exp) == REG && REG_LOOP_TEST_P (src_elt->exp)
+	      && COST (src_elt->exp) < COST (SET_SRC (x)))
+	    {
+	      rtx p, set;
+
+	      /* Look for an insn in front of LOOP_START that sets
+		 something in the desired mode to SET_SRC (x) before we hit
+		 a label or CALL_INSN.  */
+
+	      for (p = prev_nonnote_insn (loop_start);
+		   p && GET_CODE (p) != CALL_INSN
+		   && GET_CODE (p) != CODE_LABEL;
+		   p = prev_nonnote_insn  (p))
+		if ((set = single_set (p)) != 0
+		    && GET_CODE (SET_DEST (set)) == REG
+		    && GET_MODE (SET_DEST (set)) == src_elt->mode
+		    && rtx_equal_p (SET_SRC (set), SET_SRC (x)))
+		  {
+		    /* We now have to ensure that nothing between P
+		       and LOOP_START modified anything referenced in
+		       SET_SRC (x).  We know that nothing within the loop
+		       can modify it, or we would have invalidated it in
+		       the hash table.  */
+		    rtx q;
+		    rtx cse_check_loop_start_value = SET_SRC (x);
+		    for (q = p; q != loop_start; q = NEXT_INSN (q))
+		      if (INSN_P (q))
+			note_stores (PATTERN (q),
+				     cse_check_loop_start,
+				     &cse_check_loop_start_value);
+
+		    /* If nothing was changed and we can replace our
+		       SET_SRC, add an insn after P to copy its destination
+		       to what we will be replacing SET_SRC with.  */
+		    if (cse_check_loop_start_value
+			&& single_set (p)
+			&& !can_throw_internal (insn)
+			&& validate_change (insn, &SET_SRC (x),
+					    src_elt->exp, 0))
+		      {
+			/* If this creates new pseudos, this is unsafe,
+			   because the regno of new pseudo is unsuitable
+			   to index into reg_qty when cse_insn processes
+			   the new insn.  Therefore, if a new pseudo was
+			   created, discard this optimization.  */
+			int nregs = max_reg_num ();
+			rtx move
+			  = gen_move_insn (src_elt->exp, SET_DEST (set));
+			if (nregs != max_reg_num ())
+			  {
+			    if (! validate_change (insn, &SET_SRC (x),
+						   SET_SRC (set), 0))
+			      abort ();
+			  }
+			else
+			  {
+			    if (CONSTANT_P (SET_SRC (set))
+				&& ! find_reg_equal_equiv_note (insn))
+			      set_unique_reg_note (insn, REG_EQUAL,
+						   SET_SRC (set));
+			    if (control_flow_insn_p (p))
+			      /* p can cause a control flow transfer so it
+				 is the last insn of a basic block.  We can't
+				 therefore use emit_insn_after.  */
+			      emit_insn_before (move, next_nonnote_insn (p));
+			    else
+			      emit_insn_after (move, p);
+			  }
+		      }
+		    break;
+		  }
+	    }
+    }
+
+  /* Deal with the destination of X affecting the stack pointer.  */
+  addr_affects_sp_p (SET_DEST (x));
+
+  /* See comment on similar code in cse_insn for explanation of these
+     tests.  */
+  if (GET_CODE (SET_DEST (x)) == REG || GET_CODE (SET_DEST (x)) == SUBREG
+      || GET_CODE (SET_DEST (x)) == MEM)
+    invalidate (SET_DEST (x), VOIDmode);
+  else if (GET_CODE (SET_DEST (x)) == STRICT_LOW_PART
+	   || GET_CODE (SET_DEST (x)) == ZERO_EXTRACT)
+    invalidate (XEXP (SET_DEST (x), 0), GET_MODE (SET_DEST (x)));
+}
+
+/* Find the end of INSN's basic block and return its range,
+   the total number of SETs in all the insns of the block, the last insn of the
+   block, and the branch path.
+
+   The branch path indicates which branches should be followed.  If a nonzero
+   path size is specified, the block should be rescanned and a different set
+   of branches will be taken.  The branch path is only used if
+   FLAG_CSE_FOLLOW_JUMPS or FLAG_CSE_SKIP_BLOCKS is nonzero.
+
+   DATA is a pointer to a struct cse_basic_block_data, defined below, that is
+   used to describe the block.  It is filled in with the information about
+   the current block.  The incoming structure's branch path, if any, is used
+   to construct the output branch path.  */
+
+void
+cse_end_of_basic_block (rtx insn, struct cse_basic_block_data *data,
+			int follow_jumps, int after_loop, int skip_blocks)
+{
+  rtx p = insn, q;
+  int nsets = 0;
+  int low_cuid = INSN_CUID (insn), high_cuid = INSN_CUID (insn);
+  rtx next = INSN_P (insn) ? insn : next_real_insn (insn);
+  int path_size = data->path_size;
+  int path_entry = 0;
+  int i;
+
+  /* Update the previous branch path, if any.  If the last branch was
+     previously TAKEN, mark it NOT_TAKEN.  If it was previously NOT_TAKEN,
+     shorten the path by one and look at the previous branch.  We know that
+     at least one branch must have been taken if PATH_SIZE is nonzero.  */
+  while (path_size > 0)
+    {
+      if (data->path[path_size - 1].status != NOT_TAKEN)
+	{
+	  data->path[path_size - 1].status = NOT_TAKEN;
+	  break;
+	}
+      else
+	path_size--;
+    }
+
+  /* If the first instruction is marked with QImode, that means we've
+     already processed this block.  Our caller will look at DATA->LAST
+     to figure out where to go next.  We want to return the next block
+     in the instruction stream, not some branched-to block somewhere
+     else.  We accomplish this by pretending our called forbid us to
+     follow jumps, or skip blocks.  */
+  if (GET_MODE (insn) == QImode)
+    follow_jumps = skip_blocks = 0;
+
+  /* Scan to end of this basic block.  */
+  while (p && GET_CODE (p) != CODE_LABEL)
+    {
+      /* Don't cse out the end of a loop.  This makes a difference
+	 only for the unusual loops that always execute at least once;
+	 all other loops have labels there so we will stop in any case.
+	 Cse'ing out the end of the loop is dangerous because it
+	 might cause an invariant expression inside the loop
+	 to be reused after the end of the loop.  This would make it
+	 hard to move the expression out of the loop in loop.c,
+	 especially if it is one of several equivalent expressions
+	 and loop.c would like to eliminate it.
+
+	 If we are running after loop.c has finished, we can ignore
+	 the NOTE_INSN_LOOP_END.  */
+
+      if (! after_loop && GET_CODE (p) == NOTE
+	  && NOTE_LINE_NUMBER (p) == NOTE_INSN_LOOP_END)
+	break;
+
+      /* Don't cse over a call to setjmp; on some machines (eg VAX)
+	 the regs restored by the longjmp come from
+	 a later time than the setjmp.  */
+      if (PREV_INSN (p) && GET_CODE (PREV_INSN (p)) == CALL_INSN
+	  && find_reg_note (PREV_INSN (p), REG_SETJMP, NULL))
+	break;
+
+      /* A PARALLEL can have lots of SETs in it,
+	 especially if it is really an ASM_OPERANDS.  */
+      if (INSN_P (p) && GET_CODE (PATTERN (p)) == PARALLEL)
+	nsets += XVECLEN (PATTERN (p), 0);
+      else if (GET_CODE (p) != NOTE)
+	nsets += 1;
+
+      /* Ignore insns made by CSE; they cannot affect the boundaries of
+	 the basic block.  */
+
+      if (INSN_UID (p) <= max_uid && INSN_CUID (p) > high_cuid)
+	high_cuid = INSN_CUID (p);
+      if (INSN_UID (p) <= max_uid && INSN_CUID (p) < low_cuid)
+	low_cuid = INSN_CUID (p);
+
+      /* See if this insn is in our branch path.  If it is and we are to
+	 take it, do so.  */
+      if (path_entry < path_size && data->path[path_entry].branch == p)
+	{
+	  if (data->path[path_entry].status != NOT_TAKEN)
+	    p = JUMP_LABEL (p);
+
+	  /* Point to next entry in path, if any.  */
+	  path_entry++;
+	}
+
+      /* If this is a conditional jump, we can follow it if -fcse-follow-jumps
+	 was specified, we haven't reached our maximum path length, there are
+	 insns following the target of the jump, this is the only use of the
+	 jump label, and the target label is preceded by a BARRIER.
+
+	 Alternatively, we can follow the jump if it branches around a
+	 block of code and there are no other branches into the block.
+	 In this case invalidate_skipped_block will be called to invalidate any
+	 registers set in the block when following the jump.  */
+
+      else if ((follow_jumps || skip_blocks) && path_size < PARAM_VALUE (PARAM_MAX_CSE_PATH_LENGTH) - 1
+	       && GET_CODE (p) == JUMP_INSN
+	       && GET_CODE (PATTERN (p)) == SET
+	       && GET_CODE (SET_SRC (PATTERN (p))) == IF_THEN_ELSE
+	       && JUMP_LABEL (p) != 0
+	       && LABEL_NUSES (JUMP_LABEL (p)) == 1
+	       && NEXT_INSN (JUMP_LABEL (p)) != 0)
+	{
+	  for (q = PREV_INSN (JUMP_LABEL (p)); q; q = PREV_INSN (q))
+	    if ((GET_CODE (q) != NOTE
+		 || NOTE_LINE_NUMBER (q) == NOTE_INSN_LOOP_END
+		 || (PREV_INSN (q) && GET_CODE (PREV_INSN (q)) == CALL_INSN
+		     && find_reg_note (PREV_INSN (q), REG_SETJMP, NULL)))
+		&& (GET_CODE (q) != CODE_LABEL || LABEL_NUSES (q) != 0))
+	      break;
+
+	  /* If we ran into a BARRIER, this code is an extension of the
+	     basic block when the branch is taken.  */
+	  if (follow_jumps && q != 0 && GET_CODE (q) == BARRIER)
+	    {
+	      /* Don't allow ourself to keep walking around an
+		 always-executed loop.  */
+	      if (next_real_insn (q) == next)
+		{
+		  p = NEXT_INSN (p);
+		  continue;
+		}
+
+	      /* Similarly, don't put a branch in our path more than once.  */
+	      for (i = 0; i < path_entry; i++)
+		if (data->path[i].branch == p)
+		  break;
+
+	      if (i != path_entry)
+		break;
+
+	      data->path[path_entry].branch = p;
+	      data->path[path_entry++].status = TAKEN;
+
+	      /* This branch now ends our path.  It was possible that we
+		 didn't see this branch the last time around (when the
+		 insn in front of the target was a JUMP_INSN that was
+		 turned into a no-op).  */
+	      path_size = path_entry;
+
+	      p = JUMP_LABEL (p);
+	      /* Mark block so we won't scan it again later.  */
+	      PUT_MODE (NEXT_INSN (p), QImode);
+	    }
+	  /* Detect a branch around a block of code.  */
+	  else if (skip_blocks && q != 0 && GET_CODE (q) != CODE_LABEL)
+	    {
+	      rtx tmp;
+
+	      if (next_real_insn (q) == next)
+		{
+		  p = NEXT_INSN (p);
+		  continue;
+		}
+
+	      for (i = 0; i < path_entry; i++)
+		if (data->path[i].branch == p)
+		  break;
+
+	      if (i != path_entry)
+		break;
+
+	      /* This is no_labels_between_p (p, q) with an added check for
+		 reaching the end of a function (in case Q precedes P).  */
+	      for (tmp = NEXT_INSN (p); tmp && tmp != q; tmp = NEXT_INSN (tmp))
+		if (GET_CODE (tmp) == CODE_LABEL)
+		  break;
+
+	      if (tmp == q)
+		{
+		  data->path[path_entry].branch = p;
+		  data->path[path_entry++].status = AROUND;
+
+		  path_size = path_entry;
+
+		  p = JUMP_LABEL (p);
+		  /* Mark block so we won't scan it again later.  */
+		  PUT_MODE (NEXT_INSN (p), QImode);
+		}
+	    }
+	}
+      p = NEXT_INSN (p);
+    }
+
+  data->low_cuid = low_cuid;
+  data->high_cuid = high_cuid;
+  data->nsets = nsets;
+  data->last = p;
+
+  /* If all jumps in the path are not taken, set our path length to zero
+     so a rescan won't be done.  */
+  for (i = path_size - 1; i >= 0; i--)
+    if (data->path[i].status != NOT_TAKEN)
+      break;
+
+  if (i == -1)
+    data->path_size = 0;
+  else
+    data->path_size = path_size;
+
+  /* End the current branch path.  */
+  data->path[path_size].branch = 0;
+}
+
+/* Perform cse on the instructions of a function.
+   F is the first instruction.
+   NREGS is one plus the highest pseudo-reg number used in the instruction.
+
+   AFTER_LOOP is 1 if this is the cse call done after loop optimization
+   (only if -frerun-cse-after-loop).
+
+   Returns 1 if jump_optimize should be redone due to simplifications
+   in conditional jump instructions.  */
+
+int
+cse_main (rtx f, int nregs, int after_loop, FILE *file)
+{
+  struct cse_basic_block_data val;
+  rtx insn = f;
+  int i;
+
+  val.path = xmalloc (sizeof (struct branch_path)
+		      * PARAM_VALUE (PARAM_MAX_CSE_PATH_LENGTH));
+
+  cse_jumps_altered = 0;
+  recorded_label_ref = 0;
+  constant_pool_entries_cost = 0;
+  constant_pool_entries_regcost = 0;
+  val.path_size = 0;
+
+  init_recog ();
+  init_alias_analysis ();
+
+  max_reg = nregs;
+
+  max_insn_uid = get_max_uid ();
+
+  reg_eqv_table = xmalloc (nregs * sizeof (struct reg_eqv_elem));
+
+#ifdef LOAD_EXTEND_OP
+
+  /* Allocate scratch rtl here.  cse_insn will fill in the memory reference
+     and change the code and mode as appropriate.  */
+  memory_extend_rtx = gen_rtx_ZERO_EXTEND (VOIDmode, NULL_RTX);
+#endif
+
+  /* Reset the counter indicating how many elements have been made
+     thus far.  */
+  n_elements_made = 0;
+
+  /* Find the largest uid.  */
+
+  max_uid = get_max_uid ();
+  uid_cuid = xcalloc (max_uid + 1, sizeof (int));
+
+  /* Compute the mapping from uids to cuids.
+     CUIDs are numbers assigned to insns, like uids,
+     except that cuids increase monotonically through the code.
+     Don't assign cuids to line-number NOTEs, so that the distance in cuids
+     between two insns is not affected by -g.  */
+
+  for (insn = f, i = 0; insn; insn = NEXT_INSN (insn))
+    {
+      if (GET_CODE (insn) != NOTE
+	  || NOTE_LINE_NUMBER (insn) < 0)
+	INSN_CUID (insn) = ++i;
+      else
+	/* Give a line number note the same cuid as preceding insn.  */
+	INSN_CUID (insn) = i;
+    }
+
+  ggc_push_context ();
+
+  /* Loop over basic blocks.
+     Compute the maximum number of qty's needed for each basic block
+     (which is 2 for each SET).  */
+  insn = f;
+  while (insn)
+    {
+      cse_altered = 0;
+      cse_end_of_basic_block (insn, &val, flag_cse_follow_jumps, after_loop,
+			      flag_cse_skip_blocks);
+
+      /* If this basic block was already processed or has no sets, skip it.  */
+      if (val.nsets == 0 || GET_MODE (insn) == QImode)
+	{
+	  PUT_MODE (insn, VOIDmode);
+	  insn = (val.last ? NEXT_INSN (val.last) : 0);
+	  val.path_size = 0;
+	  continue;
+	}
+
+      cse_basic_block_start = val.low_cuid;
+      cse_basic_block_end = val.high_cuid;
+      max_qty = val.nsets * 2;
+
+      if (file)
+	fnotice (file, ";; Processing block from %d to %d, %d sets.\n",
+		 INSN_UID (insn), val.last ? INSN_UID (val.last) : 0,
+		 val.nsets);
+
+      /* Make MAX_QTY bigger to give us room to optimize
+	 past the end of this basic block, if that should prove useful.  */
+      if (max_qty < 500)
+	max_qty = 500;
+
+      /* If this basic block is being extended by following certain jumps,
+         (see `cse_end_of_basic_block'), we reprocess the code from the start.
+         Otherwise, we start after this basic block.  */
+      if (val.path_size > 0)
+	cse_basic_block (insn, val.last, val.path, 0);
+      else
+	{
+	  int old_cse_jumps_altered = cse_jumps_altered;
+	  rtx temp;
+
+	  /* When cse changes a conditional jump to an unconditional
+	     jump, we want to reprocess the block, since it will give
+	     us a new branch path to investigate.  */
+	  cse_jumps_altered = 0;
+	  temp = cse_basic_block (insn, val.last, val.path, ! after_loop);
+	  if (cse_jumps_altered == 0
+	      || (flag_cse_follow_jumps == 0 && flag_cse_skip_blocks == 0))
+	    insn = temp;
+
+	  cse_jumps_altered |= old_cse_jumps_altered;
+	}
+
+      if (cse_altered)
+	ggc_collect ();
+
+#ifdef USE_C_ALLOCA
+      alloca (0);
+#endif
+    }
+
+  ggc_pop_context ();
+
+  if (max_elements_made < n_elements_made)
+    max_elements_made = n_elements_made;
+
+  /* Clean up.  */
+  end_alias_analysis ();
+  free (uid_cuid);
+  free (reg_eqv_table);
+  free (val.path);
+
+  return cse_jumps_altered || recorded_label_ref;
+}
+
+/* Process a single basic block.  FROM and TO and the limits of the basic
+   block.  NEXT_BRANCH points to the branch path when following jumps or
+   a null path when not following jumps.
+
+   AROUND_LOOP is nonzero if we are to try to cse around to the start of a
+   loop.  This is true when we are being called for the last time on a
+   block and this CSE pass is before loop.c.  */
+
+static rtx
+cse_basic_block (rtx from, rtx to, struct branch_path *next_branch,
+		 int around_loop)
+{
+  rtx insn;
+  int to_usage = 0;
+  rtx libcall_insn = NULL_RTX;
+  int num_insns = 0;
+  int no_conflict = 0;
+
+  /* Allocate the space needed by qty_table.  */
+  qty_table = xmalloc (max_qty * sizeof (struct qty_table_elem));
+
+  new_basic_block ();
+
+  /* TO might be a label.  If so, protect it from being deleted.  */
+  if (to != 0 && GET_CODE (to) == CODE_LABEL)
+    ++LABEL_NUSES (to);
+
+  for (insn = from; insn != to; insn = NEXT_INSN (insn))
+    {
+      enum rtx_code code = GET_CODE (insn);
+
+      /* If we have processed 1,000 insns, flush the hash table to
+	 avoid extreme quadratic behavior.  We must not include NOTEs
+	 in the count since there may be more of them when generating
+	 debugging information.  If we clear the table at different
+	 times, code generated with -g -O might be different than code
+	 generated with -O but not -g.
+
+	 ??? This is a real kludge and needs to be done some other way.
+	 Perhaps for 2.9.  */
+      if (code != NOTE && num_insns++ > 1000)
+	{
+	  flush_hash_table ();
+	  num_insns = 0;
+	}
+
+      /* See if this is a branch that is part of the path.  If so, and it is
+	 to be taken, do so.  */
+      if (next_branch->branch == insn)
+	{
+	  enum taken status = next_branch++->status;
+	  if (status != NOT_TAKEN)
+	    {
+	      if (status == TAKEN)
+		record_jump_equiv (insn, 1);
+	      else
+		invalidate_skipped_block (NEXT_INSN (insn));
+
+	      /* Set the last insn as the jump insn; it doesn't affect cc0.
+		 Then follow this branch.  */
+#ifdef HAVE_cc0
+	      prev_insn_cc0 = 0;
+	      prev_insn = insn;
+#endif
+	      insn = JUMP_LABEL (insn);
+	      continue;
+	    }
+	}
+
+      if (GET_MODE (insn) == QImode)
+	PUT_MODE (insn, VOIDmode);
+
+      if (GET_RTX_CLASS (code) == 'i')
+	{
+	  rtx p;
+
+	  /* Process notes first so we have all notes in canonical forms when
+	     looking for duplicate operations.  */
+
+	  if (REG_NOTES (insn))
+	    REG_NOTES (insn) = cse_process_notes (REG_NOTES (insn), NULL_RTX);
+
+	  /* Track when we are inside in LIBCALL block.  Inside such a block,
+	     we do not want to record destinations.  The last insn of a
+	     LIBCALL block is not considered to be part of the block, since
+	     its destination is the result of the block and hence should be
+	     recorded.  */
+
+	  if (REG_NOTES (insn) != 0)
+	    {
+	      if ((p = find_reg_note (insn, REG_LIBCALL, NULL_RTX)))
+		libcall_insn = XEXP (p, 0);
+	      else if (find_reg_note (insn, REG_RETVAL, NULL_RTX))
+		{
+		  /* Keep libcall_insn for the last SET insn of a no-conflict
+		     block to prevent changing the destination.  */
+		  if (! no_conflict)
+		    libcall_insn = 0;
+		  else
+		    no_conflict = -1;
+		}
+	      else if (find_reg_note (insn, REG_NO_CONFLICT, NULL_RTX))
+		no_conflict = 1;
+	    }
+
+	  cse_insn (insn, libcall_insn);
+
+	  if (no_conflict == -1)
+	    {
+	      libcall_insn = 0;
+	      no_conflict = 0;
+	    }
+	    
+	  /* If we haven't already found an insn where we added a LABEL_REF,
+	     check this one.  */
+	  if (GET_CODE (insn) == INSN && ! recorded_label_ref
+	      && for_each_rtx (&PATTERN (insn), check_for_label_ref,
+			       (void *) insn))
+	    recorded_label_ref = 1;
+	}
+
+      /* If INSN is now an unconditional jump, skip to the end of our
+	 basic block by pretending that we just did the last insn in the
+	 basic block.  If we are jumping to the end of our block, show
+	 that we can have one usage of TO.  */
+
+      if (any_uncondjump_p (insn))
+	{
+	  if (to == 0)
+	    {
+	      free (qty_table);
+	      return 0;
+	    }
+
+	  if (JUMP_LABEL (insn) == to)
+	    to_usage = 1;
+
+	  /* Maybe TO was deleted because the jump is unconditional.
+	     If so, there is nothing left in this basic block.  */
+	  /* ??? Perhaps it would be smarter to set TO
+	     to whatever follows this insn,
+	     and pretend the basic block had always ended here.  */
+	  if (INSN_DELETED_P (to))
+	    break;
+
+	  insn = PREV_INSN (to);
+	}
+
+      /* See if it is ok to keep on going past the label
+	 which used to end our basic block.  Remember that we incremented
+	 the count of that label, so we decrement it here.  If we made
+	 a jump unconditional, TO_USAGE will be one; in that case, we don't
+	 want to count the use in that jump.  */
+
+      if (to != 0 && NEXT_INSN (insn) == to
+	  && GET_CODE (to) == CODE_LABEL && --LABEL_NUSES (to) == to_usage)
+	{
+	  struct cse_basic_block_data val;
+	  rtx prev;
+
+	  insn = NEXT_INSN (to);
+
+	  /* If TO was the last insn in the function, we are done.  */
+	  if (insn == 0)
+	    {
+	      free (qty_table);
+	      return 0;
+	    }
+
+	  /* If TO was preceded by a BARRIER we are done with this block
+	     because it has no continuation.  */
+	  prev = prev_nonnote_insn (to);
+	  if (prev && GET_CODE (prev) == BARRIER)
+	    {
+	      free (qty_table);
+	      return insn;
+	    }
+
+	  /* Find the end of the following block.  Note that we won't be
+	     following branches in this case.  */
+	  to_usage = 0;
+	  val.path_size = 0;
+	  val.path = xmalloc (sizeof (struct branch_path)
+			      * PARAM_VALUE (PARAM_MAX_CSE_PATH_LENGTH));
+	  cse_end_of_basic_block (insn, &val, 0, 0, 0);
+	  free (val.path);
+
+	  /* If the tables we allocated have enough space left
+	     to handle all the SETs in the next basic block,
+	     continue through it.  Otherwise, return,
+	     and that block will be scanned individually.  */
+	  if (val.nsets * 2 + next_qty > max_qty)
+	    break;
+
+	  cse_basic_block_start = val.low_cuid;
+	  cse_basic_block_end = val.high_cuid;
+	  to = val.last;
+
+	  /* Prevent TO from being deleted if it is a label.  */
+	  if (to != 0 && GET_CODE (to) == CODE_LABEL)
+	    ++LABEL_NUSES (to);
+
+	  /* Back up so we process the first insn in the extension.  */
+	  insn = PREV_INSN (insn);
+	}
+    }
+
+  if (next_qty > max_qty)
+    abort ();
+
+  /* If we are running before loop.c, we stopped on a NOTE_INSN_LOOP_END, and
+     the previous insn is the only insn that branches to the head of a loop,
+     we can cse into the loop.  Don't do this if we changed the jump
+     structure of a loop unless we aren't going to be following jumps.  */
+
+  insn = prev_nonnote_insn (to);
+  if ((cse_jumps_altered == 0
+       || (flag_cse_follow_jumps == 0 && flag_cse_skip_blocks == 0))
+      && around_loop && to != 0
+      && GET_CODE (to) == NOTE && NOTE_LINE_NUMBER (to) == NOTE_INSN_LOOP_END
+      && GET_CODE (insn) == JUMP_INSN
+      && JUMP_LABEL (insn) != 0
+      && LABEL_NUSES (JUMP_LABEL (insn)) == 1)
+    cse_around_loop (JUMP_LABEL (insn));
+
+  free (qty_table);
+
+  return to ? NEXT_INSN (to) : 0;
+}
+
+/* Called via for_each_rtx to see if an insn is using a LABEL_REF for which
+   there isn't a REG_LABEL note.  Return one if so.  DATA is the insn.  */
+
+static int
+check_for_label_ref (rtx *rtl, void *data)
+{
+  rtx insn = (rtx) data;
+
+  /* If this insn uses a LABEL_REF and there isn't a REG_LABEL note for it,
+     we must rerun jump since it needs to place the note.  If this is a
+     LABEL_REF for a CODE_LABEL that isn't in the insn chain, don't do this
+     since no REG_LABEL will be added.  */
+  return (GET_CODE (*rtl) == LABEL_REF
+	  && ! LABEL_REF_NONLOCAL_P (*rtl)
+	  && LABEL_P (XEXP (*rtl, 0))
+	  && INSN_UID (XEXP (*rtl, 0)) != 0
+	  && ! find_reg_note (insn, REG_LABEL, XEXP (*rtl, 0)));
+}
+
+/* Count the number of times registers are used (not set) in X.
+   COUNTS is an array in which we accumulate the count, INCR is how much
+   we count each register usage.  */
+
+static void
+count_reg_usage (rtx x, int *counts, int incr)
+{
+  enum rtx_code code;
+  rtx note;
+  const char *fmt;
+  int i, j;
+
+  if (x == 0)
+    return;
+
+  switch (code = GET_CODE (x))
+    {
+    case REG:
+      counts[REGNO (x)] += incr;
+      return;
+
+    case PC:
+    case CC0:
+    case CONST:
+    case CONST_INT:
+    case CONST_DOUBLE:
+    case CONST_VECTOR:
+    case SYMBOL_REF:
+    case LABEL_REF:
+      return;
+
+    case CLOBBER:
+      /* If we are clobbering a MEM, mark any registers inside the address
+         as being used.  */
+      if (GET_CODE (XEXP (x, 0)) == MEM)
+	count_reg_usage (XEXP (XEXP (x, 0), 0), counts, incr);
+      return;
+
+    case SET:
+      /* Unless we are setting a REG, count everything in SET_DEST.  */
+      if (GET_CODE (SET_DEST (x)) != REG)
+	count_reg_usage (SET_DEST (x), counts, incr);
+      count_reg_usage (SET_SRC (x), counts, incr);
+      return;
+
+    case CALL_INSN:
+      count_reg_usage (CALL_INSN_FUNCTION_USAGE (x), counts, incr);
+      /* Fall through.  */
+
+    case INSN:
+    case JUMP_INSN:
+      count_reg_usage (PATTERN (x), counts, incr);
+
+      /* Things used in a REG_EQUAL note aren't dead since loop may try to
+	 use them.  */
+
+      note = find_reg_equal_equiv_note (x);
+      if (note)
+	{
+	  rtx eqv = XEXP (note, 0);
+
+	  if (GET_CODE (eqv) == EXPR_LIST)
+	  /* This REG_EQUAL note describes the result of a function call.
+	     Process all the arguments.  */
+	    do
+	      {
+		count_reg_usage (XEXP (eqv, 0), counts, incr);
+		eqv = XEXP (eqv, 1);
+	      }
+	    while (eqv && GET_CODE (eqv) == EXPR_LIST);
+	  else
+	    count_reg_usage (eqv, counts, incr);
+	}
+      return;
+
+    case EXPR_LIST:
+      if (REG_NOTE_KIND (x) == REG_EQUAL
+	  || (REG_NOTE_KIND (x) != REG_NONNEG && GET_CODE (XEXP (x,0)) == USE)
+	  /* FUNCTION_USAGE expression lists may include (CLOBBER (mem /u)),
+	     involving registers in the address.  */
+	  || GET_CODE (XEXP (x, 0)) == CLOBBER)
+	count_reg_usage (XEXP (x, 0), counts, incr);
+
+      count_reg_usage (XEXP (x, 1), counts, incr);
+      return;
+
+    case ASM_OPERANDS:
+      /* Iterate over just the inputs, not the constraints as well.  */
+      for (i = ASM_OPERANDS_INPUT_LENGTH (x) - 1; i >= 0; i--)
+	count_reg_usage (ASM_OPERANDS_INPUT (x, i), counts, incr);
+      return;
+
+    case INSN_LIST:
+      abort ();
+
+    default:
+      break;
+    }
+
+  fmt = GET_RTX_FORMAT (code);
+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)
+    {
+      if (fmt[i] == 'e')
+	count_reg_usage (XEXP (x, i), counts, incr);
+      else if (fmt[i] == 'E')
+	for (j = XVECLEN (x, i) - 1; j >= 0; j--)
+	  count_reg_usage (XVECEXP (x, i, j), counts, incr);
+    }
+}
+
+/* Return true if set is live.  */
+static bool
+set_live_p (rtx set, rtx insn ATTRIBUTE_UNUSED, /* Only used with HAVE_cc0.  */
+	    int *counts)
+{
+#ifdef HAVE_cc0
+  rtx tem;
+#endif
+
+  if (set_noop_p (set))
+    ;
+
+#ifdef HAVE_cc0
+  else if (GET_CODE (SET_DEST (set)) == CC0
+	   && !side_effects_p (SET_SRC (set))
+	   && ((tem = next_nonnote_insn (insn)) == 0
+	       || !INSN_P (tem)
+	       || !reg_referenced_p (cc0_rtx, PATTERN (tem))))
+    return false;
+#endif
+  else if (GET_CODE (SET_DEST (set)) != REG
+	   || REGNO (SET_DEST (set)) < FIRST_PSEUDO_REGISTER
+	   || counts[REGNO (SET_DEST (set))] != 0
+	   || side_effects_p (SET_SRC (set))
+	   /* An ADDRESSOF expression can turn into a use of the
+	      internal arg pointer, so always consider the
+	      internal arg pointer live.  If it is truly dead,
+	      flow will delete the initializing insn.  */
+	   || (SET_DEST (set) == current_function_internal_arg_pointer))
+    return true;
+  return false;
+}
+
+/* Return true if insn is live.  */
+
+static bool
+insn_live_p (rtx insn, int *counts)
+{
+  int i;
+  if (flag_non_call_exceptions && may_trap_p (PATTERN (insn)))
+    return true;
+  else if (GET_CODE (PATTERN (insn)) == SET)
+    return set_live_p (PATTERN (insn), insn, counts);
+  else if (GET_CODE (PATTERN (insn)) == PARALLEL)
+    {
+      for (i = XVECLEN (PATTERN (insn), 0) - 1; i >= 0; i--)
+	{
+	  rtx elt = XVECEXP (PATTERN (insn), 0, i);
+
+	  if (GET_CODE (elt) == SET)
+	    {
+	      if (set_live_p (elt, insn, counts))
+		return true;
+	    }
+	  else if (GET_CODE (elt) != CLOBBER && GET_CODE (elt) != USE)
+	    return true;
+	}
+      return false;
+    }
+  else
+    return true;
+}
+
+/* Return true if libcall is dead as a whole.  */
+
+static bool
+dead_libcall_p (rtx insn, int *counts)
+{
+  rtx note, set, new;
+
+  /* See if there's a REG_EQUAL note on this insn and try to
+     replace the source with the REG_EQUAL expression.
+
+     We assume that insns with REG_RETVALs can only be reg->reg
+     copies at this point.  */
+  note = find_reg_note (insn, REG_EQUAL, NULL_RTX);
+  if (!note)
+    return false;
+
+  set = single_set (insn);
+  if (!set)
+    return false;
+
+  new = simplify_rtx (XEXP (note, 0));
+  if (!new)
+    new = XEXP (note, 0);
+
+  /* While changing insn, we must update the counts accordingly.  */
+  count_reg_usage (insn, counts, -1);
+
+  if (validate_change (insn, &SET_SRC (set), new, 0))
+    {
+      count_reg_usage (insn, counts, 1);
+      remove_note (insn, find_reg_note (insn, REG_RETVAL, NULL_RTX));
+      remove_note (insn, note);
+      return true;
+    }
+
+  if (CONSTANT_P (new))
+    {
+      new = force_const_mem (GET_MODE (SET_DEST (set)), new);
+      if (new && validate_change (insn, &SET_SRC (set), new, 0))
+	{
+	  count_reg_usage (insn, counts, 1);
+	  remove_note (insn, find_reg_note (insn, REG_RETVAL, NULL_RTX));
+	  remove_note (insn, note);
+	  return true;
+	}
+    }
+
+  count_reg_usage (insn, counts, 1);
+  return false;
+}
+
+/* Scan all the insns and delete any that are dead; i.e., they store a register
+   that is never used or they copy a register to itself.
+
+   This is used to remove insns made obviously dead by cse, loop or other
+   optimizations.  It improves the heuristics in loop since it won't try to
+   move dead invariants out of loops or make givs for dead quantities.  The
+   remaining passes of the compilation are also sped up.  */
+
+int
+delete_trivially_dead_insns (rtx insns, int nreg)
+{
+  int *counts;
+  rtx insn, prev;
+  int in_libcall = 0, dead_libcall = 0;
+  int ndead = 0, nlastdead, niterations = 0;
+
+  timevar_push (TV_DELETE_TRIVIALLY_DEAD);
+  /* First count the number of times each register is used.  */
+  counts = xcalloc (nreg, sizeof (int));
+  for (insn = next_real_insn (insns); insn; insn = next_real_insn (insn))
+    count_reg_usage (insn, counts, 1);
+
+  do
+    {
+      nlastdead = ndead;
+      niterations++;
+      /* Go from the last insn to the first and delete insns that only set unused
+	 registers or copy a register to itself.  As we delete an insn, remove
+	 usage counts for registers it uses.
+
+	 The first jump optimization pass may leave a real insn as the last
+	 insn in the function.   We must not skip that insn or we may end
+	 up deleting code that is not really dead.  */
+      insn = get_last_insn ();
+      if (! INSN_P (insn))
+	insn = prev_real_insn (insn);
+
+      for (; insn; insn = prev)
+	{
+	  int live_insn = 0;
+
+	  prev = prev_real_insn (insn);
+
+	  /* Don't delete any insns that are part of a libcall block unless
+	     we can delete the whole libcall block.
+
+	     Flow or loop might get confused if we did that.  Remember
+	     that we are scanning backwards.  */
+	  if (find_reg_note (insn, REG_RETVAL, NULL_RTX))
+	    {
+	      in_libcall = 1;
+	      live_insn = 1;
+	      dead_libcall = dead_libcall_p (insn, counts);
+	    }
+	  else if (in_libcall)
+	    live_insn = ! dead_libcall;
+	  else
+	    live_insn = insn_live_p (insn, counts);
+
+	  /* If this is a dead insn, delete it and show registers in it aren't
+	     being used.  */
+
+	  if (! live_insn)
+	    {
+	      count_reg_usage (insn, counts, -1);
+	      delete_insn_and_edges (insn);
+	      ndead++;
+	    }
+
+	  if (find_reg_note (insn, REG_LIBCALL, NULL_RTX))
+	    {
+	      in_libcall = 0;
+	      dead_libcall = 0;
+	    }
+	}
+    }
+  while (ndead != nlastdead);
+
+  if (rtl_dump_file && ndead)
+    fprintf (rtl_dump_file, "Deleted %i trivially dead insns; %i iterations\n",
+	     ndead, niterations);
+  /* Clean up.  */
+  free (counts);
+  timevar_pop (TV_DELETE_TRIVIALLY_DEAD);
+  return ndead;
+}
+
+/* This function is called via for_each_rtx.  The argument, NEWREG, is
+   a condition code register with the desired mode.  If we are looking
+   at the same register in a different mode, replace it with
+   NEWREG.  */
+
+static int
+cse_change_cc_mode (rtx *loc, void *data)
+{
+  rtx newreg = (rtx) data;
+
+  if (*loc
+      && GET_CODE (*loc) == REG
+      && REGNO (*loc) == REGNO (newreg)
+      && GET_MODE (*loc) != GET_MODE (newreg))
+    {
+      *loc = newreg;
+      return -1;
+    }
+  return 0;
+}
+
+/* Change the mode of any reference to the register REGNO (NEWREG) to
+   GET_MODE (NEWREG), starting at START.  Stop before END.  Stop at
+   any instruction which modifies NEWREG.  */
+
+static void
+cse_change_cc_mode_insns (rtx start, rtx end, rtx newreg)
+{
+  rtx insn;
+
+  for (insn = start; insn != end; insn = NEXT_INSN (insn))
+    {
+      if (! INSN_P (insn))
+	continue;
+
+      if (reg_set_p (newreg, insn))
+	return;
+
+      for_each_rtx (&PATTERN (insn), cse_change_cc_mode, newreg);
+      for_each_rtx (&REG_NOTES (insn), cse_change_cc_mode, newreg);
+    }
+}
+
+/* BB is a basic block which finishes with CC_REG as a condition code
+   register which is set to CC_SRC.  Look through the successors of BB
+   to find blocks which have a single predecessor (i.e., this one),
+   and look through those blocks for an assignment to CC_REG which is
+   equivalent to CC_SRC.  CAN_CHANGE_MODE indicates whether we are
+   permitted to change the mode of CC_SRC to a compatible mode.  This
+   returns VOIDmode if no equivalent assignments were found.
+   Otherwise it returns the mode which CC_SRC should wind up with.
+
+   The main complexity in this function is handling the mode issues.
+   We may have more than one duplicate which we can eliminate, and we
+   try to find a mode which will work for multiple duplicates.  */
+
+static enum machine_mode
+cse_cc_succs (basic_block bb, rtx cc_reg, rtx cc_src, bool can_change_mode)
+{
+  bool found_equiv;
+  enum machine_mode mode;
+  unsigned int insn_count;
+  edge e;
+  rtx insns[2];
+  enum machine_mode modes[2];
+  rtx last_insns[2];
+  unsigned int i;
+  rtx newreg;
+
+  /* We expect to have two successors.  Look at both before picking
+     the final mode for the comparison.  If we have more successors
+     (i.e., some sort of table jump, although that seems unlikely),
+     then we require all beyond the first two to use the same
+     mode.  */
+
+  found_equiv = false;
+  mode = GET_MODE (cc_src);
+  insn_count = 0;
+  for (e = bb->succ; e; e = e->succ_next)
+    {
+      rtx insn;
+      rtx end;
+
+      if (e->flags & EDGE_COMPLEX)
+	continue;
+
+      if (! e->dest->pred
+	  || e->dest->pred->pred_next
+	  || e->dest == EXIT_BLOCK_PTR)
+	continue;
+
+      end = NEXT_INSN (BB_END (e->dest));
+      for (insn = BB_HEAD (e->dest); insn != end; insn = NEXT_INSN (insn))
+	{
+	  rtx set;
+
+	  if (! INSN_P (insn))
+	    continue;
+
+	  /* If CC_SRC is modified, we have to stop looking for
+	     something which uses it.  */
+	  if (modified_in_p (cc_src, insn))
+	    break;
+
+	  /* Check whether INSN sets CC_REG to CC_SRC.  */
+	  set = single_set (insn);
+	  if (set
+	      && GET_CODE (SET_DEST (set)) == REG
+	      && REGNO (SET_DEST (set)) == REGNO (cc_reg))
+	    {
+	      bool found;
+	      enum machine_mode set_mode;
+	      enum machine_mode comp_mode;
+
+	      found = false;
+	      set_mode = GET_MODE (SET_SRC (set));
+	      comp_mode = set_mode;
+	      if (rtx_equal_p (cc_src, SET_SRC (set)))
+		found = true;
+	      else if (GET_CODE (cc_src) == COMPARE
+		       && GET_CODE (SET_SRC (set)) == COMPARE
+		       && mode != set_mode
+		       && rtx_equal_p (XEXP (cc_src, 0),
+				       XEXP (SET_SRC (set), 0))
+		       && rtx_equal_p (XEXP (cc_src, 1),
+				       XEXP (SET_SRC (set), 1)))
+			   
+		{
+		  comp_mode = (*targetm.cc_modes_compatible) (mode, set_mode);
+		  if (comp_mode != VOIDmode
+		      && (can_change_mode || comp_mode == mode))
+		    found = true;
+		}
+
+	      if (found)
+		{
+		  found_equiv = true;
+		  if (insn_count < ARRAY_SIZE (insns))
+		    {
+		      insns[insn_count] = insn;
+		      modes[insn_count] = set_mode;
+		      last_insns[insn_count] = end;
+		      ++insn_count;
+
+		      if (mode != comp_mode)
+			{
+			  if (! can_change_mode)
+			    abort ();
+			  mode = comp_mode;
+			  PUT_MODE (cc_src, mode);
+			}
+		    }
+		  else
+		    {
+		      if (set_mode != mode)
+			{
+			  /* We found a matching expression in the
+			     wrong mode, but we don't have room to
+			     store it in the array.  Punt.  This case
+			     should be rare.  */
+			  break;
+			}
+		      /* INSN sets CC_REG to a value equal to CC_SRC
+			 with the right mode.  We can simply delete
+			 it.  */
+		      delete_insn (insn);
+		    }
+
+		  /* We found an instruction to delete.  Keep looking,
+		     in the hopes of finding a three-way jump.  */
+		  continue;
+		}
+
+	      /* We found an instruction which sets the condition
+		 code, so don't look any farther.  */
+	      break;
+	    }
+
+	  /* If INSN sets CC_REG in some other way, don't look any
+	     farther.  */
+	  if (reg_set_p (cc_reg, insn))
+	    break;
+	}
+
+      /* If we fell off the bottom of the block, we can keep looking
+	 through successors.  We pass CAN_CHANGE_MODE as false because
+	 we aren't prepared to handle compatibility between the
+	 further blocks and this block.  */
+      if (insn == end)
+	{
+	  enum machine_mode submode;
+
+	  submode = cse_cc_succs (e->dest, cc_reg, cc_src, false);
+	  if (submode != VOIDmode)
+	    {
+	      if (submode != mode)
+		abort ();
+	      found_equiv = true;
+	      can_change_mode = false;
+	    }
+	}
+    }
+
+  if (! found_equiv)
+    return VOIDmode;
+
+  /* Now INSN_COUNT is the number of instructions we found which set
+     CC_REG to a value equivalent to CC_SRC.  The instructions are in
+     INSNS.  The modes used by those instructions are in MODES.  */
+
+  newreg = NULL_RTX;
+  for (i = 0; i < insn_count; ++i)
+    {
+      if (modes[i] != mode)
+	{
+	  /* We need to change the mode of CC_REG in INSNS[i] and
+	     subsequent instructions.  */
+	  if (! newreg)
+	    {
+	      if (GET_MODE (cc_reg) == mode)
+		newreg = cc_reg;
+	      else
+		newreg = gen_rtx_REG (mode, REGNO (cc_reg));
+	    }
+	  cse_change_cc_mode_insns (NEXT_INSN (insns[i]), last_insns[i],
+				    newreg);
+	}
+
+      delete_insn (insns[i]);
+    }
+
+  return mode;
+}
+
+/* If we have a fixed condition code register (or two), walk through
+   the instructions and try to eliminate duplicate assignments.  */
+
+void
+cse_condition_code_reg (void)
+{
+  unsigned int cc_regno_1;
+  unsigned int cc_regno_2;
+  rtx cc_reg_1;
+  rtx cc_reg_2;
+  basic_block bb;
+
+  if (! (*targetm.fixed_condition_code_regs) (&cc_regno_1, &cc_regno_2))
+    return;
+
+  cc_reg_1 = gen_rtx_REG (CCmode, cc_regno_1);
+  if (cc_regno_2 != INVALID_REGNUM)
+    cc_reg_2 = gen_rtx_REG (CCmode, cc_regno_2);
+  else
+    cc_reg_2 = NULL_RTX;
+
+  FOR_EACH_BB (bb)
+    {
+      rtx last_insn;
+      rtx cc_reg;
+      rtx insn;
+      rtx cc_src_insn;
+      rtx cc_src;
+      enum machine_mode mode;
+      enum machine_mode orig_mode;
+
+      /* Look for blocks which end with a conditional jump based on a
+	 condition code register.  Then look for the instruction which
+	 sets the condition code register.  Then look through the
+	 successor blocks for instructions which set the condition
+	 code register to the same value.  There are other possible
+	 uses of the condition code register, but these are by far the
+	 most common and the ones which we are most likely to be able
+	 to optimize.  */
+
+      last_insn = BB_END (bb);
+      if (GET_CODE (last_insn) != JUMP_INSN)
+	continue;
+
+      if (reg_referenced_p (cc_reg_1, PATTERN (last_insn)))
+	cc_reg = cc_reg_1;
+      else if (cc_reg_2 && reg_referenced_p (cc_reg_2, PATTERN (last_insn)))
+	cc_reg = cc_reg_2;
+      else
+	continue;
+
+      cc_src_insn = NULL_RTX;
+      cc_src = NULL_RTX;
+      for (insn = PREV_INSN (last_insn);
+	   insn && insn != PREV_INSN (BB_HEAD (bb));
+	   insn = PREV_INSN (insn))
+	{
+	  rtx set;
+
+	  if (! INSN_P (insn))
+	    continue;
+	  set = single_set (insn);
+	  if (set
+	      && GET_CODE (SET_DEST (set)) == REG
+	      && REGNO (SET_DEST (set)) == REGNO (cc_reg))
+	    {
+	      cc_src_insn = insn;
+	      cc_src = SET_SRC (set);
+	      break;
+	    }
+	  else if (reg_set_p (cc_reg, insn))
+	    break;
+	}
+
+      if (! cc_src_insn)
+	continue;
+
+      if (modified_between_p (cc_src, cc_src_insn, NEXT_INSN (last_insn)))
+	continue;
+
+      /* Now CC_REG is a condition code register used for a
+	 conditional jump at the end of the block, and CC_SRC, in
+	 CC_SRC_INSN, is the value to which that condition code
+	 register is set, and CC_SRC is still meaningful at the end of
+	 the basic block.  */
+
+      orig_mode = GET_MODE (cc_src);
+      mode = cse_cc_succs (bb, cc_reg, cc_src, true);
+      if (mode != VOIDmode)
+	{
+	  if (mode != GET_MODE (cc_src))
+	    abort ();
+	  if (mode != orig_mode)
+	    {
+	      rtx newreg = gen_rtx_REG (mode, REGNO (cc_reg));
+
+	      /* Change the mode of CC_REG in CC_SRC_INSN to
+		 GET_MODE (NEWREG).  */
+	      for_each_rtx (&PATTERN (cc_src_insn), cse_change_cc_mode,
+			    newreg);
+	      for_each_rtx (&REG_NOTES (cc_src_insn), cse_change_cc_mode,
+			    newreg);
+
+	      /* Do the same in the following insns that use the
+		 current value of CC_REG within BB.  */
+	      cse_change_cc_mode_insns (NEXT_INSN (cc_src_insn),
+					NEXT_INSN (last_insn),
+					newreg);
+	    }
+	}
+    }
+}
diff -Naur gcc-3.4.4/gcc/doc/invoke.texi gcc-3.4.4-ssp/gcc/doc/invoke.texi
--- gcc-3.4.4/gcc/doc/invoke.texi	2005-04-22 09:49:59.000000000 +0300
+++ gcc-3.4.4-ssp/gcc/doc/invoke.texi	2005-05-25 14:03:22.000000000 +0300
@@ -227,7 +227,7 @@
 -Wno-multichar  -Wnonnull  -Wpacked  -Wpadded @gol
 -Wparentheses  -Wpointer-arith  -Wredundant-decls @gol
 -Wreturn-type  -Wsequence-point  -Wshadow @gol
--Wsign-compare  -Wstrict-aliasing @gol
+-Wsign-compare  -Wstack-protector  -Wstrict-aliasing @gol
 -Wswitch  -Wswitch-default  -Wswitch-enum @gol
 -Wsystem-headers  -Wtrigraphs  -Wundef  -Wuninitialized @gol
 -Wunknown-pragmas  -Wunreachable-code @gol
@@ -675,6 +675,7 @@
 -fshort-double  -fshort-wchar @gol
 -fverbose-asm  -fpack-struct  -fstack-check @gol
 -fstack-limit-register=@var{reg}  -fstack-limit-symbol=@var{sym} @gol
+-fstack-protector  -fstack-protector-all @gol
 -fargument-alias  -fargument-noalias @gol
 -fargument-noalias-global  -fleading-underscore @gol
 -ftls-model=@var{model} @gol
@@ -2986,6 +2987,10 @@
 complex; GCC will refuse to optimize programs when the optimization
 itself is likely to take inordinate amounts of time.
 
+@item -Wstack-protector
+@opindex Wstack-protector
+Warn when not issuing stack smashing protection for some reason.
+
 @item -Werror
 @opindex Werror
 Make all warnings into errors.
@@ -11231,6 +11236,24 @@
 @option{-Wl,--defsym,__stack_limit=0x7ffe0000} to enforce a stack limit
 of 128KB@.  Note that this may only work with the GNU linker.
 
+@item -fstack-protector
+@item -fstack-protector-all
+@opindex fstack-protector
+@opindex fstack-protector-all
+@opindex fno-stack-protector
+Generate code to protect an application from a stack smashing
+attack. The features are (1) the insertion of random value next to the
+frame pointer to detect the integrity of the stack, (2) the reordering
+of local variables to place buffers after pointers to avoid the
+corruption of pointers that could be used to further corrupt arbitrary
+memory locations, (3) the copying of pointers in function arguments to
+an area preceding local variable buffers to prevent the corruption of
+pointers that could be used to further corrupt arbitrary memory
+locations, and the (4) omission of instrumentation code from some
+functions to decrease the performance overhead.  If the integrity
+would be broken, the program is aborted.  If stack-protector-all is
+specified, instrumentation codes are generated at every functions.
+
 @cindex aliasing of parameters
 @cindex parameters, aliased
 @item -fargument-alias
diff -Naur gcc-3.4.4/gcc/doc/invoke.texi~ gcc-3.4.4-ssp/gcc/doc/invoke.texi~
--- gcc-3.4.4/gcc/doc/invoke.texi~	1970-01-01 02:00:00.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/doc/invoke.texi~	2005-04-22 09:49:59.000000000 +0300
@@ -0,0 +1,11679 @@
+@c Copyright (C) 1988, 1989, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999,
+@c 2000, 2001, 2002, 2003, 2004, 2005 Free Software Foundation, Inc.
+@c This is part of the GCC manual.
+@c For copying conditions, see the file gcc.texi.
+
+@ignore
+@c man begin COPYRIGHT
+Copyright @copyright{} 1988, 1989, 1992, 1993, 1994, 1995, 1996, 1997,
+1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005 Free Software Foundation, Inc.
+
+Permission is granted to copy, distribute and/or modify this document
+under the terms of the GNU Free Documentation License, Version 1.2 or
+any later version published by the Free Software Foundation; with the
+Invariant Sections being ``GNU General Public License'' and ``Funding
+Free Software'', the Front-Cover texts being (a) (see below), and with
+the Back-Cover Texts being (b) (see below).  A copy of the license is
+included in the gfdl(7) man page.
+
+(a) The FSF's Front-Cover Text is:
+
+     A GNU Manual
+
+(b) The FSF's Back-Cover Text is:
+
+     You have freedom to copy and modify this GNU Manual, like GNU
+     software.  Copies published by the Free Software Foundation raise
+     funds for GNU development.
+@c man end
+@c Set file name and title for the man page.
+@setfilename gcc
+@settitle GNU project C and C++ compiler
+@c man begin SYNOPSIS
+gcc [@option{-c}|@option{-S}|@option{-E}] [@option{-std=}@var{standard}]
+    [@option{-g}] [@option{-pg}] [@option{-O}@var{level}]
+    [@option{-W}@var{warn}@dots{}] [@option{-pedantic}]
+    [@option{-I}@var{dir}@dots{}] [@option{-L}@var{dir}@dots{}]
+    [@option{-D}@var{macro}[=@var{defn}]@dots{}] [@option{-U}@var{macro}]
+    [@option{-f}@var{option}@dots{}] [@option{-m}@var{machine-option}@dots{}]
+    [@option{-o} @var{outfile}] @var{infile}@dots{}
+
+Only the most useful options are listed here; see below for the
+remainder.  @samp{g++} accepts mostly the same options as @samp{gcc}.
+@c man end
+@c man begin SEEALSO
+gpl(7), gfdl(7), fsf-funding(7),
+cpp(1), gcov(1), g77(1), as(1), ld(1), gdb(1), adb(1), dbx(1), sdb(1)
+and the Info entries for @file{gcc}, @file{cpp}, @file{g77}, @file{as},
+@file{ld}, @file{binutils} and @file{gdb}.
+@c man end
+@c man begin BUGS
+For instructions on reporting bugs, see
+@w{@uref{http://gcc.gnu.org/bugs.html}}.  Use of the @command{gccbug}
+script to report bugs is recommended.
+@c man end
+@c man begin AUTHOR
+See the Info entry for @command{gcc}, or
+@w{@uref{http://gcc.gnu.org/onlinedocs/gcc/Contributors.html}},
+for contributors to GCC@.
+@c man end
+@end ignore
+
+@node Invoking GCC
+@chapter GCC Command Options
+@cindex GCC command options
+@cindex command options
+@cindex options, GCC command
+
+@c man begin DESCRIPTION
+When you invoke GCC, it normally does preprocessing, compilation,
+assembly and linking.  The ``overall options'' allow you to stop this
+process at an intermediate stage.  For example, the @option{-c} option
+says not to run the linker.  Then the output consists of object files
+output by the assembler.
+
+Other options are passed on to one stage of processing.  Some options
+control the preprocessor and others the compiler itself.  Yet other
+options control the assembler and linker; most of these are not
+documented here, since you rarely need to use any of them.
+
+@cindex C compilation options
+Most of the command line options that you can use with GCC are useful
+for C programs; when an option is only useful with another language
+(usually C++), the explanation says so explicitly.  If the description
+for a particular option does not mention a source language, you can use
+that option with all supported languages.
+
+@cindex C++ compilation options
+@xref{Invoking G++,,Compiling C++ Programs}, for a summary of special
+options for compiling C++ programs.
+
+@cindex grouping options
+@cindex options, grouping
+The @command{gcc} program accepts options and file names as operands.  Many
+options have multi-letter names; therefore multiple single-letter options
+may @emph{not} be grouped: @option{-dr} is very different from @w{@samp{-d
+-r}}.
+
+@cindex order of options
+@cindex options, order
+You can mix options and other arguments.  For the most part, the order
+you use doesn't matter.  Order does matter when you use several options
+of the same kind; for example, if you specify @option{-L} more than once,
+the directories are searched in the order specified.
+
+Many options have long names starting with @samp{-f} or with
+@samp{-W}---for example, @option{-fforce-mem},
+@option{-fstrength-reduce}, @option{-Wformat} and so on.  Most of
+these have both positive and negative forms; the negative form of
+@option{-ffoo} would be @option{-fno-foo}.  This manual documents
+only one of these two forms, whichever one is not the default.
+
+@c man end
+
+@xref{Option Index}, for an index to GCC's options.
+
+@menu
+* Option Summary::	Brief list of all options, without explanations.
+* Overall Options::     Controlling the kind of output:
+                        an executable, object files, assembler files,
+                        or preprocessed source.
+* Invoking G++::	Compiling C++ programs.
+* C Dialect Options::   Controlling the variant of C language compiled.
+* C++ Dialect Options:: Variations on C++.
+* Objective-C Dialect Options:: Variations on Objective-C.
+* Language Independent Options:: Controlling how diagnostics should be
+                        formatted.
+* Warning Options::     How picky should the compiler be?
+* Debugging Options::   Symbol tables, measurements, and debugging dumps.
+* Optimize Options::    How much optimization?
+* Preprocessor Options:: Controlling header files and macro definitions.
+                         Also, getting dependency information for Make.
+* Assembler Options::   Passing options to the assembler.
+* Link Options::        Specifying libraries and so on.
+* Directory Options::   Where to find header files and libraries.
+                        Where to find the compiler executable files.
+* Spec Files::          How to pass switches to sub-processes.
+* Target Options::      Running a cross-compiler, or an old version of GCC.
+* Submodel Options::    Specifying minor hardware or convention variations,
+                        such as 68010 vs 68020.
+* Code Gen Options::    Specifying conventions for function calls, data layout
+                        and register usage.
+* Environment Variables:: Env vars that affect GCC.
+* Precompiled Headers:: Compiling a header once, and using it many times.
+* Running Protoize::    Automatically adding or removing function prototypes.
+@end menu
+
+@c man begin OPTIONS
+
+@node Option Summary
+@section Option Summary
+
+Here is a summary of all the options, grouped by type.  Explanations are
+in the following sections.
+
+@table @emph
+@item Overall Options
+@xref{Overall Options,,Options Controlling the Kind of Output}.
+@gccoptlist{-c  -S  -E  -o @var{file}  -pipe  -pass-exit-codes  @gol
+-x @var{language}  -v  -###  --help  --target-help  --version}
+
+@item C Language Options
+@xref{C Dialect Options,,Options Controlling C Dialect}.
+@gccoptlist{-ansi  -std=@var{standard}  -aux-info @var{filename} @gol
+-fno-asm  -fno-builtin  -fno-builtin-@var{function} @gol
+-fhosted  -ffreestanding  -fms-extensions @gol
+-trigraphs  -no-integrated-cpp  -traditional  -traditional-cpp @gol
+-fallow-single-precision  -fcond-mismatch @gol
+-fsigned-bitfields  -fsigned-char @gol
+-funsigned-bitfields  -funsigned-char @gol
+-fwritable-strings}
+
+@item C++ Language Options
+@xref{C++ Dialect Options,,Options Controlling C++ Dialect}.
+@gccoptlist{-fabi-version=@var{n}  -fno-access-control  -fcheck-new @gol
+-fconserve-space  -fno-const-strings @gol
+-fno-elide-constructors @gol
+-fno-enforce-eh-specs @gol
+-ffor-scope  -fno-for-scope  -fno-gnu-keywords @gol
+-fno-implicit-templates @gol
+-fno-implicit-inline-templates @gol
+-fno-implement-inlines  -fms-extensions @gol
+-fno-nonansi-builtins  -fno-operator-names @gol
+-fno-optional-diags  -fpermissive @gol
+-frepo  -fno-rtti  -fstats  -ftemplate-depth-@var{n} @gol
+-fuse-cxa-atexit  -fno-weak  -nostdinc++ @gol
+-fno-default-inline  -Wabi  -Wctor-dtor-privacy @gol
+-Wnon-virtual-dtor  -Wreorder @gol
+-Weffc++  -Wno-deprecated @gol
+-Wno-non-template-friend  -Wold-style-cast @gol
+-Woverloaded-virtual  -Wno-pmf-conversions @gol
+-Wsign-promo}
+
+@item Objective-C Language Options
+@xref{Objective-C Dialect Options,,Options Controlling Objective-C Dialect}.
+@gccoptlist{
+-fconstant-string-class=@var{class-name} @gol
+-fgnu-runtime  -fnext-runtime @gol
+-fno-nil-receivers @gol
+-fobjc-exceptions @gol
+-freplace-objc-classes @gol
+-fzero-link @gol
+-gen-decls @gol
+-Wno-protocol  -Wselector -Wundeclared-selector}
+
+@item Language Independent Options
+@xref{Language Independent Options,,Options to Control Diagnostic Messages Formatting}.
+@gccoptlist{-fmessage-length=@var{n}  @gol
+-fdiagnostics-show-location=@r{[}once@r{|}every-line@r{]}}
+
+@item Warning Options
+@xref{Warning Options,,Options to Request or Suppress Warnings}.
+@gccoptlist{-fsyntax-only  -pedantic  -pedantic-errors @gol
+-w  -Wextra  -Wall  -Waggregate-return @gol
+-Wcast-align  -Wcast-qual  -Wchar-subscripts  -Wcomment @gol
+-Wconversion  -Wno-deprecated-declarations @gol
+-Wdisabled-optimization  -Wno-div-by-zero  -Wendif-labels @gol
+-Werror  -Werror-implicit-function-declaration @gol
+-Wfloat-equal  -Wformat  -Wformat=2 @gol
+-Wno-format-extra-args -Wformat-nonliteral @gol
+-Wformat-security  -Wformat-y2k @gol
+-Wimplicit  -Wimplicit-function-declaration  -Wimplicit-int @gol
+-Wimport  -Wno-import  -Winit-self  -Winline @gol
+-Wno-invalid-offsetof  -Winvalid-pch @gol
+-Wlarger-than-@var{len}  -Wlong-long @gol
+-Wmain  -Wmissing-braces @gol
+-Wmissing-format-attribute  -Wmissing-noreturn @gol
+-Wno-multichar  -Wnonnull  -Wpacked  -Wpadded @gol
+-Wparentheses  -Wpointer-arith  -Wredundant-decls @gol
+-Wreturn-type  -Wsequence-point  -Wshadow @gol
+-Wsign-compare  -Wstrict-aliasing @gol
+-Wswitch  -Wswitch-default  -Wswitch-enum @gol
+-Wsystem-headers  -Wtrigraphs  -Wundef  -Wuninitialized @gol
+-Wunknown-pragmas  -Wunreachable-code @gol
+-Wunused  -Wunused-function  -Wunused-label  -Wunused-parameter @gol
+-Wunused-value  -Wunused-variable  -Wwrite-strings}
+
+@item C-only Warning Options
+@gccoptlist{-Wbad-function-cast  -Wmissing-declarations @gol
+-Wmissing-prototypes  -Wnested-externs  -Wold-style-definition @gol
+-Wstrict-prototypes  -Wtraditional @gol
+-Wdeclaration-after-statement}
+
+@item Debugging Options
+@xref{Debugging Options,,Options for Debugging Your Program or GCC}.
+@gccoptlist{-d@var{letters}  -dumpspecs  -dumpmachine  -dumpversion @gol
+-fdump-unnumbered  -fdump-translation-unit@r{[}-@var{n}@r{]} @gol
+-fdump-class-hierarchy@r{[}-@var{n}@r{]} @gol
+-fdump-tree-original@r{[}-@var{n}@r{]}  @gol
+-fdump-tree-optimized@r{[}-@var{n}@r{]} @gol
+-fdump-tree-inlined@r{[}-@var{n}@r{]} @gol
+-feliminate-dwarf2-dups -feliminate-unused-debug-types @gol
+-feliminate-unused-debug-symbols -fmem-report -fprofile-arcs @gol
+-frandom-seed=@var{string} -fsched-verbose=@var{n} @gol
+-ftest-coverage  -ftime-report @gol
+-g  -g@var{level}  -gcoff -gdwarf-2 @gol
+-ggdb  -gstabs  -gstabs+  -gvms  -gxcoff  -gxcoff+ @gol
+-p  -pg  -print-file-name=@var{library}  -print-libgcc-file-name @gol
+-print-multi-directory  -print-multi-lib @gol
+-print-prog-name=@var{program}  -print-search-dirs  -Q @gol
+-save-temps  -time}
+
+@item Optimization Options
+@xref{Optimize Options,,Options that Control Optimization}.
+@gccoptlist{-falign-functions=@var{n}  -falign-jumps=@var{n} @gol
+-falign-labels=@var{n}  -falign-loops=@var{n}  @gol
+-fbranch-probabilities -fprofile-values -fvpt -fbranch-target-load-optimize @gol
+-fbranch-target-load-optimize2 -fcaller-saves  -fcprop-registers @gol
+-fcse-follow-jumps  -fcse-skip-blocks  -fdata-sections @gol
+-fdelayed-branch  -fdelete-null-pointer-checks @gol
+-fexpensive-optimizations  -ffast-math  -ffloat-store @gol
+-fforce-addr  -fforce-mem  -ffunction-sections @gol
+-fgcse  -fgcse-lm  -fgcse-sm  -fgcse-las  -floop-optimize @gol
+-fcrossjumping  -fif-conversion  -fif-conversion2 @gol
+-finline-functions  -finline-limit=@var{n}  -fkeep-inline-functions @gol
+-fkeep-static-consts  -fmerge-constants  -fmerge-all-constants @gol
+-fmove-all-movables  -fnew-ra  -fno-branch-count-reg @gol
+-fno-default-inline  -fno-defer-pop @gol
+-fno-function-cse  -fno-guess-branch-probability @gol
+-fno-inline  -fno-math-errno  -fno-peephole  -fno-peephole2 @gol
+-funsafe-math-optimizations  -ffinite-math-only @gol
+-fno-trapping-math  -fno-zero-initialized-in-bss @gol
+-fomit-frame-pointer  -foptimize-register-move @gol
+-foptimize-sibling-calls  -fprefetch-loop-arrays @gol
+-fprofile-generate -fprofile-use @gol
+-freduce-all-givs  -fregmove  -frename-registers @gol
+-freorder-blocks  -freorder-functions @gol
+-frerun-cse-after-loop  -frerun-loop-opt @gol
+-frounding-math -fschedule-insns  -fschedule-insns2 @gol
+-fno-sched-interblock  -fno-sched-spec  -fsched-spec-load @gol
+-fsched-spec-load-dangerous  @gol
+-fsched-stalled-insns=@var{n} -sched-stalled-insns-dep=@var{n} @gol
+-fsched2-use-superblocks @gol
+-fsched2-use-traces  -fsignaling-nans @gol
+-fsingle-precision-constant  @gol
+-fstrength-reduce  -fstrict-aliasing  -ftracer  -fthread-jumps @gol
+-funroll-all-loops  -funroll-loops  -fpeel-loops @gol
+-funswitch-loops  -fold-unroll-loops  -fold-unroll-all-loops @gol
+--param @var{name}=@var{value}
+-O  -O0  -O1  -O2  -O3  -Os}
+
+@item Preprocessor Options
+@xref{Preprocessor Options,,Options Controlling the Preprocessor}.
+@gccoptlist{-A@var{question}=@var{answer} @gol
+-A-@var{question}@r{[}=@var{answer}@r{]} @gol
+-C  -dD  -dI  -dM  -dN @gol
+-D@var{macro}@r{[}=@var{defn}@r{]}  -E  -H @gol
+-idirafter @var{dir} @gol
+-include @var{file}  -imacros @var{file} @gol
+-iprefix @var{file}  -iwithprefix @var{dir} @gol
+-iwithprefixbefore @var{dir}  -isystem @var{dir} @gol
+-M  -MM  -MF  -MG  -MP  -MQ  -MT  -nostdinc  @gol
+-P  -fworking-directory  -remap @gol
+-trigraphs  -undef  -U@var{macro}  -Wp,@var{option} @gol
+-Xpreprocessor @var{option}}
+
+@item Assembler Option
+@xref{Assembler Options,,Passing Options to the Assembler}.
+@gccoptlist{-Wa,@var{option}  -Xassembler @var{option}}
+
+@item Linker Options
+@xref{Link Options,,Options for Linking}.
+@gccoptlist{@var{object-file-name}  -l@var{library} @gol
+-nostartfiles  -nodefaultlibs  -nostdlib -pie @gol
+-s  -static  -static-libgcc  -shared  -shared-libgcc  -symbolic @gol
+-Wl,@var{option}  -Xlinker @var{option} @gol
+-u @var{symbol}}
+
+@item Directory Options
+@xref{Directory Options,,Options for Directory Search}.
+@gccoptlist{-B@var{prefix}  -I@var{dir}  -I-  -L@var{dir}  -specs=@var{file}}
+
+@item Target Options
+@c I wrote this xref this way to avoid overfull hbox. -- rms
+@xref{Target Options}.
+@gccoptlist{-V @var{version}  -b @var{machine}}
+
+@item Machine Dependent Options
+@xref{Submodel Options,,Hardware Models and Configurations}.
+
+@emph{M680x0 Options}
+@gccoptlist{-m68000  -m68020  -m68020-40  -m68020-60  -m68030  -m68040 @gol
+-m68060  -mcpu32  -m5200  -m68881  -mbitfield  -mc68000  -mc68020   @gol
+-mnobitfield  -mrtd  -mshort  -msoft-float  -mpcrel @gol
+-malign-int  -mstrict-align  -msep-data  -mno-sep-data @gol
+-mshared-library-id=n  -mid-shared-library  -mno-id-shared-library}
+
+@emph{M68hc1x Options}
+@gccoptlist{-m6811  -m6812  -m68hc11  -m68hc12   -m68hcs12 @gol
+-mauto-incdec  -minmax  -mlong-calls  -mshort @gol
+-msoft-reg-count=@var{count}}
+
+@emph{VAX Options}
+@gccoptlist{-mg  -mgnu  -munix}
+
+@emph{SPARC Options}
+@gccoptlist{-mcpu=@var{cpu-type} @gol
+-mtune=@var{cpu-type} @gol
+-mcmodel=@var{code-model} @gol
+-m32  -m64  -mapp-regs  -mno-app-regs @gol
+-mfaster-structs  -mno-faster-structs @gol
+-mflat  -mno-flat  -mfpu  -mno-fpu @gol
+-mhard-float  -msoft-float @gol
+-mhard-quad-float  -msoft-quad-float @gol
+-mimpure-text  -mno-impure-text  -mlittle-endian @gol
+-mstack-bias  -mno-stack-bias @gol
+-munaligned-doubles  -mno-unaligned-doubles @gol
+-mv8plus  -mno-v8plus  -mvis  -mno-vis @gol
+-mcypress  -mf930  -mf934 @gol
+-msparclite  -msupersparc  -mv8
+-threads -pthreads}
+
+@emph{ARM Options}
+@gccoptlist{-mapcs-frame  -mno-apcs-frame @gol
+-mapcs-26  -mapcs-32 @gol
+-mapcs-stack-check  -mno-apcs-stack-check @gol
+-mapcs-float  -mno-apcs-float @gol
+-mapcs-reentrant  -mno-apcs-reentrant @gol
+-msched-prolog  -mno-sched-prolog @gol
+-mlittle-endian  -mbig-endian  -mwords-little-endian @gol
+-malignment-traps  -mno-alignment-traps @gol
+-msoft-float  -mhard-float  -mfpe @gol
+-mthumb-interwork  -mno-thumb-interwork @gol
+-mcpu=@var{name}  -march=@var{name}  -mfpe=@var{name}  @gol
+-mstructure-size-boundary=@var{n} @gol
+-mabort-on-noreturn @gol
+-mlong-calls  -mno-long-calls @gol
+-msingle-pic-base  -mno-single-pic-base @gol
+-mpic-register=@var{reg} @gol
+-mnop-fun-dllimport @gol
+-mcirrus-fix-invalid-insns -mno-cirrus-fix-invalid-insns @gol
+-mpoke-function-name @gol
+-mthumb  -marm @gol
+-mtpcs-frame  -mtpcs-leaf-frame @gol
+-mcaller-super-interworking  -mcallee-super-interworking}
+
+@emph{MN10300 Options}
+@gccoptlist{-mmult-bug  -mno-mult-bug @gol
+-mam33  -mno-am33 @gol
+-mam33-2  -mno-am33-2 @gol
+-mno-crt0  -mrelax}
+
+@emph{M32R/D Options}
+@gccoptlist{-m32r2 -m32rx -m32r @gol
+-mdebug @gol
+-malign-loops -mno-align-loops @gol
+-missue-rate=@var{number} @gol
+-mbranch-cost=@var{number} @gol
+-mmodel=@var{code-size-model-type} @gol
+-msdata=@var{sdata-type} @gol
+-mno-flush-func -mflush-func=@var{name} @gol
+-mno-flush-trap -mflush-trap=@var{number} @gol
+-G @var{num}}
+
+@emph{RS/6000 and PowerPC Options}
+@gccoptlist{-mcpu=@var{cpu-type} @gol
+-mtune=@var{cpu-type} @gol
+-mpower  -mno-power  -mpower2  -mno-power2 @gol
+-mpowerpc  -mpowerpc64  -mno-powerpc @gol
+-maltivec  -mno-altivec @gol
+-mpowerpc-gpopt  -mno-powerpc-gpopt @gol
+-mpowerpc-gfxopt  -mno-powerpc-gfxopt @gol
+-mnew-mnemonics  -mold-mnemonics @gol
+-mfull-toc   -mminimal-toc  -mno-fp-in-toc  -mno-sum-in-toc @gol
+-m64  -m32  -mxl-compat  -mno-xl-compat  -mpe @gol
+-malign-power  -malign-natural @gol
+-msoft-float  -mhard-float  -mmultiple  -mno-multiple @gol
+-mstring  -mno-string  -mupdate  -mno-update @gol
+-mfused-madd  -mno-fused-madd  -mbit-align  -mno-bit-align @gol
+-mstrict-align  -mno-strict-align  -mrelocatable @gol
+-mno-relocatable  -mrelocatable-lib  -mno-relocatable-lib @gol
+-mtoc  -mno-toc  -mlittle  -mlittle-endian  -mbig  -mbig-endian @gol
+-mdynamic-no-pic @gol
+-mprioritize-restricted-insns=@var{priority} @gol
+-msched-costly-dep=@var{dependence_type} @gol
+-minsert-sched-nops=@var{scheme} @gol
+-mcall-sysv  -mcall-netbsd @gol
+-maix-struct-return  -msvr4-struct-return @gol
+-mabi=altivec  -mabi=no-altivec @gol
+-mabi=spe  -mabi=no-spe @gol
+-misel=yes  -misel=no @gol
+-mspe=yes  -mspe=no @gol
+-mfloat-gprs=yes  -mfloat-gprs=no @gol
+-mprototype  -mno-prototype @gol
+-msim  -mmvme  -mads  -myellowknife  -memb  -msdata @gol
+-msdata=@var{opt}  -mvxworks  -mwindiss  -G @var{num}  -pthread}
+
+@emph{Darwin Options}
+@gccoptlist{-all_load  -allowable_client  -arch  -arch_errors_fatal @gol
+-arch_only  -bind_at_load  -bundle  -bundle_loader @gol
+-client_name  -compatibility_version  -current_version @gol
+-dependency-file  -dylib_file  -dylinker_install_name @gol
+-dynamic  -dynamiclib  -exported_symbols_list @gol
+-filelist  -flat_namespace  -force_cpusubtype_ALL @gol
+-force_flat_namespace  -headerpad_max_install_names @gol
+-image_base  -init  -install_name  -keep_private_externs @gol
+-multi_module  -multiply_defined  -multiply_defined_unused @gol
+-noall_load  -nofixprebinding -nomultidefs  -noprebind  -noseglinkedit @gol
+-pagezero_size  -prebind  -prebind_all_twolevel_modules @gol
+-private_bundle  -read_only_relocs  -sectalign @gol
+-sectobjectsymbols  -whyload  -seg1addr @gol
+-sectcreate  -sectobjectsymbols  -sectorder @gol
+-seg_addr_table  -seg_addr_table_filename  -seglinkedit @gol
+-segprot  -segs_read_only_addr  -segs_read_write_addr @gol
+-single_module  -static  -sub_library  -sub_umbrella @gol
+-twolevel_namespace  -umbrella  -undefined @gol
+-unexported_symbols_list  -weak_reference_mismatches @gol
+-whatsloaded}
+
+@emph{MIPS Options}
+@gccoptlist{-EL  -EB  -march=@var{arch}  -mtune=@var{arch} @gol
+-mips1  -mips2  -mips3  -mips4  -mips32  -mips32r2  -mips64 @gol
+-mips16  -mno-mips16  -mabi=@var{abi}  -mabicalls  -mno-abicalls @gol
+-mxgot  -mno-xgot  -membedded-pic  -mno-embedded-pic @gol
+-mgp32  -mgp64  -mfp32  -mfp64  -mhard-float  -msoft-float @gol
+-msingle-float  -mdouble-float  -mint64  -mlong64  -mlong32 @gol
+-G@var{num}  -membedded-data  -mno-embedded-data @gol
+-muninit-const-in-rodata  -mno-uninit-const-in-rodata @gol
+-msplit-addresses  -mno-split-addresses  @gol
+-mexplicit-relocs  -mno-explicit-relocs  @gol
+-mrnames  -mno-rnames @gol
+-mcheck-zero-division  -mno-check-zero-division @gol
+-mmemcpy  -mno-memcpy  -mlong-calls  -mno-long-calls @gol
+-mmad  -mno-mad  -mfused-madd  -mno-fused-madd  -nocpp @gol
+-mfix-sb1  -mno-fix-sb1  -mflush-func=@var{func} @gol
+-mno-flush-func  -mbranch-likely  -mno-branch-likely}
+
+@emph{i386 and x86-64 Options}
+@gccoptlist{-mtune=@var{cpu-type}  -march=@var{cpu-type} @gol
+-mfpmath=@var{unit} @gol
+-masm=@var{dialect}  -mno-fancy-math-387 @gol
+-mno-fp-ret-in-387  -msoft-float  -msvr3-shlib @gol
+-mno-wide-multiply  -mrtd  -malign-double @gol
+-mpreferred-stack-boundary=@var{num} @gol
+-mmmx  -msse  -msse2 -msse3 -m3dnow @gol
+-mthreads  -mno-align-stringops  -minline-all-stringops @gol
+-mpush-args  -maccumulate-outgoing-args  -m128bit-long-double @gol
+-m96bit-long-double  -mregparm=@var{num}  -momit-leaf-frame-pointer @gol
+-mno-red-zone -mno-tls-direct-seg-refs @gol
+-mcmodel=@var{code-model} @gol
+-m32  -m64}
+
+@emph{HPPA Options}
+@gccoptlist{-march=@var{architecture-type} @gol
+-mbig-switch  -mdisable-fpregs  -mdisable-indexing @gol
+-mfast-indirect-calls  -mgas  -mgnu-ld   -mhp-ld @gol
+-mjump-in-delay -mlinker-opt -mlong-calls @gol
+-mlong-load-store  -mno-big-switch  -mno-disable-fpregs @gol
+-mno-disable-indexing  -mno-fast-indirect-calls  -mno-gas @gol
+-mno-jump-in-delay  -mno-long-load-store @gol
+-mno-portable-runtime  -mno-soft-float @gol
+-mno-space-regs  -msoft-float  -mpa-risc-1-0 @gol
+-mpa-risc-1-1  -mpa-risc-2-0  -mportable-runtime @gol
+-mschedule=@var{cpu-type}  -mspace-regs  -msio  -mwsio @gol
+-nolibdld  -static  -threads}
+
+@emph{Intel 960 Options}
+@gccoptlist{-m@var{cpu-type}  -masm-compat  -mclean-linkage @gol
+-mcode-align  -mcomplex-addr  -mleaf-procedures @gol
+-mic-compat  -mic2.0-compat  -mic3.0-compat @gol
+-mintel-asm  -mno-clean-linkage  -mno-code-align @gol
+-mno-complex-addr  -mno-leaf-procedures @gol
+-mno-old-align  -mno-strict-align  -mno-tail-call @gol
+-mnumerics  -mold-align  -msoft-float  -mstrict-align @gol
+-mtail-call}
+
+@emph{DEC Alpha Options}
+@gccoptlist{-mno-fp-regs  -msoft-float  -malpha-as  -mgas @gol
+-mieee  -mieee-with-inexact  -mieee-conformant @gol
+-mfp-trap-mode=@var{mode}  -mfp-rounding-mode=@var{mode} @gol
+-mtrap-precision=@var{mode}  -mbuild-constants @gol
+-mcpu=@var{cpu-type}  -mtune=@var{cpu-type} @gol
+-mbwx  -mmax  -mfix  -mcix @gol
+-mfloat-vax  -mfloat-ieee @gol
+-mexplicit-relocs  -msmall-data  -mlarge-data @gol
+-msmall-text  -mlarge-text @gol
+-mmemory-latency=@var{time}}
+
+@emph{DEC Alpha/VMS Options}
+@gccoptlist{-mvms-return-codes}
+
+@emph{H8/300 Options}
+@gccoptlist{-mrelax  -mh  -ms  -mn  -mint32  -malign-300}
+
+@emph{SH Options}
+@gccoptlist{-m1  -m2  -m2e  -m3  -m3e @gol
+-m4-nofpu  -m4-single-only  -m4-single  -m4 @gol
+-m5-64media  -m5-64media-nofpu @gol
+-m5-32media  -m5-32media-nofpu @gol
+-m5-compact  -m5-compact-nofpu @gol
+-mb  -ml  -mdalign  -mrelax @gol
+-mbigtable  -mfmovd  -mhitachi  -mnomacsave @gol
+-mieee  -misize  -mpadstruct  -mspace @gol
+-mprefergot  -musermode}
+
+@emph{System V Options}
+@gccoptlist{-Qy  -Qn  -YP,@var{paths}  -Ym,@var{dir}}
+
+@emph{ARC Options}
+@gccoptlist{-EB  -EL @gol
+-mmangle-cpu  -mcpu=@var{cpu}  -mtext=@var{text-section} @gol
+-mdata=@var{data-section}  -mrodata=@var{readonly-data-section}}
+
+@emph{TMS320C3x/C4x Options}
+@gccoptlist{-mcpu=@var{cpu}  -mbig  -msmall  -mregparm  -mmemparm @gol
+-mfast-fix  -mmpyi  -mbk  -mti  -mdp-isr-reload @gol
+-mrpts=@var{count}  -mrptb  -mdb  -mloop-unsigned @gol
+-mparallel-insns  -mparallel-mpy  -mpreserve-float}
+
+@emph{V850 Options}
+@gccoptlist{-mlong-calls  -mno-long-calls  -mep  -mno-ep @gol
+-mprolog-function  -mno-prolog-function  -mspace @gol
+-mtda=@var{n}  -msda=@var{n}  -mzda=@var{n} @gol
+-mapp-regs  -mno-app-regs @gol
+-mdisable-callt  -mno-disable-callt @gol
+-mv850e1 @gol
+-mv850e @gol
+-mv850  -mbig-switch}
+
+@emph{NS32K Options}
+@gccoptlist{-m32032  -m32332  -m32532  -m32081  -m32381 @gol
+-mmult-add  -mnomult-add  -msoft-float  -mrtd  -mnortd @gol
+-mregparam  -mnoregparam  -msb  -mnosb @gol
+-mbitfield  -mnobitfield  -mhimem  -mnohimem}
+
+@emph{AVR Options}
+@gccoptlist{-mmcu=@var{mcu}  -msize  -minit-stack=@var{n}  -mno-interrupts @gol
+-mcall-prologues  -mno-tablejump  -mtiny-stack}
+
+@emph{MCore Options}
+@gccoptlist{-mhardlit  -mno-hardlit  -mdiv  -mno-div  -mrelax-immediates @gol
+-mno-relax-immediates  -mwide-bitfields  -mno-wide-bitfields @gol
+-m4byte-functions  -mno-4byte-functions  -mcallgraph-data @gol
+-mno-callgraph-data  -mslow-bytes  -mno-slow-bytes  -mno-lsim @gol
+-mlittle-endian  -mbig-endian  -m210  -m340  -mstack-increment}
+
+@emph{MMIX Options}
+@gccoptlist{-mlibfuncs  -mno-libfuncs  -mepsilon  -mno-epsilon  -mabi=gnu @gol
+-mabi=mmixware  -mzero-extend  -mknuthdiv  -mtoplevel-symbols @gol
+-melf  -mbranch-predict  -mno-branch-predict  -mbase-addresses @gol
+-mno-base-addresses  -msingle-exit  -mno-single-exit}
+
+@emph{IA-64 Options}
+@gccoptlist{-mbig-endian  -mlittle-endian  -mgnu-as  -mgnu-ld  -mno-pic @gol
+-mvolatile-asm-stop  -mb-step  -mregister-names  -mno-sdata @gol
+-mconstant-gp  -mauto-pic  -minline-float-divide-min-latency @gol
+-minline-float-divide-max-throughput @gol
+-minline-int-divide-min-latency @gol
+-minline-int-divide-max-throughput  @gol
+-minline-sqrt-min-latency -minline-sqrt-max-throughput @gol
+-mno-dwarf2-asm -mearly-stop-bits @gol
+-mfixed-range=@var{register-range} -mtls-size=@var{tls-size} @gol
+-mtune=@var{cpu-type} -mt -pthread -milp32 -mlp64}
+
+@emph{D30V Options}
+@gccoptlist{-mextmem  -mextmemory  -monchip  -mno-asm-optimize @gol
+-masm-optimize  -mbranch-cost=@var{n}  -mcond-exec=@var{n}}
+
+@emph{S/390 and zSeries Options}
+@gccoptlist{-mtune=@var{cpu-type}  -march=@var{cpu-type} @gol
+-mhard-float  -msoft-float  -mbackchain  -mno-backchain @gol
+-msmall-exec  -mno-small-exec  -mmvcle -mno-mvcle @gol
+-m64  -m31  -mdebug  -mno-debug  -mesa  -mzarch  -mfused-madd  -mno-fused-madd}
+
+@emph{CRIS Options}
+@gccoptlist{-mcpu=@var{cpu}  -march=@var{cpu}  -mtune=@var{cpu} @gol
+-mmax-stack-frame=@var{n}  -melinux-stacksize=@var{n} @gol
+-metrax4  -metrax100  -mpdebug  -mcc-init  -mno-side-effects @gol
+-mstack-align  -mdata-align  -mconst-align @gol
+-m32-bit  -m16-bit  -m8-bit  -mno-prologue-epilogue  -mno-gotplt @gol
+-melf  -maout  -melinux  -mlinux  -sim  -sim2 @gol
+-mmul-bug-workaround  -mno-mul-bug-workaround}
+
+@emph{PDP-11 Options}
+@gccoptlist{-mfpu  -msoft-float  -mac0  -mno-ac0  -m40  -m45  -m10 @gol
+-mbcopy  -mbcopy-builtin  -mint32  -mno-int16 @gol
+-mint16  -mno-int32  -mfloat32  -mno-float64 @gol
+-mfloat64  -mno-float32  -mabshi  -mno-abshi @gol
+-mbranch-expensive  -mbranch-cheap @gol
+-msplit  -mno-split  -munix-asm  -mdec-asm}
+
+@emph{Xstormy16 Options}
+@gccoptlist{-msim}
+
+@emph{Xtensa Options}
+@gccoptlist{-mconst16 -mno-const16 @gol
+-mfused-madd  -mno-fused-madd @gol
+-mtext-section-literals  -mno-text-section-literals @gol
+-mtarget-align  -mno-target-align @gol
+-mlongcalls  -mno-longcalls}
+
+@emph{FRV Options}
+@gccoptlist{-mgpr-32  -mgpr-64  -mfpr-32  -mfpr-64 @gol
+-mhard-float  -msoft-float @gol
+-malloc-cc  -mfixed-cc  -mdword  -mno-dword @gol
+-mdouble  -mno-double @gol
+-mmedia  -mno-media  -mmuladd  -mno-muladd @gol
+-mlibrary-pic  -macc-4 -macc-8 @gol
+-mpack  -mno-pack  -mno-eflags  -mcond-move  -mno-cond-move @gol
+-mscc  -mno-scc  -mcond-exec  -mno-cond-exec @gol
+-mvliw-branch  -mno-vliw-branch @gol
+-mmulti-cond-exec  -mno-multi-cond-exec  -mnested-cond-exec @gol
+-mno-nested-cond-exec  -mtomcat-stats @gol
+-mcpu=@var{cpu}}
+
+@item Code Generation Options
+@xref{Code Gen Options,,Options for Code Generation Conventions}.
+@gccoptlist{-fcall-saved-@var{reg}  -fcall-used-@var{reg} @gol
+-ffixed-@var{reg}  -fexceptions @gol
+-fnon-call-exceptions  -funwind-tables @gol
+-fasynchronous-unwind-tables @gol
+-finhibit-size-directive  -finstrument-functions @gol
+-fno-common  -fno-ident @gol
+-fpcc-struct-return  -fpic  -fPIC -fpie -fPIE @gol
+-freg-struct-return  -fshared-data  -fshort-enums @gol
+-fshort-double  -fshort-wchar @gol
+-fverbose-asm  -fpack-struct  -fstack-check @gol
+-fstack-limit-register=@var{reg}  -fstack-limit-symbol=@var{sym} @gol
+-fargument-alias  -fargument-noalias @gol
+-fargument-noalias-global  -fleading-underscore @gol
+-ftls-model=@var{model} @gol
+-ftrapv  -fwrapv  -fbounds-check}
+@end table
+
+@menu
+* Overall Options::     Controlling the kind of output:
+                        an executable, object files, assembler files,
+                        or preprocessed source.
+* C Dialect Options::   Controlling the variant of C language compiled.
+* C++ Dialect Options:: Variations on C++.
+* Objective-C Dialect Options:: Variations on Objective-C.
+* Language Independent Options:: Controlling how diagnostics should be
+                        formatted.
+* Warning Options::     How picky should the compiler be?
+* Debugging Options::   Symbol tables, measurements, and debugging dumps.
+* Optimize Options::    How much optimization?
+* Preprocessor Options:: Controlling header files and macro definitions.
+                         Also, getting dependency information for Make.
+* Assembler Options::   Passing options to the assembler.
+* Link Options::        Specifying libraries and so on.
+* Directory Options::   Where to find header files and libraries.
+                        Where to find the compiler executable files.
+* Spec Files::          How to pass switches to sub-processes.
+* Target Options::      Running a cross-compiler, or an old version of GCC.
+@end menu
+
+@node Overall Options
+@section Options Controlling the Kind of Output
+
+Compilation can involve up to four stages: preprocessing, compilation
+proper, assembly and linking, always in that order.  GCC is capable of
+preprocessing and compiling several files either into several
+assembler input files, or into one assembler input file; then each
+assembler input file produces an object file, and linking combines all
+the object files (those newly compiled, and those specified as input)
+into an executable file.
+
+@cindex file name suffix
+For any given input file, the file name suffix determines what kind of
+compilation is done:
+
+@table @gcctabopt
+@item @var{file}.c
+C source code which must be preprocessed.
+
+@item @var{file}.i
+C source code which should not be preprocessed.
+
+@item @var{file}.ii
+C++ source code which should not be preprocessed.
+
+@item @var{file}.m
+Objective-C source code.  Note that you must link with the library
+@file{libobjc.a} to make an Objective-C program work.
+
+@item @var{file}.mi
+Objective-C source code which should not be preprocessed.
+
+@item @var{file}.h
+C or C++ header file to be turned into a precompiled header.
+
+@item @var{file}.cc
+@itemx @var{file}.cp
+@itemx @var{file}.cxx
+@itemx @var{file}.cpp
+@itemx @var{file}.CPP
+@itemx @var{file}.c++
+@itemx @var{file}.C
+C++ source code which must be preprocessed.  Note that in @samp{.cxx},
+the last two letters must both be literally @samp{x}.  Likewise,
+@samp{.C} refers to a literal capital C@.
+
+@item @var{file}.hh
+@itemx @var{file}.H
+C++ header file to be turned into a precompiled header.
+
+@item @var{file}.f
+@itemx @var{file}.for
+@itemx @var{file}.FOR
+Fortran source code which should not be preprocessed.
+
+@item @var{file}.F
+@itemx @var{file}.fpp
+@itemx @var{file}.FPP
+Fortran source code which must be preprocessed (with the traditional
+preprocessor).
+
+@item @var{file}.r
+Fortran source code which must be preprocessed with a RATFOR
+preprocessor (not included with GCC)@.
+
+@xref{Overall Options,,Options Controlling the Kind of Output, g77,
+Using and Porting GNU Fortran}, for more details of the handling of
+Fortran input files.
+
+@c FIXME: Descriptions of Java file types.
+@c @var{file}.java
+@c @var{file}.class
+@c @var{file}.zip
+@c @var{file}.jar
+
+@item @var{file}.ads
+Ada source code file which contains a library unit declaration (a
+declaration of a package, subprogram, or generic, or a generic
+instantiation), or a library unit renaming declaration (a package,
+generic, or subprogram renaming declaration).  Such files are also
+called @dfn{specs}.
+
+@itemx @var{file}.adb
+Ada source code file containing a library unit body (a subprogram or
+package body).  Such files are also called @dfn{bodies}.
+
+@c GCC also knows about some suffixes for languages not yet included:
+@c Pascal:
+@c @var{file}.p
+@c @var{file}.pas
+
+@item @var{file}.s
+Assembler code.
+
+@item @var{file}.S
+Assembler code which must be preprocessed.
+
+@item @var{other}
+An object file to be fed straight into linking.
+Any file name with no recognized suffix is treated this way.
+@end table
+
+@opindex x
+You can specify the input language explicitly with the @option{-x} option:
+
+@table @gcctabopt
+@item -x @var{language}
+Specify explicitly the @var{language} for the following input files
+(rather than letting the compiler choose a default based on the file
+name suffix).  This option applies to all following input files until
+the next @option{-x} option.  Possible values for @var{language} are:
+@smallexample
+c  c-header  cpp-output
+c++  c++-header  c++-cpp-output
+objective-c  objective-c-header  objc-cpp-output
+assembler  assembler-with-cpp
+ada
+f77  f77-cpp-input  ratfor
+java
+treelang
+@end smallexample
+
+@item -x none
+Turn off any specification of a language, so that subsequent files are
+handled according to their file name suffixes (as they are if @option{-x}
+has not been used at all).
+
+@item -pass-exit-codes
+@opindex pass-exit-codes
+Normally the @command{gcc} program will exit with the code of 1 if any
+phase of the compiler returns a non-success return code.  If you specify
+@option{-pass-exit-codes}, the @command{gcc} program will instead return with
+numerically highest error produced by any phase that returned an error
+indication.
+@end table
+
+If you only want some of the stages of compilation, you can use
+@option{-x} (or filename suffixes) to tell @command{gcc} where to start, and
+one of the options @option{-c}, @option{-S}, or @option{-E} to say where
+@command{gcc} is to stop.  Note that some combinations (for example,
+@samp{-x cpp-output -E}) instruct @command{gcc} to do nothing at all.
+
+@table @gcctabopt
+@item -c
+@opindex c
+Compile or assemble the source files, but do not link.  The linking
+stage simply is not done.  The ultimate output is in the form of an
+object file for each source file.
+
+By default, the object file name for a source file is made by replacing
+the suffix @samp{.c}, @samp{.i}, @samp{.s}, etc., with @samp{.o}.
+
+Unrecognized input files, not requiring compilation or assembly, are
+ignored.
+
+@item -S
+@opindex S
+Stop after the stage of compilation proper; do not assemble.  The output
+is in the form of an assembler code file for each non-assembler input
+file specified.
+
+By default, the assembler file name for a source file is made by
+replacing the suffix @samp{.c}, @samp{.i}, etc., with @samp{.s}.
+
+Input files that don't require compilation are ignored.
+
+@item -E
+@opindex E
+Stop after the preprocessing stage; do not run the compiler proper.  The
+output is in the form of preprocessed source code, which is sent to the
+standard output.
+
+Input files which don't require preprocessing are ignored.
+
+@cindex output file option
+@item -o @var{file}
+@opindex o
+Place output in file @var{file}.  This applies regardless to whatever
+sort of output is being produced, whether it be an executable file,
+an object file, an assembler file or preprocessed C code.
+
+If you specify @option{-o} when compiling more than one input file, or
+you are producing an executable file as output, all the source files
+on the command line will be compiled at once.
+
+If @option{-o} is not specified, the default is to put an executable file
+in @file{a.out}, the object file for @file{@var{source}.@var{suffix}} in
+@file{@var{source}.o}, its assembler file in @file{@var{source}.s}, and
+all preprocessed C source on standard output.
+
+@item -v
+@opindex v
+Print (on standard error output) the commands executed to run the stages
+of compilation.  Also print the version number of the compiler driver
+program and of the preprocessor and the compiler proper.
+
+@item -###
+@opindex ###
+Like @option{-v} except the commands are not executed and all command
+arguments are quoted.  This is useful for shell scripts to capture the
+driver-generated command lines.
+
+@item -pipe
+@opindex pipe
+Use pipes rather than temporary files for communication between the
+various stages of compilation.  This fails to work on some systems where
+the assembler is unable to read from a pipe; but the GNU assembler has
+no trouble.
+
+@item --help
+@opindex help
+Print (on the standard output) a description of the command line options
+understood by @command{gcc}.  If the @option{-v} option is also specified
+then @option{--help} will also be passed on to the various processes
+invoked by @command{gcc}, so that they can display the command line options
+they accept.  If the @option{-Wextra} option is also specified then command
+line options which have no documentation associated with them will also
+be displayed.
+
+@item --target-help
+@opindex target-help
+Print (on the standard output) a description of target specific command
+line options for each tool.
+
+@item --version
+@opindex version
+Display the version number and copyrights of the invoked GCC.
+@end table
+
+@node Invoking G++
+@section Compiling C++ Programs
+
+@cindex suffixes for C++ source
+@cindex C++ source file suffixes
+C++ source files conventionally use one of the suffixes @samp{.C},
+@samp{.cc}, @samp{.cpp}, @samp{.CPP}, @samp{.c++}, @samp{.cp}, or
+@samp{.cxx}; C++ header files often use @samp{.hh} or @samp{.H}; and
+preprocessed C++ files use the suffix @samp{.ii}.  GCC recognizes
+files with these names and compiles them as C++ programs even if you
+call the compiler the same way as for compiling C programs (usually
+with the name @command{gcc}).
+
+@findex g++
+@findex c++
+However, C++ programs often require class libraries as well as a
+compiler that understands the C++ language---and under some
+circumstances, you might want to compile programs or header files from
+standard input, or otherwise without a suffix that flags them as C++
+programs.  You might also like to precompile a C header file with a
+@samp{.h} extension to be used in C++ compilations.  @command{g++} is a
+program that calls GCC with the default language set to C++, and
+automatically specifies linking against the C++ library.  On many
+systems, @command{g++} is also installed with the name @command{c++}.
+
+@cindex invoking @command{g++}
+When you compile C++ programs, you may specify many of the same
+command-line options that you use for compiling programs in any
+language; or command-line options meaningful for C and related
+languages; or options that are meaningful only for C++ programs.
+@xref{C Dialect Options,,Options Controlling C Dialect}, for
+explanations of options for languages related to C@.
+@xref{C++ Dialect Options,,Options Controlling C++ Dialect}, for
+explanations of options that are meaningful only for C++ programs.
+
+@node C Dialect Options
+@section Options Controlling C Dialect
+@cindex dialect options
+@cindex language dialect options
+@cindex options, dialect
+
+The following options control the dialect of C (or languages derived
+from C, such as C++ and Objective-C) that the compiler accepts:
+
+@table @gcctabopt
+@cindex ANSI support
+@cindex ISO support
+@item -ansi
+@opindex ansi
+In C mode, support all ISO C90 programs.  In C++ mode,
+remove GNU extensions that conflict with ISO C++.
+
+This turns off certain features of GCC that are incompatible with ISO
+C90 (when compiling C code), or of standard C++ (when compiling C++ code),
+such as the @code{asm} and @code{typeof} keywords, and
+predefined macros such as @code{unix} and @code{vax} that identify the
+type of system you are using.  It also enables the undesirable and
+rarely used ISO trigraph feature.  For the C compiler,
+it disables recognition of C++ style @samp{//} comments as well as
+the @code{inline} keyword.
+
+The alternate keywords @code{__asm__}, @code{__extension__},
+@code{__inline__} and @code{__typeof__} continue to work despite
+@option{-ansi}.  You would not want to use them in an ISO C program, of
+course, but it is useful to put them in header files that might be included
+in compilations done with @option{-ansi}.  Alternate predefined macros
+such as @code{__unix__} and @code{__vax__} are also available, with or
+without @option{-ansi}.
+
+The @option{-ansi} option does not cause non-ISO programs to be
+rejected gratuitously.  For that, @option{-pedantic} is required in
+addition to @option{-ansi}.  @xref{Warning Options}.
+
+The macro @code{__STRICT_ANSI__} is predefined when the @option{-ansi}
+option is used.  Some header files may notice this macro and refrain
+from declaring certain functions or defining certain macros that the
+ISO standard doesn't call for; this is to avoid interfering with any
+programs that might use these names for other things.
+
+Functions which would normally be built in but do not have semantics
+defined by ISO C (such as @code{alloca} and @code{ffs}) are not built-in
+functions with @option{-ansi} is used.  @xref{Other Builtins,,Other
+built-in functions provided by GCC}, for details of the functions
+affected.
+
+@item -std=
+@opindex std
+Determine the language standard.  This option is currently only
+supported when compiling C or C++.  A value for this option must be
+provided; possible values are
+
+@table @samp
+@item c89
+@itemx iso9899:1990
+ISO C90 (same as @option{-ansi}).
+
+@item iso9899:199409
+ISO C90 as modified in amendment 1.
+
+@item c99
+@itemx c9x
+@itemx iso9899:1999
+@itemx iso9899:199x
+ISO C99.  Note that this standard is not yet fully supported; see
+@w{@uref{http://gcc.gnu.org/gcc-3.4/c99status.html}} for more information.  The
+names @samp{c9x} and @samp{iso9899:199x} are deprecated.
+
+@item gnu89
+Default, ISO C90 plus GNU extensions (including some C99 features).
+
+@item gnu99
+@itemx gnu9x
+ISO C99 plus GNU extensions.  When ISO C99 is fully implemented in GCC,
+this will become the default.  The name @samp{gnu9x} is deprecated.
+
+@item c++98
+The 1998 ISO C++ standard plus amendments.
+
+@item gnu++98
+The same as @option{-std=c++98} plus GNU extensions.  This is the
+default for C++ code.
+@end table
+
+Even when this option is not specified, you can still use some of the
+features of newer standards in so far as they do not conflict with
+previous C standards.  For example, you may use @code{__restrict__} even
+when @option{-std=c99} is not specified.
+
+The @option{-std} options specifying some version of ISO C have the same
+effects as @option{-ansi}, except that features that were not in ISO C90
+but are in the specified version (for example, @samp{//} comments and
+the @code{inline} keyword in ISO C99) are not disabled.
+
+@xref{Standards,,Language Standards Supported by GCC}, for details of
+these standard versions.
+
+@item -aux-info @var{filename}
+@opindex aux-info
+Output to the given filename prototyped declarations for all functions
+declared and/or defined in a translation unit, including those in header
+files.  This option is silently ignored in any language other than C@.
+
+Besides declarations, the file indicates, in comments, the origin of
+each declaration (source file and line), whether the declaration was
+implicit, prototyped or unprototyped (@samp{I}, @samp{N} for new or
+@samp{O} for old, respectively, in the first character after the line
+number and the colon), and whether it came from a declaration or a
+definition (@samp{C} or @samp{F}, respectively, in the following
+character).  In the case of function definitions, a K&R-style list of
+arguments followed by their declarations is also provided, inside
+comments, after the declaration.
+
+@item -fno-asm
+@opindex fno-asm
+Do not recognize @code{asm}, @code{inline} or @code{typeof} as a
+keyword, so that code can use these words as identifiers.  You can use
+the keywords @code{__asm__}, @code{__inline__} and @code{__typeof__}
+instead.  @option{-ansi} implies @option{-fno-asm}.
+
+In C++, this switch only affects the @code{typeof} keyword, since
+@code{asm} and @code{inline} are standard keywords.  You may want to
+use the @option{-fno-gnu-keywords} flag instead, which has the same
+effect.  In C99 mode (@option{-std=c99} or @option{-std=gnu99}), this
+switch only affects the @code{asm} and @code{typeof} keywords, since
+@code{inline} is a standard keyword in ISO C99.
+
+@item -fno-builtin
+@itemx -fno-builtin-@var{function}
+@opindex fno-builtin
+@cindex built-in functions
+Don't recognize built-in functions that do not begin with
+@samp{__builtin_} as prefix.  @xref{Other Builtins,,Other built-in
+functions provided by GCC}, for details of the functions affected,
+including those which are not built-in functions when @option{-ansi} or
+@option{-std} options for strict ISO C conformance are used because they
+do not have an ISO standard meaning.
+
+GCC normally generates special code to handle certain built-in functions
+more efficiently; for instance, calls to @code{alloca} may become single
+instructions that adjust the stack directly, and calls to @code{memcpy}
+may become inline copy loops.  The resulting code is often both smaller
+and faster, but since the function calls no longer appear as such, you
+cannot set a breakpoint on those calls, nor can you change the behavior
+of the functions by linking with a different library.
+
+With the @option{-fno-builtin-@var{function}} option
+only the built-in function @var{function} is
+disabled.  @var{function} must not begin with @samp{__builtin_}.  If a
+function is named this is not built-in in this version of GCC, this
+option is ignored.  There is no corresponding
+@option{-fbuiltin-@var{function}} option; if you wish to enable
+built-in functions selectively when using @option{-fno-builtin} or
+@option{-ffreestanding}, you may define macros such as:
+
+@smallexample
+#define abs(n)          __builtin_abs ((n))
+#define strcpy(d, s)    __builtin_strcpy ((d), (s))
+@end smallexample
+
+@item -fhosted
+@opindex fhosted
+@cindex hosted environment
+
+Assert that compilation takes place in a hosted environment.  This implies
+@option{-fbuiltin}.  A hosted environment is one in which the
+entire standard library is available, and in which @code{main} has a return
+type of @code{int}.  Examples are nearly everything except a kernel.
+This is equivalent to @option{-fno-freestanding}.
+
+@item -ffreestanding
+@opindex ffreestanding
+@cindex hosted environment
+
+Assert that compilation takes place in a freestanding environment.  This
+implies @option{-fno-builtin}.  A freestanding environment
+is one in which the standard library may not exist, and program startup may
+not necessarily be at @code{main}.  The most obvious example is an OS kernel.
+This is equivalent to @option{-fno-hosted}.
+
+@xref{Standards,,Language Standards Supported by GCC}, for details of
+freestanding and hosted environments.
+
+@item -fms-extensions
+@opindex fms-extensions
+Accept some non-standard constructs used in Microsoft header files.
+
+@item -trigraphs
+@opindex trigraphs
+Support ISO C trigraphs.  The @option{-ansi} option (and @option{-std}
+options for strict ISO C conformance) implies @option{-trigraphs}.
+
+@item -no-integrated-cpp
+@opindex no-integrated-cpp
+Performs a compilation in two passes: preprocessing and compiling.  This
+option allows a user supplied "cc1", "cc1plus", or "cc1obj" via the
+@option{-B} option. The user supplied compilation step can then add in
+an additional preprocessing step after normal preprocessing but before
+compiling. The default is to use the integrated cpp (internal cpp)
+
+The semantics of this option will change if "cc1", "cc1plus", and
+"cc1obj" are merged.
+
+@cindex traditional C language
+@cindex C language, traditional
+@item -traditional
+@itemx -traditional-cpp
+@opindex traditional-cpp
+@opindex traditional
+Formerly, these options caused GCC to attempt to emulate a pre-standard
+C compiler.  They are now only supported with the @option{-E} switch.
+The preprocessor continues to support a pre-standard mode.  See the GNU
+CPP manual for details.
+
+@item -fcond-mismatch
+@opindex fcond-mismatch
+Allow conditional expressions with mismatched types in the second and
+third arguments.  The value of such an expression is void.  This option
+is not supported for C++.
+
+@item -funsigned-char
+@opindex funsigned-char
+Let the type @code{char} be unsigned, like @code{unsigned char}.
+
+Each kind of machine has a default for what @code{char} should
+be.  It is either like @code{unsigned char} by default or like
+@code{signed char} by default.
+
+Ideally, a portable program should always use @code{signed char} or
+@code{unsigned char} when it depends on the signedness of an object.
+But many programs have been written to use plain @code{char} and
+expect it to be signed, or expect it to be unsigned, depending on the
+machines they were written for.  This option, and its inverse, let you
+make such a program work with the opposite default.
+
+The type @code{char} is always a distinct type from each of
+@code{signed char} or @code{unsigned char}, even though its behavior
+is always just like one of those two.
+
+@item -fsigned-char
+@opindex fsigned-char
+Let the type @code{char} be signed, like @code{signed char}.
+
+Note that this is equivalent to @option{-fno-unsigned-char}, which is
+the negative form of @option{-funsigned-char}.  Likewise, the option
+@option{-fno-signed-char} is equivalent to @option{-funsigned-char}.
+
+@item -fsigned-bitfields
+@itemx -funsigned-bitfields
+@itemx -fno-signed-bitfields
+@itemx -fno-unsigned-bitfields
+@opindex fsigned-bitfields
+@opindex funsigned-bitfields
+@opindex fno-signed-bitfields
+@opindex fno-unsigned-bitfields
+These options control whether a bit-field is signed or unsigned, when the
+declaration does not use either @code{signed} or @code{unsigned}.  By
+default, such a bit-field is signed, because this is consistent: the
+basic integer types such as @code{int} are signed types.
+
+@item -fwritable-strings
+@opindex fwritable-strings
+Store string constants in the writable data segment and don't uniquize
+them.  This is for compatibility with old programs which assume they can
+write into string constants.
+
+Writing into string constants is a very bad idea; ``constants'' should
+be constant.
+
+This option is deprecated.
+@end table
+
+@node C++ Dialect Options
+@section Options Controlling C++ Dialect
+
+@cindex compiler options, C++
+@cindex C++ options, command line
+@cindex options, C++
+This section describes the command-line options that are only meaningful
+for C++ programs; but you can also use most of the GNU compiler options
+regardless of what language your program is in.  For example, you
+might compile a file @code{firstClass.C} like this:
+
+@smallexample
+g++ -g -frepo -O -c firstClass.C
+@end smallexample
+
+@noindent
+In this example, only @option{-frepo} is an option meant
+only for C++ programs; you can use the other options with any
+language supported by GCC@.
+
+Here is a list of options that are @emph{only} for compiling C++ programs:
+
+@table @gcctabopt
+
+@item -fabi-version=@var{n}
+@opindex fabi-version
+Use version @var{n} of the C++ ABI.  Version 2 is the version of the
+C++ ABI that first appeared in G++ 3.4.  Version 1 is the version of
+the C++ ABI that first appeared in G++ 3.2.  Version 0 will always be
+the version that conforms most closely to the C++ ABI specification.
+Therefore, the ABI obtained using version 0 will change as ABI bugs
+are fixed.
+
+The default is version 2.
+
+@item -fno-access-control
+@opindex fno-access-control
+Turn off all access checking.  This switch is mainly useful for working
+around bugs in the access control code.
+
+@item -fcheck-new
+@opindex fcheck-new
+Check that the pointer returned by @code{operator new} is non-null
+before attempting to modify the storage allocated.  This check is
+normally unnecessary because the C++ standard specifies that
+@code{operator new} will only return @code{0} if it is declared
+@samp{throw()}, in which case the compiler will always check the
+return value even without this option.  In all other cases, when
+@code{operator new} has a non-empty exception specification, memory
+exhaustion is signalled by throwing @code{std::bad_alloc}.  See also
+@samp{new (nothrow)}.
+
+@item -fconserve-space
+@opindex fconserve-space
+Put uninitialized or runtime-initialized global variables into the
+common segment, as C does.  This saves space in the executable at the
+cost of not diagnosing duplicate definitions.  If you compile with this
+flag and your program mysteriously crashes after @code{main()} has
+completed, you may have an object that is being destroyed twice because
+two definitions were merged.
+
+This option is no longer useful on most targets, now that support has
+been added for putting variables into BSS without making them common.
+
+@item -fno-const-strings
+@opindex fno-const-strings
+Give string constants type @code{char *} instead of type @code{const
+char *}.  By default, G++ uses type @code{const char *} as required by
+the standard.  Even if you use @option{-fno-const-strings}, you cannot
+actually modify the value of a string constant, unless you also use
+@option{-fwritable-strings}.
+
+This option might be removed in a future release of G++.  For maximum
+portability, you should structure your code so that it works with
+string constants that have type @code{const char *}.
+
+@item -fno-elide-constructors
+@opindex fno-elide-constructors
+The C++ standard allows an implementation to omit creating a temporary
+which is only used to initialize another object of the same type.
+Specifying this option disables that optimization, and forces G++ to
+call the copy constructor in all cases.
+
+@item -fno-enforce-eh-specs
+@opindex fno-enforce-eh-specs
+Don't check for violation of exception specifications at runtime.  This
+option violates the C++ standard, but may be useful for reducing code
+size in production builds, much like defining @samp{NDEBUG}.  The compiler
+will still optimize based on the exception specifications.
+
+@item -ffor-scope
+@itemx -fno-for-scope
+@opindex ffor-scope
+@opindex fno-for-scope
+If @option{-ffor-scope} is specified, the scope of variables declared in
+a @i{for-init-statement} is limited to the @samp{for} loop itself,
+as specified by the C++ standard.
+If @option{-fno-for-scope} is specified, the scope of variables declared in
+a @i{for-init-statement} extends to the end of the enclosing scope,
+as was the case in old versions of G++, and other (traditional)
+implementations of C++.
+
+The default if neither flag is given to follow the standard,
+but to allow and give a warning for old-style code that would
+otherwise be invalid, or have different behavior.
+
+@item -fno-gnu-keywords
+@opindex fno-gnu-keywords
+Do not recognize @code{typeof} as a keyword, so that code can use this
+word as an identifier.  You can use the keyword @code{__typeof__} instead.
+@option{-ansi} implies @option{-fno-gnu-keywords}.
+
+@item -fno-implicit-templates
+@opindex fno-implicit-templates
+Never emit code for non-inline templates which are instantiated
+implicitly (i.e.@: by use); only emit code for explicit instantiations.
+@xref{Template Instantiation}, for more information.
+
+@item -fno-implicit-inline-templates
+@opindex fno-implicit-inline-templates
+Don't emit code for implicit instantiations of inline templates, either.
+The default is to handle inlines differently so that compiles with and
+without optimization will need the same set of explicit instantiations.
+
+@item -fno-implement-inlines
+@opindex fno-implement-inlines
+To save space, do not emit out-of-line copies of inline functions
+controlled by @samp{#pragma implementation}.  This will cause linker
+errors if these functions are not inlined everywhere they are called.
+
+@item -fms-extensions
+@opindex fms-extensions
+Disable pedantic warnings about constructs used in MFC, such as implicit
+int and getting a pointer to member function via non-standard syntax.
+
+@item -fno-nonansi-builtins
+@opindex fno-nonansi-builtins
+Disable built-in declarations of functions that are not mandated by
+ANSI/ISO C@.  These include @code{ffs}, @code{alloca}, @code{_exit},
+@code{index}, @code{bzero}, @code{conjf}, and other related functions.
+
+@item -fno-operator-names
+@opindex fno-operator-names
+Do not treat the operator name keywords @code{and}, @code{bitand},
+@code{bitor}, @code{compl}, @code{not}, @code{or} and @code{xor} as
+synonyms as keywords.
+
+@item -fno-optional-diags
+@opindex fno-optional-diags
+Disable diagnostics that the standard says a compiler does not need to
+issue.  Currently, the only such diagnostic issued by G++ is the one for
+a name having multiple meanings within a class.
+
+@item -fpermissive
+@opindex fpermissive
+Downgrade some diagnostics about nonconformant code from errors to
+warnings.  Thus, using @option{-fpermissive} will allow some
+nonconforming code to compile.
+
+@item -frepo
+@opindex frepo
+Enable automatic template instantiation at link time.  This option also
+implies @option{-fno-implicit-templates}.  @xref{Template
+Instantiation}, for more information.
+
+@item -fno-rtti
+@opindex fno-rtti
+Disable generation of information about every class with virtual
+functions for use by the C++ runtime type identification features
+(@samp{dynamic_cast} and @samp{typeid}).  If you don't use those parts
+of the language, you can save some space by using this flag.  Note that
+exception handling uses the same information, but it will generate it as
+needed.
+
+@item -fstats
+@opindex fstats
+Emit statistics about front-end processing at the end of the compilation.
+This information is generally only useful to the G++ development team.
+
+@item -ftemplate-depth-@var{n}
+@opindex ftemplate-depth
+Set the maximum instantiation depth for template classes to @var{n}.
+A limit on the template instantiation depth is needed to detect
+endless recursions during template class instantiation.  ANSI/ISO C++
+conforming programs must not rely on a maximum depth greater than 17.
+
+@item -fuse-cxa-atexit
+@opindex fuse-cxa-atexit
+Register destructors for objects with static storage duration with the
+@code{__cxa_atexit} function rather than the @code{atexit} function.
+This option is required for fully standards-compliant handling of static
+destructors, but will only work if your C library supports
+@code{__cxa_atexit}.
+
+@item -fno-weak
+@opindex fno-weak
+Do not use weak symbol support, even if it is provided by the linker.
+By default, G++ will use weak symbols if they are available.  This
+option exists only for testing, and should not be used by end-users;
+it will result in inferior code and has no benefits.  This option may
+be removed in a future release of G++.
+
+@item -nostdinc++
+@opindex nostdinc++
+Do not search for header files in the standard directories specific to
+C++, but do still search the other standard directories.  (This option
+is used when building the C++ library.)
+@end table
+
+In addition, these optimization, warning, and code generation options
+have meanings only for C++ programs:
+
+@table @gcctabopt
+@item -fno-default-inline
+@opindex fno-default-inline
+Do not assume @samp{inline} for functions defined inside a class scope.
+@xref{Optimize Options,,Options That Control Optimization}.  Note that these
+functions will have linkage like inline functions; they just won't be
+inlined by default.
+
+@item -Wabi @r{(C++ only)}
+@opindex Wabi
+Warn when G++ generates code that is probably not compatible with the
+vendor-neutral C++ ABI.  Although an effort has been made to warn about
+all such cases, there are probably some cases that are not warned about,
+even though G++ is generating incompatible code.  There may also be
+cases where warnings are emitted even though the code that is generated
+will be compatible.
+
+You should rewrite your code to avoid these warnings if you are
+concerned about the fact that code generated by G++ may not be binary
+compatible with code generated by other compilers.
+
+The known incompatibilities at this point include:
+
+@itemize @bullet
+
+@item
+Incorrect handling of tail-padding for bit-fields.  G++ may attempt to
+pack data into the same byte as a base class.  For example:
+
+@smallexample
+struct A @{ virtual void f(); int f1 : 1; @};
+struct B : public A @{ int f2 : 1; @};
+@end smallexample
+
+@noindent
+In this case, G++ will place @code{B::f2} into the same byte
+as@code{A::f1}; other compilers will not.  You can avoid this problem
+by explicitly padding @code{A} so that its size is a multiple of the
+byte size on your platform; that will cause G++ and other compilers to
+layout @code{B} identically.
+
+@item
+Incorrect handling of tail-padding for virtual bases.  G++ does not use
+tail padding when laying out virtual bases.  For example:
+
+@smallexample
+struct A @{ virtual void f(); char c1; @};
+struct B @{ B(); char c2; @};
+struct C : public A, public virtual B @{@};
+@end smallexample
+
+@noindent
+In this case, G++ will not place @code{B} into the tail-padding for
+@code{A}; other compilers will.  You can avoid this problem by
+explicitly padding @code{A} so that its size is a multiple of its
+alignment (ignoring virtual base classes); that will cause G++ and other
+compilers to layout @code{C} identically.
+
+@item
+Incorrect handling of bit-fields with declared widths greater than that
+of their underlying types, when the bit-fields appear in a union.  For
+example:
+
+@smallexample
+union U @{ int i : 4096; @};
+@end smallexample
+
+@noindent
+Assuming that an @code{int} does not have 4096 bits, G++ will make the
+union too small by the number of bits in an @code{int}.
+
+@item
+Empty classes can be placed at incorrect offsets.  For example:
+
+@smallexample
+struct A @{@};
+
+struct B @{
+  A a;
+  virtual void f ();
+@};
+
+struct C : public B, public A @{@};
+@end smallexample
+
+@noindent
+G++ will place the @code{A} base class of @code{C} at a nonzero offset;
+it should be placed at offset zero.  G++ mistakenly believes that the
+@code{A} data member of @code{B} is already at offset zero.
+
+@item
+Names of template functions whose types involve @code{typename} or
+template template parameters can be mangled incorrectly.
+
+@smallexample
+template <typename Q>
+void f(typename Q::X) @{@}
+
+template <template <typename> class Q>
+void f(typename Q<int>::X) @{@}
+@end smallexample
+
+@noindent
+Instantiations of these templates may be mangled incorrectly.
+
+@end itemize
+
+@item -Wctor-dtor-privacy @r{(C++ only)}
+@opindex Wctor-dtor-privacy
+Warn when a class seems unusable because all the constructors or
+destructors in that class are private, and it has neither friends nor
+public static member functions.
+
+@item -Wnon-virtual-dtor @r{(C++ only)}
+@opindex Wnon-virtual-dtor
+Warn when a class appears to be polymorphic, thereby requiring a virtual
+destructor, yet it declares a non-virtual one.
+This warning is enabled by @option{-Wall}.
+
+@item -Wreorder @r{(C++ only)}
+@opindex Wreorder
+@cindex reordering, warning
+@cindex warning for reordering of member initializers
+Warn when the order of member initializers given in the code does not
+match the order in which they must be executed.  For instance:
+
+@smallexample
+struct A @{
+  int i;
+  int j;
+  A(): j (0), i (1) @{ @}
+@};
+@end smallexample
+
+The compiler will rearrange the member initializers for @samp{i}
+and @samp{j} to match the declaration order of the members, emitting
+a warning to that effect.  This warning is enabled by @option{-Wall}.
+@end table
+
+The following @option{-W@dots{}} options are not affected by @option{-Wall}.
+
+@table @gcctabopt
+@item -Weffc++ @r{(C++ only)}
+@opindex Weffc++
+Warn about violations of the following style guidelines from Scott Meyers'
+@cite{Effective C++} book:
+
+@itemize @bullet
+@item
+Item 11:  Define a copy constructor and an assignment operator for classes
+with dynamically allocated memory.
+
+@item
+Item 12:  Prefer initialization to assignment in constructors.
+
+@item
+Item 14:  Make destructors virtual in base classes.
+
+@item
+Item 15:  Have @code{operator=} return a reference to @code{*this}.
+
+@item
+Item 23:  Don't try to return a reference when you must return an object.
+
+@end itemize
+
+Also warn about violations of the following style guidelines from
+Scott Meyers' @cite{More Effective C++} book:
+
+@itemize @bullet
+@item
+Item 6:  Distinguish between prefix and postfix forms of increment and
+decrement operators.
+
+@item
+Item 7:  Never overload @code{&&}, @code{||}, or @code{,}.
+
+@end itemize
+
+When selecting this option, be aware that the standard library
+headers do not obey all of these guidelines; use @samp{grep -v}
+to filter out those warnings.
+
+@item -Wno-deprecated @r{(C++ only)}
+@opindex Wno-deprecated
+Do not warn about usage of deprecated features.  @xref{Deprecated Features}.
+
+@item -Wno-non-template-friend @r{(C++ only)}
+@opindex Wno-non-template-friend
+Disable warnings when non-templatized friend functions are declared
+within a template.  Since the advent of explicit template specification
+support in G++, if the name of the friend is an unqualified-id (i.e.,
+@samp{friend foo(int)}), the C++ language specification demands that the
+friend declare or define an ordinary, nontemplate function.  (Section
+14.5.3).  Before G++ implemented explicit specification, unqualified-ids
+could be interpreted as a particular specialization of a templatized
+function.  Because this non-conforming behavior is no longer the default
+behavior for G++, @option{-Wnon-template-friend} allows the compiler to
+check existing code for potential trouble spots and is on by default.
+This new compiler behavior can be turned off with
+@option{-Wno-non-template-friend} which keeps the conformant compiler code
+but disables the helpful warning.
+
+@item -Wold-style-cast @r{(C++ only)}
+@opindex Wold-style-cast
+Warn if an old-style (C-style) cast to a non-void type is used within
+a C++ program.  The new-style casts (@samp{static_cast},
+@samp{reinterpret_cast}, and @samp{const_cast}) are less vulnerable to
+unintended effects and much easier to search for.
+
+@item -Woverloaded-virtual @r{(C++ only)}
+@opindex Woverloaded-virtual
+@cindex overloaded virtual fn, warning
+@cindex warning for overloaded virtual fn
+Warn when a function declaration hides virtual functions from a
+base class.  For example, in:
+
+@smallexample
+struct A @{
+  virtual void f();
+@};
+
+struct B: public A @{
+  void f(int);
+@};
+@end smallexample
+
+the @code{A} class version of @code{f} is hidden in @code{B}, and code
+like:
+
+@smallexample
+B* b;
+b->f();
+@end smallexample
+
+will fail to compile.
+
+@item -Wno-pmf-conversions @r{(C++ only)}
+@opindex Wno-pmf-conversions
+Disable the diagnostic for converting a bound pointer to member function
+to a plain pointer.
+
+@item -Wsign-promo @r{(C++ only)}
+@opindex Wsign-promo
+Warn when overload resolution chooses a promotion from unsigned or
+enumerated type to a signed type, over a conversion to an unsigned type of
+the same size.  Previous versions of G++ would try to preserve
+unsignedness, but the standard mandates the current behavior.
+
+@smallexample
+struct A @{
+  operator int ();
+  A& operator = (int);
+@};
+
+main ()
+@{
+  A a,b;
+  a = b;
+@}
+@end smallexample
+
+In this example, G++ will synthesize a default @samp{A& operator =
+(const A&);}, while cfront will use the user-defined @samp{operator =}.
+@end table
+
+@node Objective-C Dialect Options
+@section Options Controlling Objective-C Dialect
+
+@cindex compiler options, Objective-C
+@cindex Objective-C options, command line
+@cindex options, Objective-C
+(NOTE: This manual does not describe the Objective-C language itself.  See
+@w{@uref{http://gcc.gnu.org/readings.html}} for references.)
+
+This section describes the command-line options that are only meaningful
+for Objective-C programs, but you can also use most of the GNU compiler
+options regardless of what language your program is in.  For example,
+you might compile a file @code{some_class.m} like this:
+
+@smallexample
+gcc -g -fgnu-runtime -O -c some_class.m
+@end smallexample
+
+@noindent
+In this example, @option{-fgnu-runtime} is an option meant only for
+Objective-C programs; you can use the other options with any language
+supported by GCC@.
+
+Here is a list of options that are @emph{only} for compiling Objective-C
+programs:
+
+@table @gcctabopt
+@item -fconstant-string-class=@var{class-name}
+@opindex fconstant-string-class
+Use @var{class-name} as the name of the class to instantiate for each
+literal string specified with the syntax @code{@@"@dots{}"}.  The default
+class name is @code{NXConstantString} if the GNU runtime is being used, and
+@code{NSConstantString} if the NeXT runtime is being used (see below).  The
+@option{-fconstant-cfstrings} option, if also present, will override the
+@option{-fconstant-string-class} setting and cause @code{@@"@dots{}"} literals
+to be laid out as constant CoreFoundation strings.
+
+@item -fgnu-runtime
+@opindex fgnu-runtime
+Generate object code compatible with the standard GNU Objective-C
+runtime.  This is the default for most types of systems.
+
+@item -fnext-runtime
+@opindex fnext-runtime
+Generate output compatible with the NeXT runtime.  This is the default
+for NeXT-based systems, including Darwin and Mac OS X@.  The macro
+@code{__NEXT_RUNTIME__} is predefined if (and only if) this option is
+used.
+
+@item -fno-nil-receivers
+@opindex fno-nil-receivers
+Assume that all Objective-C message dispatches (e.g.,
+@code{[receiver message:arg]}) in this translation unit ensure that the receiver
+is not @code{nil}.  This allows for more efficient entry points in the runtime to be
+used.  Currently, this option is only available in conjunction with
+the NeXT runtime on Mac OS X 10.3 and later.
+
+@item -fobjc-exceptions
+@opindex fobjc-exceptions
+Enable syntactic support for structured exception handling in Objective-C,
+similar to what is offered by C++ and Java.  Currently, this option is only
+available in conjunction with the NeXT runtime on Mac OS X 10.3 and later.
+
+@smallexample
+  @@try @{
+    @dots{}
+       @@throw expr;
+    @dots{}
+  @}
+  @@catch (AnObjCClass *exc) @{
+    @dots{}
+      @@throw expr;
+    @dots{}
+      @@throw;
+    @dots{}
+  @}
+  @@catch (AnotherClass *exc) @{
+    @dots{}
+  @}
+  @@catch (id allOthers) @{
+    @dots{}
+  @}
+  @@finally @{
+    @dots{}
+      @@throw expr;
+    @dots{}
+  @}
+@end smallexample
+
+The @code{@@throw} statement may appear anywhere in an Objective-C or
+Objective-C++ program; when used inside of a @code{@@catch} block, the
+@code{@@throw} may appear without an argument (as shown above), in which case
+the object caught by the @code{@@catch} will be rethrown.
+
+Note that only (pointers to) Objective-C objects may be thrown and
+caught using this scheme.  When an object is thrown, it will be caught
+by the nearest @code{@@catch} clause capable of handling objects of that type,
+analogously to how @code{catch} blocks work in C++ and Java.  A
+@code{@@catch(id @dots{})} clause (as shown above) may also be provided to catch
+any and all Objective-C exceptions not caught by previous @code{@@catch}
+clauses (if any).
+
+The @code{@@finally} clause, if present, will be executed upon exit from the
+immediately preceding @code{@@try @dots{} @@catch} section.  This will happen
+regardless of whether any exceptions are thrown, caught or rethrown
+inside the @code{@@try @dots{} @@catch} section, analogously to the behavior
+of the @code{finally} clause in Java.
+
+There are several caveats to using the new exception mechanism:
+
+@itemize @bullet
+@item
+Although currently designed to be binary compatible with @code{NS_HANDLER}-style
+idioms provided by the @code{NSException} class, the new
+exceptions can only be used on Mac OS X 10.3 (Panther) and later
+systems, due to additional functionality needed in the (NeXT) Objective-C
+runtime.
+
+@item
+As mentioned above, the new exceptions do not support handling
+types other than Objective-C objects.   Furthermore, when used from
+Objective-C++, the Objective-C exception model does not interoperate with C++
+exceptions at this time.  This means you cannot @code{@@throw} an exception
+from Objective-C and @code{catch} it in C++, or vice versa
+(i.e., @code{throw @dots{} @@catch}).
+@end itemize
+
+The @option{-fobjc-exceptions} switch also enables the use of synchronization
+blocks for thread-safe execution:
+
+@smallexample
+  @@synchronized (ObjCClass *guard) @{
+    @dots{}
+  @}
+@end smallexample
+
+Upon entering the @code{@@synchronized} block, a thread of execution shall
+first check whether a lock has been placed on the corresponding @code{guard}
+object by another thread.  If it has, the current thread shall wait until
+the other thread relinquishes its lock.  Once @code{guard} becomes available,
+the current thread will place its own lock on it, execute the code contained in
+the @code{@@synchronized} block, and finally relinquish the lock (thereby
+making @code{guard} available to other threads).
+
+Unlike Java, Objective-C does not allow for entire methods to be marked
+@code{@@synchronized}.  Note that throwing exceptions out of
+@code{@@synchronized} blocks is allowed, and will cause the guarding object
+to be unlocked properly.
+
+@item -freplace-objc-classes
+@opindex freplace-objc-classes
+Emit a special marker instructing @command{ld(1)} not to statically link in
+the resulting object file, and allow @command{dyld(1)} to load it in at
+run time instead.  This is used in conjunction with the Fix-and-Continue
+debugging mode, where the object file in question may be recompiled and
+dynamically reloaded in the course of program execution, without the need
+to restart the program itself.  Currently, Fix-and-Continue functionality
+is only available in conjunction with the NeXT runtime on Mac OS X 10.3
+and later.
+
+@item -fzero-link
+@opindex fzero-link
+When compiling for the NeXT runtime, the compiler ordinarily replaces calls
+to @code{objc_getClass("@dots{}")} (when the name of the class is known at
+compile time) with static class references that get initialized at load time,
+which improves run-time performance.  Specifying the @option{-fzero-link} flag
+suppresses this behavior and causes calls to @code{objc_getClass("@dots{}")}
+to be retained.  This is useful in Zero-Link debugging mode, since it allows
+for individual class implementations to be modified during program execution.
+
+@item -gen-decls
+@opindex gen-decls
+Dump interface declarations for all classes seen in the source file to a
+file named @file{@var{sourcename}.decl}.
+
+@item -Wno-protocol
+@opindex Wno-protocol
+If a class is declared to implement a protocol, a warning is issued for
+every method in the protocol that is not implemented by the class.  The
+default behavior is to issue a warning for every method not explicitly
+implemented in the class, even if a method implementation is inherited
+from the superclass.  If you use the @code{-Wno-protocol} option, then
+methods inherited from the superclass are considered to be implemented,
+and no warning is issued for them.
+
+@item -Wselector
+@opindex Wselector
+Warn if multiple methods of different types for the same selector are
+found during compilation.  The check is performed on the list of methods
+in the final stage of compilation.  Additionally, a check is performed
+for each selector appearing in a @code{@@selector(@dots{})}
+expression, and a corresponding method for that selector has been found
+during compilation.  Because these checks scan the method table only at
+the end of compilation, these warnings are not produced if the final
+stage of compilation is not reached, for example because an error is
+found during compilation, or because the @code{-fsyntax-only} option is
+being used.
+
+@item -Wundeclared-selector
+@opindex Wundeclared-selector
+Warn if a @code{@@selector(@dots{})} expression referring to an
+undeclared selector is found.  A selector is considered undeclared if no
+method with that name has been declared before the
+@code{@@selector(@dots{})} expression, either explicitly in an
+@code{@@interface} or @code{@@protocol} declaration, or implicitly in
+an @code{@@implementation} section.  This option always performs its
+checks as soon as a @code{@@selector(@dots{})} expression is found,
+while @code{-Wselector} only performs its checks in the final stage of
+compilation.  This also enforces the coding style convention
+that methods and selectors must be declared before being used.
+
+@item -print-objc-runtime-info
+@opindex print-objc-runtime-info
+Generate C header describing the largest structure that is passed by
+value, if any.
+
+@end table
+
+@node Language Independent Options
+@section Options to Control Diagnostic Messages Formatting
+@cindex options to control diagnostics formatting
+@cindex diagnostic messages
+@cindex message formatting
+
+Traditionally, diagnostic messages have been formatted irrespective of
+the output device's aspect (e.g.@: its width, @dots{}).  The options described
+below can be used to control the diagnostic messages formatting
+algorithm, e.g.@: how many characters per line, how often source location
+information should be reported.  Right now, only the C++ front end can
+honor these options.  However it is expected, in the near future, that
+the remaining front ends would be able to digest them correctly.
+
+@table @gcctabopt
+@item -fmessage-length=@var{n}
+@opindex fmessage-length
+Try to format error messages so that they fit on lines of about @var{n}
+characters.  The default is 72 characters for @command{g++} and 0 for the rest of
+the front ends supported by GCC@.  If @var{n} is zero, then no
+line-wrapping will be done; each error message will appear on a single
+line.
+
+@opindex fdiagnostics-show-location
+@item -fdiagnostics-show-location=once
+Only meaningful in line-wrapping mode.  Instructs the diagnostic messages
+reporter to emit @emph{once} source location information; that is, in
+case the message is too long to fit on a single physical line and has to
+be wrapped, the source location won't be emitted (as prefix) again,
+over and over, in subsequent continuation lines.  This is the default
+behavior.
+
+@item -fdiagnostics-show-location=every-line
+Only meaningful in line-wrapping mode.  Instructs the diagnostic
+messages reporter to emit the same source location information (as
+prefix) for physical lines that result from the process of breaking
+a message which is too long to fit on a single line.
+
+@end table
+
+@node Warning Options
+@section Options to Request or Suppress Warnings
+@cindex options to control warnings
+@cindex warning messages
+@cindex messages, warning
+@cindex suppressing warnings
+
+Warnings are diagnostic messages that report constructions which
+are not inherently erroneous but which are risky or suggest there
+may have been an error.
+
+You can request many specific warnings with options beginning @samp{-W},
+for example @option{-Wimplicit} to request warnings on implicit
+declarations.  Each of these specific warning options also has a
+negative form beginning @samp{-Wno-} to turn off warnings;
+for example, @option{-Wno-implicit}.  This manual lists only one of the
+two forms, whichever is not the default.
+
+The following options control the amount and kinds of warnings produced
+by GCC; for further, language-specific options also refer to
+@ref{C++ Dialect Options} and @ref{Objective-C Dialect Options}.
+
+@table @gcctabopt
+@cindex syntax checking
+@item -fsyntax-only
+@opindex fsyntax-only
+Check the code for syntax errors, but don't do anything beyond that.
+
+@item -pedantic
+@opindex pedantic
+Issue all the warnings demanded by strict ISO C and ISO C++;
+reject all programs that use forbidden extensions, and some other
+programs that do not follow ISO C and ISO C++.  For ISO C, follows the
+version of the ISO C standard specified by any @option{-std} option used.
+
+Valid ISO C and ISO C++ programs should compile properly with or without
+this option (though a rare few will require @option{-ansi} or a
+@option{-std} option specifying the required version of ISO C)@.  However,
+without this option, certain GNU extensions and traditional C and C++
+features are supported as well.  With this option, they are rejected.
+
+@option{-pedantic} does not cause warning messages for use of the
+alternate keywords whose names begin and end with @samp{__}.  Pedantic
+warnings are also disabled in the expression that follows
+@code{__extension__}.  However, only system header files should use
+these escape routes; application programs should avoid them.
+@xref{Alternate Keywords}.
+
+Some users try to use @option{-pedantic} to check programs for strict ISO
+C conformance.  They soon find that it does not do quite what they want:
+it finds some non-ISO practices, but not all---only those for which
+ISO C @emph{requires} a diagnostic, and some others for which
+diagnostics have been added.
+
+A feature to report any failure to conform to ISO C might be useful in
+some instances, but would require considerable additional work and would
+be quite different from @option{-pedantic}.  We don't have plans to
+support such a feature in the near future.
+
+Where the standard specified with @option{-std} represents a GNU
+extended dialect of C, such as @samp{gnu89} or @samp{gnu99}, there is a
+corresponding @dfn{base standard}, the version of ISO C on which the GNU
+extended dialect is based.  Warnings from @option{-pedantic} are given
+where they are required by the base standard.  (It would not make sense
+for such warnings to be given only for features not in the specified GNU
+C dialect, since by definition the GNU dialects of C include all
+features the compiler supports with the given option, and there would be
+nothing to warn about.)
+
+@item -pedantic-errors
+@opindex pedantic-errors
+Like @option{-pedantic}, except that errors are produced rather than
+warnings.
+
+@item -w
+@opindex w
+Inhibit all warning messages.
+
+@item -Wno-import
+@opindex Wno-import
+Inhibit warning messages about the use of @samp{#import}.
+
+@item -Wchar-subscripts
+@opindex Wchar-subscripts
+Warn if an array subscript has type @code{char}.  This is a common cause
+of error, as programmers often forget that this type is signed on some
+machines.
+
+@item -Wcomment
+@opindex Wcomment
+Warn whenever a comment-start sequence @samp{/*} appears in a @samp{/*}
+comment, or whenever a Backslash-Newline appears in a @samp{//} comment.
+
+@item -Wformat
+@opindex Wformat
+Check calls to @code{printf} and @code{scanf}, etc., to make sure that
+the arguments supplied have types appropriate to the format string
+specified, and that the conversions specified in the format string make
+sense.  This includes standard functions, and others specified by format
+attributes (@pxref{Function Attributes}), in the @code{printf},
+@code{scanf}, @code{strftime} and @code{strfmon} (an X/Open extension,
+not in the C standard) families.
+
+The formats are checked against the format features supported by GNU
+libc version 2.2.  These include all ISO C90 and C99 features, as well
+as features from the Single Unix Specification and some BSD and GNU
+extensions.  Other library implementations may not support all these
+features; GCC does not support warning about features that go beyond a
+particular library's limitations.  However, if @option{-pedantic} is used
+with @option{-Wformat}, warnings will be given about format features not
+in the selected standard version (but not for @code{strfmon} formats,
+since those are not in any version of the C standard).  @xref{C Dialect
+Options,,Options Controlling C Dialect}.
+
+Since @option{-Wformat} also checks for null format arguments for
+several functions, @option{-Wformat} also implies @option{-Wnonnull}.
+
+@option{-Wformat} is included in @option{-Wall}.  For more control over some
+aspects of format checking, the options @option{-Wformat-y2k},
+@option{-Wno-format-extra-args}, @option{-Wno-format-zero-length},
+@option{-Wformat-nonliteral}, @option{-Wformat-security}, and
+@option{-Wformat=2} are available, but are not included in @option{-Wall}.
+
+@item -Wformat-y2k
+@opindex Wformat-y2k
+If @option{-Wformat} is specified, also warn about @code{strftime}
+formats which may yield only a two-digit year.
+
+@item -Wno-format-extra-args
+@opindex Wno-format-extra-args
+If @option{-Wformat} is specified, do not warn about excess arguments to a
+@code{printf} or @code{scanf} format function.  The C standard specifies
+that such arguments are ignored.
+
+Where the unused arguments lie between used arguments that are
+specified with @samp{$} operand number specifications, normally
+warnings are still given, since the implementation could not know what
+type to pass to @code{va_arg} to skip the unused arguments.  However,
+in the case of @code{scanf} formats, this option will suppress the
+warning if the unused arguments are all pointers, since the Single
+Unix Specification says that such unused arguments are allowed.
+
+@item -Wno-format-zero-length
+@opindex Wno-format-zero-length
+If @option{-Wformat} is specified, do not warn about zero-length formats.
+The C standard specifies that zero-length formats are allowed.
+
+@item -Wformat-nonliteral
+@opindex Wformat-nonliteral
+If @option{-Wformat} is specified, also warn if the format string is not a
+string literal and so cannot be checked, unless the format function
+takes its format arguments as a @code{va_list}.
+
+@item -Wformat-security
+@opindex Wformat-security
+If @option{-Wformat} is specified, also warn about uses of format
+functions that represent possible security problems.  At present, this
+warns about calls to @code{printf} and @code{scanf} functions where the
+format string is not a string literal and there are no format arguments,
+as in @code{printf (foo);}.  This may be a security hole if the format
+string came from untrusted input and contains @samp{%n}.  (This is
+currently a subset of what @option{-Wformat-nonliteral} warns about, but
+in future warnings may be added to @option{-Wformat-security} that are not
+included in @option{-Wformat-nonliteral}.)
+
+@item -Wformat=2
+@opindex Wformat=2
+Enable @option{-Wformat} plus format checks not included in
+@option{-Wformat}.  Currently equivalent to @samp{-Wformat
+-Wformat-nonliteral -Wformat-security -Wformat-y2k}.
+
+@item -Wnonnull
+@opindex Wnonnull
+Warn about passing a null pointer for arguments marked as
+requiring a non-null value by the @code{nonnull} function attribute.
+
+@option{-Wnonnull} is included in @option{-Wall} and @option{-Wformat}.  It
+can be disabled with the @option{-Wno-nonnull} option.
+
+@item -Winit-self @r{(C, C++, and Objective-C only)}
+@opindex Winit-self
+Warn about uninitialized variables which are initialized with themselves.
+Note this option can only be used with the @option{-Wuninitialized} option,
+which in turn only works with @option{-O1} and above.
+
+For example, GCC will warn about @code{i} being uninitialized in the
+following snippet only when @option{-Winit-self} has been specified:
+@smallexample
+@group
+int f()
+@{
+  int i = i;
+  return i;
+@}
+@end group
+@end smallexample
+
+@item -Wimplicit-int
+@opindex Wimplicit-int
+Warn when a declaration does not specify a type.
+
+@item -Wimplicit-function-declaration
+@itemx -Werror-implicit-function-declaration
+@opindex Wimplicit-function-declaration
+@opindex Werror-implicit-function-declaration
+Give a warning (or error) whenever a function is used before being
+declared.
+
+@item -Wimplicit
+@opindex Wimplicit
+Same as @option{-Wimplicit-int} and @option{-Wimplicit-function-declaration}.
+
+@item -Wmain
+@opindex Wmain
+Warn if the type of @samp{main} is suspicious.  @samp{main} should be a
+function with external linkage, returning int, taking either zero
+arguments, two, or three arguments of appropriate types.
+
+@item -Wmissing-braces
+@opindex Wmissing-braces
+Warn if an aggregate or union initializer is not fully bracketed.  In
+the following example, the initializer for @samp{a} is not fully
+bracketed, but that for @samp{b} is fully bracketed.
+
+@smallexample
+int a[2][2] = @{ 0, 1, 2, 3 @};
+int b[2][2] = @{ @{ 0, 1 @}, @{ 2, 3 @} @};
+@end smallexample
+
+@item -Wparentheses
+@opindex Wparentheses
+Warn if parentheses are omitted in certain contexts, such
+as when there is an assignment in a context where a truth value
+is expected, or when operators are nested whose precedence people
+often get confused about.
+
+Also warn about constructions where there may be confusion to which
+@code{if} statement an @code{else} branch belongs.  Here is an example of
+such a case:
+
+@smallexample
+@group
+@{
+  if (a)
+    if (b)
+      foo ();
+  else
+    bar ();
+@}
+@end group
+@end smallexample
+
+In C, every @code{else} branch belongs to the innermost possible @code{if}
+statement, which in this example is @code{if (b)}.  This is often not
+what the programmer expected, as illustrated in the above example by
+indentation the programmer chose.  When there is the potential for this
+confusion, GCC will issue a warning when this flag is specified.
+To eliminate the warning, add explicit braces around the innermost
+@code{if} statement so there is no way the @code{else} could belong to
+the enclosing @code{if}.  The resulting code would look like this:
+
+@smallexample
+@group
+@{
+  if (a)
+    @{
+      if (b)
+        foo ();
+      else
+        bar ();
+    @}
+@}
+@end group
+@end smallexample
+
+@item -Wsequence-point
+@opindex Wsequence-point
+Warn about code that may have undefined semantics because of violations
+of sequence point rules in the C standard.
+
+The C standard defines the order in which expressions in a C program are
+evaluated in terms of @dfn{sequence points}, which represent a partial
+ordering between the execution of parts of the program: those executed
+before the sequence point, and those executed after it.  These occur
+after the evaluation of a full expression (one which is not part of a
+larger expression), after the evaluation of the first operand of a
+@code{&&}, @code{||}, @code{? :} or @code{,} (comma) operator, before a
+function is called (but after the evaluation of its arguments and the
+expression denoting the called function), and in certain other places.
+Other than as expressed by the sequence point rules, the order of
+evaluation of subexpressions of an expression is not specified.  All
+these rules describe only a partial order rather than a total order,
+since, for example, if two functions are called within one expression
+with no sequence point between them, the order in which the functions
+are called is not specified.  However, the standards committee have
+ruled that function calls do not overlap.
+
+It is not specified when between sequence points modifications to the
+values of objects take effect.  Programs whose behavior depends on this
+have undefined behavior; the C standard specifies that ``Between the
+previous and next sequence point an object shall have its stored value
+modified at most once by the evaluation of an expression.  Furthermore,
+the prior value shall be read only to determine the value to be
+stored.''.  If a program breaks these rules, the results on any
+particular implementation are entirely unpredictable.
+
+Examples of code with undefined behavior are @code{a = a++;}, @code{a[n]
+= b[n++]} and @code{a[i++] = i;}.  Some more complicated cases are not
+diagnosed by this option, and it may give an occasional false positive
+result, but in general it has been found fairly effective at detecting
+this sort of problem in programs.
+
+The present implementation of this option only works for C programs.  A
+future implementation may also work for C++ programs.
+
+The C standard is worded confusingly, therefore there is some debate
+over the precise meaning of the sequence point rules in subtle cases.
+Links to discussions of the problem, including proposed formal
+definitions, may be found on the GCC readings page, at
+@w{@uref{http://gcc.gnu.org/readings.html}}.
+
+@item -Wreturn-type
+@opindex Wreturn-type
+Warn whenever a function is defined with a return-type that defaults to
+@code{int}.  Also warn about any @code{return} statement with no
+return-value in a function whose return-type is not @code{void}.
+
+For C++, a function without return type always produces a diagnostic
+message, even when @option{-Wno-return-type} is specified.  The only
+exceptions are @samp{main} and functions defined in system headers.
+
+@item -Wswitch
+@opindex Wswitch
+Warn whenever a @code{switch} statement has an index of enumerated type
+and lacks a @code{case} for one or more of the named codes of that
+enumeration.  (The presence of a @code{default} label prevents this
+warning.)  @code{case} labels outside the enumeration range also
+provoke warnings when this option is used.
+
+@item -Wswitch-default
+@opindex Wswitch-switch
+Warn whenever a @code{switch} statement does not have a @code{default}
+case.
+
+@item -Wswitch-enum
+@opindex Wswitch-enum
+Warn whenever a @code{switch} statement has an index of enumerated type
+and lacks a @code{case} for one or more of the named codes of that
+enumeration.  @code{case} labels outside the enumeration range also
+provoke warnings when this option is used.
+
+@item -Wtrigraphs
+@opindex Wtrigraphs
+Warn if any trigraphs are encountered that might change the meaning of
+the program (trigraphs within comments are not warned about).
+
+@item -Wunused-function
+@opindex Wunused-function
+Warn whenever a static function is declared but not defined or a
+non\-inline static function is unused.
+
+@item -Wunused-label
+@opindex Wunused-label
+Warn whenever a label is declared but not used.
+
+To suppress this warning use the @samp{unused} attribute
+(@pxref{Variable Attributes}).
+
+@item -Wunused-parameter
+@opindex Wunused-parameter
+Warn whenever a function parameter is unused aside from its declaration.
+
+To suppress this warning use the @samp{unused} attribute
+(@pxref{Variable Attributes}).
+
+@item -Wunused-variable
+@opindex Wunused-variable
+Warn whenever a local variable or non-constant static variable is unused
+aside from its declaration
+
+To suppress this warning use the @samp{unused} attribute
+(@pxref{Variable Attributes}).
+
+@item -Wunused-value
+@opindex Wunused-value
+Warn whenever a statement computes a result that is explicitly not used.
+
+To suppress this warning cast the expression to @samp{void}.
+
+@item -Wunused
+@opindex Wunused
+All the above @option{-Wunused} options combined.
+
+In order to get a warning about an unused function parameter, you must
+either specify @samp{-Wextra -Wunused} (note that @samp{-Wall} implies
+@samp{-Wunused}), or separately specify @option{-Wunused-parameter}.
+
+@item -Wuninitialized
+@opindex Wuninitialized
+Warn if an automatic variable is used without first being initialized or
+if a variable may be clobbered by a @code{setjmp} call.
+
+These warnings are possible only in optimizing compilation,
+because they require data flow information that is computed only
+when optimizing.  If you don't specify @option{-O}, you simply won't
+get these warnings.
+
+If you want to warn about code which uses the uninitialized value of the
+variable in its own initializer, use the @option{-Winit-self} option.
+
+These warnings occur only for variables that are candidates for
+register allocation.  Therefore, they do not occur for a variable that
+is declared @code{volatile}, or whose address is taken, or whose size
+is other than 1, 2, 4 or 8 bytes.  Also, they do not occur for
+structures, unions or arrays, even when they are in registers.
+
+Note that there may be no warning about a variable that is used only
+to compute a value that itself is never used, because such
+computations may be deleted by data flow analysis before the warnings
+are printed.
+
+These warnings are made optional because GCC is not smart
+enough to see all the reasons why the code might be correct
+despite appearing to have an error.  Here is one example of how
+this can happen:
+
+@smallexample
+@group
+@{
+  int x;
+  switch (y)
+    @{
+    case 1: x = 1;
+      break;
+    case 2: x = 4;
+      break;
+    case 3: x = 5;
+    @}
+  foo (x);
+@}
+@end group
+@end smallexample
+
+@noindent
+If the value of @code{y} is always 1, 2 or 3, then @code{x} is
+always initialized, but GCC doesn't know this.  Here is
+another common case:
+
+@smallexample
+@{
+  int save_y;
+  if (change_y) save_y = y, y = new_y;
+  @dots{}
+  if (change_y) y = save_y;
+@}
+@end smallexample
+
+@noindent
+This has no bug because @code{save_y} is used only if it is set.
+
+@cindex @code{longjmp} warnings
+This option also warns when a non-volatile automatic variable might be
+changed by a call to @code{longjmp}.  These warnings as well are possible
+only in optimizing compilation.
+
+The compiler sees only the calls to @code{setjmp}.  It cannot know
+where @code{longjmp} will be called; in fact, a signal handler could
+call it at any point in the code.  As a result, you may get a warning
+even when there is in fact no problem because @code{longjmp} cannot
+in fact be called at the place which would cause a problem.
+
+Some spurious warnings can be avoided if you declare all the functions
+you use that never return as @code{noreturn}.  @xref{Function
+Attributes}.
+
+@item -Wunknown-pragmas
+@opindex Wunknown-pragmas
+@cindex warning for unknown pragmas
+@cindex unknown pragmas, warning
+@cindex pragmas, warning of unknown
+Warn when a #pragma directive is encountered which is not understood by
+GCC@.  If this command line option is used, warnings will even be issued
+for unknown pragmas in system header files.  This is not the case if
+the warnings were only enabled by the @option{-Wall} command line option.
+
+@item -Wstrict-aliasing
+@opindex Wstrict-aliasing
+This option is only active when @option{-fstrict-aliasing} is active.
+It warns about code which might break the strict aliasing rules that the
+compiler is using for optimization. The warning does not catch all
+cases, but does attempt to catch the more common pitfalls. It is
+included in @option{-Wall}.
+
+@item -Wall
+@opindex Wall
+All of the above @samp{-W} options combined.  This enables all the
+warnings about constructions that some users consider questionable, and
+that are easy to avoid (or modify to prevent the warning), even in
+conjunction with macros.  This also enables some language-specific
+warnings described in @ref{C++ Dialect Options} and
+@ref{Objective-C Dialect Options}.
+@end table
+
+The following @option{-W@dots{}} options are not implied by @option{-Wall}.
+Some of them warn about constructions that users generally do not
+consider questionable, but which occasionally you might wish to check
+for; others warn about constructions that are necessary or hard to avoid
+in some cases, and there is no simple way to modify the code to suppress
+the warning.
+
+@table @gcctabopt
+@item -Wextra
+@opindex W
+@opindex Wextra
+(This option used to be called @option{-W}.  The older name is still
+supported, but the newer name is more descriptive.)  Print extra warning
+messages for these events:
+
+@itemize @bullet
+@item
+A function can return either with or without a value.  (Falling
+off the end of the function body is considered returning without
+a value.)  For example, this function would evoke such a
+warning:
+
+@smallexample
+@group
+foo (a)
+@{
+  if (a > 0)
+    return a;
+@}
+@end group
+@end smallexample
+
+@item
+An expression-statement or the left-hand side of a comma expression
+contains no side effects.
+To suppress the warning, cast the unused expression to void.
+For example, an expression such as @samp{x[i,j]} will cause a warning,
+but @samp{x[(void)i,j]} will not.
+
+@item
+An unsigned value is compared against zero with @samp{<} or @samp{>=}.
+
+@item
+A comparison like @samp{x<=y<=z} appears; this is equivalent to
+@samp{(x<=y ? 1 : 0) <= z}, which is a different interpretation from
+that of ordinary mathematical notation.
+
+@item
+Storage-class specifiers like @code{static} are not the first things in
+a declaration.  According to the C Standard, this usage is obsolescent.
+
+@item
+The return type of a function has a type qualifier such as @code{const}.
+Such a type qualifier has no effect, since the value returned by a
+function is not an lvalue.  (But don't warn about the GNU extension of
+@code{volatile void} return types.  That extension will be warned about
+if @option{-pedantic} is specified.)
+
+@item
+If @option{-Wall} or @option{-Wunused} is also specified, warn about unused
+arguments.
+
+@item
+A comparison between signed and unsigned values could produce an
+incorrect result when the signed value is converted to unsigned.
+(But don't warn if @option{-Wno-sign-compare} is also specified.)
+
+@item
+An aggregate has an initializer which does not initialize all members.
+For example, the following code would cause such a warning, because
+@code{x.h} would be implicitly initialized to zero:
+
+@smallexample
+struct s @{ int f, g, h; @};
+struct s x = @{ 3, 4 @};
+@end smallexample
+
+@item
+A function parameter is declared without a type specifier in K&R-style
+functions:
+
+@smallexample
+void foo(bar) @{ @}
+@end smallexample
+
+@item
+An empty body occurs in an @samp{if} or @samp{else} statement.
+
+@item
+A pointer is compared against integer zero with @samp{<}, @samp{<=},
+@samp{>}, or @samp{>=}.
+
+@item
+A variable might be changed by @samp{longjmp} or @samp{vfork}.
+
+@item
+Any of several floating-point events that often indicate errors, such as
+overflow, underflow, loss of precision, etc.
+
+@item @r{(C++ only)}
+An enumerator and a non-enumerator both appear in a conditional expression.
+
+@item @r{(C++ only)}
+A non-static reference or non-static @samp{const} member appears in a
+class without constructors.
+
+@item @r{(C++ only)}
+Ambiguous virtual bases.
+
+@item @r{(C++ only)}
+Subscripting an array which has been declared @samp{register}.
+
+@item @r{(C++ only)}
+Taking the address of a variable which has been declared @samp{register}.
+
+@item @r{(C++ only)}
+A base class is not initialized in a derived class' copy constructor.
+@end itemize
+
+@item -Wno-div-by-zero
+@opindex Wno-div-by-zero
+@opindex Wdiv-by-zero
+Do not warn about compile-time integer division by zero.  Floating point
+division by zero is not warned about, as it can be a legitimate way of
+obtaining infinities and NaNs.
+
+@item -Wsystem-headers
+@opindex Wsystem-headers
+@cindex warnings from system headers
+@cindex system headers, warnings from
+Print warning messages for constructs found in system header files.
+Warnings from system headers are normally suppressed, on the assumption
+that they usually do not indicate real problems and would only make the
+compiler output harder to read.  Using this command line option tells
+GCC to emit warnings from system headers as if they occurred in user
+code.  However, note that using @option{-Wall} in conjunction with this
+option will @emph{not} warn about unknown pragmas in system
+headers---for that, @option{-Wunknown-pragmas} must also be used.
+
+@item -Wfloat-equal
+@opindex Wfloat-equal
+Warn if floating point values are used in equality comparisons.
+
+The idea behind this is that sometimes it is convenient (for the
+programmer) to consider floating-point values as approximations to
+infinitely precise real numbers.  If you are doing this, then you need
+to compute (by analyzing the code, or in some other way) the maximum or
+likely maximum error that the computation introduces, and allow for it
+when performing comparisons (and when producing output, but that's a
+different problem).  In particular, instead of testing for equality, you
+would check to see whether the two values have ranges that overlap; and
+this is done with the relational operators, so equality comparisons are
+probably mistaken.
+
+@item -Wtraditional @r{(C only)}
+@opindex Wtraditional
+Warn about certain constructs that behave differently in traditional and
+ISO C@.  Also warn about ISO C constructs that have no traditional C
+equivalent, and/or problematic constructs which should be avoided.
+
+@itemize @bullet
+@item
+Macro parameters that appear within string literals in the macro body.
+In traditional C macro replacement takes place within string literals,
+but does not in ISO C@.
+
+@item
+In traditional C, some preprocessor directives did not exist.
+Traditional preprocessors would only consider a line to be a directive
+if the @samp{#} appeared in column 1 on the line.  Therefore
+@option{-Wtraditional} warns about directives that traditional C
+understands but would ignore because the @samp{#} does not appear as the
+first character on the line.  It also suggests you hide directives like
+@samp{#pragma} not understood by traditional C by indenting them.  Some
+traditional implementations would not recognize @samp{#elif}, so it
+suggests avoiding it altogether.
+
+@item
+A function-like macro that appears without arguments.
+
+@item
+The unary plus operator.
+
+@item
+The @samp{U} integer constant suffix, or the @samp{F} or @samp{L} floating point
+constant suffixes.  (Traditional C does support the @samp{L} suffix on integer
+constants.)  Note, these suffixes appear in macros defined in the system
+headers of most modern systems, e.g.@: the @samp{_MIN}/@samp{_MAX} macros in @code{<limits.h>}.
+Use of these macros in user code might normally lead to spurious
+warnings, however GCC's integrated preprocessor has enough context to
+avoid warning in these cases.
+
+@item
+A function declared external in one block and then used after the end of
+the block.
+
+@item
+A @code{switch} statement has an operand of type @code{long}.
+
+@item
+A non-@code{static} function declaration follows a @code{static} one.
+This construct is not accepted by some traditional C compilers.
+
+@item
+The ISO type of an integer constant has a different width or
+signedness from its traditional type.  This warning is only issued if
+the base of the constant is ten.  I.e.@: hexadecimal or octal values, which
+typically represent bit patterns, are not warned about.
+
+@item
+Usage of ISO string concatenation is detected.
+
+@item
+Initialization of automatic aggregates.
+
+@item
+Identifier conflicts with labels.  Traditional C lacks a separate
+namespace for labels.
+
+@item
+Initialization of unions.  If the initializer is zero, the warning is
+omitted.  This is done under the assumption that the zero initializer in
+user code appears conditioned on e.g.@: @code{__STDC__} to avoid missing
+initializer warnings and relies on default initialization to zero in the
+traditional C case.
+
+@item
+Conversions by prototypes between fixed/floating point values and vice
+versa.  The absence of these prototypes when compiling with traditional
+C would cause serious problems.  This is a subset of the possible
+conversion warnings, for the full set use @option{-Wconversion}.
+
+@item
+Use of ISO C style function definitions.  This warning intentionally is
+@emph{not} issued for prototype declarations or variadic functions
+because these ISO C features will appear in your code when using
+libiberty's traditional C compatibility macros, @code{PARAMS} and
+@code{VPARAMS}.  This warning is also bypassed for nested functions
+because that feature is already a GCC extension and thus not relevant to
+traditional C compatibility.
+@end itemize
+
+@item -Wdeclaration-after-statement @r{(C only)}
+@opindex Wdeclaration-after-statement
+Warn when a declaration is found after a statement in a block.  This
+construct, known from C++, was introduced with ISO C99 and is by default
+allowed in GCC@.  It is not supported by ISO C90 and was not supported by
+GCC versions before GCC 3.0.  @xref{Mixed Declarations}.
+
+@item -Wundef
+@opindex Wundef
+Warn if an undefined identifier is evaluated in an @samp{#if} directive.
+
+@item -Wendif-labels
+@opindex Wendif-labels
+Warn whenever an @samp{#else} or an @samp{#endif} are followed by text.
+
+@item -Wshadow
+@opindex Wshadow
+Warn whenever a local variable shadows another local variable, parameter or
+global variable or whenever a built-in function is shadowed.
+
+@item -Wlarger-than-@var{len}
+@opindex Wlarger-than
+Warn whenever an object of larger than @var{len} bytes is defined.
+
+@item -Wpointer-arith
+@opindex Wpointer-arith
+Warn about anything that depends on the ``size of'' a function type or
+of @code{void}.  GNU C assigns these types a size of 1, for
+convenience in calculations with @code{void *} pointers and pointers
+to functions.
+
+@item -Wbad-function-cast @r{(C only)}
+@opindex Wbad-function-cast
+Warn whenever a function call is cast to a non-matching type.
+For example, warn if @code{int malloc()} is cast to @code{anything *}.
+
+@item -Wcast-qual
+@opindex Wcast-qual
+Warn whenever a pointer is cast so as to remove a type qualifier from
+the target type.  For example, warn if a @code{const char *} is cast
+to an ordinary @code{char *}.
+
+@item -Wcast-align
+@opindex Wcast-align
+Warn whenever a pointer is cast such that the required alignment of the
+target is increased.  For example, warn if a @code{char *} is cast to
+an @code{int *} on machines where integers can only be accessed at
+two- or four-byte boundaries.
+
+@item -Wwrite-strings
+@opindex Wwrite-strings
+When compiling C, give string constants the type @code{const
+char[@var{length}]} so that
+copying the address of one into a non-@code{const} @code{char *}
+pointer will get a warning; when compiling C++, warn about the
+deprecated conversion from string constants to @code{char *}.
+These warnings will help you find at
+compile time code that can try to write into a string constant, but
+only if you have been very careful about using @code{const} in
+declarations and prototypes.  Otherwise, it will just be a nuisance;
+this is why we did not make @option{-Wall} request these warnings.
+
+@item -Wconversion
+@opindex Wconversion
+Warn if a prototype causes a type conversion that is different from what
+would happen to the same argument in the absence of a prototype.  This
+includes conversions of fixed point to floating and vice versa, and
+conversions changing the width or signedness of a fixed point argument
+except when the same as the default promotion.
+
+Also, warn if a negative integer constant expression is implicitly
+converted to an unsigned type.  For example, warn about the assignment
+@code{x = -1} if @code{x} is unsigned.  But do not warn about explicit
+casts like @code{(unsigned) -1}.
+
+@item -Wsign-compare
+@opindex Wsign-compare
+@cindex warning for comparison of signed and unsigned values
+@cindex comparison of signed and unsigned values, warning
+@cindex signed and unsigned values, comparison warning
+Warn when a comparison between signed and unsigned values could produce
+an incorrect result when the signed value is converted to unsigned.
+This warning is also enabled by @option{-Wextra}; to get the other warnings
+of @option{-Wextra} without this warning, use @samp{-Wextra -Wno-sign-compare}.
+
+@item -Waggregate-return
+@opindex Waggregate-return
+Warn if any functions that return structures or unions are defined or
+called.  (In languages where you can return an array, this also elicits
+a warning.)
+
+@item -Wstrict-prototypes @r{(C only)}
+@opindex Wstrict-prototypes
+Warn if a function is declared or defined without specifying the
+argument types.  (An old-style function definition is permitted without
+a warning if preceded by a declaration which specifies the argument
+types.)
+
+@item -Wold-style-definition @r{(C only)}
+@opindex Wold-style-definition
+Warn if an old-style function definition is used.  A warning is given
+even if there is a previous prototype.
+
+@item -Wmissing-prototypes @r{(C only)}
+@opindex Wmissing-prototypes
+Warn if a global function is defined without a previous prototype
+declaration.  This warning is issued even if the definition itself
+provides a prototype.  The aim is to detect global functions that fail
+to be declared in header files.
+
+@item -Wmissing-declarations @r{(C only)}
+@opindex Wmissing-declarations
+Warn if a global function is defined without a previous declaration.
+Do so even if the definition itself provides a prototype.
+Use this option to detect global functions that are not declared in
+header files.
+
+@item -Wmissing-noreturn
+@opindex Wmissing-noreturn
+Warn about functions which might be candidates for attribute @code{noreturn}.
+Note these are only possible candidates, not absolute ones.  Care should
+be taken to manually verify functions actually do not ever return before
+adding the @code{noreturn} attribute, otherwise subtle code generation
+bugs could be introduced.  You will not get a warning for @code{main} in
+hosted C environments.
+
+@item -Wmissing-format-attribute
+@opindex Wmissing-format-attribute
+@opindex Wformat
+If @option{-Wformat} is enabled, also warn about functions which might be
+candidates for @code{format} attributes.  Note these are only possible
+candidates, not absolute ones.  GCC will guess that @code{format}
+attributes might be appropriate for any function that calls a function
+like @code{vprintf} or @code{vscanf}, but this might not always be the
+case, and some functions for which @code{format} attributes are
+appropriate may not be detected.  This option has no effect unless
+@option{-Wformat} is enabled (possibly by @option{-Wall}).
+
+@item -Wno-multichar
+@opindex Wno-multichar
+@opindex Wmultichar
+Do not warn if a multicharacter constant (@samp{'FOOF'}) is used.
+Usually they indicate a typo in the user's code, as they have
+implementation-defined values, and should not be used in portable code.
+
+@item -Wno-deprecated-declarations
+@opindex Wno-deprecated-declarations
+Do not warn about uses of functions, variables, and types marked as
+deprecated by using the @code{deprecated} attribute.
+(@pxref{Function Attributes}, @pxref{Variable Attributes},
+@pxref{Type Attributes}.)
+
+@item -Wpacked
+@opindex Wpacked
+Warn if a structure is given the packed attribute, but the packed
+attribute has no effect on the layout or size of the structure.
+Such structures may be mis-aligned for little benefit.  For
+instance, in this code, the variable @code{f.x} in @code{struct bar}
+will be misaligned even though @code{struct bar} does not itself
+have the packed attribute:
+
+@smallexample
+@group
+struct foo @{
+  int x;
+  char a, b, c, d;
+@} __attribute__((packed));
+struct bar @{
+  char z;
+  struct foo f;
+@};
+@end group
+@end smallexample
+
+@item -Wpadded
+@opindex Wpadded
+Warn if padding is included in a structure, either to align an element
+of the structure or to align the whole structure.  Sometimes when this
+happens it is possible to rearrange the fields of the structure to
+reduce the padding and so make the structure smaller.
+
+@item -Wredundant-decls
+@opindex Wredundant-decls
+Warn if anything is declared more than once in the same scope, even in
+cases where multiple declaration is valid and changes nothing.
+
+@item -Wnested-externs @r{(C only)}
+@opindex Wnested-externs
+Warn if an @code{extern} declaration is encountered within a function.
+
+@item -Wunreachable-code
+@opindex Wunreachable-code
+Warn if the compiler detects that code will never be executed.
+
+This option is intended to warn when the compiler detects that at
+least a whole line of source code will never be executed, because
+some condition is never satisfied or because it is after a
+procedure that never returns.
+
+It is possible for this option to produce a warning even though there
+are circumstances under which part of the affected line can be executed,
+so care should be taken when removing apparently-unreachable code.
+
+For instance, when a function is inlined, a warning may mean that the
+line is unreachable in only one inlined copy of the function.
+
+This option is not made part of @option{-Wall} because in a debugging
+version of a program there is often substantial code which checks
+correct functioning of the program and is, hopefully, unreachable
+because the program does work.  Another common use of unreachable
+code is to provide behavior which is selectable at compile-time.
+
+@item -Winline
+@opindex Winline
+Warn if a function can not be inlined and it was declared as inline.
+Even with this option, the compiler will not warn about failures to
+inline functions declared in system headers.
+
+The compiler uses a variety of heuristics to determine whether or not
+to inline a function.  For example, the compiler takes into account
+the size of the function being inlined and the the amount of inlining
+that has already been done in the current function.  Therefore,
+seemingly insignificant changes in the source program can cause the
+warnings produced by @option{-Winline} to appear or disappear.
+
+@item -Wno-invalid-offsetof @r{(C++ only)}
+@opindex Wno-invalid-offsetof
+Suppress warnings from applying the @samp{offsetof} macro to a non-POD
+type.  According to the 1998 ISO C++ standard, applying @samp{offsetof}
+to a non-POD type is undefined.  In existing C++ implementations,
+however, @samp{offsetof} typically gives meaningful results even when
+applied to certain kinds of non-POD types. (Such as a simple
+@samp{struct} that fails to be a POD type only by virtue of having a
+constructor.)  This flag is for users who are aware that they are
+writing nonportable code and who have deliberately chosen to ignore the
+warning about it.
+
+The restrictions on @samp{offsetof} may be relaxed in a future version
+of the C++ standard.
+
+@item -Winvalid-pch
+@opindex Winvalid-pch
+Warn if a precompiled header (@pxref{Precompiled Headers}) is found in
+the search path but can't be used.
+
+@item -Wlong-long
+@opindex Wlong-long
+@opindex Wno-long-long
+Warn if @samp{long long} type is used.  This is default.  To inhibit
+the warning messages, use @option{-Wno-long-long}.  Flags
+@option{-Wlong-long} and @option{-Wno-long-long} are taken into account
+only when @option{-pedantic} flag is used.
+
+@item -Wdisabled-optimization
+@opindex Wdisabled-optimization
+Warn if a requested optimization pass is disabled.  This warning does
+not generally indicate that there is anything wrong with your code; it
+merely indicates that GCC's optimizers were unable to handle the code
+effectively.  Often, the problem is that your code is too big or too
+complex; GCC will refuse to optimize programs when the optimization
+itself is likely to take inordinate amounts of time.
+
+@item -Werror
+@opindex Werror
+Make all warnings into errors.
+@end table
+
+@node Debugging Options
+@section Options for Debugging Your Program or GCC
+@cindex options, debugging
+@cindex debugging information options
+
+GCC has various special options that are used for debugging
+either your program or GCC:
+
+@table @gcctabopt
+@item -g
+@opindex g
+Produce debugging information in the operating system's native format
+(stabs, COFF, XCOFF, or DWARF)@.  GDB can work with this debugging
+information.
+
+On most systems that use stabs format, @option{-g} enables use of extra
+debugging information that only GDB can use; this extra information
+makes debugging work better in GDB but will probably make other debuggers
+crash or
+refuse to read the program.  If you want to control for certain whether
+to generate the extra information, use @option{-gstabs+}, @option{-gstabs},
+@option{-gxcoff+}, @option{-gxcoff}, or @option{-gvms} (see below).
+
+Unlike most other C compilers, GCC allows you to use @option{-g} with
+@option{-O}.  The shortcuts taken by optimized code may occasionally
+produce surprising results: some variables you declared may not exist
+at all; flow of control may briefly move where you did not expect it;
+some statements may not be executed because they compute constant
+results or their values were already at hand; some statements may
+execute in different places because they were moved out of loops.
+
+Nevertheless it proves possible to debug optimized output.  This makes
+it reasonable to use the optimizer for programs that might have bugs.
+
+The following options are useful when GCC is generated with the
+capability for more than one debugging format.
+
+@item -ggdb
+@opindex ggdb
+Produce debugging information for use by GDB@.  This means to use the
+most expressive format available (DWARF 2, stabs, or the native format
+if neither of those are supported), including GDB extensions if at all
+possible.
+
+@item -gstabs
+@opindex gstabs
+Produce debugging information in stabs format (if that is supported),
+without GDB extensions.  This is the format used by DBX on most BSD
+systems.  On MIPS, Alpha and System V Release 4 systems this option
+produces stabs debugging output which is not understood by DBX or SDB@.
+On System V Release 4 systems this option requires the GNU assembler.
+
+@item -feliminate-unused-debug-symbols
+@opindex feliminate-unused-debug-symbols
+Produce debugging information in stabs format (if that is supported),
+for only symbols that are actually used.
+
+@item -gstabs+
+@opindex gstabs+
+Produce debugging information in stabs format (if that is supported),
+using GNU extensions understood only by the GNU debugger (GDB)@.  The
+use of these extensions is likely to make other debuggers crash or
+refuse to read the program.
+
+@item -gcoff
+@opindex gcoff
+Produce debugging information in COFF format (if that is supported).
+This is the format used by SDB on most System V systems prior to
+System V Release 4.
+
+@item -gxcoff
+@opindex gxcoff
+Produce debugging information in XCOFF format (if that is supported).
+This is the format used by the DBX debugger on IBM RS/6000 systems.
+
+@item -gxcoff+
+@opindex gxcoff+
+Produce debugging information in XCOFF format (if that is supported),
+using GNU extensions understood only by the GNU debugger (GDB)@.  The
+use of these extensions is likely to make other debuggers crash or
+refuse to read the program, and may cause assemblers other than the GNU
+assembler (GAS) to fail with an error.
+
+@item -gdwarf-2
+@opindex gdwarf-2
+Produce debugging information in DWARF version 2 format (if that is
+supported).  This is the format used by DBX on IRIX 6.
+
+@item -gvms
+@opindex gvms
+Produce debugging information in VMS debug format (if that is
+supported).  This is the format used by DEBUG on VMS systems.
+
+@item -g@var{level}
+@itemx -ggdb@var{level}
+@itemx -gstabs@var{level}
+@itemx -gcoff@var{level}
+@itemx -gxcoff@var{level}
+@itemx -gvms@var{level}
+Request debugging information and also use @var{level} to specify how
+much information.  The default level is 2.
+
+Level 1 produces minimal information, enough for making backtraces in
+parts of the program that you don't plan to debug.  This includes
+descriptions of functions and external variables, but no information
+about local variables and no line numbers.
+
+Level 3 includes extra information, such as all the macro definitions
+present in the program.  Some debuggers support macro expansion when
+you use @option{-g3}.
+
+Note that in order to avoid confusion between DWARF1 debug level 2,
+and DWARF2 @option{-gdwarf-2} does not accept a concatenated debug
+level.  Instead use an additional @option{-g@var{level}} option to
+change the debug level for DWARF2.
+
+@item -feliminate-dwarf2-dups
+@opindex feliminate-dwarf2-dups
+Compress DWARF2 debugging information by eliminating duplicated
+information about each symbol.  This option only makes sense when
+generating DWARF2 debugging information with @option{-gdwarf-2}.
+
+@cindex @command{prof}
+@item -p
+@opindex p
+Generate extra code to write profile information suitable for the
+analysis program @command{prof}.  You must use this option when compiling
+the source files you want data about, and you must also use it when
+linking.
+
+@cindex @command{gprof}
+@item -pg
+@opindex pg
+Generate extra code to write profile information suitable for the
+analysis program @command{gprof}.  You must use this option when compiling
+the source files you want data about, and you must also use it when
+linking.
+
+@item -Q
+@opindex Q
+Makes the compiler print out each function name as it is compiled, and
+print some statistics about each pass when it finishes.
+
+@item -ftime-report
+@opindex ftime-report
+Makes the compiler print some statistics about the time consumed by each
+pass when it finishes.
+
+@item -fmem-report
+@opindex fmem-report
+Makes the compiler print some statistics about permanent memory
+allocation when it finishes.
+
+@item -fprofile-arcs
+@opindex fprofile-arcs
+Add code so that program flow @dfn{arcs} are instrumented.  During
+execution the program records how many times each branch and call is
+executed and how many times it is taken or returns.  When the compiled
+program exits it saves this data to a file called
+@file{@var{auxname}.gcda} for each source file. The data may be used for
+profile-directed optimizations (@option{-fbranch-probabilities}), or for
+test coverage analysis (@option{-ftest-coverage}). Each object file's
+@var{auxname} is generated from the name of the output file, if
+explicitly specified and it is not the final executable, otherwise it is
+the basename of the source file. In both cases any suffix is removed
+(e.g.  @file{foo.gcda} for input file @file{dir/foo.c}, or
+@file{dir/foo.gcda} for output file specified as @option{-o dir/foo.o}).
+
+@itemize
+
+@item
+Compile the source files with @option{-fprofile-arcs} plus optimization
+and code generation options. For test coverage analysis, use the
+additional @option{-ftest-coverage} option. You do not need to profile
+every source file in a program.
+
+@item
+Link your object files with @option{-lgcov} or @option{-fprofile-arcs}
+(the latter implies the former).
+
+@item
+Run the program on a representative workload to generate the arc profile
+information. This may be repeated any number of times. You can run
+concurrent instances of your program, and provided that the file system
+supports locking, the data files will be correctly updated. Also
+@code{fork} calls are detected and correctly handled (double counting
+will not happen).
+
+@item
+For profile-directed optimizations, compile the source files again with
+the same optimization and code generation options plus
+@option{-fbranch-probabilities} (@pxref{Optimize Options,,Options that
+Control Optimization}).
+
+@item
+For test coverage analysis, use @command{gcov} to produce human readable
+information from the @file{.gcno} and @file{.gcda} files. Refer to the
+@command{gcov} documentation for further information.
+
+@end itemize
+
+With @option{-fprofile-arcs}, for each function of your program GCC
+creates a program flow graph, then finds a spanning tree for the graph.
+Only arcs that are not on the spanning tree have to be instrumented: the
+compiler adds code to count the number of times that these arcs are
+executed.  When an arc is the only exit or only entrance to a block, the
+instrumentation code can be added to the block; otherwise, a new basic
+block must be created to hold the instrumentation code.
+
+@need 2000
+@item -ftest-coverage
+@opindex ftest-coverage
+Produce a notes file that the @command{gcov} code-coverage utility
+(@pxref{Gcov,, @command{gcov}---a Test Coverage Program}) can use to
+show program coverage. Each source file's note file is called
+@file{@var{auxname}.gcno}. Refer to the @option{-fprofile-arcs} option
+above for a description of @var{auxname} and instructions on how to
+generate test coverage data. Coverage data will match the source files
+more closely, if you do not optimize.
+
+@item -d@var{letters}
+@opindex d
+Says to make debugging dumps during compilation at times specified by
+@var{letters}.  This is used for debugging the compiler.  The file names
+for most of the dumps are made by appending a pass number and a word to
+the @var{dumpname}. @var{dumpname} is generated from the name of the
+output file, if explicitly specified and it is not an executable,
+otherwise it is the basename of the source file. In both cases any
+suffix is removed (e.g.  @file{foo.01.rtl} or @file{foo.02.sibling}).
+Here are the possible letters for use in @var{letters}, and their
+meanings:
+
+@table @samp
+@item A
+@opindex dA
+Annotate the assembler output with miscellaneous debugging information.
+@item b
+@opindex db
+Dump after computing branch probabilities, to @file{@var{file}.12.bp}.
+@item B
+@opindex dB
+Dump after block reordering, to @file{@var{file}.31.bbro}.
+@item c
+@opindex dc
+Dump after instruction combination, to the file @file{@var{file}.20.combine}.
+@item C
+@opindex dC
+Dump after the first if conversion, to the file @file{@var{file}.14.ce1}.
+Also dump after the second if conversion, to the file @file{@var{file}.21.ce2}.
+@item d
+@opindex dd
+Dump after branch target load optimization, to to @file{@var{file}.32.btl}.
+Also dump after delayed branch scheduling, to @file{@var{file}.36.dbr}.
+@item D
+@opindex dD
+Dump all macro definitions, at the end of preprocessing, in addition to
+normal output.
+@item E
+@opindex dE
+Dump after the third if conversion, to @file{@var{file}.30.ce3}.
+@item f
+@opindex df
+Dump after control and data flow analysis, to @file{@var{file}.11.cfg}.
+Also dump after life analysis, to @file{@var{file}.19.life}.
+@item F
+@opindex dF
+Dump after purging @code{ADDRESSOF} codes, to @file{@var{file}.07.addressof}.
+@item g
+@opindex dg
+Dump after global register allocation, to @file{@var{file}.25.greg}.
+@item G
+@opindex dG
+Dump after GCSE, to @file{@var{file}.08.gcse}.
+Also dump after jump bypassing and control flow optimizations, to
+@file{@var{file}.10.bypass}.
+@item h
+@opindex dh
+Dump after finalization of EH handling code, to @file{@var{file}.03.eh}.
+@item i
+@opindex di
+Dump after sibling call optimizations, to @file{@var{file}.02.sibling}.
+@item j
+@opindex dj
+Dump after the first jump optimization, to @file{@var{file}.04.jump}.
+@item k
+@opindex dk
+Dump after conversion from registers to stack, to @file{@var{file}.34.stack}.
+@item l
+@opindex dl
+Dump after local register allocation, to @file{@var{file}.24.lreg}.
+@item L
+@opindex dL
+Dump after loop optimization passes, to @file{@var{file}.09.loop} and
+@file{@var{file}.16.loop2}.
+@item M
+@opindex dM
+Dump after performing the machine dependent reorganization pass, to
+@file{@var{file}.35.mach}.
+@item n
+@opindex dn
+Dump after register renumbering, to @file{@var{file}.29.rnreg}.
+@item N
+@opindex dN
+Dump after the register move pass, to @file{@var{file}.22.regmove}.
+@item o
+@opindex do
+Dump after post-reload optimizations, to @file{@var{file}.26.postreload}.
+@item r
+@opindex dr
+Dump after RTL generation, to @file{@var{file}.01.rtl}.
+@item R
+@opindex dR
+Dump after the second scheduling pass, to @file{@var{file}.33.sched2}.
+@item s
+@opindex ds
+Dump after CSE (including the jump optimization that sometimes follows
+CSE), to @file{@var{file}.06.cse}.
+@item S
+@opindex dS
+Dump after the first scheduling pass, to @file{@var{file}.23.sched}.
+@item t
+@opindex dt
+Dump after the second CSE pass (including the jump optimization that
+sometimes follows CSE), to @file{@var{file}.18.cse2}.
+@item T
+@opindex dT
+Dump after running tracer, to @file{@var{file}.15.tracer}.
+@item u
+@opindex du
+Dump after null pointer elimination pass to @file{@var{file}.05.null}.
+@item U
+@opindex dU
+Dump callgraph and unit-at-a-time optimization @file{@var{file}.00.unit}.
+@item V
+@opindex dV
+Dump after the value profile transformations, to @file{@var{file}.13.vpt}.
+@item w
+@opindex dw
+Dump after the second flow pass, to @file{@var{file}.27.flow2}.
+@item z
+@opindex dz
+Dump after the peephole pass, to @file{@var{file}.28.peephole2}.
+@item Z
+@opindex dZ
+Dump after constructing the web, to @file{@var{file}.17.web}.
+@item a
+@opindex da
+Produce all the dumps listed above.
+@item H
+@opindex dH
+Produce a core dump whenever an error occurs.
+@item m
+@opindex dm
+Print statistics on memory usage, at the end of the run, to
+standard error.
+@item p
+@opindex dp
+Annotate the assembler output with a comment indicating which
+pattern and alternative was used.  The length of each instruction is
+also printed.
+@item P
+@opindex dP
+Dump the RTL in the assembler output as a comment before each instruction.
+Also turns on @option{-dp} annotation.
+@item v
+@opindex dv
+For each of the other indicated dump files (except for
+@file{@var{file}.01.rtl}), dump a representation of the control flow graph
+suitable for viewing with VCG to @file{@var{file}.@var{pass}.vcg}.
+@item x
+@opindex dx
+Just generate RTL for a function instead of compiling it.  Usually used
+with @samp{r}.
+@item y
+@opindex dy
+Dump debugging information during parsing, to standard error.
+@end table
+
+@item -fdump-unnumbered
+@opindex fdump-unnumbered
+When doing debugging dumps (see @option{-d} option above), suppress instruction
+numbers and line number note output.  This makes it more feasible to
+use diff on debugging dumps for compiler invocations with different
+options, in particular with and without @option{-g}.
+
+@item -fdump-translation-unit @r{(C and C++ only)}
+@itemx -fdump-translation-unit-@var{options} @r{(C and C++ only)}
+@opindex fdump-translation-unit
+Dump a representation of the tree structure for the entire translation
+unit to a file.  The file name is made by appending @file{.tu} to the
+source file name.  If the @samp{-@var{options}} form is used, @var{options}
+controls the details of the dump as described for the
+@option{-fdump-tree} options.
+
+@item -fdump-class-hierarchy @r{(C++ only)}
+@itemx -fdump-class-hierarchy-@var{options} @r{(C++ only)}
+@opindex fdump-class-hierarchy
+Dump a representation of each class's hierarchy and virtual function
+table layout to a file.  The file name is made by appending @file{.class}
+to the source file name.  If the @samp{-@var{options}} form is used,
+@var{options} controls the details of the dump as described for the
+@option{-fdump-tree} options.
+
+@item -fdump-tree-@var{switch} @r{(C++ only)}
+@itemx -fdump-tree-@var{switch}-@var{options} @r{(C++ only)}
+@opindex fdump-tree
+Control the dumping at various stages of processing the intermediate
+language tree to a file.  The file name is generated by appending a switch
+specific suffix to the source file name.  If the @samp{-@var{options}}
+form is used, @var{options} is a list of @samp{-} separated options that
+control the details of the dump. Not all options are applicable to all
+dumps, those which are not meaningful will be ignored. The following
+options are available
+
+@table @samp
+@item address
+Print the address of each node.  Usually this is not meaningful as it
+changes according to the environment and source file. Its primary use
+is for tying up a dump file with a debug environment.
+@item slim
+Inhibit dumping of members of a scope or body of a function merely
+because that scope has been reached. Only dump such items when they
+are directly reachable by some other path.
+@item all
+Turn on all options.
+@end table
+
+The following tree dumps are possible:
+@table @samp
+@item original
+Dump before any tree based optimization, to @file{@var{file}.original}.
+@item optimized
+Dump after all tree based optimization, to @file{@var{file}.optimized}.
+@item inlined
+Dump after function inlining, to @file{@var{file}.inlined}.
+@end table
+
+@item -frandom-seed=@var{string}
+@opindex frandom-string
+This option provides a seed that GCC uses when it would otherwise use
+random numbers.  It is used to generate certain symbol names
+that have to be different in every compiled file. It is also used to
+place unique stamps in coverage data files and the object files that
+produce them. You can use the @option{-frandom-seed} option to produce
+reproducibly identical object files.
+
+The @var{string} should be different for every file you compile.
+
+@item -fsched-verbose=@var{n}
+@opindex fsched-verbose
+On targets that use instruction scheduling, this option controls the
+amount of debugging output the scheduler prints.  This information is
+written to standard error, unless @option{-dS} or @option{-dR} is
+specified, in which case it is output to the usual dump
+listing file, @file{.sched} or @file{.sched2} respectively.  However
+for @var{n} greater than nine, the output is always printed to standard
+error.
+
+For @var{n} greater than zero, @option{-fsched-verbose} outputs the
+same information as @option{-dRS}.  For @var{n} greater than one, it
+also output basic block probabilities, detailed ready list information
+and unit/insn info.  For @var{n} greater than two, it includes RTL
+at abort point, control-flow and regions info.  And for @var{n} over
+four, @option{-fsched-verbose} also includes dependence info.
+
+@item -save-temps
+@opindex save-temps
+Store the usual ``temporary'' intermediate files permanently; place them
+in the current directory and name them based on the source file.  Thus,
+compiling @file{foo.c} with @samp{-c -save-temps} would produce files
+@file{foo.i} and @file{foo.s}, as well as @file{foo.o}.  This creates a
+preprocessed @file{foo.i} output file even though the compiler now
+normally uses an integrated preprocessor.
+
+@item -time
+@opindex time
+Report the CPU time taken by each subprocess in the compilation
+sequence.  For C source files, this is the compiler proper and assembler
+(plus the linker if linking is done).  The output looks like this:
+
+@smallexample
+# cc1 0.12 0.01
+# as 0.00 0.01
+@end smallexample
+
+The first number on each line is the ``user time,'' that is time spent
+executing the program itself.  The second number is ``system time,''
+time spent executing operating system routines on behalf of the program.
+Both numbers are in seconds.
+
+@item -print-file-name=@var{library}
+@opindex print-file-name
+Print the full absolute name of the library file @var{library} that
+would be used when linking---and don't do anything else.  With this
+option, GCC does not compile or link anything; it just prints the
+file name.
+
+@item -print-multi-directory
+@opindex print-multi-directory
+Print the directory name corresponding to the multilib selected by any
+other switches present in the command line.  This directory is supposed
+to exist in @env{GCC_EXEC_PREFIX}.
+
+@item -print-multi-lib
+@opindex print-multi-lib
+Print the mapping from multilib directory names to compiler switches
+that enable them.  The directory name is separated from the switches by
+@samp{;}, and each switch starts with an @samp{@@} instead of the
+@samp{-}, without spaces between multiple switches.  This is supposed to
+ease shell-processing.
+
+@item -print-prog-name=@var{program}
+@opindex print-prog-name
+Like @option{-print-file-name}, but searches for a program such as @samp{cpp}.
+
+@item -print-libgcc-file-name
+@opindex print-libgcc-file-name
+Same as @option{-print-file-name=libgcc.a}.
+
+This is useful when you use @option{-nostdlib} or @option{-nodefaultlibs}
+but you do want to link with @file{libgcc.a}.  You can do
+
+@smallexample
+gcc -nostdlib @var{files}@dots{} `gcc -print-libgcc-file-name`
+@end smallexample
+
+@item -print-search-dirs
+@opindex print-search-dirs
+Print the name of the configured installation directory and a list of
+program and library directories @command{gcc} will search---and don't do anything else.
+
+This is useful when @command{gcc} prints the error message
+@samp{installation problem, cannot exec cpp0: No such file or directory}.
+To resolve this you either need to put @file{cpp0} and the other compiler
+components where @command{gcc} expects to find them, or you can set the environment
+variable @env{GCC_EXEC_PREFIX} to the directory where you installed them.
+Don't forget the trailing '/'.
+@xref{Environment Variables}.
+
+@item -dumpmachine
+@opindex dumpmachine
+Print the compiler's target machine (for example,
+@samp{i686-pc-linux-gnu})---and don't do anything else.
+
+@item -dumpversion
+@opindex dumpversion
+Print the compiler version (for example, @samp{3.0})---and don't do
+anything else.
+
+@item -dumpspecs
+@opindex dumpspecs
+Print the compiler's built-in specs---and don't do anything else.  (This
+is used when GCC itself is being built.)  @xref{Spec Files}.
+
+@item -feliminate-unused-debug-types
+@opindex feliminate-unused-debug-types
+Normally, when producing DWARF2 output, GCC will emit debugging
+information for all types declared in a compilation
+unit, regardless of whether or not they are actually used
+in that compilation unit.  Sometimes this is useful, such as
+if, in the debugger, you want to cast a value to a type that is
+not actually used in your program (but is declared).  More often,
+however, this results in a significant amount of wasted space.
+With this option, GCC will avoid producing debug symbol output
+for types that are nowhere used in the source file being compiled.
+@end table
+
+@node Optimize Options
+@section Options That Control Optimization
+@cindex optimize options
+@cindex options, optimization
+
+These options control various sorts of optimizations.
+
+Without any optimization option, the compiler's goal is to reduce the
+cost of compilation and to make debugging produce the expected
+results.  Statements are independent: if you stop the program with a
+breakpoint between statements, you can then assign a new value to any
+variable or change the program counter to any other statement in the
+function and get exactly the results you would expect from the source
+code.
+
+Turning on optimization flags makes the compiler attempt to improve
+the performance and/or code size at the expense of compilation time
+and possibly the ability to debug the program.
+
+The compiler performs optimization based on the knowledge it has of
+the program.  Using the @option{-funit-at-a-time} flag will allow the
+compiler to consider information gained from later functions in the
+file when compiling a function.  Compiling multiple files at once to a
+single output file (and using @option{-funit-at-a-time}) will allow
+the compiler to use information gained from all of the files when
+compiling each of them.
+
+Not all optimizations are controlled directly by a flag.  Only
+optimizations that have a flag are listed.
+
+@table @gcctabopt
+@item -O
+@itemx -O1
+@opindex O
+@opindex O1
+Optimize.  Optimizing compilation takes somewhat more time, and a lot
+more memory for a large function.
+
+With @option{-O}, the compiler tries to reduce code size and execution
+time, without performing any optimizations that take a great deal of
+compilation time.
+
+@option{-O} turns on the following optimization flags:
+@gccoptlist{-fdefer-pop @gol
+-fmerge-constants @gol
+-fthread-jumps @gol
+-floop-optimize @gol
+-fif-conversion @gol
+-fif-conversion2 @gol
+-fdelayed-branch @gol
+-fguess-branch-probability @gol
+-fcprop-registers}
+
+@option{-O} also turns on @option{-fomit-frame-pointer} on machines
+where doing so does not interfere with debugging.
+
+@item -O2
+@opindex O2
+Optimize even more.  GCC performs nearly all supported optimizations
+that do not involve a space-speed tradeoff.  The compiler does not
+perform loop unrolling or function inlining when you specify @option{-O2}.
+As compared to @option{-O}, this option increases both compilation time
+and the performance of the generated code.
+
+@option{-O2} turns on all optimization flags specified by @option{-O}.  It
+also turns on the following optimization flags:
+@gccoptlist{-fforce-mem @gol
+-foptimize-sibling-calls @gol
+-fstrength-reduce @gol
+-fcse-follow-jumps  -fcse-skip-blocks @gol
+-frerun-cse-after-loop  -frerun-loop-opt @gol
+-fgcse  -fgcse-lm  -fgcse-sm  -fgcse-las @gol
+-fdelete-null-pointer-checks @gol
+-fexpensive-optimizations @gol
+-fregmove @gol
+-fschedule-insns  -fschedule-insns2 @gol
+-fsched-interblock  -fsched-spec @gol
+-fcaller-saves @gol
+-fpeephole2 @gol
+-freorder-blocks  -freorder-functions @gol
+-fstrict-aliasing @gol
+-funit-at-a-time @gol
+-falign-functions  -falign-jumps @gol
+-falign-loops  -falign-labels @gol
+-fcrossjumping}
+
+Please note the warning under @option{-fgcse} about
+invoking @option{-O2} on programs that use computed gotos.
+
+@item -O3
+@opindex O3
+Optimize yet more.  @option{-O3} turns on all optimizations specified by
+@option{-O2} and also turns on the @option{-finline-functions},
+@option{-fweb} and @option{-frename-registers} options.
+
+@item -O0
+@opindex O0
+Do not optimize.  This is the default.
+
+@item -Os
+@opindex Os
+Optimize for size.  @option{-Os} enables all @option{-O2} optimizations that
+do not typically increase code size.  It also performs further
+optimizations designed to reduce code size.
+
+@option{-Os} disables the following optimization flags:
+@gccoptlist{-falign-functions  -falign-jumps  -falign-loops @gol
+-falign-labels  -freorder-blocks  -fprefetch-loop-arrays}
+
+If you use multiple @option{-O} options, with or without level numbers,
+the last such option is the one that is effective.
+@end table
+
+Options of the form @option{-f@var{flag}} specify machine-independent
+flags.  Most flags have both positive and negative forms; the negative
+form of @option{-ffoo} would be @option{-fno-foo}.  In the table
+below, only one of the forms is listed---the one you typically will
+use.  You can figure out the other form by either removing @samp{no-}
+or adding it.
+
+The following options control specific optimizations.  They are either
+activated by @option{-O} options or are related to ones that are.  You
+can use the following flags in the rare cases when ``fine-tuning'' of
+optimizations to be performed is desired.
+
+@table @gcctabopt
+@item -fno-default-inline
+@opindex fno-default-inline
+Do not make member functions inline by default merely because they are
+defined inside the class scope (C++ only).  Otherwise, when you specify
+@w{@option{-O}}, member functions defined inside class scope are compiled
+inline by default; i.e., you don't need to add @samp{inline} in front of
+the member function name.
+
+@item -fno-defer-pop
+@opindex fno-defer-pop
+Always pop the arguments to each function call as soon as that function
+returns.  For machines which must pop arguments after a function call,
+the compiler normally lets arguments accumulate on the stack for several
+function calls and pops them all at once.
+
+Disabled at levels @option{-O}, @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fforce-mem
+@opindex fforce-mem
+Force memory operands to be copied into registers before doing
+arithmetic on them.  This produces better code by making all memory
+references potential common subexpressions.  When they are not common
+subexpressions, instruction combination should eliminate the separate
+register-load.
+
+Enabled at levels @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fforce-addr
+@opindex fforce-addr
+Force memory address constants to be copied into registers before
+doing arithmetic on them.  This may produce better code just as
+@option{-fforce-mem} may.
+
+@item -fomit-frame-pointer
+@opindex fomit-frame-pointer
+Don't keep the frame pointer in a register for functions that
+don't need one.  This avoids the instructions to save, set up and
+restore frame pointers; it also makes an extra register available
+in many functions.  @strong{It also makes debugging impossible on
+some machines.}
+
+On some machines, such as the VAX, this flag has no effect, because
+the standard calling sequence automatically handles the frame pointer
+and nothing is saved by pretending it doesn't exist.  The
+machine-description macro @code{FRAME_POINTER_REQUIRED} controls
+whether a target machine supports this flag.  @xref{Registers,,Register
+Usage, gccint, GNU Compiler Collection (GCC) Internals}.
+
+Enabled at levels @option{-O}, @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -foptimize-sibling-calls
+@opindex foptimize-sibling-calls
+Optimize sibling and tail recursive calls.
+
+Enabled at levels @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fno-inline
+@opindex fno-inline
+Don't pay attention to the @code{inline} keyword.  Normally this option
+is used to keep the compiler from expanding any functions inline.
+Note that if you are not optimizing, no functions can be expanded inline.
+
+@item -finline-functions
+@opindex finline-functions
+Integrate all simple functions into their callers.  The compiler
+heuristically decides which functions are simple enough to be worth
+integrating in this way.
+
+If all calls to a given function are integrated, and the function is
+declared @code{static}, then the function is normally not output as
+assembler code in its own right.
+
+Enabled at level @option{-O3}.
+
+@item -finline-limit=@var{n}
+@opindex finline-limit
+By default, GCC limits the size of functions that can be inlined.  This flag
+allows the control of this limit for functions that are explicitly marked as
+inline (i.e., marked with the inline keyword or defined within the class
+definition in c++).  @var{n} is the size of functions that can be inlined in
+number of pseudo instructions (not counting parameter handling).  The default
+value of @var{n} is 600.
+Increasing this value can result in more inlined code at
+the cost of compilation time and memory consumption.  Decreasing usually makes
+the compilation faster and less code will be inlined (which presumably
+means slower programs).  This option is particularly useful for programs that
+use inlining heavily such as those based on recursive templates with C++.
+
+Inlining is actually controlled by a number of parameters, which may be
+specified individually by using @option{--param @var{name}=@var{value}}.
+The @option{-finline-limit=@var{n}} option sets some of these parameters
+as follows:
+
+@table @gcctabopt
+ @item max-inline-insns-single
+  is set to @var{n}/2.
+ @item max-inline-insns-auto
+  is set to @var{n}/2.
+ @item min-inline-insns
+  is set to 130 or @var{n}/4, whichever is smaller.
+ @item max-inline-insns-rtl
+  is set to @var{n}.
+@end table
+
+See below for a documentation of the individual
+parameters controlling inlining.
+
+@emph{Note:} pseudo instruction represents, in this particular context, an
+abstract measurement of function's size.  In no way, it represents a count
+of assembly instructions and as such its exact meaning might change from one
+release to an another.
+
+@item -fkeep-inline-functions
+@opindex fkeep-inline-functions
+Even if all calls to a given function are integrated, and the function
+is declared @code{static}, nevertheless output a separate run-time
+callable version of the function.  This switch does not affect
+@code{extern inline} functions.
+
+@item -fkeep-static-consts
+@opindex fkeep-static-consts
+Emit variables declared @code{static const} when optimization isn't turned
+on, even if the variables aren't referenced.
+
+GCC enables this option by default.  If you want to force the compiler to
+check if the variable was referenced, regardless of whether or not
+optimization is turned on, use the @option{-fno-keep-static-consts} option.
+
+@item -fmerge-constants
+Attempt to merge identical constants (string constants and floating point
+constants) across compilation units.
+
+This option is the default for optimized compilation if the assembler and
+linker support it.  Use @option{-fno-merge-constants} to inhibit this
+behavior.
+
+Enabled at levels @option{-O}, @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fmerge-all-constants
+Attempt to merge identical constants and identical variables.
+
+This option implies @option{-fmerge-constants}.  In addition to
+@option{-fmerge-constants} this considers e.g. even constant initialized
+arrays or initialized constant variables with integral or floating point
+types.  Languages like C or C++ require each non-automatic variable to
+have distinct location, so using this option will result in non-conforming
+behavior.
+
+@item -fnew-ra
+@opindex fnew-ra
+Use a graph coloring register allocator.  Currently this option is meant
+only for testing.  Users should not specify this option, since it is not
+yet ready for production use.
+
+@item -fno-branch-count-reg
+@opindex fno-branch-count-reg
+Do not use ``decrement and branch'' instructions on a count register,
+but instead generate a sequence of instructions that decrement a
+register, compare it against zero, then branch based upon the result.
+This option is only meaningful on architectures that support such
+instructions, which include x86, PowerPC, IA-64 and S/390.
+
+The default is @option{-fbranch-count-reg}, enabled when
+@option{-fstrength-reduce} is enabled.
+
+@item -fno-function-cse
+@opindex fno-function-cse
+Do not put function addresses in registers; make each instruction that
+calls a constant function contain the function's address explicitly.
+
+This option results in less efficient code, but some strange hacks
+that alter the assembler output may be confused by the optimizations
+performed when this option is not used.
+
+The default is @option{-ffunction-cse}
+
+@item -fno-zero-initialized-in-bss
+@opindex fno-zero-initialized-in-bss
+If the target supports a BSS section, GCC by default puts variables that
+are initialized to zero into BSS@.  This can save space in the resulting
+code.
+
+This option turns off this behavior because some programs explicitly
+rely on variables going to the data section.  E.g., so that the
+resulting executable can find the beginning of that section and/or make
+assumptions based on that.
+
+The default is @option{-fzero-initialized-in-bss}.
+
+@item -fstrength-reduce
+@opindex fstrength-reduce
+Perform the optimizations of loop strength reduction and
+elimination of iteration variables.
+
+Enabled at levels @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fthread-jumps
+@opindex fthread-jumps
+Perform optimizations where we check to see if a jump branches to a
+location where another comparison subsumed by the first is found.  If
+so, the first branch is redirected to either the destination of the
+second branch or a point immediately following it, depending on whether
+the condition is known to be true or false.
+
+Enabled at levels @option{-O}, @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fcse-follow-jumps
+@opindex fcse-follow-jumps
+In common subexpression elimination, scan through jump instructions
+when the target of the jump is not reached by any other path.  For
+example, when CSE encounters an @code{if} statement with an
+@code{else} clause, CSE will follow the jump when the condition
+tested is false.
+
+Enabled at levels @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fcse-skip-blocks
+@opindex fcse-skip-blocks
+This is similar to @option{-fcse-follow-jumps}, but causes CSE to
+follow jumps which conditionally skip over blocks.  When CSE
+encounters a simple @code{if} statement with no else clause,
+@option{-fcse-skip-blocks} causes CSE to follow the jump around the
+body of the @code{if}.
+
+Enabled at levels @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -frerun-cse-after-loop
+@opindex frerun-cse-after-loop
+Re-run common subexpression elimination after loop optimizations has been
+performed.
+
+Enabled at levels @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -frerun-loop-opt
+@opindex frerun-loop-opt
+Run the loop optimizer twice.
+
+Enabled at levels @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fgcse
+@opindex fgcse
+Perform a global common subexpression elimination pass.
+This pass also performs global constant and copy propagation.
+
+@emph{Note:} When compiling a program using computed gotos, a GCC
+extension, you may get better runtime performance if you disable
+the global common subexpression elimination pass by adding
+@option{-fno-gcse} to the command line.
+
+Enabled at levels @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fgcse-lm
+@opindex fgcse-lm
+When @option{-fgcse-lm} is enabled, global common subexpression elimination will
+attempt to move loads which are only killed by stores into themselves.  This
+allows a loop containing a load/store sequence to be changed to a load outside
+the loop, and a copy/store within the loop.
+
+Enabled by default when gcse is enabled.
+
+@item -fgcse-sm
+@opindex fgcse-sm
+When @option{-fgcse-sm} is enabled, a store motion pass is run after
+global common subexpression elimination.  This pass will attempt to move
+stores out of loops.  When used in conjunction with @option{-fgcse-lm},
+loops containing a load/store sequence can be changed to a load before
+the loop and a store after the loop.
+
+Enabled by default when gcse is enabled.
+
+@item -fgcse-las
+@opindex fgcse-las
+When @option{-fgcse-las} is enabled, the global common subexpression
+elimination pass eliminates redundant loads that come after stores to the
+same memory location (both partial and full redundancies).
+
+Enabled by default when gcse is enabled.
+
+@item -floop-optimize
+@opindex floop-optimize
+Perform loop optimizations: move constant expressions out of loops, simplify
+exit test conditions and optionally do strength-reduction and loop unrolling as
+well.
+
+Enabled at levels @option{-O}, @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fcrossjumping
+@opindex crossjumping
+Perform cross-jumping transformation. This transformation unifies equivalent code and save code size. The
+resulting code may or may not perform better than without cross-jumping.
+
+Enabled at levels @option{-O}, @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fif-conversion
+@opindex if-conversion
+Attempt to transform conditional jumps into branch-less equivalents.  This
+include use of conditional moves, min, max, set flags and abs instructions, and
+some tricks doable by standard arithmetics.  The use of conditional execution
+on chips where it is available is controlled by @code{if-conversion2}.
+
+Enabled at levels @option{-O}, @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fif-conversion2
+@opindex if-conversion2
+Use conditional execution (where available) to transform conditional jumps into
+branch-less equivalents.
+
+Enabled at levels @option{-O}, @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fdelete-null-pointer-checks
+@opindex fdelete-null-pointer-checks
+Use global dataflow analysis to identify and eliminate useless checks
+for null pointers.  The compiler assumes that dereferencing a null
+pointer would have halted the program.  If a pointer is checked after
+it has already been dereferenced, it cannot be null.
+
+In some environments, this assumption is not true, and programs can
+safely dereference null pointers.  Use
+@option{-fno-delete-null-pointer-checks} to disable this optimization
+for programs which depend on that behavior.
+
+Enabled at levels @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fexpensive-optimizations
+@opindex fexpensive-optimizations
+Perform a number of minor optimizations that are relatively expensive.
+
+Enabled at levels @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -foptimize-register-move
+@itemx -fregmove
+@opindex foptimize-register-move
+@opindex fregmove
+Attempt to reassign register numbers in move instructions and as
+operands of other simple instructions in order to maximize the amount of
+register tying.  This is especially helpful on machines with two-operand
+instructions.
+
+Note @option{-fregmove} and @option{-foptimize-register-move} are the same
+optimization.
+
+Enabled at levels @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fdelayed-branch
+@opindex fdelayed-branch
+If supported for the target machine, attempt to reorder instructions
+to exploit instruction slots available after delayed branch
+instructions.
+
+Enabled at levels @option{-O}, @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fschedule-insns
+@opindex fschedule-insns
+If supported for the target machine, attempt to reorder instructions to
+eliminate execution stalls due to required data being unavailable.  This
+helps machines that have slow floating point or memory load instructions
+by allowing other instructions to be issued until the result of the load
+or floating point instruction is required.
+
+Enabled at levels @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fschedule-insns2
+@opindex fschedule-insns2
+Similar to @option{-fschedule-insns}, but requests an additional pass of
+instruction scheduling after register allocation has been done.  This is
+especially useful on machines with a relatively small number of
+registers and where memory load instructions take more than one cycle.
+
+Enabled at levels @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fno-sched-interblock
+@opindex fno-sched-interblock
+Don't schedule instructions across basic blocks.  This is normally
+enabled by default when scheduling before register allocation, i.e.@:
+with @option{-fschedule-insns} or at @option{-O2} or higher.
+
+@item -fno-sched-spec
+@opindex fno-sched-spec
+Don't allow speculative motion of non-load instructions.  This is normally
+enabled by default when scheduling before register allocation, i.e.@:
+with @option{-fschedule-insns} or at @option{-O2} or higher.
+
+@item -fsched-spec-load
+@opindex fsched-spec-load
+Allow speculative motion of some load instructions.  This only makes
+sense when scheduling before register allocation, i.e.@: with
+@option{-fschedule-insns} or at @option{-O2} or higher.
+
+@item -fsched-spec-load-dangerous
+@opindex fsched-spec-load-dangerous
+Allow speculative motion of more load instructions.  This only makes
+sense when scheduling before register allocation, i.e.@: with
+@option{-fschedule-insns} or at @option{-O2} or higher.
+
+@item -fsched-stalled-insns=@var{n}
+@opindex fsched-stalled-insns
+Define how many insns (if any) can be moved prematurely from the queue
+of stalled insns into the ready list, during the second scheduling pass.
+
+@item -fsched-stalled-insns-dep=@var{n}
+@opindex fsched-stalled-insns-dep
+Define how many insn groups (cycles) will be examined for a dependency
+on a stalled insn that is candidate for premature removal from the queue
+of stalled insns.  Has an effect only during the second scheduling pass,
+and only if @option{-fsched-stalled-insns} is used and its value is not zero.
+
+@item -fsched2-use-superblocks
+@opindex fsched2-use-superblocks
+When scheduling after register allocation, do use superblock scheduling
+algorithm.  Superblock scheduling allows motion across basic block boundaries
+resulting on faster schedules.  This option is experimental, as not all machine
+descriptions used by GCC model the CPU closely enough to avoid unreliable
+results from the algorithm.
+
+This only makes sense when scheduling after register allocation, i.e.@: with
+@option{-fschedule-insns2} or at @option{-O2} or higher.
+
+@item -fsched2-use-traces
+@opindex fsched2-use-traces
+Use @option{-fsched2-use-superblocks} algorithm when scheduling after register
+allocation and additionally perform code duplication in order to increase the
+size of superblocks using tracer pass.  See @option{-ftracer} for details on
+trace formation.
+
+This mode should produce faster but significantly longer programs.  Also
+without @code{-fbranch-probabilities} the traces constructed may not match the
+reality and hurt the performance.  This only makes
+sense when scheduling after register allocation, i.e.@: with
+@option{-fschedule-insns2} or at @option{-O2} or higher.
+
+@item -fcaller-saves
+@opindex fcaller-saves
+Enable values to be allocated in registers that will be clobbered by
+function calls, by emitting extra instructions to save and restore the
+registers around such calls.  Such allocation is done only when it
+seems to result in better code than would otherwise be produced.
+
+This option is always enabled by default on certain machines, usually
+those which have no call-preserved registers to use instead.
+
+Enabled at levels @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fmove-all-movables
+@opindex fmove-all-movables
+Forces all invariant computations in loops to be moved
+outside the loop.
+
+@item -freduce-all-givs
+@opindex freduce-all-givs
+Forces all general-induction variables in loops to be
+strength-reduced.
+
+@emph{Note:} When compiling programs written in Fortran,
+@option{-fmove-all-movables} and @option{-freduce-all-givs} are enabled
+by default when you use the optimizer.
+
+These options may generate better or worse code; results are highly
+dependent on the structure of loops within the source code.
+
+These two options are intended to be removed someday, once
+they have helped determine the efficacy of various
+approaches to improving loop optimizations.
+
+Please contact @w{@email{gcc@@gcc.gnu.org}}, and describe how use of
+these options affects the performance of your production code.
+Examples of code that runs @emph{slower} when these options are
+@emph{enabled} are very valuable.
+
+@item -fno-peephole
+@itemx -fno-peephole2
+@opindex fno-peephole
+@opindex fno-peephole2
+Disable any machine-specific peephole optimizations.  The difference
+between @option{-fno-peephole} and @option{-fno-peephole2} is in how they
+are implemented in the compiler; some targets use one, some use the
+other, a few use both.
+
+@option{-fpeephole} is enabled by default.
+@option{-fpeephole2} enabled at levels @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fno-guess-branch-probability
+@opindex fno-guess-branch-probability
+Do not guess branch probabilities using a randomized model.
+
+Sometimes GCC will opt to use a randomized model to guess branch
+probabilities, when none are available from either profiling feedback
+(@option{-fprofile-arcs}) or @samp{__builtin_expect}.  This means that
+different runs of the compiler on the same program may produce different
+object code.
+
+In a hard real-time system, people don't want different runs of the
+compiler to produce code that has different behavior; minimizing
+non-determinism is of paramount import.  This switch allows users to
+reduce non-determinism, possibly at the expense of inferior
+optimization.
+
+The default is @option{-fguess-branch-probability} at levels
+@option{-O}, @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -freorder-blocks
+@opindex freorder-blocks
+Reorder basic blocks in the compiled function in order to reduce number of
+taken branches and improve code locality.
+
+Enabled at levels @option{-O2}, @option{-O3}.
+
+@item -freorder-functions
+@opindex freorder-functions
+Reorder basic blocks in the compiled function in order to reduce number of
+taken branches and improve code locality. This is implemented by using special
+subsections @code{.text.hot} for most frequently executed functions and
+@code{.text.unlikely} for unlikely executed functions.  Reordering is done by
+the linker so object file format must support named sections and linker must
+place them in a reasonable way.
+
+Also profile feedback must be available in to make this option effective.  See
+@option{-fprofile-arcs} for details.
+
+Enabled at levels @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fstrict-aliasing
+@opindex fstrict-aliasing
+Allows the compiler to assume the strictest aliasing rules applicable to
+the language being compiled.  For C (and C++), this activates
+optimizations based on the type of expressions.  In particular, an
+object of one type is assumed never to reside at the same address as an
+object of a different type, unless the types are almost the same.  For
+example, an @code{unsigned int} can alias an @code{int}, but not a
+@code{void*} or a @code{double}.  A character type may alias any other
+type.
+
+Pay special attention to code like this:
+@smallexample
+union a_union @{
+  int i;
+  double d;
+@};
+
+int f() @{
+  a_union t;
+  t.d = 3.0;
+  return t.i;
+@}
+@end smallexample
+The practice of reading from a different union member than the one most
+recently written to (called ``type-punning'') is common.  Even with
+@option{-fstrict-aliasing}, type-punning is allowed, provided the memory
+is accessed through the union type.  So, the code above will work as
+expected.  However, this code might not:
+@smallexample
+int f() @{
+  a_union t;
+  int* ip;
+  t.d = 3.0;
+  ip = &t.i;
+  return *ip;
+@}
+@end smallexample
+
+Every language that wishes to perform language-specific alias analysis
+should define a function that computes, given an @code{tree}
+node, an alias set for the node.  Nodes in different alias sets are not
+allowed to alias.  For an example, see the C front-end function
+@code{c_get_alias_set}.
+
+Enabled at levels @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -falign-functions
+@itemx -falign-functions=@var{n}
+@opindex falign-functions
+Align the start of functions to the next power-of-two greater than
+@var{n}, skipping up to @var{n} bytes.  For instance,
+@option{-falign-functions=32} aligns functions to the next 32-byte
+boundary, but @option{-falign-functions=24} would align to the next
+32-byte boundary only if this can be done by skipping 23 bytes or less.
+
+@option{-fno-align-functions} and @option{-falign-functions=1} are
+equivalent and mean that functions will not be aligned.
+
+Some assemblers only support this flag when @var{n} is a power of two;
+in that case, it is rounded up.
+
+If @var{n} is not specified or is zero, use a machine-dependent default.
+
+Enabled at levels @option{-O2}, @option{-O3}.
+
+@item -falign-labels
+@itemx -falign-labels=@var{n}
+@opindex falign-labels
+Align all branch targets to a power-of-two boundary, skipping up to
+@var{n} bytes like @option{-falign-functions}.  This option can easily
+make code slower, because it must insert dummy operations for when the
+branch target is reached in the usual flow of the code.
+
+@option{-fno-align-labels} and @option{-falign-labels=1} are
+equivalent and mean that labels will not be aligned.
+
+If @option{-falign-loops} or @option{-falign-jumps} are applicable and
+are greater than this value, then their values are used instead.
+
+If @var{n} is not specified or is zero, use a machine-dependent default
+which is very likely to be @samp{1}, meaning no alignment.
+
+Enabled at levels @option{-O2}, @option{-O3}.
+
+@item -falign-loops
+@itemx -falign-loops=@var{n}
+@opindex falign-loops
+Align loops to a power-of-two boundary, skipping up to @var{n} bytes
+like @option{-falign-functions}.  The hope is that the loop will be
+executed many times, which will make up for any execution of the dummy
+operations.
+
+@option{-fno-align-loops} and @option{-falign-loops=1} are
+equivalent and mean that loops will not be aligned.
+
+If @var{n} is not specified or is zero, use a machine-dependent default.
+
+Enabled at levels @option{-O2}, @option{-O3}.
+
+@item -falign-jumps
+@itemx -falign-jumps=@var{n}
+@opindex falign-jumps
+Align branch targets to a power-of-two boundary, for branch targets
+where the targets can only be reached by jumping, skipping up to @var{n}
+bytes like @option{-falign-functions}.  In this case, no dummy operations
+need be executed.
+
+@option{-fno-align-jumps} and @option{-falign-jumps=1} are
+equivalent and mean that loops will not be aligned.
+
+If @var{n} is not specified or is zero, use a machine-dependent default.
+
+Enabled at levels @option{-O2}, @option{-O3}.
+
+@item -frename-registers
+@opindex frename-registers
+Attempt to avoid false dependencies in scheduled code by making use
+of registers left over after register allocation.  This optimization
+will most benefit processors with lots of registers.  It can, however,
+make debugging impossible, since variables will no longer stay in
+a ``home register''.
+
+@item -fweb
+@opindex fweb
+Constructs webs as commonly used for register allocation purposes and assign
+each web individual pseudo register.  This allows the register allocation pass
+to operate on pseudos directly, but also strengthens several other optimization
+passes, such as CSE, loop optimizer and trivial dead code remover.  It can,
+however, make debugging impossible, since variables will no longer stay in a
+``home register''.
+
+Enabled at levels @option{-O3}.
+
+@item -fno-cprop-registers
+@opindex fno-cprop-registers
+After register allocation and post-register allocation instruction splitting,
+we perform a copy-propagation pass to try to reduce scheduling dependencies
+and occasionally eliminate the copy.
+
+Disabled at levels @option{-O}, @option{-O2}, @option{-O3}, @option{-Os}.
+
+@item -fprofile-generate
+@opindex fprofile-generate
+
+Enable options usually used for instrumenting application to produce
+profile useful for later recompilation with profile feedback based
+optimization.  You must use @code{-fprofile-generate} both when
+compiling and when linking your program.
+
+The following options are enabled: @code{-fprofile-arcs}, @code{-fprofile-values}, @code{-fvpt}.
+
+@item -fprofile-use
+@opindex fprofile-use
+Enable profile feedback directed optimizations, and optimizations
+generally profitable only with profile feedback available.
+
+The following options are enabled: @code{-fbranch-probabilities},
+@code{-fvpt}, @code{-funroll-loops}, @code{-fpeel-loops}, @code{-ftracer}.
+
+@end table
+
+The following options control compiler behavior regarding floating
+point arithmetic.  These options trade off between speed and
+correctness.  All must be specifically enabled.
+
+@table @gcctabopt
+@item -ffloat-store
+@opindex ffloat-store
+Do not store floating point variables in registers, and inhibit other
+options that might change whether a floating point value is taken from a
+register or memory.
+
+@cindex floating point precision
+This option prevents undesirable excess precision on machines such as
+the 68000 where the floating registers (of the 68881) keep more
+precision than a @code{double} is supposed to have.  Similarly for the
+x86 architecture.  For most programs, the excess precision does only
+good, but a few programs rely on the precise definition of IEEE floating
+point.  Use @option{-ffloat-store} for such programs, after modifying
+them to store all pertinent intermediate computations into variables.
+
+@item -ffast-math
+@opindex ffast-math
+Sets @option{-fno-math-errno}, @option{-funsafe-math-optimizations}, @*
+@option{-fno-trapping-math}, @option{-ffinite-math-only},
+@option{-fno-rounding-math} and @option{-fno-signaling-nans}.
+
+This option causes the preprocessor macro @code{__FAST_MATH__} to be defined.
+
+This option should never be turned on by any @option{-O} option since
+it can result in incorrect output for programs which depend on
+an exact implementation of IEEE or ISO rules/specifications for
+math functions.
+
+@item -fno-math-errno
+@opindex fno-math-errno
+Do not set ERRNO after calling math functions that are executed
+with a single instruction, e.g., sqrt.  A program that relies on
+IEEE exceptions for math error handling may want to use this flag
+for speed while maintaining IEEE arithmetic compatibility.
+
+This option should never be turned on by any @option{-O} option since
+it can result in incorrect output for programs which depend on
+an exact implementation of IEEE or ISO rules/specifications for
+math functions.
+
+The default is @option{-fmath-errno}.
+
+@item -funsafe-math-optimizations
+@opindex funsafe-math-optimizations
+Allow optimizations for floating-point arithmetic that (a) assume
+that arguments and results are valid and (b) may violate IEEE or
+ANSI standards.  When used at link-time, it may include libraries
+or startup files that change the default FPU control word or other
+similar optimizations.
+
+This option should never be turned on by any @option{-O} option since
+it can result in incorrect output for programs which depend on
+an exact implementation of IEEE or ISO rules/specifications for
+math functions.
+
+The default is @option{-fno-unsafe-math-optimizations}.
+
+@item -ffinite-math-only
+@opindex ffinite-math-only
+Allow optimizations for floating-point arithmetic that assume
+that arguments and results are not NaNs or +-Infs.
+
+This option should never be turned on by any @option{-O} option since
+it can result in incorrect output for programs which depend on
+an exact implementation of IEEE or ISO rules/specifications.
+
+The default is @option{-fno-finite-math-only}.
+
+@item -fno-trapping-math
+@opindex fno-trapping-math
+Compile code assuming that floating-point operations cannot generate
+user-visible traps.  These traps include division by zero, overflow,
+underflow, inexact result and invalid operation.  This option implies
+@option{-fno-signaling-nans}.  Setting this option may allow faster
+code if one relies on ``non-stop'' IEEE arithmetic, for example.
+
+This option should never be turned on by any @option{-O} option since
+it can result in incorrect output for programs which depend on
+an exact implementation of IEEE or ISO rules/specifications for
+math functions.
+
+The default is @option{-ftrapping-math}.
+
+@item -frounding-math
+@opindex frounding-math
+Disable transformations and optimizations that assume default floating
+point rounding behavior.  This is round-to-zero for all floating point
+to integer conversions, and round-to-nearest for all other arithmetic
+truncations.  This option should be specified for programs that change
+the FP rounding mode dynamically, or that may be executed with a
+non-default rounding mode.  This option disables constant folding of
+floating point expressions at compile-time (which may be affected by
+rounding mode) and arithmetic transformations that are unsafe in the
+presence of sign-dependent rounding modes.
+
+The default is @option{-fno-rounding-math}.
+
+This option is experimental and does not currently guarantee to
+disable all GCC optimizations that are affected by rounding mode.
+Future versions of GCC may provide finer control of this setting
+using C99's @code{FENV_ACCESS} pragma.  This command line option
+will be used to specify the default state for @code{FENV_ACCESS}.
+
+@item -fsignaling-nans
+@opindex fsignaling-nans
+Compile code assuming that IEEE signaling NaNs may generate user-visible
+traps during floating-point operations.  Setting this option disables
+optimizations that may change the number of exceptions visible with
+signaling NaNs.  This option implies @option{-ftrapping-math}.
+
+This option causes the preprocessor macro @code{__SUPPORT_SNAN__} to
+be defined.
+
+The default is @option{-fno-signaling-nans}.
+
+This option is experimental and does not currently guarantee to
+disable all GCC optimizations that affect signaling NaN behavior.
+
+@item -fsingle-precision-constant
+@opindex fsingle-precision-constant
+Treat floating point constant as single precision constant instead of
+implicitly converting it to double precision constant.
+
+
+@end table
+
+The following options control optimizations that may improve
+performance, but are not enabled by any @option{-O} options.  This
+section includes experimental options that may produce broken code.
+
+@table @gcctabopt
+@item -fbranch-probabilities
+@opindex fbranch-probabilities
+After running a program compiled with @option{-fprofile-arcs}
+(@pxref{Debugging Options,, Options for Debugging Your Program or
+@command{gcc}}), you can compile it a second time using
+@option{-fbranch-probabilities}, to improve optimizations based on
+the number of times each branch was taken.  When the program
+compiled with @option{-fprofile-arcs} exits it saves arc execution
+counts to a file called @file{@var{sourcename}.gcda} for each source
+file  The information in this data file is very dependent on the
+structure of the generated code, so you must use the same source code
+and the same optimization options for both compilations.
+
+With @option{-fbranch-probabilities}, GCC puts a
+@samp{REG_BR_PROB} note on each @samp{JUMP_INSN} and @samp{CALL_INSN}.
+These can be used to improve optimization.  Currently, they are only
+used in one place: in @file{reorg.c}, instead of guessing which path a
+branch is mostly to take, the @samp{REG_BR_PROB} values are used to
+exactly determine which path is taken more often.
+
+@item -fprofile-values
+@opindex fprofile-values
+If combined with @option{-fprofile-arcs}, it adds code so that some
+data about values of expressions in the program is gathered.
+
+With @option{-fbranch-probabilities}, it reads back the data gathered
+from profiling values of expressions and adds @samp{REG_VALUE_PROFILE}
+notes to instructions for their later usage in optimizations.
+
+@item -fvpt
+@opindex fvpt
+If combined with @option{-fprofile-arcs}, it instructs the compiler to add
+a code to gather information about values of expressions.
+
+With @option{-fbranch-probabilities}, it reads back the data gathered
+and actually performs the optimizations based on them.
+Currently the optimizations include specialization of division operation
+using the knowledge about the value of the denominator.
+
+@item -fnew-ra
+@opindex fnew-ra
+Use a graph coloring register allocator.  Currently this option is meant
+for testing, so we are interested to hear about miscompilations with
+@option{-fnew-ra}.
+
+@item -ftracer
+@opindex ftracer
+Perform tail duplication to enlarge superblock size. This transformation
+simplifies the control flow of the function allowing other optimizations to do
+better job.
+
+@item -funit-at-a-time
+@opindex funit-at-a-time
+Parse the whole compilation unit before starting to produce code.
+This allows some extra optimizations to take place but consumes more
+memory.
+
+@item -funroll-loops
+@opindex funroll-loops
+Unroll loops whose number of iterations can be determined at compile time or
+upon entry to the loop.  @option{-funroll-loops} implies
+@option{-frerun-cse-after-loop}.  It also turns on complete loop peeling
+(i.e. complete removal of loops with small constant number of iterations).
+This option makes code larger, and may or may not make it run faster.
+
+@item -funroll-all-loops
+@opindex funroll-all-loops
+Unroll all loops, even if their number of iterations is uncertain when
+the loop is entered.  This usually makes programs run more slowly.
+@option{-funroll-all-loops} implies the same options as
+@option{-funroll-loops}.
+
+@item -fpeel-loops
+@opindex fpeel-loops
+Peels the loops for that there is enough information that they do not
+roll much (from profile feedback).  It also turns on complete loop peeling
+(i.e. complete removal of loops with small constant number of iterations).
+
+@item -funswitch-loops
+@opindex funswitch-loops
+Move branches with loop invariant conditions out of the loop, with duplicates
+of the loop on both branches (modified according to result of the condition).
+
+@item -fold-unroll-loops
+@opindex fold-unroll-loops
+Unroll loops whose number of iterations can be determined at compile
+time or upon entry to the loop, using the old loop unroller whose loop
+recognition is based on notes from frontend.  @option{-fold-unroll-loops} implies
+both @option{-fstrength-reduce} and @option{-frerun-cse-after-loop}.  This
+option makes code larger, and may or may not make it run faster.
+
+@item -fold-unroll-all-loops
+@opindex fold-unroll-all-loops
+Unroll all loops, even if their number of iterations is uncertain when
+the loop is entered. This is done using the old loop unroller whose loop
+recognition is based on notes from frontend.  This usually makes programs run more slowly.
+@option{-fold-unroll-all-loops} implies the same options as
+@option{-fold-unroll-loops}.
+
+@item -funswitch-loops
+@opindex funswitch-loops
+Move branches with loop invariant conditions out of the loop, with duplicates
+of the loop on both branches (modified according to result of the condition).
+
+@item -funswitch-loops
+@opindex funswitch-loops
+Move branches with loop invariant conditions out of the loop, with duplicates
+of the loop on both branches (modified according to result of the condition).
+
+@item -fprefetch-loop-arrays
+@opindex fprefetch-loop-arrays
+If supported by the target machine, generate instructions to prefetch
+memory to improve the performance of loops that access large arrays.
+
+Disabled at level @option{-Os}.
+
+@item -ffunction-sections
+@itemx -fdata-sections
+@opindex ffunction-sections
+@opindex fdata-sections
+Place each function or data item into its own section in the output
+file if the target supports arbitrary sections.  The name of the
+function or the name of the data item determines the section's name
+in the output file.
+
+Use these options on systems where the linker can perform optimizations
+to improve locality of reference in the instruction space.  Most systems
+using the ELF object format and SPARC processors running Solaris 2 have
+linkers with such optimizations.  AIX may have these optimizations in
+the future.
+
+Only use these options when there are significant benefits from doing
+so.  When you specify these options, the assembler and linker will
+create larger object and executable files and will also be slower.
+You will not be able to use @code{gprof} on all systems if you
+specify this option and you may have problems with debugging if
+you specify both this option and @option{-g}.
+
+@item -fbranch-target-load-optimize
+@opindex fbranch-target-load-optimize
+Perform branch target register load optimization before prologue / epilogue
+threading.
+The use of target registers can typically be exposed only during reload,
+thus hoisting loads out of loops and doing inter-block scheduling needs
+a separate optimization pass.
+
+@item -fbranch-target-load-optimize2
+@opindex fbranch-target-load-optimize2
+Perform branch target register load optimization after prologue / epilogue
+threading.
+
+@item --param @var{name}=@var{value}
+@opindex param
+In some places, GCC uses various constants to control the amount of
+optimization that is done.  For example, GCC will not inline functions
+that contain more that a certain number of instructions.  You can
+control some of these constants on the command-line using the
+@option{--param} option.
+
+The names of specific parameters, and the meaning of the values, are
+tied to the internals of the compiler, and are subject to change
+without notice in future releases.
+
+In each case, the @var{value} is an integer.  The allowable choices for
+@var{name} are given in the following table:
+
+@table @gcctabopt
+@item max-crossjump-edges
+The maximum number of incoming edges to consider for crossjumping.
+The algorithm used by @option{-fcrossjumping} is @math{O(N^2)} in
+the number of edges incoming to each block.  Increasing values mean
+more aggressive optimization, making the compile time increase with
+probably small improvement in executable size.
+
+@item max-delay-slot-insn-search
+The maximum number of instructions to consider when looking for an
+instruction to fill a delay slot.  If more than this arbitrary number of
+instructions is searched, the time savings from filling the delay slot
+will be minimal so stop searching.  Increasing values mean more
+aggressive optimization, making the compile time increase with probably
+small improvement in executable run time.
+
+@item max-delay-slot-live-search
+When trying to fill delay slots, the maximum number of instructions to
+consider when searching for a block with valid live register
+information.  Increasing this arbitrarily chosen value means more
+aggressive optimization, increasing the compile time.  This parameter
+should be removed when the delay slot code is rewritten to maintain the
+control-flow graph.
+
+@item max-gcse-memory
+The approximate maximum amount of memory that will be allocated in
+order to perform the global common subexpression elimination
+optimization.  If more memory than specified is required, the
+optimization will not be done.
+
+@item max-gcse-passes
+The maximum number of passes of GCSE to run.
+
+@item max-pending-list-length
+The maximum number of pending dependencies scheduling will allow
+before flushing the current state and starting over.  Large functions
+with few branches or calls can create excessively large lists which
+needlessly consume memory and resources.
+
+@item max-inline-insns-single
+Several parameters control the tree inliner used in gcc.
+This number sets the maximum number of instructions (counted in GCC's
+internal representation) in a single function that the tree inliner
+will consider for inlining.  This only affects functions declared
+inline and methods implemented in a class declaration (C++).
+The default value is 500.
+
+@item max-inline-insns-auto
+When you use @option{-finline-functions} (included in @option{-O3}),
+a lot of functions that would otherwise not be considered for inlining
+by the compiler will be investigated.  To those functions, a different
+(more restrictive) limit compared to functions declared inline can
+be applied.
+The default value is 100.
+
+@item large-function-insns
+The limit specifying really large functions.  For functions greater than this
+limit inlining is constrained by @option{--param large-function-growth}.
+This parameter is useful primarily to avoid extreme compilation time caused by non-linear
+algorithms used by the backend.
+This parameter is ignored when @option{-funit-at-a-time} is not used.
+The default value is 3000.
+
+@item large-function-growth
+Specifies maximal growth of large function caused by inlining in percents.
+This parameter is ignored when @option{-funit-at-a-time} is not used.
+The default value is 200.
+
+@item inline-unit-growth
+Specifies maximal overall growth of the compilation unit caused by inlining.
+This parameter is ignored when @option{-funit-at-a-time} is not used.
+The default value is 150.
+
+@item max-inline-insns-rtl
+For languages that use the RTL inliner (this happens at a later stage
+than tree inlining), you can set the maximum allowable size (counted
+in RTL instructions) for the RTL inliner with this parameter.
+The default value is 600.
+
+@item max-unrolled-insns
+The maximum number of instructions that a loop should have if that loop
+is unrolled, and if the loop is unrolled, it determines how many times
+the loop code is unrolled.
+
+@item max-average-unrolled-insns
+The maximum number of instructions biased by probabilities of their execution
+that a loop should have if that loop is unrolled, and if the loop is unrolled,
+it determines how many times the loop code is unrolled.
+
+@item max-unroll-times
+The maximum number of unrollings of a single loop.
+
+@item max-peeled-insns
+The maximum number of instructions that a loop should have if that loop
+is peeled, and if the loop is peeled, it determines how many times
+the loop code is peeled.
+
+@item max-peel-times
+The maximum number of peelings of a single loop.
+
+@item max-completely-peeled-insns
+The maximum number of insns of a completely peeled loop.
+
+@item max-completely-peel-times
+The maximum number of iterations of a loop to be suitable for complete peeling.
+
+@item max-unswitch-insns
+The maximum number of insns of an unswitched loop.
+
+@item max-unswitch-level
+The maximum number of branches unswitched in a single loop.
+
+@item hot-bb-count-fraction
+Select fraction of the maximal count of repetitions of basic block in program
+given basic block needs to have to be considered hot.
+
+@item hot-bb-frequency-fraction
+Select fraction of the maximal frequency of executions of basic block in
+function given basic block needs to have to be considered hot
+
+@item tracer-dynamic-coverage
+@itemx tracer-dynamic-coverage-feedback
+
+This value is used to limit superblock formation once the given percentage of
+executed instructions is covered.  This limits unnecessary code size
+expansion.
+
+The @option{tracer-dynamic-coverage-feedback} is used only when profile
+feedback is available.  The real profiles (as opposed to statically estimated
+ones) are much less balanced allowing the threshold to be larger value.
+
+@item tracer-max-code-growth
+Stop tail duplication once code growth has reached given percentage.  This is
+rather hokey argument, as most of the duplicates will be eliminated later in
+cross jumping, so it may be set to much higher values than is the desired code
+growth.
+
+@item tracer-min-branch-ratio
+
+Stop reverse growth when the reverse probability of best edge is less than this
+threshold (in percent).
+
+@item tracer-min-branch-ratio
+@itemx tracer-min-branch-ratio-feedback
+
+Stop forward growth if the best edge do have probability lower than this
+threshold.
+
+Similarly to @option{tracer-dynamic-coverage} two values are present, one for
+compilation for profile feedback and one for compilation without.  The value
+for compilation with profile feedback needs to be more conservative (higher) in
+order to make tracer effective.
+
+@item max-cse-path-length
+
+Maximum number of basic blocks on path that cse considers.
+
+@item max-last-value-rtl
+
+The maximum size measured as number of RTLs that can be recorded in an
+expression in combiner for a pseudo register as last known value of that
+register.  The default is 10000.
+
+@item ggc-min-expand
+
+GCC uses a garbage collector to manage its own memory allocation.  This
+parameter specifies the minimum percentage by which the garbage
+collector's heap should be allowed to expand between collections.
+Tuning this may improve compilation speed; it has no effect on code
+generation.
+
+The default is 30% + 70% * (RAM/1GB) with an upper bound of 100% when
+RAM >= 1GB.  If @code{getrlimit} is available, the notion of "RAM" is
+the smallest of actual RAM, RLIMIT_RSS, RLIMIT_DATA and RLIMIT_AS.  If
+GCC is not able to calculate RAM on a particular platform, the lower
+bound of 30% is used.  Setting this parameter and
+@option{ggc-min-heapsize} to zero causes a full collection to occur at
+every opportunity.  This is extremely slow, but can be useful for
+debugging.
+
+@item ggc-min-heapsize
+
+Minimum size of the garbage collector's heap before it begins bothering
+to collect garbage.  The first collection occurs after the heap expands
+by @option{ggc-min-expand}% beyond @option{ggc-min-heapsize}.  Again,
+tuning this may improve compilation speed, and has no effect on code
+generation.
+
+The default is RAM/8, with a lower bound of 4096 (four megabytes) and an
+upper bound of 131072 (128 megabytes).  If @code{getrlimit} is
+available, the notion of "RAM" is the smallest of actual RAM,
+RLIMIT_RSS, RLIMIT_DATA and RLIMIT_AS.  If GCC is not able to calculate
+RAM on a particular platform, the lower bound is used.  Setting this
+parameter very large effectively disables garbage collection.  Setting
+this parameter and @option{ggc-min-expand} to zero causes a full
+collection to occur at every opportunity.
+
+@item max-reload-search-insns
+The maximum number of instruction reload should look backward for equivalent
+register.  Increasing values mean more aggressive optimization, making the
+compile time increase with probably slightly better performance.  The default
+value is 100.
+
+@item max-cselib-memory-location
+The maximum number of memory locations cselib should take into acount.
+Increasing values mean more aggressive optimization, making the compile time
+increase with probably slightly better performance.  The default value is 500.
+
+@item reorder-blocks-duplicate
+@itemx reorder-blocks-duplicate-feedback
+
+Used by basic block reordering pass to decide whether to use unconditional
+branch or duplicate the code on its destination.  Code is duplicated when its
+estimated size is smaller than this value multiplied by the estimated size of
+unconditional jump in the hot spots of the program.
+
+The @option{reorder-block-duplicate-feedback} is used only when profile
+feedback is available and may be set to higher values than
+@option{reorder-block-duplicate} since information about the hot spots is more
+accurate.
+@end table
+@end table
+
+@node Preprocessor Options
+@section Options Controlling the Preprocessor
+@cindex preprocessor options
+@cindex options, preprocessor
+
+These options control the C preprocessor, which is run on each C source
+file before actual compilation.
+
+If you use the @option{-E} option, nothing is done except preprocessing.
+Some of these options make sense only together with @option{-E} because
+they cause the preprocessor output to be unsuitable for actual
+compilation.
+
+@table @gcctabopt
+@opindex Wp
+You can use @option{-Wp,@var{option}} to bypass the compiler driver
+and pass @var{option} directly through to the preprocessor.  If
+@var{option} contains commas, it is split into multiple options at the
+commas.  However, many options are modified, translated or interpreted
+by the compiler driver before being passed to the preprocessor, and
+@option{-Wp} forcibly bypasses this phase.  The preprocessor's direct
+interface is undocumented and subject to change, so whenever possible
+you should avoid using @option{-Wp} and let the driver handle the
+options instead.
+
+@item -Xpreprocessor @var{option}
+@opindex preprocessor
+Pass @var{option} as an option to the preprocessor.  You can use this to
+supply system-specific preprocessor options which GCC does not know how to
+recognize.
+
+If you want to pass an option that takes an argument, you must use
+@option{-Xpreprocessor} twice, once for the option and once for the argument.
+@end table
+
+@include cppopts.texi
+
+@node Assembler Options
+@section Passing Options to the Assembler
+
+@c prevent bad page break with this line
+You can pass options to the assembler.
+
+@table @gcctabopt
+@item -Wa,@var{option}
+@opindex Wa
+Pass @var{option} as an option to the assembler.  If @var{option}
+contains commas, it is split into multiple options at the commas.
+
+@item -Xassembler @var{option}
+@opindex Xassembler
+Pass @var{option} as an option to the assembler.  You can use this to
+supply system-specific assembler options which GCC does not know how to
+recognize.
+
+If you want to pass an option that takes an argument, you must use
+@option{-Xassembler} twice, once for the option and once for the argument.
+
+@end table
+
+@node Link Options
+@section Options for Linking
+@cindex link options
+@cindex options, linking
+
+These options come into play when the compiler links object files into
+an executable output file.  They are meaningless if the compiler is
+not doing a link step.
+
+@table @gcctabopt
+@cindex file names
+@item @var{object-file-name}
+A file name that does not end in a special recognized suffix is
+considered to name an object file or library.  (Object files are
+distinguished from libraries by the linker according to the file
+contents.)  If linking is done, these object files are used as input
+to the linker.
+
+@item -c
+@itemx -S
+@itemx -E
+@opindex c
+@opindex S
+@opindex E
+If any of these options is used, then the linker is not run, and
+object file names should not be used as arguments.  @xref{Overall
+Options}.
+
+@cindex Libraries
+@item -l@var{library}
+@itemx -l @var{library}
+@opindex l
+Search the library named @var{library} when linking.  (The second
+alternative with the library as a separate argument is only for
+POSIX compliance and is not recommended.)
+
+It makes a difference where in the command you write this option; the
+linker searches and processes libraries and object files in the order they
+are specified.  Thus, @samp{foo.o -lz bar.o} searches library @samp{z}
+after file @file{foo.o} but before @file{bar.o}.  If @file{bar.o} refers
+to functions in @samp{z}, those functions may not be loaded.
+
+The linker searches a standard list of directories for the library,
+which is actually a file named @file{lib@var{library}.a}.  The linker
+then uses this file as if it had been specified precisely by name.
+
+The directories searched include several standard system directories
+plus any that you specify with @option{-L}.
+
+Normally the files found this way are library files---archive files
+whose members are object files.  The linker handles an archive file by
+scanning through it for members which define symbols that have so far
+been referenced but not defined.  But if the file that is found is an
+ordinary object file, it is linked in the usual fashion.  The only
+difference between using an @option{-l} option and specifying a file name
+is that @option{-l} surrounds @var{library} with @samp{lib} and @samp{.a}
+and searches several directories.
+
+@item -lobjc
+@opindex lobjc
+You need this special case of the @option{-l} option in order to
+link an Objective-C program.
+
+@item -nostartfiles
+@opindex nostartfiles
+Do not use the standard system startup files when linking.
+The standard system libraries are used normally, unless @option{-nostdlib}
+or @option{-nodefaultlibs} is used.
+
+@item -nodefaultlibs
+@opindex nodefaultlibs
+Do not use the standard system libraries when linking.
+Only the libraries you specify will be passed to the linker.
+The standard startup files are used normally, unless @option{-nostartfiles}
+is used.  The compiler may generate calls to memcmp, memset, and memcpy
+for System V (and ISO C) environments or to bcopy and bzero for
+BSD environments.  These entries are usually resolved by entries in
+libc.  These entry points should be supplied through some other
+mechanism when this option is specified.
+
+@item -nostdlib
+@opindex nostdlib
+Do not use the standard system startup files or libraries when linking.
+No startup files and only the libraries you specify will be passed to
+the linker.  The compiler may generate calls to memcmp, memset, and memcpy
+for System V (and ISO C) environments or to bcopy and bzero for
+BSD environments.  These entries are usually resolved by entries in
+libc.  These entry points should be supplied through some other
+mechanism when this option is specified.
+
+@cindex @option{-lgcc}, use with @option{-nostdlib}
+@cindex @option{-nostdlib} and unresolved references
+@cindex unresolved references and @option{-nostdlib}
+@cindex @option{-lgcc}, use with @option{-nodefaultlibs}
+@cindex @option{-nodefaultlibs} and unresolved references
+@cindex unresolved references and @option{-nodefaultlibs}
+One of the standard libraries bypassed by @option{-nostdlib} and
+@option{-nodefaultlibs} is @file{libgcc.a}, a library of internal subroutines
+that GCC uses to overcome shortcomings of particular machines, or special
+needs for some languages.
+(@xref{Interface,,Interfacing to GCC Output,gccint,GNU Compiler
+Collection (GCC) Internals},
+for more discussion of @file{libgcc.a}.)
+In most cases, you need @file{libgcc.a} even when you want to avoid
+other standard libraries.  In other words, when you specify @option{-nostdlib}
+or @option{-nodefaultlibs} you should usually specify @option{-lgcc} as well.
+This ensures that you have no unresolved references to internal GCC
+library subroutines.  (For example, @samp{__main}, used to ensure C++
+constructors will be called; @pxref{Collect2,,@code{collect2}, gccint,
+GNU Compiler Collection (GCC) Internals}.)
+
+@item -pie
+@opindex pie
+Produce a position independent executable on targets which support it.
+For predictable results, you must also specify the same set of options
+that were used to generate code (@option{-fpie}, @option{-fPIE},
+or model suboptions) when you specify this option.
+
+@item -s
+@opindex s
+Remove all symbol table and relocation information from the executable.
+
+@item -static
+@opindex static
+On systems that support dynamic linking, this prevents linking with the shared
+libraries.  On other systems, this option has no effect.
+
+@item -shared
+@opindex shared
+Produce a shared object which can then be linked with other objects to
+form an executable.  Not all systems support this option.  For predictable
+results, you must also specify the same set of options that were used to
+generate code (@option{-fpic}, @option{-fPIC}, or model suboptions)
+when you specify this option.@footnote{On some systems, @samp{gcc -shared}
+needs to build supplementary stub code for constructors to work.  On
+multi-libbed systems, @samp{gcc -shared} must select the correct support
+libraries to link against.  Failing to supply the correct flags may lead
+to subtle defects.  Supplying them in cases where they are not necessary
+is innocuous.}
+
+@item -shared-libgcc
+@itemx -static-libgcc
+@opindex shared-libgcc
+@opindex static-libgcc
+On systems that provide @file{libgcc} as a shared library, these options
+force the use of either the shared or static version respectively.
+If no shared version of @file{libgcc} was built when the compiler was
+configured, these options have no effect.
+
+There are several situations in which an application should use the
+shared @file{libgcc} instead of the static version.  The most common
+of these is when the application wishes to throw and catch exceptions
+across different shared libraries.  In that case, each of the libraries
+as well as the application itself should use the shared @file{libgcc}.
+
+Therefore, the G++ and GCJ drivers automatically add
+@option{-shared-libgcc} whenever you build a shared library or a main
+executable, because C++ and Java programs typically use exceptions, so
+this is the right thing to do.
+
+If, instead, you use the GCC driver to create shared libraries, you may
+find that they will not always be linked with the shared @file{libgcc}.
+If GCC finds, at its configuration time, that you have a non-GNU linker
+or a GNU linker that does not support option @option{--eh-frame-hdr},
+it will link the shared version of @file{libgcc} into shared libraries
+by default.  Otherwise, it will take advantage of the linker and optimize
+away the linking with the shared version of @file{libgcc}, linking with
+the static version of libgcc by default.  This allows exceptions to
+propagate through such shared libraries, without incurring relocation
+costs at library load time.
+
+However, if a library or main executable is supposed to throw or catch
+exceptions, you must link it using the G++ or GCJ driver, as appropriate
+for the languages used in the program, or using the option
+@option{-shared-libgcc}, such that it is linked with the shared
+@file{libgcc}.
+
+@item -symbolic
+@opindex symbolic
+Bind references to global symbols when building a shared object.  Warn
+about any unresolved references (unless overridden by the link editor
+option @samp{-Xlinker -z -Xlinker defs}).  Only a few systems support
+this option.
+
+@item -Xlinker @var{option}
+@opindex Xlinker
+Pass @var{option} as an option to the linker.  You can use this to
+supply system-specific linker options which GCC does not know how to
+recognize.
+
+If you want to pass an option that takes an argument, you must use
+@option{-Xlinker} twice, once for the option and once for the argument.
+For example, to pass @option{-assert definitions}, you must write
+@samp{-Xlinker -assert -Xlinker definitions}.  It does not work to write
+@option{-Xlinker "-assert definitions"}, because this passes the entire
+string as a single argument, which is not what the linker expects.
+
+@item -Wl,@var{option}
+@opindex Wl
+Pass @var{option} as an option to the linker.  If @var{option} contains
+commas, it is split into multiple options at the commas.
+
+@item -u @var{symbol}
+@opindex u
+Pretend the symbol @var{symbol} is undefined, to force linking of
+library modules to define it.  You can use @option{-u} multiple times with
+different symbols to force loading of additional library modules.
+@end table
+
+@node Directory Options
+@section Options for Directory Search
+@cindex directory options
+@cindex options, directory search
+@cindex search path
+
+These options specify directories to search for header files, for
+libraries and for parts of the compiler:
+
+@table @gcctabopt
+@item -I@var{dir}
+@opindex I
+Add the directory @var{dir} to the head of the list of directories to be
+searched for header files.  This can be used to override a system header
+file, substituting your own version, since these directories are
+searched before the system header file directories.  However, you should
+not use this option to add directories that contain vendor-supplied
+system header files (use @option{-isystem} for that).  If you use more than
+one @option{-I} option, the directories are scanned in left-to-right
+order; the standard system directories come after.
+
+If a standard system include directory, or a directory specified with
+@option{-isystem}, is also specified with @option{-I}, the @option{-I}
+option will be ignored.  The directory will still be searched but as a
+system directory at its normal position in the system include chain.
+This is to ensure that GCC's procedure to fix buggy system headers and
+the ordering for the include_next directive are not inadvertently changed.
+If you really need to change the search order for system directories,
+use the @option{-nostdinc} and/or @option{-isystem} options.
+
+@item -I-
+@opindex I-
+Any directories you specify with @option{-I} options before the @option{-I-}
+option are searched only for the case of @samp{#include "@var{file}"};
+they are not searched for @samp{#include <@var{file}>}.
+
+If additional directories are specified with @option{-I} options after
+the @option{-I-}, these directories are searched for all @samp{#include}
+directives.  (Ordinarily @emph{all} @option{-I} directories are used
+this way.)
+
+In addition, the @option{-I-} option inhibits the use of the current
+directory (where the current input file came from) as the first search
+directory for @samp{#include "@var{file}"}.  There is no way to
+override this effect of @option{-I-}.  With @option{-I.} you can specify
+searching the directory which was current when the compiler was
+invoked.  That is not exactly the same as what the preprocessor does
+by default, but it is often satisfactory.
+
+@option{-I-} does not inhibit the use of the standard system directories
+for header files.  Thus, @option{-I-} and @option{-nostdinc} are
+independent.
+
+@item -L@var{dir}
+@opindex L
+Add directory @var{dir} to the list of directories to be searched
+for @option{-l}.
+
+@item -B@var{prefix}
+@opindex B
+This option specifies where to find the executables, libraries,
+include files, and data files of the compiler itself.
+
+The compiler driver program runs one or more of the subprograms
+@file{cpp}, @file{cc1}, @file{as} and @file{ld}.  It tries
+@var{prefix} as a prefix for each program it tries to run, both with and
+without @samp{@var{machine}/@var{version}/} (@pxref{Target Options}).
+
+For each subprogram to be run, the compiler driver first tries the
+@option{-B} prefix, if any.  If that name is not found, or if @option{-B}
+was not specified, the driver tries two standard prefixes, which are
+@file{/usr/lib/gcc/} and @file{/usr/local/lib/gcc/}.  If neither of
+those results in a file name that is found, the unmodified program
+name is searched for using the directories specified in your
+@env{PATH} environment variable.
+
+The compiler will check to see if the path provided by the @option{-B}
+refers to a directory, and if necessary it will add a directory
+separator character at the end of the path.
+
+@option{-B} prefixes that effectively specify directory names also apply
+to libraries in the linker, because the compiler translates these
+options into @option{-L} options for the linker.  They also apply to
+includes files in the preprocessor, because the compiler translates these
+options into @option{-isystem} options for the preprocessor.  In this case,
+the compiler appends @samp{include} to the prefix.
+
+The run-time support file @file{libgcc.a} can also be searched for using
+the @option{-B} prefix, if needed.  If it is not found there, the two
+standard prefixes above are tried, and that is all.  The file is left
+out of the link if it is not found by those means.
+
+Another way to specify a prefix much like the @option{-B} prefix is to use
+the environment variable @env{GCC_EXEC_PREFIX}.  @xref{Environment
+Variables}.
+
+As a special kludge, if the path provided by @option{-B} is
+@file{[dir/]stage@var{N}/}, where @var{N} is a number in the range 0 to
+9, then it will be replaced by @file{[dir/]include}.  This is to help
+with boot-strapping the compiler.
+
+@item -specs=@var{file}
+@opindex specs
+Process @var{file} after the compiler reads in the standard @file{specs}
+file, in order to override the defaults that the @file{gcc} driver
+program uses when determining what switches to pass to @file{cc1},
+@file{cc1plus}, @file{as}, @file{ld}, etc.  More than one
+@option{-specs=@var{file}} can be specified on the command line, and they
+are processed in order, from left to right.
+@end table
+
+@c man end
+
+@node Spec Files
+@section Specifying subprocesses and the switches to pass to them
+@cindex Spec Files
+
+@command{gcc} is a driver program.  It performs its job by invoking a
+sequence of other programs to do the work of compiling, assembling and
+linking.  GCC interprets its command-line parameters and uses these to
+deduce which programs it should invoke, and which command-line options
+it ought to place on their command lines.  This behavior is controlled
+by @dfn{spec strings}.  In most cases there is one spec string for each
+program that GCC can invoke, but a few programs have multiple spec
+strings to control their behavior.  The spec strings built into GCC can
+be overridden by using the @option{-specs=} command-line switch to specify
+a spec file.
+
+@dfn{Spec files} are plaintext files that are used to construct spec
+strings.  They consist of a sequence of directives separated by blank
+lines.  The type of directive is determined by the first non-whitespace
+character on the line and it can be one of the following:
+
+@table @code
+@item %@var{command}
+Issues a @var{command} to the spec file processor.  The commands that can
+appear here are:
+
+@table @code
+@item %include <@var{file}>
+@cindex %include
+Search for @var{file} and insert its text at the current point in the
+specs file.
+
+@item %include_noerr <@var{file}>
+@cindex %include_noerr
+Just like @samp{%include}, but do not generate an error message if the include
+file cannot be found.
+
+@item %rename @var{old_name} @var{new_name}
+@cindex %rename
+Rename the spec string @var{old_name} to @var{new_name}.
+
+@end table
+
+@item *[@var{spec_name}]:
+This tells the compiler to create, override or delete the named spec
+string.  All lines after this directive up to the next directive or
+blank line are considered to be the text for the spec string.  If this
+results in an empty string then the spec will be deleted.  (Or, if the
+spec did not exist, then nothing will happened.)  Otherwise, if the spec
+does not currently exist a new spec will be created.  If the spec does
+exist then its contents will be overridden by the text of this
+directive, unless the first character of that text is the @samp{+}
+character, in which case the text will be appended to the spec.
+
+@item [@var{suffix}]:
+Creates a new @samp{[@var{suffix}] spec} pair.  All lines after this directive
+and up to the next directive or blank line are considered to make up the
+spec string for the indicated suffix.  When the compiler encounters an
+input file with the named suffix, it will processes the spec string in
+order to work out how to compile that file.  For example:
+
+@smallexample
+.ZZ:
+z-compile -input %i
+@end smallexample
+
+This says that any input file whose name ends in @samp{.ZZ} should be
+passed to the program @samp{z-compile}, which should be invoked with the
+command-line switch @option{-input} and with the result of performing the
+@samp{%i} substitution.  (See below.)
+
+As an alternative to providing a spec string, the text that follows a
+suffix directive can be one of the following:
+
+@table @code
+@item @@@var{language}
+This says that the suffix is an alias for a known @var{language}.  This is
+similar to using the @option{-x} command-line switch to GCC to specify a
+language explicitly.  For example:
+
+@smallexample
+.ZZ:
+@@c++
+@end smallexample
+
+Says that .ZZ files are, in fact, C++ source files.
+
+@item #@var{name}
+This causes an error messages saying:
+
+@smallexample
+@var{name} compiler not installed on this system.
+@end smallexample
+@end table
+
+GCC already has an extensive list of suffixes built into it.
+This directive will add an entry to the end of the list of suffixes, but
+since the list is searched from the end backwards, it is effectively
+possible to override earlier entries using this technique.
+
+@end table
+
+GCC has the following spec strings built into it.  Spec files can
+override these strings or create their own.  Note that individual
+targets can also add their own spec strings to this list.
+
+@smallexample
+asm          Options to pass to the assembler
+asm_final    Options to pass to the assembler post-processor
+cpp          Options to pass to the C preprocessor
+cc1          Options to pass to the C compiler
+cc1plus      Options to pass to the C++ compiler
+endfile      Object files to include at the end of the link
+link         Options to pass to the linker
+lib          Libraries to include on the command line to the linker
+libgcc       Decides which GCC support library to pass to the linker
+linker       Sets the name of the linker
+predefines   Defines to be passed to the C preprocessor
+signed_char  Defines to pass to CPP to say whether @code{char} is signed
+             by default
+startfile    Object files to include at the start of the link
+@end smallexample
+
+Here is a small example of a spec file:
+
+@smallexample
+%rename lib                 old_lib
+
+*lib:
+--start-group -lgcc -lc -leval1 --end-group %(old_lib)
+@end smallexample
+
+This example renames the spec called @samp{lib} to @samp{old_lib} and
+then overrides the previous definition of @samp{lib} with a new one.
+The new definition adds in some extra command-line options before
+including the text of the old definition.
+
+@dfn{Spec strings} are a list of command-line options to be passed to their
+corresponding program.  In addition, the spec strings can contain
+@samp{%}-prefixed sequences to substitute variable text or to
+conditionally insert text into the command line.  Using these constructs
+it is possible to generate quite complex command lines.
+
+Here is a table of all defined @samp{%}-sequences for spec
+strings.  Note that spaces are not generated automatically around the
+results of expanding these sequences.  Therefore you can concatenate them
+together or combine them with constant text in a single argument.
+
+@table @code
+@item %%
+Substitute one @samp{%} into the program name or argument.
+
+@item %i
+Substitute the name of the input file being processed.
+
+@item %b
+Substitute the basename of the input file being processed.
+This is the substring up to (and not including) the last period
+and not including the directory.
+
+@item %B
+This is the same as @samp{%b}, but include the file suffix (text after
+the last period).
+
+@item %d
+Marks the argument containing or following the @samp{%d} as a
+temporary file name, so that that file will be deleted if GCC exits
+successfully.  Unlike @samp{%g}, this contributes no text to the
+argument.
+
+@item %g@var{suffix}
+Substitute a file name that has suffix @var{suffix} and is chosen
+once per compilation, and mark the argument in the same way as
+@samp{%d}.  To reduce exposure to denial-of-service attacks, the file
+name is now chosen in a way that is hard to predict even when previously
+chosen file names are known.  For example, @samp{%g.s @dots{} %g.o @dots{} %g.s}
+might turn into @samp{ccUVUUAU.s ccXYAXZ12.o ccUVUUAU.s}.  @var{suffix} matches
+the regexp @samp{[.A-Za-z]*} or the special string @samp{%O}, which is
+treated exactly as if @samp{%O} had been preprocessed.  Previously, @samp{%g}
+was simply substituted with a file name chosen once per compilation,
+without regard to any appended suffix (which was therefore treated
+just like ordinary text), making such attacks more likely to succeed.
+
+@item %u@var{suffix}
+Like @samp{%g}, but generates a new temporary file name even if
+@samp{%u@var{suffix}} was already seen.
+
+@item %U@var{suffix}
+Substitutes the last file name generated with @samp{%u@var{suffix}}, generating a
+new one if there is no such last file name.  In the absence of any
+@samp{%u@var{suffix}}, this is just like @samp{%g@var{suffix}}, except they don't share
+the same suffix @emph{space}, so @samp{%g.s @dots{} %U.s @dots{} %g.s @dots{} %U.s}
+would involve the generation of two distinct file names, one
+for each @samp{%g.s} and another for each @samp{%U.s}.  Previously, @samp{%U} was
+simply substituted with a file name chosen for the previous @samp{%u},
+without regard to any appended suffix.
+
+@item %j@var{suffix}
+Substitutes the name of the @code{HOST_BIT_BUCKET}, if any, and if it is
+writable, and if save-temps is off; otherwise, substitute the name
+of a temporary file, just like @samp{%u}.  This temporary file is not
+meant for communication between processes, but rather as a junk
+disposal mechanism.
+
+@item %|@var{suffix}
+@itemx %m@var{suffix}
+Like @samp{%g}, except if @option{-pipe} is in effect.  In that case
+@samp{%|} substitutes a single dash and @samp{%m} substitutes nothing at
+all.  These are the two most common ways to instruct a program that it
+should read from standard input or write to standard output.  If you
+need something more elaborate you can use an @samp{%@{pipe:@code{X}@}}
+construct: see for example @file{f/lang-specs.h}.
+
+@item %.@var{SUFFIX}
+Substitutes @var{.SUFFIX} for the suffixes of a matched switch's args
+when it is subsequently output with @samp{%*}.  @var{SUFFIX} is
+terminated by the next space or %.
+
+@item %w
+Marks the argument containing or following the @samp{%w} as the
+designated output file of this compilation.  This puts the argument
+into the sequence of arguments that @samp{%o} will substitute later.
+
+@item %o
+Substitutes the names of all the output files, with spaces
+automatically placed around them.  You should write spaces
+around the @samp{%o} as well or the results are undefined.
+@samp{%o} is for use in the specs for running the linker.
+Input files whose names have no recognized suffix are not compiled
+at all, but they are included among the output files, so they will
+be linked.
+
+@item %O
+Substitutes the suffix for object files.  Note that this is
+handled specially when it immediately follows @samp{%g, %u, or %U},
+because of the need for those to form complete file names.  The
+handling is such that @samp{%O} is treated exactly as if it had already
+been substituted, except that @samp{%g, %u, and %U} do not currently
+support additional @var{suffix} characters following @samp{%O} as they would
+following, for example, @samp{.o}.
+
+@item %p
+Substitutes the standard macro predefinitions for the
+current target machine.  Use this when running @code{cpp}.
+
+@item %P
+Like @samp{%p}, but puts @samp{__} before and after the name of each
+predefined macro, except for macros that start with @samp{__} or with
+@samp{_@var{L}}, where @var{L} is an uppercase letter.  This is for ISO
+C@.
+
+@item %I
+Substitute any of @option{-iprefix} (made from @env{GCC_EXEC_PREFIX}),
+@option{-isysroot} (made from @env{TARGET_SYSTEM_ROOT}), and
+@option{-isystem} (made from @env{COMPILER_PATH} and @option{-B} options)
+as necessary.
+
+@item %s
+Current argument is the name of a library or startup file of some sort.
+Search for that file in a standard list of directories and substitute
+the full name found.
+
+@item %e@var{str}
+Print @var{str} as an error message.  @var{str} is terminated by a newline.
+Use this when inconsistent options are detected.
+
+@item %(@var{name})
+Substitute the contents of spec string @var{name} at this point.
+
+@item %[@var{name}]
+Like @samp{%(@dots{})} but put @samp{__} around @option{-D} arguments.
+
+@item %x@{@var{option}@}
+Accumulate an option for @samp{%X}.
+
+@item %X
+Output the accumulated linker options specified by @option{-Wl} or a @samp{%x}
+spec string.
+
+@item %Y
+Output the accumulated assembler options specified by @option{-Wa}.
+
+@item %Z
+Output the accumulated preprocessor options specified by @option{-Wp}.
+
+@item %a
+Process the @code{asm} spec.  This is used to compute the
+switches to be passed to the assembler.
+
+@item %A
+Process the @code{asm_final} spec.  This is a spec string for
+passing switches to an assembler post-processor, if such a program is
+needed.
+
+@item %l
+Process the @code{link} spec.  This is the spec for computing the
+command line passed to the linker.  Typically it will make use of the
+@samp{%L %G %S %D and %E} sequences.
+
+@item %D
+Dump out a @option{-L} option for each directory that GCC believes might
+contain startup files.  If the target supports multilibs then the
+current multilib directory will be prepended to each of these paths.
+
+@item %M
+Output the multilib directory with directory separators replaced with
+@samp{_}.  If multilib directories are not set, or the multilib directory is
+@file{.} then this option emits nothing.
+
+@item %L
+Process the @code{lib} spec.  This is a spec string for deciding which
+libraries should be included on the command line to the linker.
+
+@item %G
+Process the @code{libgcc} spec.  This is a spec string for deciding
+which GCC support library should be included on the command line to the linker.
+
+@item %S
+Process the @code{startfile} spec.  This is a spec for deciding which
+object files should be the first ones passed to the linker.  Typically
+this might be a file named @file{crt0.o}.
+
+@item %E
+Process the @code{endfile} spec.  This is a spec string that specifies
+the last object files that will be passed to the linker.
+
+@item %C
+Process the @code{cpp} spec.  This is used to construct the arguments
+to be passed to the C preprocessor.
+
+@item %c
+Process the @code{signed_char} spec.  This is intended to be used
+to tell cpp whether a char is signed.  It typically has the definition:
+@smallexample
+%@{funsigned-char:-D__CHAR_UNSIGNED__@}
+@end smallexample
+
+@item %1
+Process the @code{cc1} spec.  This is used to construct the options to be
+passed to the actual C compiler (@samp{cc1}).
+
+@item %2
+Process the @code{cc1plus} spec.  This is used to construct the options to be
+passed to the actual C++ compiler (@samp{cc1plus}).
+
+@item %*
+Substitute the variable part of a matched option.  See below.
+Note that each comma in the substituted string is replaced by
+a single space.
+
+@item %<@code{S}
+Remove all occurrences of @code{-S} from the command line.  Note---this
+command is position dependent.  @samp{%} commands in the spec string
+before this one will see @code{-S}, @samp{%} commands in the spec string
+after this one will not.
+
+@item %:@var{function}(@var{args})
+Call the named function @var{function}, passing it @var{args}.
+@var{args} is first processed as a nested spec string, then split
+into an argument vector in the usual fashion.  The function returns
+a string which is processed as if it had appeared literally as part
+of the current spec.
+
+The following built-in spec functions are provided:
+
+@table @code
+@item @code{if-exists}
+The @code{if-exists} spec function takes one argument, an absolute
+pathname to a file.  If the file exists, @code{if-exists} returns the
+pathname.  Here is a small example of its usage:
+
+@smallexample
+*startfile:
+crt0%O%s %:if-exists(crti%O%s) crtbegin%O%s
+@end smallexample
+
+@item @code{if-exists-else}
+The @code{if-exists-else} spec function is similar to the @code{if-exists}
+spec function, except that it takes two arguments.  The first argument is
+an absolute pathname to a file.  If the file exists, @code{if-exists-else}
+returns the pathname.  If it does not exist, it returns the second argument.
+This way, @code{if-exists-else} can be used to select one file or another,
+based on the existence of the first.  Here is a small example of its usage:
+
+@smallexample
+*startfile:
+crt0%O%s %:if-exists(crti%O%s) \
+%:if-exists-else(crtbeginT%O%s crtbegin%O%s)
+@end smallexample
+@end table
+
+@item %@{@code{S}@}
+Substitutes the @code{-S} switch, if that switch was given to GCC@.
+If that switch was not specified, this substitutes nothing.  Note that
+the leading dash is omitted when specifying this option, and it is
+automatically inserted if the substitution is performed.  Thus the spec
+string @samp{%@{foo@}} would match the command-line option @option{-foo}
+and would output the command line option @option{-foo}.
+
+@item %W@{@code{S}@}
+Like %@{@code{S}@} but mark last argument supplied within as a file to be
+deleted on failure.
+
+@item %@{@code{S}*@}
+Substitutes all the switches specified to GCC whose names start
+with @code{-S}, but which also take an argument.  This is used for
+switches like @option{-o}, @option{-D}, @option{-I}, etc.
+GCC considers @option{-o foo} as being
+one switch whose names starts with @samp{o}.  %@{o*@} would substitute this
+text, including the space.  Thus two arguments would be generated.
+
+@item %@{@code{S}*&@code{T}*@}
+Like %@{@code{S}*@}, but preserve order of @code{S} and @code{T} options
+(the order of @code{S} and @code{T} in the spec is not significant).
+There can be any number of ampersand-separated variables; for each the
+wild card is optional.  Useful for CPP as @samp{%@{D*&U*&A*@}}.
+
+@item %@{@code{S}:@code{X}@}
+Substitutes @code{X}, if the @samp{-S} switch was given to GCC@.
+
+@item %@{!@code{S}:@code{X}@}
+Substitutes @code{X}, if the @samp{-S} switch was @emph{not} given to GCC@.
+
+@item %@{@code{S}*:@code{X}@}
+Substitutes @code{X} if one or more switches whose names start with
+@code{-S} are specified to GCC@.  Normally @code{X} is substituted only
+once, no matter how many such switches appeared.  However, if @code{%*}
+appears somewhere in @code{X}, then @code{X} will be substituted once
+for each matching switch, with the @code{%*} replaced by the part of
+that switch that matched the @code{*}.
+
+@item %@{.@code{S}:@code{X}@}
+Substitutes @code{X}, if processing a file with suffix @code{S}.
+
+@item %@{!.@code{S}:@code{X}@}
+Substitutes @code{X}, if @emph{not} processing a file with suffix @code{S}.
+
+@item %@{@code{S}|@code{P}:@code{X}@}
+Substitutes @code{X} if either @code{-S} or @code{-P} was given to GCC@.
+This may be combined with @samp{!}, @samp{.}, and @code{*} sequences as well,
+although they have a stronger binding than the @samp{|}.  If @code{%*}
+appears in @code{X}, all of the alternatives must be starred, and only
+the first matching alternative is substituted.
+
+For example, a spec string like this:
+
+@smallexample
+%@{.c:-foo@} %@{!.c:-bar@} %@{.c|d:-baz@} %@{!.c|d:-boggle@}
+@end smallexample
+
+will output the following command-line options from the following input
+command-line options:
+
+@smallexample
+fred.c        -foo -baz
+jim.d         -bar -boggle
+-d fred.c     -foo -baz -boggle
+-d jim.d      -bar -baz -boggle
+@end smallexample
+
+@item %@{S:X; T:Y; :D@}
+
+If @code{S} was given to GCC, substitutes @code{X}; else if @code{T} was
+given to GCC, substitutes @code{Y}; else substitutes @code{D}.  There can
+be as many clauses as you need.  This may be combined with @code{.},
+@code{!}, @code{|}, and @code{*} as needed.
+
+
+@end table
+
+The conditional text @code{X} in a %@{@code{S}:@code{X}@} or similar
+construct may contain other nested @samp{%} constructs or spaces, or
+even newlines.  They are processed as usual, as described above.
+Trailing white space in @code{X} is ignored.  White space may also
+appear anywhere on the left side of the colon in these constructs,
+except between @code{.} or @code{*} and the corresponding word.
+
+The @option{-O}, @option{-f}, @option{-m}, and @option{-W} switches are
+handled specifically in these constructs.  If another value of
+@option{-O} or the negated form of a @option{-f}, @option{-m}, or
+@option{-W} switch is found later in the command line, the earlier
+switch value is ignored, except with @{@code{S}*@} where @code{S} is
+just one letter, which passes all matching options.
+
+The character @samp{|} at the beginning of the predicate text is used to
+indicate that a command should be piped to the following command, but
+only if @option{-pipe} is specified.
+
+It is built into GCC which switches take arguments and which do not.
+(You might think it would be useful to generalize this to allow each
+compiler's spec to say which switches take arguments.  But this cannot
+be done in a consistent fashion.  GCC cannot even decide which input
+files have been specified without knowing which switches take arguments,
+and it must know which input files to compile in order to tell which
+compilers to run).
+
+GCC also knows implicitly that arguments starting in @option{-l} are to be
+treated as compiler output files, and passed to the linker in their
+proper position among the other output files.
+
+@c man begin OPTIONS
+
+@node Target Options
+@section Specifying Target Machine and Compiler Version
+@cindex target options
+@cindex cross compiling
+@cindex specifying machine version
+@cindex specifying compiler version and target machine
+@cindex compiler version, specifying
+@cindex target machine, specifying
+
+The usual way to run GCC is to run the executable called @file{gcc}, or
+@file{<machine>-gcc} when cross-compiling, or
+@file{<machine>-gcc-<version>} to run a version other than the one that
+was installed last.  Sometimes this is inconvenient, so GCC provides
+options that will switch to another cross-compiler or version.
+
+@table @gcctabopt
+@item -b @var{machine}
+@opindex b
+The argument @var{machine} specifies the target machine for compilation.
+
+The value to use for @var{machine} is the same as was specified as the
+machine type when configuring GCC as a cross-compiler.  For
+example, if a cross-compiler was configured with @samp{configure
+i386v}, meaning to compile for an 80386 running System V, then you
+would specify @option{-b i386v} to run that cross compiler.
+
+@item -V @var{version}
+@opindex V
+The argument @var{version} specifies which version of GCC to run.
+This is useful when multiple versions are installed.  For example,
+@var{version} might be @samp{2.0}, meaning to run GCC version 2.0.
+@end table
+
+The @option{-V} and @option{-b} options work by running the
+@file{<machine>-gcc-<version>} executable, so there's no real reason to
+use them if you can just run that directly.
+
+@node Submodel Options
+@section Hardware Models and Configurations
+@cindex submodel options
+@cindex specifying hardware config
+@cindex hardware models and configurations, specifying
+@cindex machine dependent options
+
+Earlier we discussed the standard option @option{-b} which chooses among
+different installed compilers for completely different target
+machines, such as VAX vs.@: 68000 vs.@: 80386.
+
+In addition, each of these target machine types can have its own
+special options, starting with @samp{-m}, to choose among various
+hardware models or configurations---for example, 68010 vs 68020,
+floating coprocessor or none.  A single installed version of the
+compiler can compile for any model or configuration, according to the
+options specified.
+
+Some configurations of the compiler also support additional special
+options, usually for compatibility with other compilers on the same
+platform.
+
+These options are defined by the macro @code{TARGET_SWITCHES} in the
+machine description.  The default for the options is also defined by
+that macro, which enables you to change the defaults.
+
+@menu
+* M680x0 Options::
+* M68hc1x Options::
+* VAX Options::
+* SPARC Options::
+* ARM Options::
+* MN10300 Options::
+* M32R/D Options::
+* RS/6000 and PowerPC Options::
+* Darwin Options::
+* MIPS Options::
+* i386 and x86-64 Options::
+* HPPA Options::
+* Intel 960 Options::
+* DEC Alpha Options::
+* DEC Alpha/VMS Options::
+* H8/300 Options::
+* SH Options::
+* System V Options::
+* TMS320C3x/C4x Options::
+* V850 Options::
+* ARC Options::
+* NS32K Options::
+* AVR Options::
+* MCore Options::
+* IA-64 Options::
+* D30V Options::
+* S/390 and zSeries Options::
+* CRIS Options::
+* MMIX Options::
+* PDP-11 Options::
+* Xstormy16 Options::
+* Xtensa Options::
+* FRV Options::
+@end menu
+
+@node M680x0 Options
+@subsection M680x0 Options
+@cindex M680x0 options
+
+These are the @samp{-m} options defined for the 68000 series.  The default
+values for these options depends on which style of 68000 was selected when
+the compiler was configured; the defaults for the most common choices are
+given below.
+
+@table @gcctabopt
+@item -m68000
+@itemx -mc68000
+@opindex m68000
+@opindex mc68000
+Generate output for a 68000.  This is the default
+when the compiler is configured for 68000-based systems.
+
+Use this option for microcontrollers with a 68000 or EC000 core,
+including the 68008, 68302, 68306, 68307, 68322, 68328 and 68356.
+
+@item -m68020
+@itemx -mc68020
+@opindex m68020
+@opindex mc68020
+Generate output for a 68020.  This is the default
+when the compiler is configured for 68020-based systems.
+
+@item -m68881
+@opindex m68881
+Generate output containing 68881 instructions for floating point.
+This is the default for most 68020 systems unless @option{--nfp} was
+specified when the compiler was configured.
+
+@item -m68030
+@opindex m68030
+Generate output for a 68030.  This is the default when the compiler is
+configured for 68030-based systems.
+
+@item -m68040
+@opindex m68040
+Generate output for a 68040.  This is the default when the compiler is
+configured for 68040-based systems.
+
+This option inhibits the use of 68881/68882 instructions that have to be
+emulated by software on the 68040.  Use this option if your 68040 does not
+have code to emulate those instructions.
+
+@item -m68060
+@opindex m68060
+Generate output for a 68060.  This is the default when the compiler is
+configured for 68060-based systems.
+
+This option inhibits the use of 68020 and 68881/68882 instructions that
+have to be emulated by software on the 68060.  Use this option if your 68060
+does not have code to emulate those instructions.
+
+@item -mcpu32
+@opindex mcpu32
+Generate output for a CPU32.  This is the default
+when the compiler is configured for CPU32-based systems.
+
+Use this option for microcontrollers with a
+CPU32 or CPU32+ core, including the 68330, 68331, 68332, 68333, 68334,
+68336, 68340, 68341, 68349 and 68360.
+
+@item -m5200
+@opindex m5200
+Generate output for a 520X ``coldfire'' family cpu.  This is the default
+when the compiler is configured for 520X-based systems.
+
+Use this option for microcontroller with a 5200 core, including
+the MCF5202, MCF5203, MCF5204 and MCF5202.
+
+
+@item -m68020-40
+@opindex m68020-40
+Generate output for a 68040, without using any of the new instructions.
+This results in code which can run relatively efficiently on either a
+68020/68881 or a 68030 or a 68040.  The generated code does use the
+68881 instructions that are emulated on the 68040.
+
+@item -m68020-60
+@opindex m68020-60
+Generate output for a 68060, without using any of the new instructions.
+This results in code which can run relatively efficiently on either a
+68020/68881 or a 68030 or a 68040.  The generated code does use the
+68881 instructions that are emulated on the 68060.
+
+@item -msoft-float
+@opindex msoft-float
+Generate output containing library calls for floating point.
+@strong{Warning:} the requisite libraries are not available for all m68k
+targets.  Normally the facilities of the machine's usual C compiler are
+used, but this can't be done directly in cross-compilation.  You must
+make your own arrangements to provide suitable library functions for
+cross-compilation.  The embedded targets @samp{m68k-*-aout} and
+@samp{m68k-*-coff} do provide software floating point support.
+
+@item -mshort
+@opindex mshort
+Consider type @code{int} to be 16 bits wide, like @code{short int}.
+
+@item -mnobitfield
+@opindex mnobitfield
+Do not use the bit-field instructions.  The @option{-m68000}, @option{-mcpu32}
+and @option{-m5200} options imply @w{@option{-mnobitfield}}.
+
+@item -mbitfield
+@opindex mbitfield
+Do use the bit-field instructions.  The @option{-m68020} option implies
+@option{-mbitfield}.  This is the default if you use a configuration
+designed for a 68020.
+
+@item -mrtd
+@opindex mrtd
+Use a different function-calling convention, in which functions
+that take a fixed number of arguments return with the @code{rtd}
+instruction, which pops their arguments while returning.  This
+saves one instruction in the caller since there is no need to pop
+the arguments there.
+
+This calling convention is incompatible with the one normally
+used on Unix, so you cannot use it if you need to call libraries
+compiled with the Unix compiler.
+
+Also, you must provide function prototypes for all functions that
+take variable numbers of arguments (including @code{printf});
+otherwise incorrect code will be generated for calls to those
+functions.
+
+In addition, seriously incorrect code will result if you call a
+function with too many arguments.  (Normally, extra arguments are
+harmlessly ignored.)
+
+The @code{rtd} instruction is supported by the 68010, 68020, 68030,
+68040, 68060 and CPU32 processors, but not by the 68000 or 5200.
+
+@item -malign-int
+@itemx -mno-align-int
+@opindex malign-int
+@opindex mno-align-int
+Control whether GCC aligns @code{int}, @code{long}, @code{long long},
+@code{float}, @code{double}, and @code{long double} variables on a 32-bit
+boundary (@option{-malign-int}) or a 16-bit boundary (@option{-mno-align-int}).
+Aligning variables on 32-bit boundaries produces code that runs somewhat
+faster on processors with 32-bit busses at the expense of more memory.
+
+@strong{Warning:} if you use the @option{-malign-int} switch, GCC will
+align structures containing the above types  differently than
+most published application binary interface specifications for the m68k.
+
+@item -mpcrel
+@opindex mpcrel
+Use the pc-relative addressing mode of the 68000 directly, instead of
+using a global offset table.  At present, this option implies @option{-fpic},
+allowing at most a 16-bit offset for pc-relative addressing.  @option{-fPIC} is
+not presently supported with @option{-mpcrel}, though this could be supported for
+68020 and higher processors.
+
+@item -mno-strict-align
+@itemx -mstrict-align
+@opindex mno-strict-align
+@opindex mstrict-align
+Do not (do) assume that unaligned memory references will be handled by
+the system.
+
+@item -msep-data
+Generate code that allows the data segment to be located in a different
+area of memory from the text segment.  This allows for execute in place in
+an environment without virtual memory management.  This option implies -fPIC.
+
+@item -mno-sep-data
+Generate code that assumes that the data segment follows the text segment.
+This is the default.
+
+@item -mid-shared-library
+Generate code that supports shared libraries via the library ID method.
+This allows for execute in place and shared libraries in an environment
+without virtual memory management.  This option implies -fPIC.
+
+@item -mno-id-shared-library
+Generate code that doesn't assume ID based shared libraries are being used.
+This is the default.
+
+@item -mshared-library-id=n
+Specified the identification number of the ID based shared library being
+compiled.  Specifying a value of 0 will generate more compact code, specifying
+other values will force the allocation of that number to the current
+library but is no more space or time efficient than omitting this option.
+
+@end table
+
+@node M68hc1x Options
+@subsection M68hc1x Options
+@cindex M68hc1x options
+
+These are the @samp{-m} options defined for the 68hc11 and 68hc12
+microcontrollers.  The default values for these options depends on
+which style of microcontroller was selected when the compiler was configured;
+the defaults for the most common choices are given below.
+
+@table @gcctabopt
+@item -m6811
+@itemx -m68hc11
+@opindex m6811
+@opindex m68hc11
+Generate output for a 68HC11.  This is the default
+when the compiler is configured for 68HC11-based systems.
+
+@item -m6812
+@itemx -m68hc12
+@opindex m6812
+@opindex m68hc12
+Generate output for a 68HC12.  This is the default
+when the compiler is configured for 68HC12-based systems.
+
+@item -m68S12
+@itemx -m68hcs12
+@opindex m68S12
+@opindex m68hcs12
+Generate output for a 68HCS12.
+
+@item -mauto-incdec
+@opindex mauto-incdec
+Enable the use of 68HC12 pre and post auto-increment and auto-decrement
+addressing modes.
+
+@item -minmax
+@itemx -nominmax
+@opindex minmax
+@opindex mnominmax
+Enable the use of 68HC12 min and max instructions.
+
+@item -mlong-calls
+@itemx -mno-long-calls
+@opindex mlong-calls
+@opindex mno-long-calls
+Treat all calls as being far away (near).  If calls are assumed to be
+far away, the compiler will use the @code{call} instruction to
+call a function and the @code{rtc} instruction for returning.
+
+@item -mshort
+@opindex mshort
+Consider type @code{int} to be 16 bits wide, like @code{short int}.
+
+@item -msoft-reg-count=@var{count}
+@opindex msoft-reg-count
+Specify the number of pseudo-soft registers which are used for the
+code generation.  The maximum number is 32.  Using more pseudo-soft
+register may or may not result in better code depending on the program.
+The default is 4 for 68HC11 and 2 for 68HC12.
+
+@end table
+
+@node VAX Options
+@subsection VAX Options
+@cindex VAX options
+
+These @samp{-m} options are defined for the VAX:
+
+@table @gcctabopt
+@item -munix
+@opindex munix
+Do not output certain jump instructions (@code{aobleq} and so on)
+that the Unix assembler for the VAX cannot handle across long
+ranges.
+
+@item -mgnu
+@opindex mgnu
+Do output those jump instructions, on the assumption that you
+will assemble with the GNU assembler.
+
+@item -mg
+@opindex mg
+Output code for g-format floating point numbers instead of d-format.
+@end table
+
+@node SPARC Options
+@subsection SPARC Options
+@cindex SPARC options
+
+These @samp{-m} options are supported on the SPARC:
+
+@table @gcctabopt
+@item -mno-app-regs
+@itemx -mapp-regs
+@opindex mno-app-regs
+@opindex mapp-regs
+Specify @option{-mapp-regs} to generate output using the global registers
+2 through 4, which the SPARC SVR4 ABI reserves for applications.  This
+is the default, except on Solaris.
+
+To be fully SVR4 ABI compliant at the cost of some performance loss,
+specify @option{-mno-app-regs}.  You should compile libraries and system
+software with this option.
+
+@item -mfpu
+@itemx -mhard-float
+@opindex mfpu
+@opindex mhard-float
+Generate output containing floating point instructions.  This is the
+default.
+
+@item -mno-fpu
+@itemx -msoft-float
+@opindex mno-fpu
+@opindex msoft-float
+Generate output containing library calls for floating point.
+@strong{Warning:} the requisite libraries are not available for all SPARC
+targets.  Normally the facilities of the machine's usual C compiler are
+used, but this cannot be done directly in cross-compilation.  You must make
+your own arrangements to provide suitable library functions for
+cross-compilation.  The embedded targets @samp{sparc-*-aout} and
+@samp{sparclite-*-*} do provide software floating point support.
+
+@option{-msoft-float} changes the calling convention in the output file;
+therefore, it is only useful if you compile @emph{all} of a program with
+this option.  In particular, you need to compile @file{libgcc.a}, the
+library that comes with GCC, with @option{-msoft-float} in order for
+this to work.
+
+@item -mhard-quad-float
+@opindex mhard-quad-float
+Generate output containing quad-word (long double) floating point
+instructions.
+
+@item -msoft-quad-float
+@opindex msoft-quad-float
+Generate output containing library calls for quad-word (long double)
+floating point instructions.  The functions called are those specified
+in the SPARC ABI@.  This is the default.
+
+As of this writing, there are no SPARC implementations that have hardware
+support for the quad-word floating point instructions.  They all invoke
+a trap handler for one of these instructions, and then the trap handler
+emulates the effect of the instruction.  Because of the trap handler overhead,
+this is much slower than calling the ABI library routines.  Thus the
+@option{-msoft-quad-float} option is the default.
+
+@item -mno-flat
+@itemx -mflat
+@opindex mno-flat
+@opindex mflat
+With @option{-mflat}, the compiler does not generate save/restore instructions
+and will use a ``flat'' or single register window calling convention.
+This model uses %i7 as the frame pointer and is compatible with the normal
+register window model.  Code from either may be intermixed.
+The local registers and the input registers (0--5) are still treated as
+``call saved'' registers and will be saved on the stack as necessary.
+
+With @option{-mno-flat} (the default), the compiler emits save/restore
+instructions (except for leaf functions) and is the normal mode of operation.
+
+These options are deprecated and will be deleted in a future GCC release.
+
+@item -mno-unaligned-doubles
+@itemx -munaligned-doubles
+@opindex mno-unaligned-doubles
+@opindex munaligned-doubles
+Assume that doubles have 8 byte alignment.  This is the default.
+
+With @option{-munaligned-doubles}, GCC assumes that doubles have 8 byte
+alignment only if they are contained in another type, or if they have an
+absolute address.  Otherwise, it assumes they have 4 byte alignment.
+Specifying this option avoids some rare compatibility problems with code
+generated by other compilers.  It is not the default because it results
+in a performance loss, especially for floating point code.
+
+@item -mno-faster-structs
+@itemx -mfaster-structs
+@opindex mno-faster-structs
+@opindex mfaster-structs
+With @option{-mfaster-structs}, the compiler assumes that structures
+should have 8 byte alignment.  This enables the use of pairs of
+@code{ldd} and @code{std} instructions for copies in structure
+assignment, in place of twice as many @code{ld} and @code{st} pairs.
+However, the use of this changed alignment directly violates the SPARC
+ABI@.  Thus, it's intended only for use on targets where the developer
+acknowledges that their resulting code will not be directly in line with
+the rules of the ABI@.
+
+@item -mimpure-text
+@opindex mimpure-text
+@option{-mimpure-text}, used in addition to @option{-shared}, tells
+the compiler to not pass @option{-z text} to the linker when linking a
+shared object.  Using this option, you can link position-dependent
+code into a shared object.
+
+@option{-mimpure-text} suppresses the ``relocations remain against
+allocatable but non-writable sections'' linker error message.
+However, the necessary relocations will trigger copy-on-write, and the
+shared object is not actually shared across processes.  Instead of
+using @option{-mimpure-text}, you should compile all source code with
+@option{-fpic} or @option{-fPIC}.
+
+This option is only available on SunOS and Solaris.
+
+@item -mv8
+@itemx -msparclite
+@opindex mv8
+@opindex msparclite
+These two options select variations on the SPARC architecture.
+These options are deprecated and will be deleted in a future GCC release.
+They have been replaced with @option{-mcpu=xxx}.
+
+@item -mcypress
+@itemx -msupersparc
+@itemx -mf930
+@itemx -mf934
+@opindex mcypress
+@opindex msupersparc
+@opindex -mf930
+@opindex -mf934
+These four options select the processor for which the code is optimized.
+These options are deprecated and will be deleted in a future GCC release.
+They have been replaced with @option{-mcpu=xxx}.
+
+@item -mcpu=@var{cpu_type}
+@opindex mcpu
+Set the instruction set, register set, and instruction scheduling parameters
+for machine type @var{cpu_type}.  Supported values for @var{cpu_type} are
+@samp{v7}, @samp{cypress}, @samp{v8}, @samp{supersparc}, @samp{sparclite},
+@samp{f930}, @samp{f934}, @samp{hypersparc}, @samp{sparclite86x},
+@samp{sparclet}, @samp{tsc701}, @samp{v9}, @samp{ultrasparc}, and
+@samp{ultrasparc3}.
+
+Default instruction scheduling parameters are used for values that select
+an architecture and not an implementation.  These are @samp{v7}, @samp{v8},
+@samp{sparclite}, @samp{sparclet}, @samp{v9}.
+
+Here is a list of each supported architecture and their supported
+implementations.
+
+@smallexample
+    v7:             cypress
+    v8:             supersparc, hypersparc
+    sparclite:      f930, f934, sparclite86x
+    sparclet:       tsc701
+    v9:             ultrasparc, ultrasparc3
+@end smallexample
+
+By default (unless configured otherwise), GCC generates code for the V7
+variant of the SPARC architecture.  With @option{-mcpu=cypress}, the compiler
+additionally optimizes it for the Cypress CY7C602 chip, as used in the
+SPARCStation/SPARCServer 3xx series.  This is also appropriate for the older
+SPARCStation 1, 2, IPX etc.
+
+With @option{-mcpu=v8}, GCC generates code for the V8 variant of the SPARC
+architecture.  The only difference from V7 code is that the compiler emits
+the integer multiply and integer divide instructions which exist in SPARC-V8
+but not in SPARC-V7.  With @option{-mcpu=supersparc}, the compiler additionally
+optimizes it for the SuperSPARC chip, as used in the SPARCStation 10, 1000 and
+2000 series.
+
+With @option{-mcpu=sparclite}, GCC generates code for the SPARClite variant of
+the SPARC architecture.  This adds the integer multiply, integer divide step
+and scan (@code{ffs}) instructions which exist in SPARClite but not in SPARC-V7.
+With @option{-mcpu=f930}, the compiler additionally optimizes it for the
+Fujitsu MB86930 chip, which is the original SPARClite, with no FPU.  With
+@option{-mcpu=f934}, the compiler additionally optimizes it for the Fujitsu
+MB86934 chip, which is the more recent SPARClite with FPU.
+
+With @option{-mcpu=sparclet}, GCC generates code for the SPARClet variant of
+the SPARC architecture.  This adds the integer multiply, multiply/accumulate,
+integer divide step and scan (@code{ffs}) instructions which exist in SPARClet
+but not in SPARC-V7.  With @option{-mcpu=tsc701}, the compiler additionally
+optimizes it for the TEMIC SPARClet chip.
+
+With @option{-mcpu=v9}, GCC generates code for the V9 variant of the SPARC
+architecture.  This adds 64-bit integer and floating-point move instructions,
+3 additional floating-point condition code registers and conditional move
+instructions.  With @option{-mcpu=ultrasparc}, the compiler additionally
+optimizes it for the Sun UltraSPARC I/II chips.  With
+@option{-mcpu=ultrasparc3}, the compiler additionally optimizes it for the
+Sun UltraSPARC III chip.
+
+@item -mtune=@var{cpu_type}
+@opindex mtune
+Set the instruction scheduling parameters for machine type
+@var{cpu_type}, but do not set the instruction set or register set that the
+option @option{-mcpu=@var{cpu_type}} would.
+
+The same values for @option{-mcpu=@var{cpu_type}} can be used for
+@option{-mtune=@var{cpu_type}}, but the only useful values are those
+that select a particular cpu implementation.  Those are @samp{cypress},
+@samp{supersparc}, @samp{hypersparc}, @samp{f930}, @samp{f934},
+@samp{sparclite86x}, @samp{tsc701}, @samp{ultrasparc}, and
+@samp{ultrasparc3}.
+
+@item -mv8plus
+@itemx -mno-v8plus
+@opindex mv8plus
+@opindex mno-v8plus
+With @option{-mv8plus}, GCC generates code for the SPARC-V8+ ABI.  The
+difference from the V8 ABI is that the global and out registers are
+considered 64-bit wide.  This is enabled by default on Solaris in 32-bit
+mode for all SPARC-V9 processors.
+
+@item -mvis
+@itemx -mno-vis
+@opindex mvis
+@opindex mno-vis
+With @option{-mvis}, GCC generates code that takes advantage of the UltraSPARC
+Visual Instruction Set extensions.  The default is @option{-mno-vis}.
+@end table
+
+These @samp{-m} options are supported in addition to the above
+on SPARC-V9 processors in 64-bit environments:
+
+@table @gcctabopt
+@item -mlittle-endian
+@opindex mlittle-endian
+Generate code for a processor running in little-endian mode. It is only
+available for a few configurations and most notably not on Solaris and Linux.
+
+@item -m32
+@itemx -m64
+@opindex m32
+@opindex m64
+Generate code for a 32-bit or 64-bit environment.
+The 32-bit environment sets int, long and pointer to 32 bits.
+The 64-bit environment sets int to 32 bits and long and pointer
+to 64 bits.
+
+@item -mcmodel=medlow
+@opindex mcmodel=medlow
+Generate code for the Medium/Low code model: 64-bit addresses, programs
+must be linked in the low 32 bits of memory.  Programs can be statically
+or dynamically linked.
+
+@item -mcmodel=medmid
+@opindex mcmodel=medmid
+Generate code for the Medium/Middle code model: 64-bit addresses, programs
+must be linked in the low 44 bits of memory, the text and data segments must
+be less than 2GB in size and the data segment must be located within 2GB of
+the text segment.
+
+@item -mcmodel=medany
+@opindex mcmodel=medany
+Generate code for the Medium/Anywhere code model: 64-bit addresses, programs
+may be linked anywhere in memory, the text and data segments must be less
+than 2GB in size and the data segment must be located within 2GB of the
+text segment.
+
+@item -mcmodel=embmedany
+@opindex mcmodel=embmedany
+Generate code for the Medium/Anywhere code model for embedded systems:
+64-bit addresses, the text and data segments must be less than 2GB in
+size, both starting anywhere in memory (determined at link time).  The
+global register %g4 points to the base of the data segment.  Programs
+are statically linked and PIC is not supported.
+
+@item -mstack-bias
+@itemx -mno-stack-bias
+@opindex mstack-bias
+@opindex mno-stack-bias
+With @option{-mstack-bias}, GCC assumes that the stack pointer, and
+frame pointer if present, are offset by @minus{}2047 which must be added back
+when making stack frame references.  This is the default in 64-bit mode.
+Otherwise, assume no such offset is present.
+@end table
+
+These switches are supported in addition to the above on Solaris:
+
+@table @gcctabopt
+@item -threads
+@opindex threads
+Add support for multithreading using the Solaris threads library.  This
+option sets flags for both the preprocessor and linker.  This option does
+not affect the thread safety of object code produced by the compiler or
+that of libraries supplied with it.
+
+@item -pthreads
+@opindex pthreads
+Add support for multithreading using the POSIX threads library.  This
+option sets flags for both the preprocessor and linker.  This option does
+not affect the thread safety of object code produced  by the compiler or
+that of libraries supplied with it.
+@end table
+
+@node ARM Options
+@subsection ARM Options
+@cindex ARM options
+
+These @samp{-m} options are defined for Advanced RISC Machines (ARM)
+architectures:
+
+@table @gcctabopt
+@item -mapcs-frame
+@opindex mapcs-frame
+Generate a stack frame that is compliant with the ARM Procedure Call
+Standard for all functions, even if this is not strictly necessary for
+correct execution of the code.  Specifying @option{-fomit-frame-pointer}
+with this option will cause the stack frames not to be generated for
+leaf functions.  The default is @option{-mno-apcs-frame}.
+
+@item -mapcs
+@opindex mapcs
+This is a synonym for @option{-mapcs-frame}.
+
+@item -mapcs-26
+@opindex mapcs-26
+Generate code for a processor running with a 26-bit program counter,
+and conforming to the function calling standards for the APCS 26-bit
+option.
+
+This option is deprecated.  Future releases of the GCC will only support
+generating code that runs in apcs-32 mode.
+
+@item -mapcs-32
+@opindex mapcs-32
+Generate code for a processor running with a 32-bit program counter,
+and conforming to the function calling standards for the APCS 32-bit
+option.
+
+This flag is deprecated.  Future releases of GCC will make this flag
+unconditional.
+
+@ignore
+@c not currently implemented
+@item -mapcs-stack-check
+@opindex mapcs-stack-check
+Generate code to check the amount of stack space available upon entry to
+every function (that actually uses some stack space).  If there is
+insufficient space available then either the function
+@samp{__rt_stkovf_split_small} or @samp{__rt_stkovf_split_big} will be
+called, depending upon the amount of stack space required.  The run time
+system is required to provide these functions.  The default is
+@option{-mno-apcs-stack-check}, since this produces smaller code.
+
+@c not currently implemented
+@item -mapcs-float
+@opindex mapcs-float
+Pass floating point arguments using the float point registers.  This is
+one of the variants of the APCS@.  This option is recommended if the
+target hardware has a floating point unit or if a lot of floating point
+arithmetic is going to be performed by the code.  The default is
+@option{-mno-apcs-float}, since integer only code is slightly increased in
+size if @option{-mapcs-float} is used.
+
+@c not currently implemented
+@item -mapcs-reentrant
+@opindex mapcs-reentrant
+Generate reentrant, position independent code.  The default is
+@option{-mno-apcs-reentrant}.
+@end ignore
+
+@item -mthumb-interwork
+@opindex mthumb-interwork
+Generate code which supports calling between the ARM and Thumb
+instruction sets.  Without this option the two instruction sets cannot
+be reliably used inside one program.  The default is
+@option{-mno-thumb-interwork}, since slightly larger code is generated
+when @option{-mthumb-interwork} is specified.
+
+@item -mno-sched-prolog
+@opindex mno-sched-prolog
+Prevent the reordering of instructions in the function prolog, or the
+merging of those instruction with the instructions in the function's
+body.  This means that all functions will start with a recognizable set
+of instructions (or in fact one of a choice from a small set of
+different function prologues), and this information can be used to
+locate the start if functions inside an executable piece of code.  The
+default is @option{-msched-prolog}.
+
+@item -mhard-float
+@opindex mhard-float
+Generate output containing floating point instructions.  This is the
+default.
+
+@item -msoft-float
+@opindex msoft-float
+Generate output containing library calls for floating point.
+@strong{Warning:} the requisite libraries are not available for all ARM
+targets.  Normally the facilities of the machine's usual C compiler are
+used, but this cannot be done directly in cross-compilation.  You must make
+your own arrangements to provide suitable library functions for
+cross-compilation.
+
+@option{-msoft-float} changes the calling convention in the output file;
+therefore, it is only useful if you compile @emph{all} of a program with
+this option.  In particular, you need to compile @file{libgcc.a}, the
+library that comes with GCC, with @option{-msoft-float} in order for
+this to work.
+
+@item -mlittle-endian
+@opindex mlittle-endian
+Generate code for a processor running in little-endian mode.  This is
+the default for all standard configurations.
+
+@item -mbig-endian
+@opindex mbig-endian
+Generate code for a processor running in big-endian mode; the default is
+to compile code for a little-endian processor.
+
+@item -mwords-little-endian
+@opindex mwords-little-endian
+This option only applies when generating code for big-endian processors.
+Generate code for a little-endian word order but a big-endian byte
+order.  That is, a byte order of the form @samp{32107654}.  Note: this
+option should only be used if you require compatibility with code for
+big-endian ARM processors generated by versions of the compiler prior to
+2.8.
+
+@item -malignment-traps
+@opindex malignment-traps
+Generate code that will not trap if the MMU has alignment traps enabled.
+On ARM architectures prior to ARMv4, there were no instructions to
+access half-word objects stored in memory.  However, when reading from
+memory a feature of the ARM architecture allows a word load to be used,
+even if the address is unaligned, and the processor core will rotate the
+data as it is being loaded.  This option tells the compiler that such
+misaligned accesses will cause a MMU trap and that it should instead
+synthesize the access as a series of byte accesses.  The compiler can
+still use word accesses to load half-word data if it knows that the
+address is aligned to a word boundary.
+
+This option has no effect when compiling for ARM architecture 4 or later,
+since these processors have instructions to directly access half-word
+objects in memory.
+
+@item -mno-alignment-traps
+@opindex mno-alignment-traps
+Generate code that assumes that the MMU will not trap unaligned
+accesses.  This produces better code when the target instruction set
+does not have half-word memory operations (i.e.@: implementations prior to
+ARMv4).
+
+Note that you cannot use this option to access unaligned word objects,
+since the processor will only fetch one 32-bit aligned object from
+memory.
+
+The default setting is @option{-malignment-traps}, since this produces
+code that will also run on processors implementing ARM architecture
+version 6 or later.
+
+This option is deprecated and will be removed in the next release of GCC.
+
+@item -mcpu=@var{name}
+@opindex mcpu
+This specifies the name of the target ARM processor.  GCC uses this name
+to determine what kind of instructions it can emit when generating
+assembly code.  Permissible names are: @samp{arm2}, @samp{arm250},
+@samp{arm3}, @samp{arm6}, @samp{arm60}, @samp{arm600}, @samp{arm610},
+@samp{arm620}, @samp{arm7}, @samp{arm7m}, @samp{arm7d}, @samp{arm7dm},
+@samp{arm7di}, @samp{arm7dmi}, @samp{arm70}, @samp{arm700},
+@samp{arm700i}, @samp{arm710}, @samp{arm710c}, @samp{arm7100},
+@samp{arm7500}, @samp{arm7500fe}, @samp{arm7tdmi}, @samp{arm8},
+@samp{strongarm}, @samp{strongarm110}, @samp{strongarm1100},
+@samp{arm8}, @samp{arm810}, @samp{arm9}, @samp{arm9e}, @samp{arm920},
+@samp{arm920t}, @samp{arm926ejs}, @samp{arm940t}, @samp{arm9tdmi},
+@samp{arm10tdmi}, @samp{arm1020t}, @samp{arm1026ejs},
+@samp{arm1136js}, @samp{arm1136jfs} ,@samp{xscale}, @samp{iwmmxt},
+@samp{ep9312}.
+
+@itemx -mtune=@var{name}
+@opindex mtune
+This option is very similar to the @option{-mcpu=} option, except that
+instead of specifying the actual target processor type, and hence
+restricting which instructions can be used, it specifies that GCC should
+tune the performance of the code as if the target were of the type
+specified in this option, but still choosing the instructions that it
+will generate based on the cpu specified by a @option{-mcpu=} option.
+For some ARM implementations better performance can be obtained by using
+this option.
+
+@item -march=@var{name}
+@opindex march
+This specifies the name of the target ARM architecture.  GCC uses this
+name to determine what kind of instructions it can emit when generating
+assembly code.  This option can be used in conjunction with or instead
+of the @option{-mcpu=} option.  Permissible names are: @samp{armv2},
+@samp{armv2a}, @samp{armv3}, @samp{armv3m}, @samp{armv4}, @samp{armv4t},
+@samp{armv5}, @samp{armv5t}, @samp{armv5te}, @samp{armv6j},
+@samp{iwmmxt}, @samp{ep9312}.
+
+@item -mfpe=@var{number}
+@itemx -mfp=@var{number}
+@opindex mfpe
+@opindex mfp
+This specifies the version of the floating point emulation available on
+the target.  Permissible values are 2 and 3.  @option{-mfp=} is a synonym
+for @option{-mfpe=}, for compatibility with older versions of GCC@.
+
+@item -mstructure-size-boundary=@var{n}
+@opindex mstructure-size-boundary
+The size of all structures and unions will be rounded up to a multiple
+of the number of bits set by this option.  Permissible values are 8 and
+32.  The default value varies for different toolchains.  For the COFF
+targeted toolchain the default value is 8.  Specifying the larger number
+can produce faster, more efficient code, but can also increase the size
+of the program.  The two values are potentially incompatible.  Code
+compiled with one value cannot necessarily expect to work with code or
+libraries compiled with the other value, if they exchange information
+using structures or unions.
+
+@item -mabort-on-noreturn
+@opindex mabort-on-noreturn
+Generate a call to the function @code{abort} at the end of a
+@code{noreturn} function.  It will be executed if the function tries to
+return.
+
+@item -mlong-calls
+@itemx -mno-long-calls
+@opindex mlong-calls
+@opindex mno-long-calls
+Tells the compiler to perform function calls by first loading the
+address of the function into a register and then performing a subroutine
+call on this register.  This switch is needed if the target function
+will lie outside of the 64 megabyte addressing range of the offset based
+version of subroutine call instruction.
+
+Even if this switch is enabled, not all function calls will be turned
+into long calls.  The heuristic is that static functions, functions
+which have the @samp{short-call} attribute, functions that are inside
+the scope of a @samp{#pragma no_long_calls} directive and functions whose
+definitions have already been compiled within the current compilation
+unit, will not be turned into long calls.  The exception to this rule is
+that weak function definitions, functions with the @samp{long-call}
+attribute or the @samp{section} attribute, and functions that are within
+the scope of a @samp{#pragma long_calls} directive, will always be
+turned into long calls.
+
+This feature is not enabled by default.  Specifying
+@option{-mno-long-calls} will restore the default behavior, as will
+placing the function calls within the scope of a @samp{#pragma
+long_calls_off} directive.  Note these switches have no effect on how
+the compiler generates code to handle function calls via function
+pointers.
+
+@item -mnop-fun-dllimport
+@opindex mnop-fun-dllimport
+Disable support for the @code{dllimport} attribute.
+
+@item -msingle-pic-base
+@opindex msingle-pic-base
+Treat the register used for PIC addressing as read-only, rather than
+loading it in the prologue for each function.  The run-time system is
+responsible for initializing this register with an appropriate value
+before execution begins.
+
+@item -mpic-register=@var{reg}
+@opindex mpic-register
+Specify the register to be used for PIC addressing.  The default is R10
+unless stack-checking is enabled, when R9 is used.
+
+@item -mcirrus-fix-invalid-insns
+@opindex mcirrus-fix-invalid-insns
+@opindex mno-cirrus-fix-invalid-insns
+Insert NOPs into the instruction stream to in order to work around
+problems with invalid Maverick instruction combinations.  This option
+is only valid if the @option{-mcpu=ep9312} option has been used to
+enable generation of instructions for the Cirrus Maverick floating
+point co-processor.  This option is not enabled by default, since the
+problem is only present in older Maverick implementations.  The default
+can be re-enabled by use of the @option{-mno-cirrus-fix-invalid-insns}
+switch.
+
+@item -mpoke-function-name
+@opindex mpoke-function-name
+Write the name of each function into the text section, directly
+preceding the function prologue.  The generated code is similar to this:
+
+@smallexample
+     t0
+         .ascii "arm_poke_function_name", 0
+         .align
+     t1
+         .word 0xff000000 + (t1 - t0)
+     arm_poke_function_name
+         mov     ip, sp
+         stmfd   sp!, @{fp, ip, lr, pc@}
+         sub     fp, ip, #4
+@end smallexample
+
+When performing a stack backtrace, code can inspect the value of
+@code{pc} stored at @code{fp + 0}.  If the trace function then looks at
+location @code{pc - 12} and the top 8 bits are set, then we know that
+there is a function name embedded immediately preceding this location
+and has length @code{((pc[-3]) & 0xff000000)}.
+
+@item -mthumb
+@opindex mthumb
+Generate code for the 16-bit Thumb instruction set.  The default is to
+use the 32-bit ARM instruction set.
+
+@item -mtpcs-frame
+@opindex mtpcs-frame
+Generate a stack frame that is compliant with the Thumb Procedure Call
+Standard for all non-leaf functions.  (A leaf function is one that does
+not call any other functions.)  The default is @option{-mno-tpcs-frame}.
+
+@item -mtpcs-leaf-frame
+@opindex mtpcs-leaf-frame
+Generate a stack frame that is compliant with the Thumb Procedure Call
+Standard for all leaf functions.  (A leaf function is one that does
+not call any other functions.)  The default is @option{-mno-apcs-leaf-frame}.
+
+@item -mcallee-super-interworking
+@opindex mcallee-super-interworking
+Gives all externally visible functions in the file being compiled an ARM
+instruction set header which switches to Thumb mode before executing the
+rest of the function.  This allows these functions to be called from
+non-interworking code.
+
+@item -mcaller-super-interworking
+@opindex mcaller-super-interworking
+Allows calls via function pointers (including virtual functions) to
+execute correctly regardless of whether the target code has been
+compiled for interworking or not.  There is a small overhead in the cost
+of executing a function pointer if this option is enabled.
+
+@end table
+
+@node MN10300 Options
+@subsection MN10300 Options
+@cindex MN10300 options
+
+These @option{-m} options are defined for Matsushita MN10300 architectures:
+
+@table @gcctabopt
+@item -mmult-bug
+@opindex mmult-bug
+Generate code to avoid bugs in the multiply instructions for the MN10300
+processors.  This is the default.
+
+@item -mno-mult-bug
+@opindex mno-mult-bug
+Do not generate code to avoid bugs in the multiply instructions for the
+MN10300 processors.
+
+@item -mam33
+@opindex mam33
+Generate code which uses features specific to the AM33 processor.
+
+@item -mno-am33
+@opindex mno-am33
+Do not generate code which uses features specific to the AM33 processor.  This
+is the default.
+
+@item -mno-crt0
+@opindex mno-crt0
+Do not link in the C run-time initialization object file.
+
+@item -mrelax
+@opindex mrelax
+Indicate to the linker that it should perform a relaxation optimization pass
+to shorten branches, calls and absolute memory addresses.  This option only
+has an effect when used on the command line for the final link step.
+
+This option makes symbolic debugging impossible.
+@end table
+
+
+@node M32R/D Options
+@subsection M32R/D Options
+@cindex M32R/D options
+
+These @option{-m} options are defined for Renesas M32R/D architectures:
+
+@table @gcctabopt
+@item -m32r2
+@opindex m32r2
+Generate code for the M32R/2@.
+
+@item -m32rx
+@opindex m32rx
+Generate code for the M32R/X@.
+
+@item -m32r
+@opindex m32r
+Generate code for the M32R@.  This is the default.
+
+@item -mmodel=small
+@opindex mmodel=small
+Assume all objects live in the lower 16MB of memory (so that their addresses
+can be loaded with the @code{ld24} instruction), and assume all subroutines
+are reachable with the @code{bl} instruction.
+This is the default.
+
+The addressability of a particular object can be set with the
+@code{model} attribute.
+
+@item -mmodel=medium
+@opindex mmodel=medium
+Assume objects may be anywhere in the 32-bit address space (the compiler
+will generate @code{seth/add3} instructions to load their addresses), and
+assume all subroutines are reachable with the @code{bl} instruction.
+
+@item -mmodel=large
+@opindex mmodel=large
+Assume objects may be anywhere in the 32-bit address space (the compiler
+will generate @code{seth/add3} instructions to load their addresses), and
+assume subroutines may not be reachable with the @code{bl} instruction
+(the compiler will generate the much slower @code{seth/add3/jl}
+instruction sequence).
+
+@item -msdata=none
+@opindex msdata=none
+Disable use of the small data area.  Variables will be put into
+one of @samp{.data}, @samp{bss}, or @samp{.rodata} (unless the
+@code{section} attribute has been specified).
+This is the default.
+
+The small data area consists of sections @samp{.sdata} and @samp{.sbss}.
+Objects may be explicitly put in the small data area with the
+@code{section} attribute using one of these sections.
+
+@item -msdata=sdata
+@opindex msdata=sdata
+Put small global and static data in the small data area, but do not
+generate special code to reference them.
+
+@item -msdata=use
+@opindex msdata=use
+Put small global and static data in the small data area, and generate
+special instructions to reference them.
+
+@item -G @var{num}
+@opindex G
+@cindex smaller data references
+Put global and static objects less than or equal to @var{num} bytes
+into the small data or bss sections instead of the normal data or bss
+sections.  The default value of @var{num} is 8.
+The @option{-msdata} option must be set to one of @samp{sdata} or @samp{use}
+for this option to have any effect.
+
+All modules should be compiled with the same @option{-G @var{num}} value.
+Compiling with different values of @var{num} may or may not work; if it
+doesn't the linker will give an error message---incorrect code will not be
+generated.
+
+@item -mdebug
+@opindex mdebug
+Makes the M32R specific code in the compiler display some statistics
+that might help in debugging programs.
+
+@item -malign-loops
+@opindex malign-loops
+Align all loops to a 32-byte boundary.
+
+@item -mno-align-loops
+@opindex mno-align-loops
+Do not enforce a 32-byte alignment for loops.  This is the default.
+
+@item -missue-rate=@var{number}
+@opindex missue-rate=@var{number}
+Issue @var{number} instructions per cycle.  @var{number} can only be 1
+or 2.
+
+@item -mbranch-cost=@var{number}
+@opindex mbranch-cost=@var{number}
+@var{number} can only be 1 or 2.  If it is 1 then branches will be
+preferred over conditional code, if it is 2, then the opposite will
+apply.
+
+@item -mflush-trap=@var{number}
+@opindex mflush-trap=@var{number}
+Specifies the trap number to use to flush the cache.  The default is
+12.  Valid numbers are between 0 and 15 inclusive.
+
+@item -mno-flush-trap
+@opindex mno-flush-trap
+Specifies that the cache cannot be flushed by using a trap.
+
+@item -mflush-func=@var{name}
+@opindex mflush-func=@var{name}
+Specifies the name of the operating system function to call to flush
+the cache.  The default is @emph{_flush_cache}, but a function call
+will only be used if a trap is not available.
+
+@item -mno-flush-func
+@opindex mno-flush-func
+Indicates that there is no OS function for flushing the cache.
+
+@end table
+
+@node RS/6000 and PowerPC Options
+@subsection IBM RS/6000 and PowerPC Options
+@cindex RS/6000 and PowerPC Options
+@cindex IBM RS/6000 and PowerPC Options
+
+These @samp{-m} options are defined for the IBM RS/6000 and PowerPC:
+@table @gcctabopt
+@item -mpower
+@itemx -mno-power
+@itemx -mpower2
+@itemx -mno-power2
+@itemx -mpowerpc
+@itemx -mno-powerpc
+@itemx -mpowerpc-gpopt
+@itemx -mno-powerpc-gpopt
+@itemx -mpowerpc-gfxopt
+@itemx -mno-powerpc-gfxopt
+@itemx -mpowerpc64
+@itemx -mno-powerpc64
+@opindex mpower
+@opindex mno-power
+@opindex mpower2
+@opindex mno-power2
+@opindex mpowerpc
+@opindex mno-powerpc
+@opindex mpowerpc-gpopt
+@opindex mno-powerpc-gpopt
+@opindex mpowerpc-gfxopt
+@opindex mno-powerpc-gfxopt
+@opindex mpowerpc64
+@opindex mno-powerpc64
+GCC supports two related instruction set architectures for the
+RS/6000 and PowerPC@.  The @dfn{POWER} instruction set are those
+instructions supported by the @samp{rios} chip set used in the original
+RS/6000 systems and the @dfn{PowerPC} instruction set is the
+architecture of the Motorola MPC5xx, MPC6xx, MPC8xx microprocessors, and
+the IBM 4xx microprocessors.
+
+Neither architecture is a subset of the other.  However there is a
+large common subset of instructions supported by both.  An MQ
+register is included in processors supporting the POWER architecture.
+
+You use these options to specify which instructions are available on the
+processor you are using.  The default value of these options is
+determined when configuring GCC@.  Specifying the
+@option{-mcpu=@var{cpu_type}} overrides the specification of these
+options.  We recommend you use the @option{-mcpu=@var{cpu_type}} option
+rather than the options listed above.
+
+The @option{-mpower} option allows GCC to generate instructions that
+are found only in the POWER architecture and to use the MQ register.
+Specifying @option{-mpower2} implies @option{-power} and also allows GCC
+to generate instructions that are present in the POWER2 architecture but
+not the original POWER architecture.
+
+The @option{-mpowerpc} option allows GCC to generate instructions that
+are found only in the 32-bit subset of the PowerPC architecture.
+Specifying @option{-mpowerpc-gpopt} implies @option{-mpowerpc} and also allows
+GCC to use the optional PowerPC architecture instructions in the
+General Purpose group, including floating-point square root.  Specifying
+@option{-mpowerpc-gfxopt} implies @option{-mpowerpc} and also allows GCC to
+use the optional PowerPC architecture instructions in the Graphics
+group, including floating-point select.
+
+The @option{-mpowerpc64} option allows GCC to generate the additional
+64-bit instructions that are found in the full PowerPC64 architecture
+and to treat GPRs as 64-bit, doubleword quantities.  GCC defaults to
+@option{-mno-powerpc64}.
+
+If you specify both @option{-mno-power} and @option{-mno-powerpc}, GCC
+will use only the instructions in the common subset of both
+architectures plus some special AIX common-mode calls, and will not use
+the MQ register.  Specifying both @option{-mpower} and @option{-mpowerpc}
+permits GCC to use any instruction from either architecture and to
+allow use of the MQ register; specify this for the Motorola MPC601.
+
+@item -mnew-mnemonics
+@itemx -mold-mnemonics
+@opindex mnew-mnemonics
+@opindex mold-mnemonics
+Select which mnemonics to use in the generated assembler code.  With
+@option{-mnew-mnemonics}, GCC uses the assembler mnemonics defined for
+the PowerPC architecture.  With @option{-mold-mnemonics} it uses the
+assembler mnemonics defined for the POWER architecture.  Instructions
+defined in only one architecture have only one mnemonic; GCC uses that
+mnemonic irrespective of which of these options is specified.
+
+GCC defaults to the mnemonics appropriate for the architecture in
+use.  Specifying @option{-mcpu=@var{cpu_type}} sometimes overrides the
+value of these option.  Unless you are building a cross-compiler, you
+should normally not specify either @option{-mnew-mnemonics} or
+@option{-mold-mnemonics}, but should instead accept the default.
+
+@item -mcpu=@var{cpu_type}
+@opindex mcpu
+Set architecture type, register usage, choice of mnemonics, and
+instruction scheduling parameters for machine type @var{cpu_type}.
+Supported values for @var{cpu_type} are @samp{401}, @samp{403},
+@samp{405}, @samp{405fp}, @samp{440}, @samp{440fp}, @samp{505},
+@samp{601}, @samp{602}, @samp{603}, @samp{603e}, @samp{604},
+@samp{604e}, @samp{620}, @samp{630}, @samp{740}, @samp{7400},
+@samp{7450}, @samp{750}, @samp{801}, @samp{821}, @samp{823},
+@samp{860}, @samp{970}, @samp{8540}, @samp{common}, @samp{ec603e}, @samp{G3},
+@samp{G4}, @samp{G5}, @samp{power}, @samp{power2}, @samp{power3},
+@samp{power4}, @samp{power5}, @samp{powerpc}, @samp{powerpc64},
+@samp{rios}, @samp{rios1}, @samp{rios2}, @samp{rsc}, and @samp{rs64a}.
+
+@option{-mcpu=common} selects a completely generic processor.  Code
+generated under this option will run on any POWER or PowerPC processor.
+GCC will use only the instructions in the common subset of both
+architectures, and will not use the MQ register.  GCC assumes a generic
+processor model for scheduling purposes.
+
+@option{-mcpu=power}, @option{-mcpu=power2}, @option{-mcpu=powerpc}, and
+@option{-mcpu=powerpc64} specify generic POWER, POWER2, pure 32-bit
+PowerPC (i.e., not MPC601), and 64-bit PowerPC architecture machine
+types, with an appropriate, generic processor model assumed for
+scheduling purposes.
+
+The other options specify a specific processor.  Code generated under
+those options will run best on that processor, and may not run at all on
+others.
+
+The @option{-mcpu} options automatically enable or disable the
+following options: @option{-maltivec}, @option{-mhard-float},
+@option{-mmfcrf}, @option{-mmultiple}, @option{-mnew-mnemonics},
+@option{-mpower}, @option{-mpower2}, @option{-mpowerpc64},
+@option{-mpowerpc-gpopt}, @option{-mpowerpc-gfxopt},
+@option{-mstring}.  The particular options set for any particular CPU
+will vary between compiler versions, depending on what setting seems
+to produce optimal code for that CPU; it doesn't necessarily reflect
+the actual hardware's capabilities.  If you wish to set an individual
+option to a particular value, you may specify it after the
+@option{-mcpu} option, like @samp{-mcpu=970 -mno-altivec}.
+
+On AIX, the @option{-maltivec} and @option{-mpowerpc64} options are
+not enabled or disabled by the @option{-mcpu} option at present, since
+AIX does not have full support for these options.  You may still
+enable or disable them individually if you're sure it'll work in your
+environment.
+
+@item -mtune=@var{cpu_type}
+@opindex mtune
+Set the instruction scheduling parameters for machine type
+@var{cpu_type}, but do not set the architecture type, register usage, or
+choice of mnemonics, as @option{-mcpu=@var{cpu_type}} would.  The same
+values for @var{cpu_type} are used for @option{-mtune} as for
+@option{-mcpu}.  If both are specified, the code generated will use the
+architecture, registers, and mnemonics set by @option{-mcpu}, but the
+scheduling parameters set by @option{-mtune}.
+
+@item -maltivec
+@itemx -mno-altivec
+@opindex maltivec
+@opindex mno-altivec
+These switches enable or disable the use of built-in functions that
+allow access to the AltiVec instruction set.  You may also need to set
+@option{-mabi=altivec} to adjust the current ABI with AltiVec ABI
+enhancements.
+
+@item -mabi=spe
+@opindex mabi=spe
+Extend the current ABI with SPE ABI extensions.  This does not change
+the default ABI, instead it adds the SPE ABI extensions to the current
+ABI@.
+
+@item -mabi=no-spe
+@opindex mabi=no-spe
+Disable Booke SPE ABI extensions for the current ABI.
+
+@item -misel=@var{yes/no}
+@itemx -misel
+@opindex misel
+This switch enables or disables the generation of ISEL instructions.
+
+@item -mspe=@var{yes/no}
+@itemx -mspe
+@opindex mspe
+This switch enables or disables the generation of SPE simd
+instructions.
+
+@item -mfloat-gprs=@var{yes/no}
+@itemx -mfloat-gprs
+@opindex mfloat-gprs
+This switch enables or disables the generation of floating point
+operations on the general purpose registers for architectures that
+support it.  This option is currently only available on the MPC8540.
+
+@item -mfull-toc
+@itemx -mno-fp-in-toc
+@itemx -mno-sum-in-toc
+@itemx -mminimal-toc
+@opindex mfull-toc
+@opindex mno-fp-in-toc
+@opindex mno-sum-in-toc
+@opindex mminimal-toc
+Modify generation of the TOC (Table Of Contents), which is created for
+every executable file.  The @option{-mfull-toc} option is selected by
+default.  In that case, GCC will allocate at least one TOC entry for
+each unique non-automatic variable reference in your program.  GCC
+will also place floating-point constants in the TOC@.  However, only
+16,384 entries are available in the TOC@.
+
+If you receive a linker error message that saying you have overflowed
+the available TOC space, you can reduce the amount of TOC space used
+with the @option{-mno-fp-in-toc} and @option{-mno-sum-in-toc} options.
+@option{-mno-fp-in-toc} prevents GCC from putting floating-point
+constants in the TOC and @option{-mno-sum-in-toc} forces GCC to
+generate code to calculate the sum of an address and a constant at
+run-time instead of putting that sum into the TOC@.  You may specify one
+or both of these options.  Each causes GCC to produce very slightly
+slower and larger code at the expense of conserving TOC space.
+
+If you still run out of space in the TOC even when you specify both of
+these options, specify @option{-mminimal-toc} instead.  This option causes
+GCC to make only one TOC entry for every file.  When you specify this
+option, GCC will produce code that is slower and larger but which
+uses extremely little TOC space.  You may wish to use this option
+only on files that contain less frequently executed code.
+
+@item -maix64
+@itemx -maix32
+@opindex maix64
+@opindex maix32
+Enable 64-bit AIX ABI and calling convention: 64-bit pointers, 64-bit
+@code{long} type, and the infrastructure needed to support them.
+Specifying @option{-maix64} implies @option{-mpowerpc64} and
+@option{-mpowerpc}, while @option{-maix32} disables the 64-bit ABI and
+implies @option{-mno-powerpc64}.  GCC defaults to @option{-maix32}.
+
+@item -mxl-compat
+@itemx -mno-xl-compat
+@opindex mxl-compat
+@opindex mno-xl-compat
+Produce code that conforms more closely to IBM XLC semantics when using
+AIX-compatible ABI.  Pass floating-point arguments to prototyped
+functions beyond the register save area (RSA) on the stack in addition
+to argument FPRs.  Do not assume that most significant double in 128
+bit long double value is properly rounded when comparing values.
+
+The AIX calling convention was extended but not initially documented to
+handle an obscure K&R C case of calling a function that takes the
+address of its arguments with fewer arguments than declared.  AIX XL
+compilers access floating point arguments which do not fit in the
+RSA from the stack when a subroutine is compiled without
+optimization.  Because always storing floating-point arguments on the
+stack is inefficient and rarely needed, this option is not enabled by
+default and only is necessary when calling subroutines compiled by AIX
+XL compilers without optimization.
+
+@item -mpe
+@opindex mpe
+Support @dfn{IBM RS/6000 SP} @dfn{Parallel Environment} (PE)@.  Link an
+application written to use message passing with special startup code to
+enable the application to run.  The system must have PE installed in the
+standard location (@file{/usr/lpp/ppe.poe/}), or the @file{specs} file
+must be overridden with the @option{-specs=} option to specify the
+appropriate directory location.  The Parallel Environment does not
+support threads, so the @option{-mpe} option and the @option{-pthread}
+option are incompatible.
+
+@item -malign-natural
+@itemx -malign-power
+@opindex malign-natural
+@opindex malign-power
+On AIX, Darwin, and 64-bit PowerPC GNU/Linux, the option
+@option{-malign-natural} overrides the ABI-defined alignment of larger
+types, such as floating-point doubles, on their natural size-based boundary.
+The option @option{-malign-power} instructs GCC to follow the ABI-specified
+alignment rules.  GCC defaults to the standard alignment defined in the ABI.
+
+@item -msoft-float
+@itemx -mhard-float
+@opindex msoft-float
+@opindex mhard-float
+Generate code that does not use (uses) the floating-point register set.
+Software floating point emulation is provided if you use the
+@option{-msoft-float} option, and pass the option to GCC when linking.
+
+@item -mmultiple
+@itemx -mno-multiple
+@opindex mmultiple
+@opindex mno-multiple
+Generate code that uses (does not use) the load multiple word
+instructions and the store multiple word instructions.  These
+instructions are generated by default on POWER systems, and not
+generated on PowerPC systems.  Do not use @option{-mmultiple} on little
+endian PowerPC systems, since those instructions do not work when the
+processor is in little endian mode.  The exceptions are PPC740 and
+PPC750 which permit the instructions usage in little endian mode.
+
+@item -mstring
+@itemx -mno-string
+@opindex mstring
+@opindex mno-string
+Generate code that uses (does not use) the load string instructions
+and the store string word instructions to save multiple registers and
+do small block moves.  These instructions are generated by default on
+POWER systems, and not generated on PowerPC systems.  Do not use
+@option{-mstring} on little endian PowerPC systems, since those
+instructions do not work when the processor is in little endian mode.
+The exceptions are PPC740 and PPC750 which permit the instructions
+usage in little endian mode.
+
+@item -mupdate
+@itemx -mno-update
+@opindex mupdate
+@opindex mno-update
+Generate code that uses (does not use) the load or store instructions
+that update the base register to the address of the calculated memory
+location.  These instructions are generated by default.  If you use
+@option{-mno-update}, there is a small window between the time that the
+stack pointer is updated and the address of the previous frame is
+stored, which means code that walks the stack frame across interrupts or
+signals may get corrupted data.
+
+@item -mfused-madd
+@itemx -mno-fused-madd
+@opindex mfused-madd
+@opindex mno-fused-madd
+Generate code that uses (does not use) the floating point multiply and
+accumulate instructions.  These instructions are generated by default if
+hardware floating is used.
+
+@item -mno-bit-align
+@itemx -mbit-align
+@opindex mno-bit-align
+@opindex mbit-align
+On System V.4 and embedded PowerPC systems do not (do) force structures
+and unions that contain bit-fields to be aligned to the base type of the
+bit-field.
+
+For example, by default a structure containing nothing but 8
+@code{unsigned} bit-fields of length 1 would be aligned to a 4 byte
+boundary and have a size of 4 bytes.  By using @option{-mno-bit-align},
+the structure would be aligned to a 1 byte boundary and be one byte in
+size.
+
+@item -mno-strict-align
+@itemx -mstrict-align
+@opindex mno-strict-align
+@opindex mstrict-align
+On System V.4 and embedded PowerPC systems do not (do) assume that
+unaligned memory references will be handled by the system.
+
+@item -mrelocatable
+@itemx -mno-relocatable
+@opindex mrelocatable
+@opindex mno-relocatable
+On embedded PowerPC systems generate code that allows (does not allow)
+the program to be relocated to a different address at runtime.  If you
+use @option{-mrelocatable} on any module, all objects linked together must
+be compiled with @option{-mrelocatable} or @option{-mrelocatable-lib}.
+
+@item -mrelocatable-lib
+@itemx -mno-relocatable-lib
+@opindex mrelocatable-lib
+@opindex mno-relocatable-lib
+On embedded PowerPC systems generate code that allows (does not allow)
+the program to be relocated to a different address at runtime.  Modules
+compiled with @option{-mrelocatable-lib} can be linked with either modules
+compiled without @option{-mrelocatable} and @option{-mrelocatable-lib} or
+with modules compiled with the @option{-mrelocatable} options.
+
+@item -mno-toc
+@itemx -mtoc
+@opindex mno-toc
+@opindex mtoc
+On System V.4 and embedded PowerPC systems do not (do) assume that
+register 2 contains a pointer to a global area pointing to the addresses
+used in the program.
+
+@item -mlittle
+@itemx -mlittle-endian
+@opindex mlittle
+@opindex mlittle-endian
+On System V.4 and embedded PowerPC systems compile code for the
+processor in little endian mode.  The @option{-mlittle-endian} option is
+the same as @option{-mlittle}.
+
+@item -mbig
+@itemx -mbig-endian
+@opindex mbig
+@opindex mbig-endian
+On System V.4 and embedded PowerPC systems compile code for the
+processor in big endian mode.  The @option{-mbig-endian} option is
+the same as @option{-mbig}.
+
+@item -mdynamic-no-pic
+@opindex mdynamic-no-pic
+On Darwin and Mac OS X systems, compile code so that it is not
+relocatable, but that its external references are relocatable.  The
+resulting code is suitable for applications, but not shared
+libraries.
+
+@item -mprioritize-restricted-insns=@var{priority}
+@opindex mprioritize-restricted-insns
+This option controls the priority that is assigned to
+dispatch-slot restricted instructions during the second scheduling
+pass.  The argument @var{priority} takes the value @var{0/1/2} to assign
+@var{no/highest/second-highest} priority to dispatch slot restricted
+instructions.
+
+@item -msched-costly-dep=@var{dependence_type}
+@opindex msched-costly-dep
+This option controls which dependences are considered costly
+by the target during instruction scheduling.  The argument
+@var{dependence_type} takes one of the following values:
+@var{no}: no dependence is costly,
+@var{all}: all dependences are costly,
+@var{true_store_to_load}: a true dependence from store to load is costly,
+@var{store_to_load}: any dependence from store to load is costly,
+@var{number}: any dependence which latency >= @var{number} is costly.
+
+@item -minsert-sched-nops=@var{scheme}
+@opindex minsert-sched-nops
+This option controls which nop insertion scheme will be used during
+the second scheduling pass. The argument @var{scheme} takes one of the
+following values:
+@var{no}: Don't insert nops.
+@var{pad}: Pad with nops any dispatch group which has vacant issue slots,
+according to the scheduler's grouping.
+@var{regroup_exact}: Insert nops to force costly dependent insns into
+separate groups.  Insert exactly as many nops as needed to force an insn
+to a new group, according to the estimated processor grouping.
+@var{number}: Insert nops to force costly dependent insns into
+separate groups.  Insert @var{number} nops to force an insn to a new group.
+
+@item -mcall-sysv
+@opindex mcall-sysv
+On System V.4 and embedded PowerPC systems compile code using calling
+conventions that adheres to the March 1995 draft of the System V
+Application Binary Interface, PowerPC processor supplement.  This is the
+default unless you configured GCC using @samp{powerpc-*-eabiaix}.
+
+@item -mcall-sysv-eabi
+@opindex mcall-sysv-eabi
+Specify both @option{-mcall-sysv} and @option{-meabi} options.
+
+@item -mcall-sysv-noeabi
+@opindex mcall-sysv-noeabi
+Specify both @option{-mcall-sysv} and @option{-mno-eabi} options.
+
+@item -mcall-solaris
+@opindex mcall-solaris
+On System V.4 and embedded PowerPC systems compile code for the Solaris
+operating system.
+
+@item -mcall-linux
+@opindex mcall-linux
+On System V.4 and embedded PowerPC systems compile code for the
+Linux-based GNU system.
+
+@item -mcall-gnu
+@opindex mcall-gnu
+On System V.4 and embedded PowerPC systems compile code for the
+Hurd-based GNU system.
+
+@item -mcall-netbsd
+@opindex mcall-netbsd
+On System V.4 and embedded PowerPC systems compile code for the
+NetBSD operating system.
+
+@item -maix-struct-return
+@opindex maix-struct-return
+Return all structures in memory (as specified by the AIX ABI)@.
+
+@item -msvr4-struct-return
+@opindex msvr4-struct-return
+Return structures smaller than 8 bytes in registers (as specified by the
+SVR4 ABI)@.
+
+@item -mabi=altivec
+@opindex mabi=altivec
+Extend the current ABI with AltiVec ABI extensions.  This does not
+change the default ABI, instead it adds the AltiVec ABI extensions to
+the current ABI@.
+
+@item -mabi=no-altivec
+@opindex mabi=no-altivec
+Disable AltiVec ABI extensions for the current ABI.
+
+@item -mprototype
+@itemx -mno-prototype
+@opindex mprototype
+@opindex mno-prototype
+On System V.4 and embedded PowerPC systems assume that all calls to
+variable argument functions are properly prototyped.  Otherwise, the
+compiler must insert an instruction before every non prototyped call to
+set or clear bit 6 of the condition code register (@var{CR}) to
+indicate whether floating point values were passed in the floating point
+registers in case the function takes a variable arguments.  With
+@option{-mprototype}, only calls to prototyped variable argument functions
+will set or clear the bit.
+
+@item -msim
+@opindex msim
+On embedded PowerPC systems, assume that the startup module is called
+@file{sim-crt0.o} and that the standard C libraries are @file{libsim.a} and
+@file{libc.a}.  This is the default for @samp{powerpc-*-eabisim}.
+configurations.
+
+@item -mmvme
+@opindex mmvme
+On embedded PowerPC systems, assume that the startup module is called
+@file{crt0.o} and the standard C libraries are @file{libmvme.a} and
+@file{libc.a}.
+
+@item -mads
+@opindex mads
+On embedded PowerPC systems, assume that the startup module is called
+@file{crt0.o} and the standard C libraries are @file{libads.a} and
+@file{libc.a}.
+
+@item -myellowknife
+@opindex myellowknife
+On embedded PowerPC systems, assume that the startup module is called
+@file{crt0.o} and the standard C libraries are @file{libyk.a} and
+@file{libc.a}.
+
+@item -mvxworks
+@opindex mvxworks
+On System V.4 and embedded PowerPC systems, specify that you are
+compiling for a VxWorks system.
+
+@item -mwindiss
+@opindex mwindiss
+Specify that you are compiling for the WindISS simulation environment.
+
+@item -memb
+@opindex memb
+On embedded PowerPC systems, set the @var{PPC_EMB} bit in the ELF flags
+header to indicate that @samp{eabi} extended relocations are used.
+
+@item -meabi
+@itemx -mno-eabi
+@opindex meabi
+@opindex mno-eabi
+On System V.4 and embedded PowerPC systems do (do not) adhere to the
+Embedded Applications Binary Interface (eabi) which is a set of
+modifications to the System V.4 specifications.  Selecting @option{-meabi}
+means that the stack is aligned to an 8 byte boundary, a function
+@code{__eabi} is called to from @code{main} to set up the eabi
+environment, and the @option{-msdata} option can use both @code{r2} and
+@code{r13} to point to two separate small data areas.  Selecting
+@option{-mno-eabi} means that the stack is aligned to a 16 byte boundary,
+do not call an initialization function from @code{main}, and the
+@option{-msdata} option will only use @code{r13} to point to a single
+small data area.  The @option{-meabi} option is on by default if you
+configured GCC using one of the @samp{powerpc*-*-eabi*} options.
+
+@item -msdata=eabi
+@opindex msdata=eabi
+On System V.4 and embedded PowerPC systems, put small initialized
+@code{const} global and static data in the @samp{.sdata2} section, which
+is pointed to by register @code{r2}.  Put small initialized
+non-@code{const} global and static data in the @samp{.sdata} section,
+which is pointed to by register @code{r13}.  Put small uninitialized
+global and static data in the @samp{.sbss} section, which is adjacent to
+the @samp{.sdata} section.  The @option{-msdata=eabi} option is
+incompatible with the @option{-mrelocatable} option.  The
+@option{-msdata=eabi} option also sets the @option{-memb} option.
+
+@item -msdata=sysv
+@opindex msdata=sysv
+On System V.4 and embedded PowerPC systems, put small global and static
+data in the @samp{.sdata} section, which is pointed to by register
+@code{r13}.  Put small uninitialized global and static data in the
+@samp{.sbss} section, which is adjacent to the @samp{.sdata} section.
+The @option{-msdata=sysv} option is incompatible with the
+@option{-mrelocatable} option.
+
+@item -msdata=default
+@itemx -msdata
+@opindex msdata=default
+@opindex msdata
+On System V.4 and embedded PowerPC systems, if @option{-meabi} is used,
+compile code the same as @option{-msdata=eabi}, otherwise compile code the
+same as @option{-msdata=sysv}.
+
+@item -msdata-data
+@opindex msdata-data
+On System V.4 and embedded PowerPC systems, put small global and static
+data in the @samp{.sdata} section.  Put small uninitialized global and
+static data in the @samp{.sbss} section.  Do not use register @code{r13}
+to address small data however.  This is the default behavior unless
+other @option{-msdata} options are used.
+
+@item -msdata=none
+@itemx -mno-sdata
+@opindex msdata=none
+@opindex mno-sdata
+On embedded PowerPC systems, put all initialized global and static data
+in the @samp{.data} section, and all uninitialized data in the
+@samp{.bss} section.
+
+@item -G @var{num}
+@opindex G
+@cindex smaller data references (PowerPC)
+@cindex .sdata/.sdata2 references (PowerPC)
+On embedded PowerPC systems, put global and static items less than or
+equal to @var{num} bytes into the small data or bss sections instead of
+the normal data or bss section.  By default, @var{num} is 8.  The
+@option{-G @var{num}} switch is also passed to the linker.
+All modules should be compiled with the same @option{-G @var{num}} value.
+
+@item -mregnames
+@itemx -mno-regnames
+@opindex mregnames
+@opindex mno-regnames
+On System V.4 and embedded PowerPC systems do (do not) emit register
+names in the assembly language output using symbolic forms.
+
+@item -mlongcall
+@itemx -mno-longcall
+@opindex mlongcall
+@opindex mno-longcall
+Default to making all function calls via pointers, so that functions
+which reside further than 64 megabytes (67,108,864 bytes) from the
+current location can be called.  This setting can be overridden by the
+@code{shortcall} function attribute, or by @code{#pragma longcall(0)}.
+
+Some linkers are capable of detecting out-of-range calls and generating
+glue code on the fly.  On these systems, long calls are unnecessary and
+generate slower code.  As of this writing, the AIX linker can do this,
+as can the GNU linker for PowerPC/64.  It is planned to add this feature
+to the GNU linker for 32-bit PowerPC systems as well.
+
+On Mach-O (Darwin) systems, this option directs the compiler emit to
+the glue for every direct call, and the Darwin linker decides whether
+to use or discard it.
+
+In the future, we may cause GCC to ignore all longcall specifications
+when the linker is known to generate glue.
+
+@item -pthread
+@opindex pthread
+Adds support for multithreading with the @dfn{pthreads} library.
+This option sets flags for both the preprocessor and linker.
+
+@end table
+
+@node Darwin Options
+@subsection Darwin Options
+@cindex Darwin options
+
+These options are defined for all architectures running the Darwin operating
+system.  They are useful for compatibility with other Mac OS compilers.
+
+@table @gcctabopt
+@item -all_load
+@opindex all_load
+Loads all members of static archive libraries.
+See man ld(1) for more information.
+
+@item -arch_errors_fatal
+@opindex arch_errors_fatal
+Cause the errors having to do with files that have the wrong architecture
+to be fatal.
+
+@item -bind_at_load
+@opindex bind_at_load
+Causes the output file to be marked such that the dynamic linker will
+bind all undefined references when the file is loaded or launched.
+
+@item -bundle
+@opindex bundle
+Produce a Mach-o bundle format file.
+See man ld(1) for more information.
+
+@item -bundle_loader @var{executable}
+@opindex bundle_loader
+This specifies the @var{executable} that will be loading the build
+output file being linked. See man ld(1) for more information.
+
+@item -allowable_client  @var{client_name}
+@itemx -arch_only
+
+@itemx -client_name
+@itemx -compatibility_version
+@itemx -current_version
+@itemx -dependency-file
+@itemx -dylib_file
+@itemx -dylinker_install_name
+@itemx -dynamic
+@itemx -dynamiclib
+@itemx -exported_symbols_list
+@itemx -filelist
+@itemx -flat_namespace
+@itemx -force_cpusubtype_ALL
+@itemx -force_flat_namespace
+@itemx -headerpad_max_install_names
+@itemx -image_base
+@itemx -init
+@itemx -install_name
+@itemx -keep_private_externs
+@itemx -multi_module
+@itemx -multiply_defined
+@itemx -multiply_defined_unused
+@itemx -noall_load
+@itemx -nofixprebinding
+@itemx -nomultidefs
+@itemx -noprebind
+@itemx -noseglinkedit
+@itemx -pagezero_size
+@itemx -prebind
+@itemx -prebind_all_twolevel_modules
+@itemx -private_bundle
+@itemx -read_only_relocs
+@itemx -sectalign
+@itemx -sectobjectsymbols
+@itemx -whyload
+@itemx -seg1addr
+@itemx -sectcreate
+@itemx -sectobjectsymbols
+@itemx -sectorder
+@itemx -seg_addr_table
+@itemx -seg_addr_table_filename
+@itemx -seglinkedit
+@itemx -segprot
+@itemx -segs_read_only_addr
+@itemx -segs_read_write_addr
+@itemx -single_module
+@itemx -static
+@itemx -sub_library
+@itemx -sub_umbrella
+@itemx -twolevel_namespace
+@itemx -umbrella
+@itemx -undefined
+@itemx -unexported_symbols_list
+@itemx -weak_reference_mismatches
+@itemx -whatsloaded
+
+@opindex allowable_client
+@opindex arch_only
+@opindex client_name
+@opindex compatibility_version
+@opindex current_version
+@opindex dependency-file
+@opindex dylib_file
+@opindex dylinker_install_name
+@opindex dynamic
+@opindex dynamiclib
+@opindex exported_symbols_list
+@opindex filelist
+@opindex flat_namespace
+@opindex force_cpusubtype_ALL
+@opindex force_flat_namespace
+@opindex headerpad_max_install_names
+@opindex image_base
+@opindex init
+@opindex install_name
+@opindex keep_private_externs
+@opindex multi_module
+@opindex multiply_defined
+@opindex multiply_defined_unused
+@opindex noall_load
+@opindex nofixprebinding
+@opindex nomultidefs
+@opindex noprebind
+@opindex noseglinkedit
+@opindex pagezero_size
+@opindex prebind
+@opindex prebind_all_twolevel_modules
+@opindex private_bundle
+@opindex read_only_relocs
+@opindex sectalign
+@opindex sectobjectsymbols
+@opindex whyload
+@opindex seg1addr
+@opindex sectcreate
+@opindex sectobjectsymbols
+@opindex sectorder
+@opindex seg_addr_table
+@opindex seg_addr_table_filename
+@opindex seglinkedit
+@opindex segprot
+@opindex segs_read_only_addr
+@opindex segs_read_write_addr
+@opindex single_module
+@opindex static
+@opindex sub_library
+@opindex sub_umbrella
+@opindex twolevel_namespace
+@opindex umbrella
+@opindex undefined
+@opindex unexported_symbols_list
+@opindex weak_reference_mismatches
+@opindex whatsloaded
+
+These options are available for Darwin linker. Darwin linker man page
+describes them in detail.
+@end table
+
+
+@node MIPS Options
+@subsection MIPS Options
+@cindex MIPS options
+
+@table @gcctabopt
+
+@item -EB
+@opindex EB
+Generate big-endian code.
+
+@item -EL
+@opindex EL
+Generate little-endian code.  This is the default for @samp{mips*el-*-*}
+configurations.
+
+@item -march=@var{arch}
+@opindex march
+Generate code that will run on @var{arch}, which can be the name of a
+generic MIPS ISA, or the name of a particular processor.
+The ISA names are:
+@samp{mips1}, @samp{mips2}, @samp{mips3}, @samp{mips4},
+@samp{mips32}, @samp{mips32r2}, and @samp{mips64}.
+The processor names are:
+@samp{4kc}, @samp{4kp}, @samp{5kc}, @samp{20kc},
+@samp{m4k},
+@samp{r2000}, @samp{r3000}, @samp{r3900}, @samp{r4000}, @samp{r4400},
+@samp{r4600}, @samp{r4650}, @samp{r6000}, @samp{r8000}, @samp{rm7000},
+@samp{rm9000},
+@samp{orion},
+@samp{sb1},
+@samp{vr4100}, @samp{vr4111}, @samp{vr4120}, @samp{vr4300},
+@samp{vr5000}, @samp{vr5400} and @samp{vr5500}.
+The special value @samp{from-abi} selects the
+most compatible architecture for the selected ABI (that is,
+@samp{mips1} for 32-bit ABIs and @samp{mips3} for 64-bit ABIs)@.
+
+In processor names, a final @samp{000} can be abbreviated as @samp{k}
+(for example, @samp{-march=r2k}).  Prefixes are optional, and
+@samp{vr} may be written @samp{r}.
+
+GCC defines two macros based on the value of this option.  The first
+is @samp{_MIPS_ARCH}, which gives the name of target architecture, as
+a string.  The second has the form @samp{_MIPS_ARCH_@var{foo}},
+where @var{foo} is the capitalized value of @samp{_MIPS_ARCH}@.
+For example, @samp{-march=r2000} will set @samp{_MIPS_ARCH}
+to @samp{"r2000"} and define the macro @samp{_MIPS_ARCH_R2000}.
+
+Note that the @samp{_MIPS_ARCH} macro uses the processor names given
+above.  In other words, it will have the full prefix and will not
+abbreviate @samp{000} as @samp{k}.  In the case of @samp{from-abi},
+the macro names the resolved architecture (either @samp{"mips1"} or
+@samp{"mips3"}).  It names the default architecture when no
+@option{-march} option is given.
+
+@item -mtune=@var{arch}
+@opindex mtune
+Optimize for @var{arch}.  Among other things, this option controls
+the way instructions are scheduled, and the perceived cost of arithmetic
+operations.  The list of @var{arch} values is the same as for
+@option{-march}.
+
+When this option is not used, GCC will optimize for the processor
+specified by @option{-march}.  By using @option{-march} and
+@option{-mtune} together, it is possible to generate code that will
+run on a family of processors, but optimize the code for one
+particular member of that family.
+
+@samp{-mtune} defines the macros @samp{_MIPS_TUNE} and
+@samp{_MIPS_TUNE_@var{foo}}, which work in the same way as the
+@samp{-march} ones described above.
+
+@item -mips1
+@opindex mips1
+Equivalent to @samp{-march=mips1}.
+
+@item -mips2
+@opindex mips2
+Equivalent to @samp{-march=mips2}.
+
+@item -mips3
+@opindex mips3
+Equivalent to @samp{-march=mips3}.
+
+@item -mips4
+@opindex mips4
+Equivalent to @samp{-march=mips4}.
+
+@item -mips32
+@opindex mips32
+Equivalent to @samp{-march=mips32}.
+
+@item -mips32r2
+@opindex mips32r2
+Equivalent to @samp{-march=mips32r2}.
+
+@item -mips64
+@opindex mips64
+Equivalent to @samp{-march=mips64}.
+
+@item -mips16
+@itemx -mno-mips16
+@opindex mips16
+@opindex mno-mips16
+Use (do not use) the MIPS16 ISA.
+
+@item -mabi=32
+@itemx -mabi=o64
+@itemx -mabi=n32
+@itemx -mabi=64
+@itemx -mabi=eabi
+@opindex mabi=32
+@opindex mabi=o64
+@opindex mabi=n32
+@opindex mabi=64
+@opindex mabi=eabi
+Generate code for the given ABI@.
+
+Note that the EABI has a 32-bit and a 64-bit variant.  GCC normally
+generates 64-bit code when you select a 64-bit architecture, but you
+can use @option{-mgp32} to get 32-bit code instead.
+
+@item -mabicalls
+@itemx -mno-abicalls
+@opindex mabicalls
+@opindex mno-abicalls
+Generate (do not generate) SVR4-style position-independent code.
+@option{-mabicalls} is the default for SVR4-based systems.
+
+@item -mxgot
+@itemx -mno-xgot
+@opindex mxgot
+@opindex mno-xgot
+Lift (do not lift) the usual restrictions on the size of the global
+offset table.
+
+GCC normally uses a single instruction to load values from the GOT.
+While this is relatively efficient, it will only work if the GOT
+is smaller than about 64k.  Anything larger will cause the linker
+to report an error such as:
+
+@cindex relocation truncated to fit (MIPS)
+@smallexample
+relocation truncated to fit: R_MIPS_GOT16 foobar
+@end smallexample
+
+If this happens, you should recompile your code with @option{-mxgot}.
+It should then work with very large GOTs, although it will also be
+less efficient, since it will take three instructions to fetch the
+value of a global symbol.
+
+Note that some linkers can create multiple GOTs.  If you have such a
+linker, you should only need to use @option{-mxgot} when a single object
+file accesses more than 64k's worth of GOT entries.  Very few do.
+
+These options have no effect unless GCC is generating position
+independent code.
+
+@item -membedded-pic
+@itemx -mno-embedded-pic
+@opindex membedded-pic
+@opindex mno-embedded-pic
+Generate (do not generate) position-independent code suitable for some
+embedded systems.  All calls are made using PC relative addresses, and
+all data is addressed using the $gp register.  No more than 65536
+bytes of global data may be used.  This requires GNU as and GNU ld,
+which do most of the work.
+
+@item -mgp32
+@opindex mgp32
+Assume that general-purpose registers are 32 bits wide.
+
+@item -mgp64
+@opindex mgp64
+Assume that general-purpose registers are 64 bits wide.
+
+@item -mfp32
+@opindex mfp32
+Assume that floating-point registers are 32 bits wide.
+
+@item -mfp64
+@opindex mfp64
+Assume that floating-point registers are 64 bits wide.
+
+@item -mhard-float
+@opindex mhard-float
+Use floating-point coprocessor instructions.
+
+@item -msoft-float
+@opindex msoft-float
+Do not use floating-point coprocessor instructions.  Implement
+floating-point calculations using library calls instead.
+
+@item -msingle-float
+@opindex msingle-float
+Assume that the floating-point coprocessor only supports single-precision
+operations.
+
+@itemx -mdouble-float
+@opindex mdouble-float
+Assume that the floating-point coprocessor supports double-precision
+operations.  This is the default.
+
+@item -mint64
+@opindex mint64
+Force @code{int} and @code{long} types to be 64 bits wide.  See
+@option{-mlong32} for an explanation of the default and the way
+that the pointer size is determined.
+
+@item -mlong64
+@opindex mlong64
+Force @code{long} types to be 64 bits wide.  See @option{-mlong32} for
+an explanation of the default and the way that the pointer size is
+determined.
+
+@item -mlong32
+@opindex mlong32
+Force @code{long}, @code{int}, and pointer types to be 32 bits wide.
+
+The default size of @code{int}s, @code{long}s and pointers depends on
+the ABI@.  All the supported ABIs use 32-bit @code{int}s.  The n64 ABI
+uses 64-bit @code{long}s, as does the 64-bit EABI; the others use
+32-bit @code{long}s.  Pointers are the same size as @code{long}s,
+or the same size as integer registers, whichever is smaller.
+
+@item -G @var{num}
+@opindex G
+@cindex smaller data references (MIPS)
+@cindex gp-relative references (MIPS)
+Put global and static items less than or equal to @var{num} bytes into
+the small data or bss section instead of the normal data or bss section.
+This allows the data to be accessed using a single instruction.
+
+All modules should be compiled with the same @option{-G @var{num}}
+value.
+
+@item -membedded-data
+@itemx -mno-embedded-data
+@opindex membedded-data
+@opindex mno-embedded-data
+Allocate variables to the read-only data section first if possible, then
+next in the small data section if possible, otherwise in data.  This gives
+slightly slower code than the default, but reduces the amount of RAM required
+when executing, and thus may be preferred for some embedded systems.
+
+@item -muninit-const-in-rodata
+@itemx -mno-uninit-const-in-rodata
+@opindex muninit-const-in-rodata
+@opindex mno-uninit-const-in-rodata
+Put uninitialized @code{const} variables in the read-only data section.
+This option is only meaningful in conjunction with @option{-membedded-data}.
+
+@item -msplit-addresses
+@itemx -mno-split-addresses
+@opindex msplit-addresses
+@opindex mno-split-addresses
+Enable (disable) use of the @code{%hi()} and @code{%lo()} assembler
+relocation operators.  This option has been superceded by
+@option{-mexplicit-relocs} but is retained for backwards compatibility.
+
+@item -mexplicit-relocs
+@itemx -mno-explicit-relocs
+@opindex mexplicit-relocs
+@opindex mno-explicit-relocs
+Use (do not use) assembler relocation operators when dealing with symbolic
+addresses.  The alternative, selected by @option{-mno-explicit-relocs},
+is to use assembler macros instead.
+
+@option{-mexplicit-relocs} is usually the default if GCC was
+configured to use an assembler that supports relocation operators.
+However, there are two exceptions:
+
+@itemize @bullet
+@item
+GCC is not yet able to generate explicit relocations for the combination
+of @option{-mabi=64} and @option{-mno-abicalls}.  This will be addressed
+in a future release.
+
+@item
+The combination of @option{-mabicalls} and @option{-fno-unit-at-a-time}
+implies @option{-mno-explicit-relocs} unless explicitly overridden.
+This is because, when generating abicalls, the choice of relocation
+depends on whether a symbol is local or global.  In some rare cases,
+GCC will not be able to decide this until the whole compilation unit
+has been read.
+@end itemize
+
+@item -mrnames
+@itemx -mno-rnames
+@opindex mrnames
+@opindex mno-rnames
+Generate (do not generate) code that refers to registers using their
+software names.  The default is @option{-mno-rnames}, which tells GCC
+to use hardware names like @samp{$4} instead of software names like
+@samp{a0}.  The only assembler known to support @option{-rnames} is
+the Algorithmics assembler.
+
+@item -mcheck-zero-division
+@itemx -mno-check-zero-division
+@opindex mcheck-zero-division
+@opindex mno-check-zero-division
+Trap (do not trap) on integer division by zero.  The default is
+@option{-mcheck-zero-division}.
+
+@item -mmemcpy
+@itemx -mno-memcpy
+@opindex mmemcpy
+@opindex mno-memcpy
+Force (do not force) the use of @code{memcpy()} for non-trivial block
+moves.  The default is @option{-mno-memcpy}, which allows GCC to inline
+most constant-sized copies.
+
+@item -mlong-calls
+@itemx -mno-long-calls
+@opindex mlong-calls
+@opindex mno-long-calls
+Disable (do not disable) use of the @code{jal} instruction.  Calling
+functions using @code{jal} is more efficient but requires the caller
+and callee to be in the same 256 megabyte segment.
+
+This option has no effect on abicalls code.  The default is
+@option{-mno-long-calls}.
+
+@item -mmad
+@itemx -mno-mad
+@opindex mmad
+@opindex mno-mad
+Enable (disable) use of the @code{mad}, @code{madu} and @code{mul}
+instructions, as provided by the R4650 ISA.
+
+@item -mfused-madd
+@itemx -mno-fused-madd
+@opindex mfused-madd
+@opindex mno-fused-madd
+Enable (disable) use of the floating point multiply-accumulate
+instructions, when they are available.  The default is
+@option{-mfused-madd}.
+
+When multiply-accumulate instructions are used, the intermediate
+product is calculated to infinite precision and is not subject to
+the FCSR Flush to Zero bit.  This may be undesirable in some
+circumstances.
+
+@item -nocpp
+@opindex nocpp
+Tell the MIPS assembler to not run its preprocessor over user
+assembler files (with a @samp{.s} suffix) when assembling them.
+
+@item -mfix-sb1
+@itemx -mno-fix-sb1
+@opindex mfix-sb1
+Work around certain SB-1 CPU core errata.
+(This flag currently works around the SB-1 revision 2
+``F1'' and ``F2'' floating point errata.)
+
+@item -mflush-func=@var{func}
+@itemx -mno-flush-func
+@opindex mflush-func
+Specifies the function to call to flush the I and D caches, or to not
+call any such function.  If called, the function must take the same
+arguments as the common @code{_flush_func()}, that is, the address of the
+memory range for which the cache is being flushed, the size of the
+memory range, and the number 3 (to flush both caches).  The default
+depends on the target GCC was configured for, but commonly is either
+@samp{_flush_func} or @samp{__cpu_flush}.
+
+@item -mbranch-likely
+@itemx -mno-branch-likely
+@opindex mbranch-likely
+@opindex mno-branch-likely
+Enable or disable use of Branch Likely instructions, regardless of the
+default for the selected architecture.  By default, Branch Likely
+instructions may be generated if they are supported by the selected
+architecture.  An exception is for the MIPS32 and MIPS64 architectures
+and processors which implement those architectures; for those, Branch
+Likely instructions will not be generated by default because the MIPS32
+and MIPS64 architectures specifically deprecate their use.
+@end table
+
+@node i386 and x86-64 Options
+@subsection Intel 386 and AMD x86-64 Options
+@cindex i386 Options
+@cindex x86-64 Options
+@cindex Intel 386 Options
+@cindex AMD x86-64 Options
+
+These @samp{-m} options are defined for the i386 and x86-64 family of
+computers:
+
+@table @gcctabopt
+@item -mtune=@var{cpu-type}
+@opindex mtune
+Tune to @var{cpu-type} everything applicable about the generated code, except
+for the ABI and the set of available instructions.  The choices for
+@var{cpu-type} are:
+@table @emph
+@item i386
+Original Intel's i386 CPU.
+@item i486
+Intel's i486 CPU.  (No scheduling is implemented for this chip.)
+@item i586, pentium
+Intel Pentium CPU with no MMX support.
+@item pentium-mmx
+Intel PentiumMMX CPU based on Pentium core with MMX instruction set support.
+@item i686, pentiumpro
+Intel PentiumPro CPU.
+@item pentium2
+Intel Pentium2 CPU based on PentiumPro core with MMX instruction set support.
+@item pentium3, pentium3m
+Intel Pentium3 CPU based on PentiumPro core with MMX and SSE instruction set
+support.
+@item pentium-m
+Low power version of Intel Pentium3 CPU with MMX, SSE and SSE2 instruction set
+support.  Used by Centrino notebooks.
+@item pentium4, pentium4m
+Intel Pentium4 CPU with MMX, SSE and SSE2 instruction set support.
+@item prescott
+Improved version of Intel Pentium4 CPU with MMX, SSE, SSE2 and SSE3 instruction
+set support.
+@item nocona
+Improved version of Intel Pentium4 CPU with 64-bit extensions, MMX, SSE,
+SSE2 and SSE3 instruction set support.
+@item k6
+AMD K6 CPU with MMX instruction set support.
+@item k6-2, k6-3
+Improved versions of AMD K6 CPU with MMX and 3dNOW! instruction set support.
+@item athlon, athlon-tbird
+AMD Athlon CPU with MMX, 3dNOW!, enhanced 3dNOW! and SSE prefetch instructions
+support.
+@item athlon-4, athlon-xp, athlon-mp
+Improved AMD Athlon CPU with MMX, 3dNOW!, enhanced 3dNOW! and full SSE
+instruction set support.
+@item k8, opteron, athlon64, athlon-fx
+AMD K8 core based CPUs with x86-64 instruction set support.  (This supersets
+MMX, SSE, SSE2, 3dNOW!, enhanced 3dNOW! and 64-bit instruction set extensions.)
+@item winchip-c6
+IDT Winchip C6 CPU, dealt in same way as i486 with additional MMX instruction
+set support.
+@item winchip2
+IDT Winchip2 CPU, dealt in same way as i486 with additional MMX and 3dNOW!
+instruction set support.
+@item c3
+Via C3 CPU with MMX and 3dNOW!  instruction set support.  (No scheduling is
+implemented for this chip.)
+@item c3-2
+Via C3-2 CPU with MMX and SSE instruction set support.  (No scheduling is
+implemented for this chip.)
+@end table
+
+While picking a specific @var{cpu-type} will schedule things appropriately
+for that particular chip, the compiler will not generate any code that
+does not run on the i386 without the @option{-march=@var{cpu-type}} option
+being used.
+
+@item -march=@var{cpu-type}
+@opindex march
+Generate instructions for the machine type @var{cpu-type}.  The choices
+for @var{cpu-type} are the same as for @option{-mtune}.  Moreover,
+specifying @option{-march=@var{cpu-type}} implies @option{-mtune=@var{cpu-type}}.
+
+@item -mcpu=@var{cpu-type}
+@opindex mcpu
+A deprecated synonym for @option{-mtune}.
+
+@item -m386
+@itemx -m486
+@itemx -mpentium
+@itemx -mpentiumpro
+@opindex m386
+@opindex m486
+@opindex mpentium
+@opindex mpentiumpro
+These options are synonyms for @option{-mtune=i386}, @option{-mtune=i486},
+@option{-mtune=pentium}, and @option{-mtune=pentiumpro} respectively.
+These synonyms are deprecated.
+
+@item -mfpmath=@var{unit}
+@opindex march
+Generate floating point arithmetics for selected unit @var{unit}.  The choices
+for @var{unit} are:
+
+@table @samp
+@item 387
+Use the standard 387 floating point coprocessor present majority of chips and
+emulated otherwise.  Code compiled with this option will run almost everywhere.
+The temporary results are computed in 80bit precision instead of precision
+specified by the type resulting in slightly different results compared to most
+of other chips. See @option{-ffloat-store} for more detailed description.
+
+This is the default choice for i386 compiler.
+
+@item sse
+Use scalar floating point instructions present in the SSE instruction set.
+This instruction set is supported by Pentium3 and newer chips, in the AMD line
+by Athlon-4, Athlon-xp and Athlon-mp chips.  The earlier version of SSE
+instruction set supports only single precision arithmetics, thus the double and
+extended precision arithmetics is still done using 387.  Later version, present
+only in Pentium4 and the future AMD x86-64 chips supports double precision
+arithmetics too.
+
+For i387 you need to use @option{-march=@var{cpu-type}}, @option{-msse} or
+@option{-msse2} switches to enable SSE extensions and make this option
+effective.  For x86-64 compiler, these extensions are enabled by default.
+
+The resulting code should be considerably faster in the majority of cases and avoid
+the numerical instability problems of 387 code, but may break some existing
+code that expects temporaries to be 80bit.
+
+This is the default choice for the x86-64 compiler.
+
+@item sse,387
+Attempt to utilize both instruction sets at once.  This effectively double the
+amount of available registers and on chips with separate execution units for
+387 and SSE the execution resources too.  Use this option with care, as it is
+still experimental, because the GCC register allocator does not model separate
+functional units well resulting in instable performance.
+@end table
+
+@item -masm=@var{dialect}
+@opindex masm=@var{dialect}
+Output asm instructions using selected @var{dialect}. Supported choices are
+@samp{intel} or @samp{att} (the default one).
+
+@item -mieee-fp
+@itemx -mno-ieee-fp
+@opindex mieee-fp
+@opindex mno-ieee-fp
+Control whether or not the compiler uses IEEE floating point
+comparisons.  These handle correctly the case where the result of a
+comparison is unordered.
+
+@item -msoft-float
+@opindex msoft-float
+Generate output containing library calls for floating point.
+@strong{Warning:} the requisite libraries are not part of GCC@.
+Normally the facilities of the machine's usual C compiler are used, but
+this can't be done directly in cross-compilation.  You must make your
+own arrangements to provide suitable library functions for
+cross-compilation.
+
+On machines where a function returns floating point results in the 80387
+register stack, some floating point opcodes may be emitted even if
+@option{-msoft-float} is used.
+
+@item -mno-fp-ret-in-387
+@opindex mno-fp-ret-in-387
+Do not use the FPU registers for return values of functions.
+
+The usual calling convention has functions return values of types
+@code{float} and @code{double} in an FPU register, even if there
+is no FPU@.  The idea is that the operating system should emulate
+an FPU@.
+
+The option @option{-mno-fp-ret-in-387} causes such values to be returned
+in ordinary CPU registers instead.
+
+@item -mno-fancy-math-387
+@opindex mno-fancy-math-387
+Some 387 emulators do not support the @code{sin}, @code{cos} and
+@code{sqrt} instructions for the 387.  Specify this option to avoid
+generating those instructions.  This option is the default on FreeBSD,
+OpenBSD and NetBSD@.  This option is overridden when @option{-march}
+indicates that the target cpu will always have an FPU and so the
+instruction will not need emulation.  As of revision 2.6.1, these
+instructions are not generated unless you also use the
+@option{-funsafe-math-optimizations} switch.
+
+@item -malign-double
+@itemx -mno-align-double
+@opindex malign-double
+@opindex mno-align-double
+Control whether GCC aligns @code{double}, @code{long double}, and
+@code{long long} variables on a two word boundary or a one word
+boundary.  Aligning @code{double} variables on a two word boundary will
+produce code that runs somewhat faster on a @samp{Pentium} at the
+expense of more memory.
+
+@strong{Warning:} if you use the @option{-malign-double} switch,
+structures containing the above types will be aligned differently than
+the published application binary interface specifications for the 386
+and will not be binary compatible with structures in code compiled
+without that switch.
+
+@item -m96bit-long-double
+@itemx -m128bit-long-double
+@opindex m96bit-long-double
+@opindex m128bit-long-double
+These switches control the size of @code{long double} type. The i386
+application binary interface specifies the size to be 96 bits,
+so @option{-m96bit-long-double} is the default in 32 bit mode.
+
+Modern architectures (Pentium and newer) would prefer @code{long double}
+to be aligned to an 8 or 16 byte boundary.  In arrays or structures
+conforming to the ABI, this would not be possible.  So specifying a
+@option{-m128bit-long-double} will align @code{long double}
+to a 16 byte boundary by padding the @code{long double} with an additional
+32 bit zero.
+
+In the x86-64 compiler, @option{-m128bit-long-double} is the default choice as
+its ABI specifies that @code{long double} is to be aligned on 16 byte boundary.
+
+Notice that neither of these options enable any extra precision over the x87
+standard of 80 bits for a @code{long double}.
+
+@strong{Warning:} if you override the default value for your target ABI, the
+structures and arrays containing @code{long double} variables will change
+their size as well as function calling convention for function taking
+@code{long double} will be modified.  Hence they will not be binary
+compatible with arrays or structures in code compiled without that switch.
+
+
+@item -msvr3-shlib
+@itemx -mno-svr3-shlib
+@opindex msvr3-shlib
+@opindex mno-svr3-shlib
+Control whether GCC places uninitialized local variables into the
+@code{bss} or @code{data} segments.  @option{-msvr3-shlib} places them
+into @code{bss}.  These options are meaningful only on System V Release 3.
+
+@item -mrtd
+@opindex mrtd
+Use a different function-calling convention, in which functions that
+take a fixed number of arguments return with the @code{ret} @var{num}
+instruction, which pops their arguments while returning.  This saves one
+instruction in the caller since there is no need to pop the arguments
+there.
+
+You can specify that an individual function is called with this calling
+sequence with the function attribute @samp{stdcall}.  You can also
+override the @option{-mrtd} option by using the function attribute
+@samp{cdecl}.  @xref{Function Attributes}.
+
+@strong{Warning:} this calling convention is incompatible with the one
+normally used on Unix, so you cannot use it if you need to call
+libraries compiled with the Unix compiler.
+
+Also, you must provide function prototypes for all functions that
+take variable numbers of arguments (including @code{printf});
+otherwise incorrect code will be generated for calls to those
+functions.
+
+In addition, seriously incorrect code will result if you call a
+function with too many arguments.  (Normally, extra arguments are
+harmlessly ignored.)
+
+@item -mregparm=@var{num}
+@opindex mregparm
+Control how many registers are used to pass integer arguments.  By
+default, no registers are used to pass arguments, and at most 3
+registers can be used.  You can control this behavior for a specific
+function by using the function attribute @samp{regparm}.
+@xref{Function Attributes}.
+
+@strong{Warning:} if you use this switch, and
+@var{num} is nonzero, then you must build all modules with the same
+value, including any libraries.  This includes the system libraries and
+startup modules.
+
+@item -mpreferred-stack-boundary=@var{num}
+@opindex mpreferred-stack-boundary
+Attempt to keep the stack boundary aligned to a 2 raised to @var{num}
+byte boundary.  If @option{-mpreferred-stack-boundary} is not specified,
+the default is 4 (16 bytes or 128 bits), except when optimizing for code
+size (@option{-Os}), in which case the default is the minimum correct
+alignment (4 bytes for x86, and 8 bytes for x86-64).
+
+On Pentium and PentiumPro, @code{double} and @code{long double} values
+should be aligned to an 8 byte boundary (see @option{-malign-double}) or
+suffer significant run time performance penalties.  On Pentium III, the
+Streaming SIMD Extension (SSE) data type @code{__m128} suffers similar
+penalties if it is not 16 byte aligned.
+
+To ensure proper alignment of this values on the stack, the stack boundary
+must be as aligned as that required by any value stored on the stack.
+Further, every function must be generated such that it keeps the stack
+aligned.  Thus calling a function compiled with a higher preferred
+stack boundary from a function compiled with a lower preferred stack
+boundary will most likely misalign the stack.  It is recommended that
+libraries that use callbacks always use the default setting.
+
+This extra alignment does consume extra stack space, and generally
+increases code size.  Code that is sensitive to stack space usage, such
+as embedded systems and operating system kernels, may want to reduce the
+preferred alignment to @option{-mpreferred-stack-boundary=2}.
+
+@item -mmmx
+@itemx -mno-mmx
+@item -msse
+@itemx -mno-sse
+@item -msse2
+@itemx -mno-sse2
+@item -msse3
+@itemx -mno-sse3
+@item -m3dnow
+@itemx -mno-3dnow
+@opindex mmmx
+@opindex mno-mmx
+@opindex msse
+@opindex mno-sse
+@opindex m3dnow
+@opindex mno-3dnow
+These switches enable or disable the use of built-in functions that allow
+direct access to the MMX, SSE, SSE2, SSE3 and 3Dnow extensions of the
+instruction set.
+
+@xref{X86 Built-in Functions}, for details of the functions enabled
+and disabled by these switches.
+
+To have SSE/SSE2 instructions generated automatically from floating-point
+code, see @option{-mfpmath=sse}.
+
+@item -mpush-args
+@itemx -mno-push-args
+@opindex mpush-args
+@opindex mno-push-args
+Use PUSH operations to store outgoing parameters.  This method is shorter
+and usually equally fast as method using SUB/MOV operations and is enabled
+by default.  In some cases disabling it may improve performance because of
+improved scheduling and reduced dependencies.
+
+@item -maccumulate-outgoing-args
+@opindex maccumulate-outgoing-args
+If enabled, the maximum amount of space required for outgoing arguments will be
+computed in the function prologue.  This is faster on most modern CPUs
+because of reduced dependencies, improved scheduling and reduced stack usage
+when preferred stack boundary is not equal to 2.  The drawback is a notable
+increase in code size.  This switch implies @option{-mno-push-args}.
+
+@item -mthreads
+@opindex mthreads
+Support thread-safe exception handling on @samp{Mingw32}.  Code that relies
+on thread-safe exception handling must compile and link all code with the
+@option{-mthreads} option.  When compiling, @option{-mthreads} defines
+@option{-D_MT}; when linking, it links in a special thread helper library
+@option{-lmingwthrd} which cleans up per thread exception handling data.
+
+@item -mno-align-stringops
+@opindex mno-align-stringops
+Do not align destination of inlined string operations.  This switch reduces
+code size and improves performance in case the destination is already aligned,
+but GCC doesn't know about it.
+
+@item -minline-all-stringops
+@opindex minline-all-stringops
+By default GCC inlines string operations only when destination is known to be
+aligned at least to 4 byte boundary.  This enables more inlining, increase code
+size, but may improve performance of code that depends on fast memcpy, strlen
+and memset for short lengths.
+
+@item -momit-leaf-frame-pointer
+@opindex momit-leaf-frame-pointer
+Don't keep the frame pointer in a register for leaf functions.  This
+avoids the instructions to save, set up and restore frame pointers and
+makes an extra register available in leaf functions.  The option
+@option{-fomit-frame-pointer} removes the frame pointer for all functions
+which might make debugging harder.
+
+@item -mtls-direct-seg-refs
+@itemx -mno-tls-direct-seg-refs
+@opindex mtls-direct-seg-refs
+Controls whether TLS variables may be accessed with offsets from the
+TLS segment register (@code{%gs} for 32-bit, @code{%fs} for 64-bit),
+or whether the thread base pointer must be added.  Whether or not this
+is legal depends on the operating system, and whether it maps the
+segment to cover the entire TLS area.
+
+For systems that use GNU libc, the default is on.
+@end table
+
+These @samp{-m} switches are supported in addition to the above
+on AMD x86-64 processors in 64-bit environments.
+
+@table @gcctabopt
+@item -m32
+@itemx -m64
+@opindex m32
+@opindex m64
+Generate code for a 32-bit or 64-bit environment.
+The 32-bit environment sets int, long and pointer to 32 bits and
+generates code that runs on any i386 system.
+The 64-bit environment sets int to 32 bits and long and pointer
+to 64 bits and generates code for AMD's x86-64 architecture.
+
+@item -mno-red-zone
+@opindex no-red-zone
+Do not use a so called red zone for x86-64 code.  The red zone is mandated
+by the x86-64 ABI, it is a 128-byte area beyond the location of the
+stack pointer that will not be modified by signal or interrupt handlers
+and therefore can be used for temporary data without adjusting the stack
+pointer.  The flag @option{-mno-red-zone} disables this red zone.
+
+@item -mcmodel=small
+@opindex mcmodel=small
+Generate code for the small code model: the program and its symbols must
+be linked in the lower 2 GB of the address space.  Pointers are 64 bits.
+Programs can be statically or dynamically linked.  This is the default
+code model.
+
+@item -mcmodel=kernel
+@opindex mcmodel=kernel
+Generate code for the kernel code model.  The kernel runs in the
+negative 2 GB of the address space.
+This model has to be used for Linux kernel code.
+
+@item -mcmodel=medium
+@opindex mcmodel=medium
+Generate code for the medium model: The program is linked in the lower 2
+GB of the address space but symbols can be located anywhere in the
+address space.  Programs can be statically or dynamically linked, but
+building of shared libraries are not supported with the medium model.
+
+@item -mcmodel=large
+@opindex mcmodel=large
+Generate code for the large model: This model makes no assumptions
+about addresses and sizes of sections.  Currently GCC does not implement
+this model.
+@end table
+
+@node HPPA Options
+@subsection HPPA Options
+@cindex HPPA Options
+
+These @samp{-m} options are defined for the HPPA family of computers:
+
+@table @gcctabopt
+@item -march=@var{architecture-type}
+@opindex march
+Generate code for the specified architecture.  The choices for
+@var{architecture-type} are @samp{1.0} for PA 1.0, @samp{1.1} for PA
+1.1, and @samp{2.0} for PA 2.0 processors.  Refer to
+@file{/usr/lib/sched.models} on an HP-UX system to determine the proper
+architecture option for your machine.  Code compiled for lower numbered
+architectures will run on higher numbered architectures, but not the
+other way around.
+
+PA 2.0 support currently requires gas snapshot 19990413 or later.  The
+next release of binutils (current is 2.9.1) will probably contain PA 2.0
+support.
+
+@item -mpa-risc-1-0
+@itemx -mpa-risc-1-1
+@itemx -mpa-risc-2-0
+@opindex mpa-risc-1-0
+@opindex mpa-risc-1-1
+@opindex mpa-risc-2-0
+Synonyms for @option{-march=1.0}, @option{-march=1.1}, and @option{-march=2.0} respectively.
+
+@item -mbig-switch
+@opindex mbig-switch
+Generate code suitable for big switch tables.  Use this option only if
+the assembler/linker complain about out of range branches within a switch
+table.
+
+@item -mjump-in-delay
+@opindex mjump-in-delay
+Fill delay slots of function calls with unconditional jump instructions
+by modifying the return pointer for the function call to be the target
+of the conditional jump.
+
+@item -mdisable-fpregs
+@opindex mdisable-fpregs
+Prevent floating point registers from being used in any manner.  This is
+necessary for compiling kernels which perform lazy context switching of
+floating point registers.  If you use this option and attempt to perform
+floating point operations, the compiler will abort.
+
+@item -mdisable-indexing
+@opindex mdisable-indexing
+Prevent the compiler from using indexing address modes.  This avoids some
+rather obscure problems when compiling MIG generated code under MACH@.
+
+@item -mno-space-regs
+@opindex mno-space-regs
+Generate code that assumes the target has no space registers.  This allows
+GCC to generate faster indirect calls and use unscaled index address modes.
+
+Such code is suitable for level 0 PA systems and kernels.
+
+@item -mfast-indirect-calls
+@opindex mfast-indirect-calls
+Generate code that assumes calls never cross space boundaries.  This
+allows GCC to emit code which performs faster indirect calls.
+
+This option will not work in the presence of shared libraries or nested
+functions.
+
+@item -mlong-load-store
+@opindex mlong-load-store
+Generate 3-instruction load and store sequences as sometimes required by
+the HP-UX 10 linker.  This is equivalent to the @samp{+k} option to
+the HP compilers.
+
+@item -mportable-runtime
+@opindex mportable-runtime
+Use the portable calling conventions proposed by HP for ELF systems.
+
+@item -mgas
+@opindex mgas
+Enable the use of assembler directives only GAS understands.
+
+@item -mschedule=@var{cpu-type}
+@opindex mschedule
+Schedule code according to the constraints for the machine type
+@var{cpu-type}.  The choices for @var{cpu-type} are @samp{700}
+@samp{7100}, @samp{7100LC}, @samp{7200}, @samp{7300} and @samp{8000}.  Refer
+to @file{/usr/lib/sched.models} on an HP-UX system to determine the
+proper scheduling option for your machine.  The default scheduling is
+@samp{8000}.
+
+@item -mlinker-opt
+@opindex mlinker-opt
+Enable the optimization pass in the HP-UX linker.  Note this makes symbolic
+debugging impossible.  It also triggers a bug in the HP-UX 8 and HP-UX 9
+linkers in which they give bogus error messages when linking some programs.
+
+@item -msoft-float
+@opindex msoft-float
+Generate output containing library calls for floating point.
+@strong{Warning:} the requisite libraries are not available for all HPPA
+targets.  Normally the facilities of the machine's usual C compiler are
+used, but this cannot be done directly in cross-compilation.  You must make
+your own arrangements to provide suitable library functions for
+cross-compilation.  The embedded target @samp{hppa1.1-*-pro}
+does provide software floating point support.
+
+@option{-msoft-float} changes the calling convention in the output file;
+therefore, it is only useful if you compile @emph{all} of a program with
+this option.  In particular, you need to compile @file{libgcc.a}, the
+library that comes with GCC, with @option{-msoft-float} in order for
+this to work.
+
+@item -msio
+@opindex msio
+Generate the predefine, @code{_SIO}, for server IO.  The default is
+@option{-mwsio}.  This generates the predefines, @code{__hp9000s700},
+@code{__hp9000s700__} and @code{_WSIO}, for workstation IO.  These
+options are available under HP-UX and HI-UX.
+
+@item -mgnu-ld
+@opindex gnu-ld
+Use GNU ld specific options.  This passes @option{-shared} to ld when
+building a shared library.  It is the default when GCC is configured,
+explicitly or implicitly, with the GNU linker.  This option does not
+have any affect on which ld is called, it only changes what parameters
+are passed to that ld.  The ld that is called is determined by the
+@option{--with-ld} configure option, GCC's program search path, and
+finally by the user's @env{PATH}.  The linker used by GCC can be printed
+using @samp{which `gcc -print-prog-name=ld`}.
+
+@item -mhp-ld
+@opindex hp-ld
+Use HP ld specific options.  This passes @option{-b} to ld when building
+a shared library and passes @option{+Accept TypeMismatch} to ld on all
+links.  It is the default when GCC is configured, explicitly or
+implicitly, with the HP linker.  This option does not have any affect on
+which ld is called, it only changes what parameters are passed to that
+ld.  The ld that is called is determined by the @option{--with-ld}
+configure option, GCC's program search path, and finally by the user's
+@env{PATH}.  The linker used by GCC can be printed using @samp{which
+`gcc -print-prog-name=ld`}.
+
+@item -mlong-calls
+@opindex mno-long-calls
+Generate code that uses long call sequences.  This ensures that a call
+is always able to reach linker generated stubs.  The default is to generate
+long calls only when the distance from the call site to the beginning
+of the function or translation unit, as the case may be, exceeds a
+predefined limit set by the branch type being used.  The limits for
+normal calls are 7,600,000 and 240,000 bytes, respectively for the
+PA 2.0 and PA 1.X architectures.  Sibcalls are always limited at
+240,000 bytes.
+
+Distances are measured from the beginning of functions when using the
+@option{-ffunction-sections} option, or when using the @option{-mgas}
+and @option{-mno-portable-runtime} options together under HP-UX with
+the SOM linker.
+
+It is normally not desirable to use this option as it will degrade
+performance.  However, it may be useful in large applications,
+particularly when partial linking is used to build the application.
+
+The types of long calls used depends on the capabilities of the
+assembler and linker, and the type of code being generated.  The
+impact on systems that support long absolute calls, and long pic
+symbol-difference or pc-relative calls should be relatively small.
+However, an indirect call is used on 32-bit ELF systems in pic code
+and it is quite long.
+
+@item -nolibdld
+@opindex nolibdld
+Suppress the generation of link options to search libdld.sl when the
+@option{-static} option is specified on HP-UX 10 and later.
+
+@item -static
+@opindex static
+The HP-UX implementation of setlocale in libc has a dependency on
+libdld.sl.  There isn't an archive version of libdld.sl.  Thus,
+when the @option{-static} option is specified, special link options
+are needed to resolve this dependency.
+
+On HP-UX 10 and later, the GCC driver adds the necessary options to
+link with libdld.sl when the @option{-static} option is specified.
+This causes the resulting binary to be dynamic.  On the 64-bit port,
+the linkers generate dynamic binaries by default in any case.  The
+@option{-nolibdld} option can be used to prevent the GCC driver from
+adding these link options.
+
+@item -threads
+@opindex threads
+Add support for multithreading with the @dfn{dce thread} library
+under HP-UX.  This option sets flags for both the preprocessor and
+linker.
+@end table
+
+@node Intel 960 Options
+@subsection Intel 960 Options
+
+These @samp{-m} options are defined for the Intel 960 implementations:
+
+@table @gcctabopt
+@item -m@var{cpu-type}
+@opindex mka
+@opindex mkb
+@opindex mmc
+@opindex mca
+@opindex mcf
+@opindex msa
+@opindex msb
+Assume the defaults for the machine type @var{cpu-type} for some of
+the other options, including instruction scheduling, floating point
+support, and addressing modes.  The choices for @var{cpu-type} are
+@samp{ka}, @samp{kb}, @samp{mc}, @samp{ca}, @samp{cf},
+@samp{sa}, and @samp{sb}.
+The default is
+@samp{kb}.
+
+@item -mnumerics
+@itemx -msoft-float
+@opindex mnumerics
+@opindex msoft-float
+The @option{-mnumerics} option indicates that the processor does support
+floating-point instructions.  The @option{-msoft-float} option indicates
+that floating-point support should not be assumed.
+
+@item -mleaf-procedures
+@itemx -mno-leaf-procedures
+@opindex mleaf-procedures
+@opindex mno-leaf-procedures
+Do (or do not) attempt to alter leaf procedures to be callable with the
+@code{bal} instruction as well as @code{call}.  This will result in more
+efficient code for explicit calls when the @code{bal} instruction can be
+substituted by the assembler or linker, but less efficient code in other
+cases, such as calls via function pointers, or using a linker that doesn't
+support this optimization.
+
+@item -mtail-call
+@itemx -mno-tail-call
+@opindex mtail-call
+@opindex mno-tail-call
+Do (or do not) make additional attempts (beyond those of the
+machine-independent portions of the compiler) to optimize tail-recursive
+calls into branches.  You may not want to do this because the detection of
+cases where this is not valid is not totally complete.  The default is
+@option{-mno-tail-call}.
+
+@item -mcomplex-addr
+@itemx -mno-complex-addr
+@opindex mcomplex-addr
+@opindex mno-complex-addr
+Assume (or do not assume) that the use of a complex addressing mode is a
+win on this implementation of the i960.  Complex addressing modes may not
+be worthwhile on the K-series, but they definitely are on the C-series.
+The default is currently @option{-mcomplex-addr} for all processors except
+the CB and CC@.
+
+@item -mcode-align
+@itemx -mno-code-align
+@opindex mcode-align
+@opindex mno-code-align
+Align code to 8-byte boundaries for faster fetching (or don't bother).
+Currently turned on by default for C-series implementations only.
+
+@ignore
+@item -mclean-linkage
+@itemx -mno-clean-linkage
+@opindex mclean-linkage
+@opindex mno-clean-linkage
+These options are not fully implemented.
+@end ignore
+
+@item -mic-compat
+@itemx -mic2.0-compat
+@itemx -mic3.0-compat
+@opindex mic-compat
+@opindex mic2.0-compat
+@opindex mic3.0-compat
+Enable compatibility with iC960 v2.0 or v3.0.
+
+@item -masm-compat
+@itemx -mintel-asm
+@opindex masm-compat
+@opindex mintel-asm
+Enable compatibility with the iC960 assembler.
+
+@item -mstrict-align
+@itemx -mno-strict-align
+@opindex mstrict-align
+@opindex mno-strict-align
+Do not permit (do permit) unaligned accesses.
+
+@item -mold-align
+@opindex mold-align
+Enable structure-alignment compatibility with Intel's gcc release version
+1.3 (based on gcc 1.37).  This option implies @option{-mstrict-align}.
+
+@item -mlong-double-64
+@opindex mlong-double-64
+Implement type @samp{long double} as 64-bit floating point numbers.
+Without the option @samp{long double} is implemented by 80-bit
+floating point numbers.  The only reason we have it because there is
+no 128-bit @samp{long double} support in @samp{fp-bit.c} yet.  So it
+is only useful for people using soft-float targets.  Otherwise, we
+should recommend against use of it.
+
+@end table
+
+@node DEC Alpha Options
+@subsection DEC Alpha Options
+
+These @samp{-m} options are defined for the DEC Alpha implementations:
+
+@table @gcctabopt
+@item -mno-soft-float
+@itemx -msoft-float
+@opindex mno-soft-float
+@opindex msoft-float
+Use (do not use) the hardware floating-point instructions for
+floating-point operations.  When @option{-msoft-float} is specified,
+functions in @file{libgcc.a} will be used to perform floating-point
+operations.  Unless they are replaced by routines that emulate the
+floating-point operations, or compiled in such a way as to call such
+emulations routines, these routines will issue floating-point
+operations.   If you are compiling for an Alpha without floating-point
+operations, you must ensure that the library is built so as not to call
+them.
+
+Note that Alpha implementations without floating-point operations are
+required to have floating-point registers.
+
+@item -mfp-reg
+@itemx -mno-fp-regs
+@opindex mfp-reg
+@opindex mno-fp-regs
+Generate code that uses (does not use) the floating-point register set.
+@option{-mno-fp-regs} implies @option{-msoft-float}.  If the floating-point
+register set is not used, floating point operands are passed in integer
+registers as if they were integers and floating-point results are passed
+in @code{$0} instead of @code{$f0}.  This is a non-standard calling sequence,
+so any function with a floating-point argument or return value called by code
+compiled with @option{-mno-fp-regs} must also be compiled with that
+option.
+
+A typical use of this option is building a kernel that does not use,
+and hence need not save and restore, any floating-point registers.
+
+@item -mieee
+@opindex mieee
+The Alpha architecture implements floating-point hardware optimized for
+maximum performance.  It is mostly compliant with the IEEE floating
+point standard.  However, for full compliance, software assistance is
+required.  This option generates code fully IEEE compliant code
+@emph{except} that the @var{inexact-flag} is not maintained (see below).
+If this option is turned on, the preprocessor macro @code{_IEEE_FP} is
+defined during compilation.  The resulting code is less efficient but is
+able to correctly support denormalized numbers and exceptional IEEE
+values such as not-a-number and plus/minus infinity.  Other Alpha
+compilers call this option @option{-ieee_with_no_inexact}.
+
+@item -mieee-with-inexact
+@opindex mieee-with-inexact
+This is like @option{-mieee} except the generated code also maintains
+the IEEE @var{inexact-flag}.  Turning on this option causes the
+generated code to implement fully-compliant IEEE math.  In addition to
+@code{_IEEE_FP}, @code{_IEEE_FP_EXACT} is defined as a preprocessor
+macro.  On some Alpha implementations the resulting code may execute
+significantly slower than the code generated by default.  Since there is
+very little code that depends on the @var{inexact-flag}, you should
+normally not specify this option.  Other Alpha compilers call this
+option @option{-ieee_with_inexact}.
+
+@item -mfp-trap-mode=@var{trap-mode}
+@opindex mfp-trap-mode
+This option controls what floating-point related traps are enabled.
+Other Alpha compilers call this option @option{-fptm @var{trap-mode}}.
+The trap mode can be set to one of four values:
+
+@table @samp
+@item n
+This is the default (normal) setting.  The only traps that are enabled
+are the ones that cannot be disabled in software (e.g., division by zero
+trap).
+
+@item u
+In addition to the traps enabled by @samp{n}, underflow traps are enabled
+as well.
+
+@item su
+Like @samp{su}, but the instructions are marked to be safe for software
+completion (see Alpha architecture manual for details).
+
+@item sui
+Like @samp{su}, but inexact traps are enabled as well.
+@end table
+
+@item -mfp-rounding-mode=@var{rounding-mode}
+@opindex mfp-rounding-mode
+Selects the IEEE rounding mode.  Other Alpha compilers call this option
+@option{-fprm @var{rounding-mode}}.  The @var{rounding-mode} can be one
+of:
+
+@table @samp
+@item n
+Normal IEEE rounding mode.  Floating point numbers are rounded towards
+the nearest machine number or towards the even machine number in case
+of a tie.
+
+@item m
+Round towards minus infinity.
+
+@item c
+Chopped rounding mode.  Floating point numbers are rounded towards zero.
+
+@item d
+Dynamic rounding mode.  A field in the floating point control register
+(@var{fpcr}, see Alpha architecture reference manual) controls the
+rounding mode in effect.  The C library initializes this register for
+rounding towards plus infinity.  Thus, unless your program modifies the
+@var{fpcr}, @samp{d} corresponds to round towards plus infinity.
+@end table
+
+@item -mtrap-precision=@var{trap-precision}
+@opindex mtrap-precision
+In the Alpha architecture, floating point traps are imprecise.  This
+means without software assistance it is impossible to recover from a
+floating trap and program execution normally needs to be terminated.
+GCC can generate code that can assist operating system trap handlers
+in determining the exact location that caused a floating point trap.
+Depending on the requirements of an application, different levels of
+precisions can be selected:
+
+@table @samp
+@item p
+Program precision.  This option is the default and means a trap handler
+can only identify which program caused a floating point exception.
+
+@item f
+Function precision.  The trap handler can determine the function that
+caused a floating point exception.
+
+@item i
+Instruction precision.  The trap handler can determine the exact
+instruction that caused a floating point exception.
+@end table
+
+Other Alpha compilers provide the equivalent options called
+@option{-scope_safe} and @option{-resumption_safe}.
+
+@item -mieee-conformant
+@opindex mieee-conformant
+This option marks the generated code as IEEE conformant.  You must not
+use this option unless you also specify @option{-mtrap-precision=i} and either
+@option{-mfp-trap-mode=su} or @option{-mfp-trap-mode=sui}.  Its only effect
+is to emit the line @samp{.eflag 48} in the function prologue of the
+generated assembly file.  Under DEC Unix, this has the effect that
+IEEE-conformant math library routines will be linked in.
+
+@item -mbuild-constants
+@opindex mbuild-constants
+Normally GCC examines a 32- or 64-bit integer constant to
+see if it can construct it from smaller constants in two or three
+instructions.  If it cannot, it will output the constant as a literal and
+generate code to load it from the data segment at runtime.
+
+Use this option to require GCC to construct @emph{all} integer constants
+using code, even if it takes more instructions (the maximum is six).
+
+You would typically use this option to build a shared library dynamic
+loader.  Itself a shared library, it must relocate itself in memory
+before it can find the variables and constants in its own data segment.
+
+@item -malpha-as
+@itemx -mgas
+@opindex malpha-as
+@opindex mgas
+Select whether to generate code to be assembled by the vendor-supplied
+assembler (@option{-malpha-as}) or by the GNU assembler @option{-mgas}.
+
+@item -mbwx
+@itemx -mno-bwx
+@itemx -mcix
+@itemx -mno-cix
+@itemx -mfix
+@itemx -mno-fix
+@itemx -mmax
+@itemx -mno-max
+@opindex mbwx
+@opindex mno-bwx
+@opindex mcix
+@opindex mno-cix
+@opindex mfix
+@opindex mno-fix
+@opindex mmax
+@opindex mno-max
+Indicate whether GCC should generate code to use the optional BWX,
+CIX, FIX and MAX instruction sets.  The default is to use the instruction
+sets supported by the CPU type specified via @option{-mcpu=} option or that
+of the CPU on which GCC was built if none was specified.
+
+@item -mfloat-vax
+@itemx -mfloat-ieee
+@opindex mfloat-vax
+@opindex mfloat-ieee
+Generate code that uses (does not use) VAX F and G floating point
+arithmetic instead of IEEE single and double precision.
+
+@item -mexplicit-relocs
+@itemx -mno-explicit-relocs
+@opindex mexplicit-relocs
+@opindex mno-explicit-relocs
+Older Alpha assemblers provided no way to generate symbol relocations
+except via assembler macros.  Use of these macros does not allow
+optimal instruction scheduling.  GNU binutils as of version 2.12
+supports a new syntax that allows the compiler to explicitly mark
+which relocations should apply to which instructions.  This option
+is mostly useful for debugging, as GCC detects the capabilities of
+the assembler when it is built and sets the default accordingly.
+
+@item -msmall-data
+@itemx -mlarge-data
+@opindex msmall-data
+@opindex mlarge-data
+When @option{-mexplicit-relocs} is in effect, static data is
+accessed via @dfn{gp-relative} relocations.  When @option{-msmall-data}
+is used, objects 8 bytes long or smaller are placed in a @dfn{small data area}
+(the @code{.sdata} and @code{.sbss} sections) and are accessed via
+16-bit relocations off of the @code{$gp} register.  This limits the
+size of the small data area to 64KB, but allows the variables to be
+directly accessed via a single instruction.
+
+The default is @option{-mlarge-data}.  With this option the data area
+is limited to just below 2GB.  Programs that require more than 2GB of
+data must use @code{malloc} or @code{mmap} to allocate the data in the
+heap instead of in the program's data segment.
+
+When generating code for shared libraries, @option{-fpic} implies
+@option{-msmall-data} and @option{-fPIC} implies @option{-mlarge-data}.
+
+@item -msmall-text
+@itemx -mlarge-text
+@opindex msmall-text
+@opindex mlarge-text
+When @option{-msmall-text} is used, the compiler assumes that the
+code of the entire program (or shared library) fits in 4MB, and is
+thus reachable with a branch instruction.  When @option{-msmall-data}
+is used, the compiler can assume that all local symbols share the
+same @code{$gp} value, and thus reduce the number of instructions
+required for a function call from 4 to 1.
+
+The default is @option{-mlarge-text}.
+
+@item -mcpu=@var{cpu_type}
+@opindex mcpu
+Set the instruction set and instruction scheduling parameters for
+machine type @var{cpu_type}.  You can specify either the @samp{EV}
+style name or the corresponding chip number.  GCC supports scheduling
+parameters for the EV4, EV5 and EV6 family of processors and will
+choose the default values for the instruction set from the processor
+you specify.  If you do not specify a processor type, GCC will default
+to the processor on which the compiler was built.
+
+Supported values for @var{cpu_type} are
+
+@table @samp
+@item ev4
+@itemx ev45
+@itemx 21064
+Schedules as an EV4 and has no instruction set extensions.
+
+@item ev5
+@itemx 21164
+Schedules as an EV5 and has no instruction set extensions.
+
+@item ev56
+@itemx 21164a
+Schedules as an EV5 and supports the BWX extension.
+
+@item pca56
+@itemx 21164pc
+@itemx 21164PC
+Schedules as an EV5 and supports the BWX and MAX extensions.
+
+@item ev6
+@itemx 21264
+Schedules as an EV6 and supports the BWX, FIX, and MAX extensions.
+
+@item ev67
+@itemx 21264a
+Schedules as an EV6 and supports the BWX, CIX, FIX, and MAX extensions.
+@end table
+
+@item -mtune=@var{cpu_type}
+@opindex mtune
+Set only the instruction scheduling parameters for machine type
+@var{cpu_type}.  The instruction set is not changed.
+
+@item -mmemory-latency=@var{time}
+@opindex mmemory-latency
+Sets the latency the scheduler should assume for typical memory
+references as seen by the application.  This number is highly
+dependent on the memory access patterns used by the application
+and the size of the external cache on the machine.
+
+Valid options for @var{time} are
+
+@table @samp
+@item @var{number}
+A decimal number representing clock cycles.
+
+@item L1
+@itemx L2
+@itemx L3
+@itemx main
+The compiler contains estimates of the number of clock cycles for
+``typical'' EV4 & EV5 hardware for the Level 1, 2 & 3 caches
+(also called Dcache, Scache, and Bcache), as well as to main memory.
+Note that L3 is only valid for EV5.
+
+@end table
+@end table
+
+@node DEC Alpha/VMS Options
+@subsection DEC Alpha/VMS Options
+
+These @samp{-m} options are defined for the DEC Alpha/VMS implementations:
+
+@table @gcctabopt
+@item -mvms-return-codes
+@opindex mvms-return-codes
+Return VMS condition codes from main.  The default is to return POSIX
+style condition (e.g.@ error) codes.
+@end table
+
+@node H8/300 Options
+@subsection H8/300 Options
+
+These @samp{-m} options are defined for the H8/300 implementations:
+
+@table @gcctabopt
+@item -mrelax
+@opindex mrelax
+Shorten some address references at link time, when possible; uses the
+linker option @option{-relax}.  @xref{H8/300,, @code{ld} and the H8/300,
+ld, Using ld}, for a fuller description.
+
+@item -mh
+@opindex mh
+Generate code for the H8/300H@.
+
+@item -ms
+@opindex ms
+Generate code for the H8S@.
+
+@item -mn
+@opindex mn
+Generate code for the H8S and H8/300H in the normal mode.  This switch
+must be used either with -mh or -ms.
+
+@item -ms2600
+@opindex ms2600
+Generate code for the H8S/2600.  This switch must be used with @option{-ms}.
+
+@item -mint32
+@opindex mint32
+Make @code{int} data 32 bits by default.
+
+@item -malign-300
+@opindex malign-300
+On the H8/300H and H8S, use the same alignment rules as for the H8/300.
+The default for the H8/300H and H8S is to align longs and floats on 4
+byte boundaries.
+@option{-malign-300} causes them to be aligned on 2 byte boundaries.
+This option has no effect on the H8/300.
+@end table
+
+@node SH Options
+@subsection SH Options
+
+These @samp{-m} options are defined for the SH implementations:
+
+@table @gcctabopt
+@item -m1
+@opindex m1
+Generate code for the SH1.
+
+@item -m2
+@opindex m2
+Generate code for the SH2.
+
+@item -m2e
+Generate code for the SH2e.
+
+@item -m3
+@opindex m3
+Generate code for the SH3.
+
+@item -m3e
+@opindex m3e
+Generate code for the SH3e.
+
+@item -m4-nofpu
+@opindex m4-nofpu
+Generate code for the SH4 without a floating-point unit.
+
+@item -m4-single-only
+@opindex m4-single-only
+Generate code for the SH4 with a floating-point unit that only
+supports single-precision arithmetic.
+
+@item -m4-single
+@opindex m4-single
+Generate code for the SH4 assuming the floating-point unit is in
+single-precision mode by default.
+
+@item -m4
+@opindex m4
+Generate code for the SH4.
+
+@item -mb
+@opindex mb
+Compile code for the processor in big endian mode.
+
+@item -ml
+@opindex ml
+Compile code for the processor in little endian mode.
+
+@item -mdalign
+@opindex mdalign
+Align doubles at 64-bit boundaries.  Note that this changes the calling
+conventions, and thus some functions from the standard C library will
+not work unless you recompile it first with @option{-mdalign}.
+
+@item -mrelax
+@opindex mrelax
+Shorten some address references at link time, when possible; uses the
+linker option @option{-relax}.
+
+@item -mbigtable
+@opindex mbigtable
+Use 32-bit offsets in @code{switch} tables.  The default is to use
+16-bit offsets.
+
+@item -mfmovd
+@opindex mfmovd
+Enable the use of the instruction @code{fmovd}.
+
+@item -mhitachi
+@opindex mhitachi
+Comply with the calling conventions defined by Renesas.
+
+@item -mnomacsave
+@opindex mnomacsave
+Mark the @code{MAC} register as call-clobbered, even if
+@option{-mhitachi} is given.
+
+@item -mieee
+@opindex mieee
+Increase IEEE-compliance of floating-point code.
+
+@item -misize
+@opindex misize
+Dump instruction size and location in the assembly code.
+
+@item -mpadstruct
+@opindex mpadstruct
+This option is deprecated.  It pads structures to multiple of 4 bytes,
+which is incompatible with the SH ABI@.
+
+@item -mspace
+@opindex mspace
+Optimize for space instead of speed.  Implied by @option{-Os}.
+
+@item -mprefergot
+@opindex mprefergot
+When generating position-independent code, emit function calls using
+the Global Offset Table instead of the Procedure Linkage Table.
+
+@item -musermode
+@opindex musermode
+Generate a library function call to invalidate instruction cache
+entries, after fixing up a trampoline.  This library function call
+doesn't assume it can write to the whole memory address space.  This
+is the default when the target is @code{sh-*-linux*}.
+@end table
+
+@node System V Options
+@subsection Options for System V
+
+These additional options are available on System V Release 4 for
+compatibility with other compilers on those systems:
+
+@table @gcctabopt
+@item -G
+@opindex G
+Create a shared object.
+It is recommended that @option{-symbolic} or @option{-shared} be used instead.
+
+@item -Qy
+@opindex Qy
+Identify the versions of each tool used by the compiler, in a
+@code{.ident} assembler directive in the output.
+
+@item -Qn
+@opindex Qn
+Refrain from adding @code{.ident} directives to the output file (this is
+the default).
+
+@item -YP,@var{dirs}
+@opindex YP
+Search the directories @var{dirs}, and no others, for libraries
+specified with @option{-l}.
+
+@item -Ym,@var{dir}
+@opindex Ym
+Look in the directory @var{dir} to find the M4 preprocessor.
+The assembler uses this option.
+@c This is supposed to go with a -Yd for predefined M4 macro files, but
+@c the generic assembler that comes with Solaris takes just -Ym.
+@end table
+
+@node TMS320C3x/C4x Options
+@subsection TMS320C3x/C4x Options
+@cindex TMS320C3x/C4x Options
+
+These @samp{-m} options are defined for TMS320C3x/C4x implementations:
+
+@table @gcctabopt
+
+@item -mcpu=@var{cpu_type}
+@opindex mcpu
+Set the instruction set, register set, and instruction scheduling
+parameters for machine type @var{cpu_type}.  Supported values for
+@var{cpu_type} are @samp{c30}, @samp{c31}, @samp{c32}, @samp{c40}, and
+@samp{c44}.  The default is @samp{c40} to generate code for the
+TMS320C40.
+
+@item -mbig-memory
+@itemx -mbig
+@itemx -msmall-memory
+@itemx -msmall
+@opindex mbig-memory
+@opindex mbig
+@opindex msmall-memory
+@opindex msmall
+Generates code for the big or small memory model.  The small memory
+model assumed that all data fits into one 64K word page.  At run-time
+the data page (DP) register must be set to point to the 64K page
+containing the .bss and .data program sections.  The big memory model is
+the default and requires reloading of the DP register for every direct
+memory access.
+
+@item -mbk
+@itemx -mno-bk
+@opindex mbk
+@opindex mno-bk
+Allow (disallow) allocation of general integer operands into the block
+count register BK@.
+
+@item -mdb
+@itemx -mno-db
+@opindex mdb
+@opindex mno-db
+Enable (disable) generation of code using decrement and branch,
+DBcond(D), instructions.  This is enabled by default for the C4x.  To be
+on the safe side, this is disabled for the C3x, since the maximum
+iteration count on the C3x is @math{2^{23} + 1} (but who iterates loops more than
+@math{2^{23}} times on the C3x?).  Note that GCC will try to reverse a loop so
+that it can utilize the decrement and branch instruction, but will give
+up if there is more than one memory reference in the loop.  Thus a loop
+where the loop counter is decremented can generate slightly more
+efficient code, in cases where the RPTB instruction cannot be utilized.
+
+@item -mdp-isr-reload
+@itemx -mparanoid
+@opindex mdp-isr-reload
+@opindex mparanoid
+Force the DP register to be saved on entry to an interrupt service
+routine (ISR), reloaded to point to the data section, and restored on
+exit from the ISR@.  This should not be required unless someone has
+violated the small memory model by modifying the DP register, say within
+an object library.
+
+@item -mmpyi
+@itemx -mno-mpyi
+@opindex mmpyi
+@opindex mno-mpyi
+For the C3x use the 24-bit MPYI instruction for integer multiplies
+instead of a library call to guarantee 32-bit results.  Note that if one
+of the operands is a constant, then the multiplication will be performed
+using shifts and adds.  If the @option{-mmpyi} option is not specified for the C3x,
+then squaring operations are performed inline instead of a library call.
+
+@item -mfast-fix
+@itemx -mno-fast-fix
+@opindex mfast-fix
+@opindex mno-fast-fix
+The C3x/C4x FIX instruction to convert a floating point value to an
+integer value chooses the nearest integer less than or equal to the
+floating point value rather than to the nearest integer.  Thus if the
+floating point number is negative, the result will be incorrectly
+truncated an additional code is necessary to detect and correct this
+case.  This option can be used to disable generation of the additional
+code required to correct the result.
+
+@item -mrptb
+@itemx -mno-rptb
+@opindex mrptb
+@opindex mno-rptb
+Enable (disable) generation of repeat block sequences using the RPTB
+instruction for zero overhead looping.  The RPTB construct is only used
+for innermost loops that do not call functions or jump across the loop
+boundaries.  There is no advantage having nested RPTB loops due to the
+overhead required to save and restore the RC, RS, and RE registers.
+This is enabled by default with @option{-O2}.
+
+@item -mrpts=@var{count}
+@itemx -mno-rpts
+@opindex mrpts
+@opindex mno-rpts
+Enable (disable) the use of the single instruction repeat instruction
+RPTS@.  If a repeat block contains a single instruction, and the loop
+count can be guaranteed to be less than the value @var{count}, GCC will
+emit a RPTS instruction instead of a RPTB@.  If no value is specified,
+then a RPTS will be emitted even if the loop count cannot be determined
+at compile time.  Note that the repeated instruction following RPTS does
+not have to be reloaded from memory each iteration, thus freeing up the
+CPU buses for operands.  However, since interrupts are blocked by this
+instruction, it is disabled by default.
+
+@item -mloop-unsigned
+@itemx -mno-loop-unsigned
+@opindex mloop-unsigned
+@opindex mno-loop-unsigned
+The maximum iteration count when using RPTS and RPTB (and DB on the C40)
+is @math{2^{31} + 1} since these instructions test if the iteration count is
+negative to terminate the loop.  If the iteration count is unsigned
+there is a possibility than the @math{2^{31} + 1} maximum iteration count may be
+exceeded.  This switch allows an unsigned iteration count.
+
+@item -mti
+@opindex mti
+Try to emit an assembler syntax that the TI assembler (asm30) is happy
+with.  This also enforces compatibility with the API employed by the TI
+C3x C compiler.  For example, long doubles are passed as structures
+rather than in floating point registers.
+
+@item -mregparm
+@itemx -mmemparm
+@opindex mregparm
+@opindex mmemparm
+Generate code that uses registers (stack) for passing arguments to functions.
+By default, arguments are passed in registers where possible rather
+than by pushing arguments on to the stack.
+
+@item -mparallel-insns
+@itemx -mno-parallel-insns
+@opindex mparallel-insns
+@opindex mno-parallel-insns
+Allow the generation of parallel instructions.  This is enabled by
+default with @option{-O2}.
+
+@item -mparallel-mpy
+@itemx -mno-parallel-mpy
+@opindex mparallel-mpy
+@opindex mno-parallel-mpy
+Allow the generation of MPY||ADD and MPY||SUB parallel instructions,
+provided @option{-mparallel-insns} is also specified.  These instructions have
+tight register constraints which can pessimize the code generation
+of large functions.
+
+@end table
+
+@node V850 Options
+@subsection V850 Options
+@cindex V850 Options
+
+These @samp{-m} options are defined for V850 implementations:
+
+@table @gcctabopt
+@item -mlong-calls
+@itemx -mno-long-calls
+@opindex mlong-calls
+@opindex mno-long-calls
+Treat all calls as being far away (near).  If calls are assumed to be
+far away, the compiler will always load the functions address up into a
+register, and call indirect through the pointer.
+
+@item -mno-ep
+@itemx -mep
+@opindex mno-ep
+@opindex mep
+Do not optimize (do optimize) basic blocks that use the same index
+pointer 4 or more times to copy pointer into the @code{ep} register, and
+use the shorter @code{sld} and @code{sst} instructions.  The @option{-mep}
+option is on by default if you optimize.
+
+@item -mno-prolog-function
+@itemx -mprolog-function
+@opindex mno-prolog-function
+@opindex mprolog-function
+Do not use (do use) external functions to save and restore registers
+at the prologue and epilogue of a function.  The external functions
+are slower, but use less code space if more than one function saves
+the same number of registers.  The @option{-mprolog-function} option
+is on by default if you optimize.
+
+@item -mspace
+@opindex mspace
+Try to make the code as small as possible.  At present, this just turns
+on the @option{-mep} and @option{-mprolog-function} options.
+
+@item -mtda=@var{n}
+@opindex mtda
+Put static or global variables whose size is @var{n} bytes or less into
+the tiny data area that register @code{ep} points to.  The tiny data
+area can hold up to 256 bytes in total (128 bytes for byte references).
+
+@item -msda=@var{n}
+@opindex msda
+Put static or global variables whose size is @var{n} bytes or less into
+the small data area that register @code{gp} points to.  The small data
+area can hold up to 64 kilobytes.
+
+@item -mzda=@var{n}
+@opindex mzda
+Put static or global variables whose size is @var{n} bytes or less into
+the first 32 kilobytes of memory.
+
+@item -mv850
+@opindex mv850
+Specify that the target processor is the V850.
+
+@item -mbig-switch
+@opindex mbig-switch
+Generate code suitable for big switch tables.  Use this option only if
+the assembler/linker complain about out of range branches within a switch
+table.
+
+@item -mapp-regs
+@opindex mapp-regs
+This option will cause r2 and r5 to be used in the code generated by
+the compiler.  This setting is the default.
+
+@item -mno-app-regs
+@opindex mno-app-regs
+This option will cause r2 and r5 to be treated as fixed registers.
+
+@item -mv850e1
+@opindex mv850e1
+Specify that the target processor is the V850E1.  The preprocessor
+constants @samp{__v850e1__} and @samp{__v850e__} will be defined if
+this option is used.
+
+@item -mv850e
+@opindex mv850e
+Specify that the target processor is the V850E.  The preprocessor
+constant @samp{__v850e__} will be defined if this option is used.
+
+If neither @option{-mv850} nor @option{-mv850e} nor @option{-mv850e1}
+are defined then a default target processor will be chosen and the
+relevant @samp{__v850*__} preprocessor constant will be defined.
+
+The preprocessor constants @samp{__v850} and @samp{__v851__} are always
+defined, regardless of which processor variant is the target.
+
+@item -mdisable-callt
+@opindex mdisable-callt
+This option will suppress generation of the CALLT instruction for the
+v850e and v850e1 flavors of the v850 architecture.  The default is
+@option{-mno-disable-callt} which allows the CALLT instruction to be used.
+
+@end table
+
+@node ARC Options
+@subsection ARC Options
+@cindex ARC Options
+
+These options are defined for ARC implementations:
+
+@table @gcctabopt
+@item -EL
+@opindex EL
+Compile code for little endian mode.  This is the default.
+
+@item -EB
+@opindex EB
+Compile code for big endian mode.
+
+@item -mmangle-cpu
+@opindex mmangle-cpu
+Prepend the name of the cpu to all public symbol names.
+In multiple-processor systems, there are many ARC variants with different
+instruction and register set characteristics.  This flag prevents code
+compiled for one cpu to be linked with code compiled for another.
+No facility exists for handling variants that are ``almost identical''.
+This is an all or nothing option.
+
+@item -mcpu=@var{cpu}
+@opindex mcpu
+Compile code for ARC variant @var{cpu}.
+Which variants are supported depend on the configuration.
+All variants support @option{-mcpu=base}, this is the default.
+
+@item -mtext=@var{text-section}
+@itemx -mdata=@var{data-section}
+@itemx -mrodata=@var{readonly-data-section}
+@opindex mtext
+@opindex mdata
+@opindex mrodata
+Put functions, data, and readonly data in @var{text-section},
+@var{data-section}, and @var{readonly-data-section} respectively
+by default.  This can be overridden with the @code{section} attribute.
+@xref{Variable Attributes}.
+
+@end table
+
+@node NS32K Options
+@subsection NS32K Options
+@cindex NS32K options
+
+These are the @samp{-m} options defined for the 32000 series.  The default
+values for these options depends on which style of 32000 was selected when
+the compiler was configured; the defaults for the most common choices are
+given below.
+
+@table @gcctabopt
+@item -m32032
+@itemx -m32032
+@opindex m32032
+@opindex m32032
+Generate output for a 32032.  This is the default
+when the compiler is configured for 32032 and 32016 based systems.
+
+@item -m32332
+@itemx -m32332
+@opindex m32332
+@opindex m32332
+Generate output for a 32332.  This is the default
+when the compiler is configured for 32332-based systems.
+
+@item -m32532
+@itemx -m32532
+@opindex m32532
+@opindex m32532
+Generate output for a 32532.  This is the default
+when the compiler is configured for 32532-based systems.
+
+@item -m32081
+@opindex m32081
+Generate output containing 32081 instructions for floating point.
+This is the default for all systems.
+
+@item -m32381
+@opindex m32381
+Generate output containing 32381 instructions for floating point.  This
+also implies @option{-m32081}.  The 32381 is only compatible with the 32332
+and 32532 cpus.  This is the default for the pc532-netbsd configuration.
+
+@item -mmulti-add
+@opindex mmulti-add
+Try and generate multiply-add floating point instructions @code{polyF}
+and @code{dotF}.  This option is only available if the @option{-m32381}
+option is in effect.  Using these instructions requires changes to
+register allocation which generally has a negative impact on
+performance.  This option should only be enabled when compiling code
+particularly likely to make heavy use of multiply-add instructions.
+
+@item -mnomulti-add
+@opindex mnomulti-add
+Do not try and generate multiply-add floating point instructions
+@code{polyF} and @code{dotF}.  This is the default on all platforms.
+
+@item -msoft-float
+@opindex msoft-float
+Generate output containing library calls for floating point.
+@strong{Warning:} the requisite libraries may not be available.
+
+@item -mieee-compare
+@itemx -mno-ieee-compare
+@opindex mieee-compare
+@opindex mno-ieee-compare
+Control whether or not the compiler uses IEEE floating point
+comparisons.  These handle correctly the case where the result of a
+comparison is unordered.
+@strong{Warning:} the requisite kernel support may not be available.
+
+@item -mnobitfield
+@opindex mnobitfield
+Do not use the bit-field instructions.  On some machines it is faster to
+use shifting and masking operations.  This is the default for the pc532.
+
+@item -mbitfield
+@opindex mbitfield
+Do use the bit-field instructions.  This is the default for all platforms
+except the pc532.
+
+@item -mrtd
+@opindex mrtd
+Use a different function-calling convention, in which functions
+that take a fixed number of arguments return pop their
+arguments on return with the @code{ret} instruction.
+
+This calling convention is incompatible with the one normally
+used on Unix, so you cannot use it if you need to call libraries
+compiled with the Unix compiler.
+
+Also, you must provide function prototypes for all functions that
+take variable numbers of arguments (including @code{printf});
+otherwise incorrect code will be generated for calls to those
+functions.
+
+In addition, seriously incorrect code will result if you call a
+function with too many arguments.  (Normally, extra arguments are
+harmlessly ignored.)
+
+This option takes its name from the 680x0 @code{rtd} instruction.
+
+
+@item -mregparam
+@opindex mregparam
+Use a different function-calling convention where the first two arguments
+are passed in registers.
+
+This calling convention is incompatible with the one normally
+used on Unix, so you cannot use it if you need to call libraries
+compiled with the Unix compiler.
+
+@item -mnoregparam
+@opindex mnoregparam
+Do not pass any arguments in registers.  This is the default for all
+targets.
+
+@item -msb
+@opindex msb
+It is OK to use the sb as an index register which is always loaded with
+zero.  This is the default for the pc532-netbsd target.
+
+@item -mnosb
+@opindex mnosb
+The sb register is not available for use or has not been initialized to
+zero by the run time system.  This is the default for all targets except
+the pc532-netbsd.  It is also implied whenever @option{-mhimem} or
+@option{-fpic} is set.
+
+@item -mhimem
+@opindex mhimem
+Many ns32000 series addressing modes use displacements of up to 512MB@.
+If an address is above 512MB then displacements from zero can not be used.
+This option causes code to be generated which can be loaded above 512MB@.
+This may be useful for operating systems or ROM code.
+
+@item -mnohimem
+@opindex mnohimem
+Assume code will be loaded in the first 512MB of virtual address space.
+This is the default for all platforms.
+
+
+@end table
+
+@node AVR Options
+@subsection AVR Options
+@cindex AVR Options
+
+These options are defined for AVR implementations:
+
+@table @gcctabopt
+@item -mmcu=@var{mcu}
+@opindex mmcu
+Specify ATMEL AVR instruction set or MCU type.
+
+Instruction set avr1 is for the minimal AVR core, not supported by the C
+compiler, only for assembler programs (MCU types: at90s1200, attiny10,
+attiny11, attiny12, attiny15, attiny28).
+
+Instruction set avr2 (default) is for the classic AVR core with up to
+8K program memory space (MCU types: at90s2313, at90s2323, attiny22,
+at90s2333, at90s2343, at90s4414, at90s4433, at90s4434, at90s8515,
+at90c8534, at90s8535).
+
+Instruction set avr3 is for the classic AVR core with up to 128K program
+memory space (MCU types: atmega103, atmega603, at43usb320, at76c711).
+
+Instruction set avr4 is for the enhanced AVR core with up to 8K program
+memory space (MCU types: atmega8, atmega83, atmega85).
+
+Instruction set avr5 is for the enhanced AVR core with up to 128K program
+memory space (MCU types: atmega16, atmega161, atmega163, atmega32, atmega323,
+atmega64, atmega128, at43usb355, at94k).
+
+@item -msize
+@opindex msize
+Output instruction sizes to the asm file.
+
+@item -minit-stack=@var{N}
+@opindex minit-stack
+Specify the initial stack address, which may be a symbol or numeric value,
+@samp{__stack} is the default.
+
+@item -mno-interrupts
+@opindex mno-interrupts
+Generated code is not compatible with hardware interrupts.
+Code size will be smaller.
+
+@item -mcall-prologues
+@opindex mcall-prologues
+Functions prologues/epilogues expanded as call to appropriate
+subroutines.  Code size will be smaller.
+
+@item -mno-tablejump
+@opindex mno-tablejump
+Do not generate tablejump insns which sometimes increase code size.
+
+@item -mtiny-stack
+@opindex mtiny-stack
+Change only the low 8 bits of the stack pointer.
+@end table
+
+@node MCore Options
+@subsection MCore Options
+@cindex MCore options
+
+These are the @samp{-m} options defined for the Motorola M*Core
+processors.
+
+@table @gcctabopt
+
+@item -mhardlit
+@itemx -mno-hardlit
+@opindex mhardlit
+@opindex mno-hardlit
+Inline constants into the code stream if it can be done in two
+instructions or less.
+
+@item -mdiv
+@itemx -mno-div
+@opindex mdiv
+@opindex mno-div
+Use the divide instruction.  (Enabled by default).
+
+@item -mrelax-immediate
+@itemx -mno-relax-immediate
+@opindex mrelax-immediate
+@opindex mno-relax-immediate
+Allow arbitrary sized immediates in bit operations.
+
+@item -mwide-bitfields
+@itemx -mno-wide-bitfields
+@opindex mwide-bitfields
+@opindex mno-wide-bitfields
+Always treat bit-fields as int-sized.
+
+@item -m4byte-functions
+@itemx -mno-4byte-functions
+@opindex m4byte-functions
+@opindex mno-4byte-functions
+Force all functions to be aligned to a four byte boundary.
+
+@item -mcallgraph-data
+@itemx -mno-callgraph-data
+@opindex mcallgraph-data
+@opindex mno-callgraph-data
+Emit callgraph information.
+
+@item -mslow-bytes
+@itemx -mno-slow-bytes
+@opindex mslow-bytes
+@opindex mno-slow-bytes
+Prefer word access when reading byte quantities.
+
+@item -mlittle-endian
+@itemx -mbig-endian
+@opindex mlittle-endian
+@opindex mbig-endian
+Generate code for a little endian target.
+
+@item -m210
+@itemx -m340
+@opindex m210
+@opindex m340
+Generate code for the 210 processor.
+@end table
+
+@node IA-64 Options
+@subsection IA-64 Options
+@cindex IA-64 Options
+
+These are the @samp{-m} options defined for the Intel IA-64 architecture.
+
+@table @gcctabopt
+@item -mbig-endian
+@opindex mbig-endian
+Generate code for a big endian target.  This is the default for HP-UX@.
+
+@item -mlittle-endian
+@opindex mlittle-endian
+Generate code for a little endian target.  This is the default for AIX5
+and GNU/Linux.
+
+@item -mgnu-as
+@itemx -mno-gnu-as
+@opindex mgnu-as
+@opindex mno-gnu-as
+Generate (or don't) code for the GNU assembler.  This is the default.
+@c Also, this is the default if the configure option @option{--with-gnu-as}
+@c is used.
+
+@item -mgnu-ld
+@itemx -mno-gnu-ld
+@opindex mgnu-ld
+@opindex mno-gnu-ld
+Generate (or don't) code for the GNU linker.  This is the default.
+@c Also, this is the default if the configure option @option{--with-gnu-ld}
+@c is used.
+
+@item -mno-pic
+@opindex mno-pic
+Generate code that does not use a global pointer register.  The result
+is not position independent code, and violates the IA-64 ABI@.
+
+@item -mvolatile-asm-stop
+@itemx -mno-volatile-asm-stop
+@opindex mvolatile-asm-stop
+@opindex mno-volatile-asm-stop
+Generate (or don't) a stop bit immediately before and after volatile asm
+statements.
+
+@item -mb-step
+@opindex mb-step
+Generate code that works around Itanium B step errata.
+
+@item -mregister-names
+@itemx -mno-register-names
+@opindex mregister-names
+@opindex mno-register-names
+Generate (or don't) @samp{in}, @samp{loc}, and @samp{out} register names for
+the stacked registers.  This may make assembler output more readable.
+
+@item -mno-sdata
+@itemx -msdata
+@opindex mno-sdata
+@opindex msdata
+Disable (or enable) optimizations that use the small data section.  This may
+be useful for working around optimizer bugs.
+
+@item -mconstant-gp
+@opindex mconstant-gp
+Generate code that uses a single constant global pointer value.  This is
+useful when compiling kernel code.
+
+@item -mauto-pic
+@opindex mauto-pic
+Generate code that is self-relocatable.  This implies @option{-mconstant-gp}.
+This is useful when compiling firmware code.
+
+@item -minline-float-divide-min-latency
+@opindex minline-float-divide-min-latency
+Generate code for inline divides of floating point values
+using the minimum latency algorithm.
+
+@item -minline-float-divide-max-throughput
+@opindex minline-float-divide-max-throughput
+Generate code for inline divides of floating point values
+using the maximum throughput algorithm.
+
+@item -minline-int-divide-min-latency
+@opindex minline-int-divide-min-latency
+Generate code for inline divides of integer values
+using the minimum latency algorithm.
+
+@item -minline-int-divide-max-throughput
+@opindex minline-int-divide-max-throughput
+Generate code for inline divides of integer values
+using the maximum throughput algorithm.
+
+@item -minline-sqrt-min-latency
+@opindex minline-sqrt-min-latency
+Generate code for inline square roots
+using the minimum latency algorithm.
+
+@item -minline-sqrt-max-throughput
+@opindex minline-sqrt-max-throughput
+Generate code for inline square roots
+using the maximum throughput algorithm.
+
+@item -mno-dwarf2-asm
+@itemx -mdwarf2-asm
+@opindex mno-dwarf2-asm
+@opindex mdwarf2-asm
+Don't (or do) generate assembler code for the DWARF2 line number debugging
+info.  This may be useful when not using the GNU assembler.
+
+@item -mearly-stop-bits
+@itemx -mno-early-stop-bits
+@opindex mearly-stop-bits
+@opindex mno-early-stop-bits
+Allow stop bits to be placed earlier than immediately preceding the
+instruction that triggered the stop bit.  This can improve instruction
+scheduling, but does not always do so.
+
+@item -mfixed-range=@var{register-range}
+@opindex mfixed-range
+Generate code treating the given register range as fixed registers.
+A fixed register is one that the register allocator can not use.  This is
+useful when compiling kernel code.  A register range is specified as
+two registers separated by a dash.  Multiple register ranges can be
+specified separated by a comma.
+
+@item -mtls-size=@var{tls-size}
+@opindex mtls-size
+Specify bit size of immediate TLS offsets.  Valid values are 14, 22, and
+64.
+
+@item -mtune-arch=@var{cpu-type}
+@opindex mtune-arch
+Tune the instruction scheduling for a particular CPU, Valid values are
+itanium, itanium1, merced, itanium2, and mckinley.
+
+@item -mt
+@itemx -pthread
+@opindex mt
+@opindex pthread
+Add support for multithreading using the POSIX threads library.  This
+option sets flags for both the preprocessor and linker.  It does
+not affect the thread safety of object code produced by the compiler or
+that of libraries supplied with it.  These are HP-UX specific flags.
+
+@item -milp32
+@itemx -mlp64
+@opindex milp32
+@opindex mlp64
+Generate code for a 32-bit or 64-bit environment.
+The 32-bit environment sets int, long and pointer to 32 bits.
+The 64-bit environment sets int to 32 bits and long and pointer
+to 64 bits.  These are HP-UX specific flags.
+
+@end table
+
+@node D30V Options
+@subsection D30V Options
+@cindex D30V Options
+
+These @samp{-m} options are defined for D30V implementations:
+
+@table @gcctabopt
+@item -mextmem
+@opindex mextmem
+Link the @samp{.text}, @samp{.data}, @samp{.bss}, @samp{.strings},
+@samp{.rodata}, @samp{.rodata1}, @samp{.data1} sections into external
+memory, which starts at location @code{0x80000000}.
+
+@item -mextmemory
+@opindex mextmemory
+Same as the @option{-mextmem} switch.
+
+@item -monchip
+@opindex monchip
+Link the @samp{.text} section into onchip text memory, which starts at
+location @code{0x0}.  Also link @samp{.data}, @samp{.bss},
+@samp{.strings}, @samp{.rodata}, @samp{.rodata1}, @samp{.data1} sections
+into onchip data memory, which starts at location @code{0x20000000}.
+
+@item -mno-asm-optimize
+@itemx -masm-optimize
+@opindex mno-asm-optimize
+@opindex masm-optimize
+Disable (enable) passing @option{-O} to the assembler when optimizing.
+The assembler uses the @option{-O} option to automatically parallelize
+adjacent short instructions where possible.
+
+@item -mbranch-cost=@var{n}
+@opindex mbranch-cost
+Increase the internal costs of branches to @var{n}.  Higher costs means
+that the compiler will issue more instructions to avoid doing a branch.
+The default is 2.
+
+@item -mcond-exec=@var{n}
+@opindex mcond-exec
+Specify the maximum number of conditionally executed instructions that
+replace a branch.  The default is 4.
+@end table
+
+@node S/390 and zSeries Options
+@subsection S/390 and zSeries Options
+@cindex S/390 and zSeries Options
+
+These are the @samp{-m} options defined for the S/390 and zSeries architecture.
+
+@table @gcctabopt
+@item -mhard-float
+@itemx -msoft-float
+@opindex mhard-float
+@opindex msoft-float
+Use (do not use) the hardware floating-point instructions and registers
+for floating-point operations.  When @option{-msoft-float} is specified,
+functions in @file{libgcc.a} will be used to perform floating-point
+operations.  When @option{-mhard-float} is specified, the compiler
+generates IEEE floating-point instructions.  This is the default.
+
+@item -mbackchain
+@itemx -mno-backchain
+@opindex mbackchain
+@opindex mno-backchain
+Generate (or do not generate) code which maintains an explicit
+backchain within the stack frame that points to the caller's frame.
+This may be needed to allow debugging using tools that do not understand
+DWARF-2 call frame information.  The default is not to generate the
+backchain.
+
+@item -msmall-exec
+@itemx -mno-small-exec
+@opindex msmall-exec
+@opindex mno-small-exec
+Generate (or do not generate) code using the @code{bras} instruction
+to do subroutine calls.
+This only works reliably if the total executable size does not
+exceed 64k.  The default is to use the @code{basr} instruction instead,
+which does not have this limitation.
+
+@item -m64
+@itemx -m31
+@opindex m64
+@opindex m31
+When @option{-m31} is specified, generate code compliant to the
+GNU/Linux for S/390 ABI@.  When @option{-m64} is specified, generate
+code compliant to the GNU/Linux for zSeries ABI@.  This allows GCC in
+particular to generate 64-bit instructions.  For the @samp{s390}
+targets, the default is @option{-m31}, while the @samp{s390x}
+targets default to @option{-m64}.
+
+@item -mzarch
+@itemx -mesa
+@opindex mzarch
+@opindex mesa
+When @option{-mzarch} is specified, generate code using the
+instructions available on z/Architecture.
+When @option{-mesa} is specified, generate code using the
+instructions available on ESA/390. Note that @option{-mesa} is
+not possible with @option{-m64}.
+When generating code compliant to the GNU/Linux for S/390 ABI,
+the default is @option{-mesa}.  When generating code compliant
+to the GNU/Linux for zSeries ABI, the default is @option{-mzarch}.
+
+@item -mmvcle
+@itemx -mno-mvcle
+@opindex mmvcle
+@opindex mno-mvcle
+Generate (or do not generate) code using the @code{mvcle} instruction
+to perform block moves.  When @option{-mno-mvcle} is specified,
+use a @code{mvc} loop instead.  This is the default.
+
+@item -mdebug
+@itemx -mno-debug
+@opindex mdebug
+@opindex mno-debug
+Print (or do not print) additional debug information when compiling.
+The default is to not print debug information.
+
+@item -march=@var{cpu-type}
+@opindex march
+Generate code that will run on @var{cpu-type}, which is the name of a system
+representing a certain processor type. Possible values for
+@var{cpu-type} are @samp{g5}, @samp{g6}, @samp{z900}, and @samp{z990}.
+When generating code using the instructions available on z/Architecture,
+the default is @option{-march=z900}.  Otherwise, the default is
+@option{-march=g5}.
+
+@item -mtune=@var{cpu-type}
+@opindex mtune
+Tune to @var{cpu-type} everything applicable about the generated code,
+except for the ABI and the set of available instructions.
+The list of @var{cpu-type} values is the same as for @option{-march}.
+The default is the value used for @option{-march}.
+
+@item -mfused-madd
+@itemx -mno-fused-madd
+@opindex mfused-madd
+@opindex mno-fused-madd
+Generate code that uses (does not use) the floating point multiply and
+accumulate instructions.  These instructions are generated by default if
+hardware floating point is used.
+@end table
+
+@node CRIS Options
+@subsection CRIS Options
+@cindex CRIS Options
+
+These options are defined specifically for the CRIS ports.
+
+@table @gcctabopt
+@item -march=@var{architecture-type}
+@itemx -mcpu=@var{architecture-type}
+@opindex march
+@opindex mcpu
+Generate code for the specified architecture.  The choices for
+@var{architecture-type} are @samp{v3}, @samp{v8} and @samp{v10} for
+respectively ETRAX@w{ }4, ETRAX@w{ }100, and ETRAX@w{ }100@w{ }LX.
+Default is @samp{v0} except for cris-axis-linux-gnu, where the default is
+@samp{v10}.
+
+@item -mtune=@var{architecture-type}
+@opindex mtune
+Tune to @var{architecture-type} everything applicable about the generated
+code, except for the ABI and the set of available instructions.  The
+choices for @var{architecture-type} are the same as for
+@option{-march=@var{architecture-type}}.
+
+@item -mmax-stack-frame=@var{n}
+@opindex mmax-stack-frame
+Warn when the stack frame of a function exceeds @var{n} bytes.
+
+@item -melinux-stacksize=@var{n}
+@opindex melinux-stacksize
+Only available with the @samp{cris-axis-aout} target.  Arranges for
+indications in the program to the kernel loader that the stack of the
+program should be set to @var{n} bytes.
+
+@item -metrax4
+@itemx -metrax100
+@opindex metrax4
+@opindex metrax100
+The options @option{-metrax4} and @option{-metrax100} are synonyms for
+@option{-march=v3} and @option{-march=v8} respectively.
+
+@item -mmul-bug-workaround
+@itemx -mno-mul-bug-workaround
+@opindex mmul-bug-workaround
+@opindex mno-mul-bug-workaround
+Work around a bug in the @code{muls} and @code{mulu} instructions for CPU
+models where it applies.  This option is active by default.
+
+@item -mpdebug
+@opindex mpdebug
+Enable CRIS-specific verbose debug-related information in the assembly
+code.  This option also has the effect to turn off the @samp{#NO_APP}
+formatted-code indicator to the assembler at the beginning of the
+assembly file.
+
+@item -mcc-init
+@opindex mcc-init
+Do not use condition-code results from previous instruction; always emit
+compare and test instructions before use of condition codes.
+
+@item -mno-side-effects
+@opindex mno-side-effects
+Do not emit instructions with side-effects in addressing modes other than
+post-increment.
+
+@item -mstack-align
+@itemx -mno-stack-align
+@itemx -mdata-align
+@itemx -mno-data-align
+@itemx -mconst-align
+@itemx -mno-const-align
+@opindex mstack-align
+@opindex mno-stack-align
+@opindex mdata-align
+@opindex mno-data-align
+@opindex mconst-align
+@opindex mno-const-align
+These options (no-options) arranges (eliminate arrangements) for the
+stack-frame, individual data and constants to be aligned for the maximum
+single data access size for the chosen CPU model.  The default is to
+arrange for 32-bit alignment.  ABI details such as structure layout are
+not affected by these options.
+
+@item -m32-bit
+@itemx -m16-bit
+@itemx -m8-bit
+@opindex m32-bit
+@opindex m16-bit
+@opindex m8-bit
+Similar to the stack- data- and const-align options above, these options
+arrange for stack-frame, writable data and constants to all be 32-bit,
+16-bit or 8-bit aligned.  The default is 32-bit alignment.
+
+@item -mno-prologue-epilogue
+@itemx -mprologue-epilogue
+@opindex mno-prologue-epilogue
+@opindex mprologue-epilogue
+With @option{-mno-prologue-epilogue}, the normal function prologue and
+epilogue that sets up the stack-frame are omitted and no return
+instructions or return sequences are generated in the code.  Use this
+option only together with visual inspection of the compiled code: no
+warnings or errors are generated when call-saved registers must be saved,
+or storage for local variable needs to be allocated.
+
+@item -mno-gotplt
+@itemx -mgotplt
+@opindex mno-gotplt
+@opindex mgotplt
+With @option{-fpic} and @option{-fPIC}, don't generate (do generate)
+instruction sequences that load addresses for functions from the PLT part
+of the GOT rather than (traditional on other architectures) calls to the
+PLT.  The default is @option{-mgotplt}.
+
+@item -maout
+@opindex maout
+Legacy no-op option only recognized with the cris-axis-aout target.
+
+@item -melf
+@opindex melf
+Legacy no-op option only recognized with the cris-axis-elf and
+cris-axis-linux-gnu targets.
+
+@item -melinux
+@opindex melinux
+Only recognized with the cris-axis-aout target, where it selects a
+GNU/linux-like multilib, include files and instruction set for
+@option{-march=v8}.
+
+@item -mlinux
+@opindex mlinux
+Legacy no-op option only recognized with the cris-axis-linux-gnu target.
+
+@item -sim
+@opindex sim
+This option, recognized for the cris-axis-aout and cris-axis-elf arranges
+to link with input-output functions from a simulator library.  Code,
+initialized data and zero-initialized data are allocated consecutively.
+
+@item -sim2
+@opindex sim2
+Like @option{-sim}, but pass linker options to locate initialized data at
+0x40000000 and zero-initialized data at 0x80000000.
+@end table
+
+@node MMIX Options
+@subsection MMIX Options
+@cindex MMIX Options
+
+These options are defined for the MMIX:
+
+@table @gcctabopt
+@item -mlibfuncs
+@itemx -mno-libfuncs
+@opindex mlibfuncs
+@opindex mno-libfuncs
+Specify that intrinsic library functions are being compiled, passing all
+values in registers, no matter the size.
+
+@item -mepsilon
+@itemx -mno-epsilon
+@opindex mepsilon
+@opindex mno-epsilon
+Generate floating-point comparison instructions that compare with respect
+to the @code{rE} epsilon register.
+
+@item -mabi=mmixware
+@itemx -mabi=gnu
+@opindex mabi-mmixware
+@opindex mabi=gnu
+Generate code that passes function parameters and return values that (in
+the called function) are seen as registers @code{$0} and up, as opposed to
+the GNU ABI which uses global registers @code{$231} and up.
+
+@item -mzero-extend
+@itemx -mno-zero-extend
+@opindex mzero-extend
+@opindex mno-zero-extend
+When reading data from memory in sizes shorter than 64 bits, use (do not
+use) zero-extending load instructions by default, rather than
+sign-extending ones.
+
+@item -mknuthdiv
+@itemx -mno-knuthdiv
+@opindex mknuthdiv
+@opindex mno-knuthdiv
+Make the result of a division yielding a remainder have the same sign as
+the divisor.  With the default, @option{-mno-knuthdiv}, the sign of the
+remainder follows the sign of the dividend.  Both methods are
+arithmetically valid, the latter being almost exclusively used.
+
+@item -mtoplevel-symbols
+@itemx -mno-toplevel-symbols
+@opindex mtoplevel-symbols
+@opindex mno-toplevel-symbols
+Prepend (do not prepend) a @samp{:} to all global symbols, so the assembly
+code can be used with the @code{PREFIX} assembly directive.
+
+@item -melf
+@opindex melf
+Generate an executable in the ELF format, rather than the default
+@samp{mmo} format used by the @command{mmix} simulator.
+
+@item -mbranch-predict
+@itemx -mno-branch-predict
+@opindex mbranch-predict
+@opindex mno-branch-predict
+Use (do not use) the probable-branch instructions, when static branch
+prediction indicates a probable branch.
+
+@item -mbase-addresses
+@itemx -mno-base-addresses
+@opindex mbase-addresses
+@opindex mno-base-addresses
+Generate (do not generate) code that uses @emph{base addresses}.  Using a
+base address automatically generates a request (handled by the assembler
+and the linker) for a constant to be set up in a global register.  The
+register is used for one or more base address requests within the range 0
+to 255 from the value held in the register.  The generally leads to short
+and fast code, but the number of different data items that can be
+addressed is limited.  This means that a program that uses lots of static
+data may require @option{-mno-base-addresses}.
+
+@item -msingle-exit
+@itemx -mno-single-exit
+@opindex msingle-exit
+@opindex mno-single-exit
+Force (do not force) generated code to have a single exit point in each
+function.
+@end table
+
+@node PDP-11 Options
+@subsection PDP-11 Options
+@cindex PDP-11 Options
+
+These options are defined for the PDP-11:
+
+@table @gcctabopt
+@item -mfpu
+@opindex mfpu
+Use hardware FPP floating point.  This is the default.  (FIS floating
+point on the PDP-11/40 is not supported.)
+
+@item -msoft-float
+@opindex msoft-float
+Do not use hardware floating point.
+
+@item -mac0
+@opindex mac0
+Return floating-point results in ac0 (fr0 in Unix assembler syntax).
+
+@item -mno-ac0
+@opindex mno-ac0
+Return floating-point results in memory.  This is the default.
+
+@item -m40
+@opindex m40
+Generate code for a PDP-11/40.
+
+@item -m45
+@opindex m45
+Generate code for a PDP-11/45.  This is the default.
+
+@item -m10
+@opindex m10
+Generate code for a PDP-11/10.
+
+@item -mbcopy-builtin
+@opindex bcopy-builtin
+Use inline @code{movstrhi} patterns for copying memory.  This is the
+default.
+
+@item -mbcopy
+@opindex mbcopy
+Do not use inline @code{movstrhi} patterns for copying memory.
+
+@item -mint16
+@itemx -mno-int32
+@opindex mint16
+@opindex mno-int32
+Use 16-bit @code{int}.  This is the default.
+
+@item -mint32
+@itemx -mno-int16
+@opindex mint32
+@opindex mno-int16
+Use 32-bit @code{int}.
+
+@item -mfloat64
+@itemx -mno-float32
+@opindex mfloat64
+@opindex mno-float32
+Use 64-bit @code{float}.  This is the default.
+
+@item -mfloat32
+@itemx -mno-float64
+@opindex mfloat32
+@opindex mno-float64
+Use 32-bit @code{float}.
+
+@item -mabshi
+@opindex mabshi
+Use @code{abshi2} pattern.  This is the default.
+
+@item -mno-abshi
+@opindex mno-abshi
+Do not use @code{abshi2} pattern.
+
+@item -mbranch-expensive
+@opindex mbranch-expensive
+Pretend that branches are expensive.  This is for experimenting with
+code generation only.
+
+@item -mbranch-cheap
+@opindex mbranch-cheap
+Do not pretend that branches are expensive.  This is the default.
+
+@item -msplit
+@opindex msplit
+Generate code for a system with split I&D.
+
+@item -mno-split
+@opindex mno-split
+Generate code for a system without split I&D.  This is the default.
+
+@item -munix-asm
+@opindex munix-asm
+Use Unix assembler syntax.  This is the default when configured for
+@samp{pdp11-*-bsd}.
+
+@item -mdec-asm
+@opindex mdec-asm
+Use DEC assembler syntax.  This is the default when configured for any
+PDP-11 target other than @samp{pdp11-*-bsd}.
+@end table
+
+@node Xstormy16 Options
+@subsection Xstormy16 Options
+@cindex Xstormy16 Options
+
+These options are defined for Xstormy16:
+
+@table @gcctabopt
+@item -msim
+@opindex msim
+Choose startup files and linker script suitable for the simulator.
+@end table
+
+@node FRV Options
+@subsection FRV Options
+@cindex FRV Options
+
+@table @gcctabopt
+@item -mgpr-32
+@opindex mgpr-32
+
+Only use the first 32 general purpose registers.
+
+@item -mgpr-64
+@opindex mgpr-64
+
+Use all 64 general purpose registers.
+
+@item -mfpr-32
+@opindex mfpr-32
+
+Use only the first 32 floating point registers.
+
+@item -mfpr-64
+@opindex mfpr-64
+
+Use all 64 floating point registers
+
+@item -mhard-float
+@opindex mhard-float
+
+Use hardware instructions for floating point operations.
+
+@item -msoft-float
+@opindex msoft-float
+
+Use library routines for floating point operations.
+
+@item -malloc-cc
+@opindex malloc-cc
+
+Dynamically allocate condition code registers.
+
+@item -mfixed-cc
+@opindex mfixed-cc
+
+Do not try to dynamically allocate condition code registers, only
+use @code{icc0} and @code{fcc0}.
+
+@item -mdword
+@opindex mdword
+
+Change ABI to use double word insns.
+
+@item -mno-dword
+@opindex mno-dword
+
+Do not use double word instructions.
+
+@item -mdouble
+@opindex mdouble
+
+Use floating point double instructions.
+
+@item -mno-double
+@opindex mno-double
+
+Do not use floating point double instructions.
+
+@item -mmedia
+@opindex mmedia
+
+Use media instructions.
+
+@item -mno-media
+@opindex mno-media
+
+Do not use media instructions.
+
+@item -mmuladd
+@opindex mmuladd
+
+Use multiply and add/subtract instructions.
+
+@item -mno-muladd
+@opindex mno-muladd
+
+Do not use multiply and add/subtract instructions.
+
+@item -mlibrary-pic
+@opindex mlibrary-pic
+
+Enable PIC support for building libraries
+
+@item -macc-4
+@opindex macc-4
+
+Use only the first four media accumulator registers.
+
+@item -macc-8
+@opindex macc-8
+
+Use all eight media accumulator registers.
+
+@item -mpack
+@opindex mpack
+
+Pack VLIW instructions.
+
+@item -mno-pack
+@opindex mno-pack
+
+Do not pack VLIW instructions.
+
+@item -mno-eflags
+@opindex mno-eflags
+
+Do not mark ABI switches in e_flags.
+
+@item -mcond-move
+@opindex mcond-move
+
+Enable the use of conditional-move instructions (default).
+
+This switch is mainly for debugging the compiler and will likely be removed
+in a future version.
+
+@item -mno-cond-move
+@opindex mno-cond-move
+
+Disable the use of conditional-move instructions.
+
+This switch is mainly for debugging the compiler and will likely be removed
+in a future version.
+
+@item -mscc
+@opindex mscc
+
+Enable the use of conditional set instructions (default).
+
+This switch is mainly for debugging the compiler and will likely be removed
+in a future version.
+
+@item -mno-scc
+@opindex mno-scc
+
+Disable the use of conditional set instructions.
+
+This switch is mainly for debugging the compiler and will likely be removed
+in a future version.
+
+@item -mcond-exec
+@opindex mcond-exec
+
+Enable the use of conditional execution (default).
+
+This switch is mainly for debugging the compiler and will likely be removed
+in a future version.
+
+@item -mno-cond-exec
+@opindex mno-cond-exec
+
+Disable the use of conditional execution.
+
+This switch is mainly for debugging the compiler and will likely be removed
+in a future version.
+
+@item -mvliw-branch
+@opindex mvliw-branch
+
+Run a pass to pack branches into VLIW instructions (default).
+
+This switch is mainly for debugging the compiler and will likely be removed
+in a future version.
+
+@item -mno-vliw-branch
+@opindex mno-vliw-branch
+
+Do not run a pass to pack branches into VLIW instructions.
+
+This switch is mainly for debugging the compiler and will likely be removed
+in a future version.
+
+@item -mmulti-cond-exec
+@opindex mmulti-cond-exec
+
+Enable optimization of @code{&&} and @code{||} in conditional execution
+(default).
+
+This switch is mainly for debugging the compiler and will likely be removed
+in a future version.
+
+@item -mno-multi-cond-exec
+@opindex mno-multi-cond-exec
+
+Disable optimization of @code{&&} and @code{||} in conditional execution.
+
+This switch is mainly for debugging the compiler and will likely be removed
+in a future version.
+
+@item -mnested-cond-exec
+@opindex mnested-cond-exec
+
+Enable nested conditional execution optimizations (default).
+
+This switch is mainly for debugging the compiler and will likely be removed
+in a future version.
+
+@item -mno-nested-cond-exec
+@opindex mno-nested-cond-exec
+
+Disable nested conditional execution optimizations.
+
+This switch is mainly for debugging the compiler and will likely be removed
+in a future version.
+
+@item -mtomcat-stats
+@opindex mtomcat-stats
+
+Cause gas to print out tomcat statistics.
+
+@item -mcpu=@var{cpu}
+@opindex mcpu
+
+Select the processor type for which to generate code.  Possible values are
+@samp{simple}, @samp{tomcat}, @samp{fr500}, @samp{fr400}, @samp{fr300},
+@samp{frv}.
+
+@end table
+
+@node Xtensa Options
+@subsection Xtensa Options
+@cindex Xtensa Options
+
+These options are supported for Xtensa targets:
+
+@table @gcctabopt
+@item -mconst16
+@itemx -mno-const16
+@opindex mconst16
+@opindex mno-const16
+Enable or disable use of @code{CONST16} instructions for loading
+constant values.  The @code{CONST16} instruction is currently not a
+standard option from Tensilica.  When enabled, @code{CONST16}
+instructions are always used in place of the standard @code{L32R}
+instructions.  The use of @code{CONST16} is enabled by default only if
+the @code{L32R} instruction is not available.
+
+@item -mfused-madd
+@itemx -mno-fused-madd
+@opindex mfused-madd
+@opindex mno-fused-madd
+Enable or disable use of fused multiply/add and multiply/subtract
+instructions in the floating-point option.  This has no effect if the
+floating-point option is not also enabled.  Disabling fused multiply/add
+and multiply/subtract instructions forces the compiler to use separate
+instructions for the multiply and add/subtract operations.  This may be
+desirable in some cases where strict IEEE 754-compliant results are
+required: the fused multiply add/subtract instructions do not round the
+intermediate result, thereby producing results with @emph{more} bits of
+precision than specified by the IEEE standard.  Disabling fused multiply
+add/subtract instructions also ensures that the program output is not
+sensitive to the compiler's ability to combine multiply and add/subtract
+operations.
+
+@item -mtext-section-literals
+@itemx -mno-text-section-literals
+@opindex mtext-section-literals
+@opindex mno-text-section-literals
+Control the treatment of literal pools.  The default is
+@option{-mno-text-section-literals}, which places literals in a separate
+section in the output file.  This allows the literal pool to be placed
+in a data RAM/ROM, and it also allows the linker to combine literal
+pools from separate object files to remove redundant literals and
+improve code size.  With @option{-mtext-section-literals}, the literals
+are interspersed in the text section in order to keep them as close as
+possible to their references.  This may be necessary for large assembly
+files.
+
+@item -mtarget-align
+@itemx -mno-target-align
+@opindex mtarget-align
+@opindex mno-target-align
+When this option is enabled, GCC instructs the assembler to
+automatically align instructions to reduce branch penalties at the
+expense of some code density.  The assembler attempts to widen density
+instructions to align branch targets and the instructions following call
+instructions.  If there are not enough preceding safe density
+instructions to align a target, no widening will be performed.  The
+default is @option{-mtarget-align}.  These options do not affect the
+treatment of auto-aligned instructions like @code{LOOP}, which the
+assembler will always align, either by widening density instructions or
+by inserting no-op instructions.
+
+@item -mlongcalls
+@itemx -mno-longcalls
+@opindex mlongcalls
+@opindex mno-longcalls
+When this option is enabled, GCC instructs the assembler to translate
+direct calls to indirect calls unless it can determine that the target
+of a direct call is in the range allowed by the call instruction.  This
+translation typically occurs for calls to functions in other source
+files.  Specifically, the assembler translates a direct @code{CALL}
+instruction into an @code{L32R} followed by a @code{CALLX} instruction.
+The default is @option{-mno-longcalls}.  This option should be used in
+programs where the call target can potentially be out of range.  This
+option is implemented in the assembler, not the compiler, so the
+assembly code generated by GCC will still show direct call
+instructions---look at the disassembled object code to see the actual
+instructions.  Note that the assembler will use an indirect call for
+every cross-file call, not just those that really will be out of range.
+@end table
+
+@node Code Gen Options
+@section Options for Code Generation Conventions
+@cindex code generation conventions
+@cindex options, code generation
+@cindex run-time options
+
+These machine-independent options control the interface conventions
+used in code generation.
+
+Most of them have both positive and negative forms; the negative form
+of @option{-ffoo} would be @option{-fno-foo}.  In the table below, only
+one of the forms is listed---the one which is not the default.  You
+can figure out the other form by either removing @samp{no-} or adding
+it.
+
+@table @gcctabopt
+@item -fbounds-check
+@opindex fbounds-check
+For front-ends that support it, generate additional code to check that
+indices used to access arrays are within the declared range.  This is
+currently only supported by the Java and Fortran 77 front-ends, where
+this option defaults to true and false respectively.
+
+@item -ftrapv
+@opindex ftrapv
+This option generates traps for signed overflow on addition, subtraction,
+multiplication operations.
+
+@item -fwrapv
+@opindex fwrapv
+This option instructs the compiler to assume that signed arithmetic
+overflow of addition, subtraction and multiplication wraps around
+using twos-complement representation.  This flag enables some optimizations
+and disables other.  This option is enabled by default for the Java
+front-end, as required by the Java language specification.
+
+@item -fexceptions
+@opindex fexceptions
+Enable exception handling.  Generates extra code needed to propagate
+exceptions.  For some targets, this implies GCC will generate frame
+unwind information for all functions, which can produce significant data
+size overhead, although it does not affect execution.  If you do not
+specify this option, GCC will enable it by default for languages like
+C++ which normally require exception handling, and disable it for
+languages like C that do not normally require it.  However, you may need
+to enable this option when compiling C code that needs to interoperate
+properly with exception handlers written in C++.  You may also wish to
+disable this option if you are compiling older C++ programs that don't
+use exception handling.
+
+@item -fnon-call-exceptions
+@opindex fnon-call-exceptions
+Generate code that allows trapping instructions to throw exceptions.
+Note that this requires platform-specific runtime support that does
+not exist everywhere.  Moreover, it only allows @emph{trapping}
+instructions to throw exceptions, i.e.@: memory references or floating
+point instructions.  It does not allow exceptions to be thrown from
+arbitrary signal handlers such as @code{SIGALRM}.
+
+@item -funwind-tables
+@opindex funwind-tables
+Similar to @option{-fexceptions}, except that it will just generate any needed
+static data, but will not affect the generated code in any other way.
+You will normally not enable this option; instead, a language processor
+that needs this handling would enable it on your behalf.
+
+@item -fasynchronous-unwind-tables
+@opindex funwind-tables
+Generate unwind table in dwarf2 format, if supported by target machine.  The
+table is exact at each instruction boundary, so it can be used for stack
+unwinding from asynchronous events (such as debugger or garbage collector).
+
+@item -fpcc-struct-return
+@opindex fpcc-struct-return
+Return ``short'' @code{struct} and @code{union} values in memory like
+longer ones, rather than in registers.  This convention is less
+efficient, but it has the advantage of allowing intercallability between
+GCC-compiled files and files compiled with other compilers, particularly
+the Portable C Compiler (pcc).
+
+The precise convention for returning structures in memory depends
+on the target configuration macros.
+
+Short structures and unions are those whose size and alignment match
+that of some integer type.
+
+@strong{Warning:} code compiled with the @option{-fpcc-struct-return}
+switch is not binary compatible with code compiled with the
+@option{-freg-struct-return} switch.
+Use it to conform to a non-default application binary interface.
+
+@item -freg-struct-return
+@opindex freg-struct-return
+Return @code{struct} and @code{union} values in registers when possible.
+This is more efficient for small structures than
+@option{-fpcc-struct-return}.
+
+If you specify neither @option{-fpcc-struct-return} nor
+@option{-freg-struct-return}, GCC defaults to whichever convention is
+standard for the target.  If there is no standard convention, GCC
+defaults to @option{-fpcc-struct-return}, except on targets where GCC is
+the principal compiler.  In those cases, we can choose the standard, and
+we chose the more efficient register return alternative.
+
+@strong{Warning:} code compiled with the @option{-freg-struct-return}
+switch is not binary compatible with code compiled with the
+@option{-fpcc-struct-return} switch.
+Use it to conform to a non-default application binary interface.
+
+@item -fshort-enums
+@opindex fshort-enums
+Allocate to an @code{enum} type only as many bytes as it needs for the
+declared range of possible values.  Specifically, the @code{enum} type
+will be equivalent to the smallest integer type which has enough room.
+
+@strong{Warning:} the @option{-fshort-enums} switch causes GCC to generate
+code that is not binary compatible with code generated without that switch.
+Use it to conform to a non-default application binary interface.
+
+@item -fshort-double
+@opindex fshort-double
+Use the same size for @code{double} as for @code{float}.
+
+@strong{Warning:} the @option{-fshort-double} switch causes GCC to generate
+code that is not binary compatible with code generated without that switch.
+Use it to conform to a non-default application binary interface.
+
+@item -fshort-wchar
+@opindex fshort-wchar
+Override the underlying type for @samp{wchar_t} to be @samp{short
+unsigned int} instead of the default for the target.  This option is
+useful for building programs to run under WINE@.
+
+@strong{Warning:} the @option{-fshort-wchar} switch causes GCC to generate
+code that is not binary compatible with code generated without that switch.
+Use it to conform to a non-default application binary interface.
+
+@item -fshared-data
+@opindex fshared-data
+Requests that the data and non-@code{const} variables of this
+compilation be shared data rather than private data.  The distinction
+makes sense only on certain operating systems, where shared data is
+shared between processes running the same program, while private data
+exists in one copy per process.
+
+@item -fno-common
+@opindex fno-common
+In C, allocate even uninitialized global variables in the data section of the
+object file, rather than generating them as common blocks.  This has the
+effect that if the same variable is declared (without @code{extern}) in
+two different compilations, you will get an error when you link them.
+The only reason this might be useful is if you wish to verify that the
+program will work on other systems which always work this way.
+
+@item -fno-ident
+@opindex fno-ident
+Ignore the @samp{#ident} directive.
+
+@item -finhibit-size-directive
+@opindex finhibit-size-directive
+Don't output a @code{.size} assembler directive, or anything else that
+would cause trouble if the function is split in the middle, and the
+two halves are placed at locations far apart in memory.  This option is
+used when compiling @file{crtstuff.c}; you should not need to use it
+for anything else.
+
+@item -fverbose-asm
+@opindex fverbose-asm
+Put extra commentary information in the generated assembly code to
+make it more readable.  This option is generally only of use to those
+who actually need to read the generated assembly code (perhaps while
+debugging the compiler itself).
+
+@option{-fno-verbose-asm}, the default, causes the
+extra information to be omitted and is useful when comparing two assembler
+files.
+
+@item -fpic
+@opindex fpic
+@cindex global offset table
+@cindex PIC
+Generate position-independent code (PIC) suitable for use in a shared
+library, if supported for the target machine.  Such code accesses all
+constant addresses through a global offset table (GOT)@.  The dynamic
+loader resolves the GOT entries when the program starts (the dynamic
+loader is not part of GCC; it is part of the operating system).  If
+the GOT size for the linked executable exceeds a machine-specific
+maximum size, you get an error message from the linker indicating that
+@option{-fpic} does not work; in that case, recompile with @option{-fPIC}
+instead.  (These maximums are 8k on the SPARC and 32k
+on the m68k and RS/6000.  The 386 has no such limit.)
+
+Position-independent code requires special support, and therefore works
+only on certain machines.  For the 386, GCC supports PIC for System V
+but not for the Sun 386i.  Code generated for the IBM RS/6000 is always
+position-independent.
+
+@item -fPIC
+@opindex fPIC
+If supported for the target machine, emit position-independent code,
+suitable for dynamic linking and avoiding any limit on the size of the
+global offset table.  This option makes a difference on the m68k
+and the SPARC.
+
+Position-independent code requires special support, and therefore works
+only on certain machines.
+
+@item -fpie
+@itemx -fPIE
+@opindex fpie
+@opindex fPIE
+These options are similar to @option{-fpic} and @option{-fPIC}, but
+generated position independent code can be only linked into executables.
+Usually these options are used when @option{-pie} GCC option will be
+used during linking.
+
+@item -ffixed-@var{reg}
+@opindex ffixed
+Treat the register named @var{reg} as a fixed register; generated code
+should never refer to it (except perhaps as a stack pointer, frame
+pointer or in some other fixed role).
+
+@var{reg} must be the name of a register.  The register names accepted
+are machine-specific and are defined in the @code{REGISTER_NAMES}
+macro in the machine description macro file.
+
+This flag does not have a negative form, because it specifies a
+three-way choice.
+
+@item -fcall-used-@var{reg}
+@opindex fcall-used
+Treat the register named @var{reg} as an allocable register that is
+clobbered by function calls.  It may be allocated for temporaries or
+variables that do not live across a call.  Functions compiled this way
+will not save and restore the register @var{reg}.
+
+It is an error to used this flag with the frame pointer or stack pointer.
+Use of this flag for other registers that have fixed pervasive roles in
+the machine's execution model will produce disastrous results.
+
+This flag does not have a negative form, because it specifies a
+three-way choice.
+
+@item -fcall-saved-@var{reg}
+@opindex fcall-saved
+Treat the register named @var{reg} as an allocable register saved by
+functions.  It may be allocated even for temporaries or variables that
+live across a call.  Functions compiled this way will save and restore
+the register @var{reg} if they use it.
+
+It is an error to used this flag with the frame pointer or stack pointer.
+Use of this flag for other registers that have fixed pervasive roles in
+the machine's execution model will produce disastrous results.
+
+A different sort of disaster will result from the use of this flag for
+a register in which function values may be returned.
+
+This flag does not have a negative form, because it specifies a
+three-way choice.
+
+@item -fpack-struct
+@opindex fpack-struct
+Pack all structure members together without holes.
+
+@strong{Warning:} the @option{-fpack-struct} switch causes GCC to generate
+code that is not binary compatible with code generated without that switch.
+Additionally, it makes the code suboptimal.
+Use it to conform to a non-default application binary interface.
+
+@item -finstrument-functions
+@opindex finstrument-functions
+Generate instrumentation calls for entry and exit to functions.  Just
+after function entry and just before function exit, the following
+profiling functions will be called with the address of the current
+function and its call site.  (On some platforms,
+@code{__builtin_return_address} does not work beyond the current
+function, so the call site information may not be available to the
+profiling functions otherwise.)
+
+@smallexample
+void __cyg_profile_func_enter (void *this_fn,
+                               void *call_site);
+void __cyg_profile_func_exit  (void *this_fn,
+                               void *call_site);
+@end smallexample
+
+The first argument is the address of the start of the current function,
+which may be looked up exactly in the symbol table.
+
+This currently disables function inlining.  This restriction is
+expected to be removed in future releases.
+
+A function may be given the attribute @code{no_instrument_function}, in
+which case this instrumentation will not be done.  This can be used, for
+example, for the profiling functions listed above, high-priority
+interrupt routines, and any functions from which the profiling functions
+cannot safely be called (perhaps signal handlers, if the profiling
+routines generate output or allocate memory).
+
+@item -fstack-check
+@opindex fstack-check
+Generate code to verify that you do not go beyond the boundary of the
+stack.  You should specify this flag if you are running in an
+environment with multiple threads, but only rarely need to specify it in
+a single-threaded environment since stack overflow is automatically
+detected on nearly all systems if there is only one stack.
+
+Note that this switch does not actually cause checking to be done; the
+operating system must do that.  The switch causes generation of code
+to ensure that the operating system sees the stack being extended.
+
+@item -fstack-limit-register=@var{reg}
+@itemx -fstack-limit-symbol=@var{sym}
+@itemx -fno-stack-limit
+@opindex fstack-limit-register
+@opindex fstack-limit-symbol
+@opindex fno-stack-limit
+Generate code to ensure that the stack does not grow beyond a certain value,
+either the value of a register or the address of a symbol.  If the stack
+would grow beyond the value, a signal is raised.  For most targets,
+the signal is raised before the stack overruns the boundary, so
+it is possible to catch the signal without taking special precautions.
+
+For instance, if the stack starts at absolute address @samp{0x80000000}
+and grows downwards, you can use the flags
+@option{-fstack-limit-symbol=__stack_limit} and
+@option{-Wl,--defsym,__stack_limit=0x7ffe0000} to enforce a stack limit
+of 128KB@.  Note that this may only work with the GNU linker.
+
+@cindex aliasing of parameters
+@cindex parameters, aliased
+@item -fargument-alias
+@itemx -fargument-noalias
+@itemx -fargument-noalias-global
+@opindex fargument-alias
+@opindex fargument-noalias
+@opindex fargument-noalias-global
+Specify the possible relationships among parameters and between
+parameters and global data.
+
+@option{-fargument-alias} specifies that arguments (parameters) may
+alias each other and may alias global storage.@*
+@option{-fargument-noalias} specifies that arguments do not alias
+each other, but may alias global storage.@*
+@option{-fargument-noalias-global} specifies that arguments do not
+alias each other and do not alias global storage.
+
+Each language will automatically use whatever option is required by
+the language standard.  You should not need to use these options yourself.
+
+@item -fleading-underscore
+@opindex fleading-underscore
+This option and its counterpart, @option{-fno-leading-underscore}, forcibly
+change the way C symbols are represented in the object file.  One use
+is to help link with legacy assembly code.
+
+@strong{Warning:} the @option{-fleading-underscore} switch causes GCC to
+generate code that is not binary compatible with code generated without that
+switch.  Use it to conform to a non-default application binary interface.
+Not all targets provide complete support for this switch.
+
+@item -ftls-model=@var{model}
+Alter the thread-local storage model to be used (@pxref{Thread-Local}).
+The @var{model} argument should be one of @code{global-dynamic},
+@code{local-dynamic}, @code{initial-exec} or @code{local-exec}.
+
+The default without @option{-fpic} is @code{initial-exec}; with
+@option{-fpic} the default is @code{global-dynamic}.
+@end table
+
+@c man end
+
+@node Environment Variables
+@section Environment Variables Affecting GCC
+@cindex environment variables
+
+@c man begin ENVIRONMENT
+This section describes several environment variables that affect how GCC
+operates.  Some of them work by specifying directories or prefixes to use
+when searching for various kinds of files.  Some are used to specify other
+aspects of the compilation environment.
+
+Note that you can also specify places to search using options such as
+@option{-B}, @option{-I} and @option{-L} (@pxref{Directory Options}).  These
+take precedence over places specified using environment variables, which
+in turn take precedence over those specified by the configuration of GCC@.
+@xref{Driver,, Controlling the Compilation Driver @file{gcc}, gccint,
+GNU Compiler Collection (GCC) Internals}.
+
+@table @env
+@item LANG
+@itemx LC_CTYPE
+@c @itemx LC_COLLATE
+@itemx LC_MESSAGES
+@c @itemx LC_MONETARY
+@c @itemx LC_NUMERIC
+@c @itemx LC_TIME
+@itemx LC_ALL
+@findex LANG
+@findex LC_CTYPE
+@c @findex LC_COLLATE
+@findex LC_MESSAGES
+@c @findex LC_MONETARY
+@c @findex LC_NUMERIC
+@c @findex LC_TIME
+@findex LC_ALL
+@cindex locale
+These environment variables control the way that GCC uses
+localization information that allow GCC to work with different
+national conventions.  GCC inspects the locale categories
+@env{LC_CTYPE} and @env{LC_MESSAGES} if it has been configured to do
+so.  These locale categories can be set to any value supported by your
+installation.  A typical value is @samp{en_GB.UTF-8} for English in the United
+Kingdom encoded in UTF-8.
+
+The @env{LC_CTYPE} environment variable specifies character
+classification.  GCC uses it to determine the character boundaries in
+a string; this is needed for some multibyte encodings that contain quote
+and escape characters that would otherwise be interpreted as a string
+end or escape.
+
+The @env{LC_MESSAGES} environment variable specifies the language to
+use in diagnostic messages.
+
+If the @env{LC_ALL} environment variable is set, it overrides the value
+of @env{LC_CTYPE} and @env{LC_MESSAGES}; otherwise, @env{LC_CTYPE}
+and @env{LC_MESSAGES} default to the value of the @env{LANG}
+environment variable.  If none of these variables are set, GCC
+defaults to traditional C English behavior.
+
+@item TMPDIR
+@findex TMPDIR
+If @env{TMPDIR} is set, it specifies the directory to use for temporary
+files.  GCC uses temporary files to hold the output of one stage of
+compilation which is to be used as input to the next stage: for example,
+the output of the preprocessor, which is the input to the compiler
+proper.
+
+@item GCC_EXEC_PREFIX
+@findex GCC_EXEC_PREFIX
+If @env{GCC_EXEC_PREFIX} is set, it specifies a prefix to use in the
+names of the subprograms executed by the compiler.  No slash is added
+when this prefix is combined with the name of a subprogram, but you can
+specify a prefix that ends with a slash if you wish.
+
+If @env{GCC_EXEC_PREFIX} is not set, GCC will attempt to figure out
+an appropriate prefix to use based on the pathname it was invoked with.
+
+If GCC cannot find the subprogram using the specified prefix, it
+tries looking in the usual places for the subprogram.
+
+The default value of @env{GCC_EXEC_PREFIX} is
+@file{@var{prefix}/lib/gcc/} where @var{prefix} is the value
+of @code{prefix} when you ran the @file{configure} script.
+
+Other prefixes specified with @option{-B} take precedence over this prefix.
+
+This prefix is also used for finding files such as @file{crt0.o} that are
+used for linking.
+
+In addition, the prefix is used in an unusual way in finding the
+directories to search for header files.  For each of the standard
+directories whose name normally begins with @samp{/usr/local/lib/gcc}
+(more precisely, with the value of @env{GCC_INCLUDE_DIR}), GCC tries
+replacing that beginning with the specified prefix to produce an
+alternate directory name.  Thus, with @option{-Bfoo/}, GCC will search
+@file{foo/bar} where it would normally search @file{/usr/local/lib/bar}.
+These alternate directories are searched first; the standard directories
+come next.
+
+@item COMPILER_PATH
+@findex COMPILER_PATH
+The value of @env{COMPILER_PATH} is a colon-separated list of
+directories, much like @env{PATH}.  GCC tries the directories thus
+specified when searching for subprograms, if it can't find the
+subprograms using @env{GCC_EXEC_PREFIX}.
+
+@item LIBRARY_PATH
+@findex LIBRARY_PATH
+The value of @env{LIBRARY_PATH} is a colon-separated list of
+directories, much like @env{PATH}.  When configured as a native compiler,
+GCC tries the directories thus specified when searching for special
+linker files, if it can't find them using @env{GCC_EXEC_PREFIX}.  Linking
+using GCC also uses these directories when searching for ordinary
+libraries for the @option{-l} option (but directories specified with
+@option{-L} come first).
+
+@item LANG
+@findex LANG
+@cindex locale definition
+This variable is used to pass locale information to the compiler.  One way in
+which this information is used is to determine the character set to be used
+when character literals, string literals and comments are parsed in C and C++.
+When the compiler is configured to allow multibyte characters,
+the following values for @env{LANG} are recognized:
+
+@table @samp
+@item C-JIS
+Recognize JIS characters.
+@item C-SJIS
+Recognize SJIS characters.
+@item C-EUCJP
+Recognize EUCJP characters.
+@end table
+
+If @env{LANG} is not defined, or if it has some other value, then the
+compiler will use mblen and mbtowc as defined by the default locale to
+recognize and translate multibyte characters.
+@end table
+
+@noindent
+Some additional environments variables affect the behavior of the
+preprocessor.
+
+@include cppenv.texi
+
+@c man end
+
+@node Precompiled Headers
+@section Using Precompiled Headers
+@cindex precompiled headers
+@cindex speed of compilation
+
+Often large projects have many header files that are included in every
+source file.  The time the compiler takes to process these header files
+over and over again can account for nearly all of the time required to
+build the project.  To make builds faster, GCC allows users to
+`precompile' a header file; then, if builds can use the precompiled
+header file they will be much faster.
+
+@strong{Caution:} There are a few known situations where GCC will
+crash when trying to use a precompiled header.  If you have trouble
+with a precompiled header, you should remove the precompiled header
+and compile without it.  In addition, please use GCC's on-line
+defect-tracking system to report any problems you encounter with
+precompiled headers.  @xref{Bugs}.
+
+To create a precompiled header file, simply compile it as you would any
+other file, if necessary using the @option{-x} option to make the driver
+treat it as a C or C++ header file.  You will probably want to use a
+tool like @command{make} to keep the precompiled header up-to-date when
+the headers it contains change.
+
+A precompiled header file will be searched for when @code{#include} is
+seen in the compilation.  As it searches for the included file
+(@pxref{Search Path,,Search Path,cpp,The C Preprocessor}) the
+compiler looks for a precompiled header in each directory just before it
+looks for the include file in that directory.  The name searched for is
+the name specified in the @code{#include} with @samp{.gch} appended.  If
+the precompiled header file can't be used, it is ignored.
+
+For instance, if you have @code{#include "all.h"}, and you have
+@file{all.h.gch} in the same directory as @file{all.h}, then the
+precompiled header file will be used if possible, and the original
+header will be used otherwise.
+
+Alternatively, you might decide to put the precompiled header file in a
+directory and use @option{-I} to ensure that directory is searched
+before (or instead of) the directory containing the original header.
+Then, if you want to check that the precompiled header file is always
+used, you can put a file of the same name as the original header in this
+directory containing an @code{#error} command.
+
+This also works with @option{-include}.  So yet another way to use
+precompiled headers, good for projects not designed with precompiled
+header files in mind, is to simply take most of the header files used by
+a project, include them from another header file, precompile that header
+file, and @option{-include} the precompiled header.  If the header files
+have guards against multiple inclusion, they will be skipped because
+they've already been included (in the precompiled header).
+
+If you need to precompile the same header file for different
+languages, targets, or compiler options, you can instead make a
+@emph{directory} named like @file{all.h.gch}, and put each precompiled
+header in the directory.  (It doesn't matter what you call the files
+in the directory, every precompiled header in the directory will be
+considered.)  The first precompiled header encountered in the
+directory that is valid for this compilation will be used; they're
+searched in no particular order.
+
+There are many other possibilities, limited only by your imagination,
+good sense, and the constraints of your build system.
+
+A precompiled header file can be used only when these conditions apply:
+
+@itemize
+@item
+Only one precompiled header can be used in a particular compilation.
+@item
+A precompiled header can't be used once the first C token is seen.  You
+can have preprocessor directives before a precompiled header; you can
+even include a precompiled header from inside another header, so long as
+there are no C tokens before the @code{#include}.
+@item
+The precompiled header file must be produced for the same language as
+the current compilation.  You can't use a C precompiled header for a C++
+compilation.
+@item
+The precompiled header file must be produced by the same compiler
+version and configuration as the current compilation is using.
+The easiest way to guarantee this is to use the same compiler binary
+for creating and using precompiled headers.
+@item
+Any macros defined before the precompiled header (including with
+@option{-D}) must either be defined in the same way as when the
+precompiled header was generated, or must not affect the precompiled
+header, which usually means that the they don't appear in the
+precompiled header at all.
+@item
+Certain command-line options must be defined in the same way as when the
+precompiled header was generated.  At present, it's not clear which
+options are safe to change and which are not; the safest choice is to
+use exactly the same options when generating and using the precompiled
+header.
+@end itemize
+
+For all of these but the last, the compiler will automatically ignore
+the precompiled header if the conditions aren't met.  For the last item,
+some option changes will cause the precompiled header to be rejected,
+but not all incompatible option combinations have yet been found.  If
+you find a new incompatible combination, please consider filing a bug
+report, see @ref{Bugs}.
+
+@node Running Protoize
+@section Running Protoize
+
+The program @code{protoize} is an optional part of GCC@.  You can use
+it to add prototypes to a program, thus converting the program to ISO
+C in one respect.  The companion program @code{unprotoize} does the
+reverse: it removes argument types from any prototypes that are found.
+
+When you run these programs, you must specify a set of source files as
+command line arguments.  The conversion programs start out by compiling
+these files to see what functions they define.  The information gathered
+about a file @var{foo} is saved in a file named @file{@var{foo}.X}.
+
+After scanning comes actual conversion.  The specified files are all
+eligible to be converted; any files they include (whether sources or
+just headers) are eligible as well.
+
+But not all the eligible files are converted.  By default,
+@code{protoize} and @code{unprotoize} convert only source and header
+files in the current directory.  You can specify additional directories
+whose files should be converted with the @option{-d @var{directory}}
+option.  You can also specify particular files to exclude with the
+@option{-x @var{file}} option.  A file is converted if it is eligible, its
+directory name matches one of the specified directory names, and its
+name within the directory has not been excluded.
+
+Basic conversion with @code{protoize} consists of rewriting most
+function definitions and function declarations to specify the types of
+the arguments.  The only ones not rewritten are those for varargs
+functions.
+
+@code{protoize} optionally inserts prototype declarations at the
+beginning of the source file, to make them available for any calls that
+precede the function's definition.  Or it can insert prototype
+declarations with block scope in the blocks where undeclared functions
+are called.
+
+Basic conversion with @code{unprotoize} consists of rewriting most
+function declarations to remove any argument types, and rewriting
+function definitions to the old-style pre-ISO form.
+
+Both conversion programs print a warning for any function declaration or
+definition that they can't convert.  You can suppress these warnings
+with @option{-q}.
+
+The output from @code{protoize} or @code{unprotoize} replaces the
+original source file.  The original file is renamed to a name ending
+with @samp{.save} (for DOS, the saved filename ends in @samp{.sav}
+without the original @samp{.c} suffix).  If the @samp{.save} (@samp{.sav}
+for DOS) file already exists, then the source file is simply discarded.
+
+@code{protoize} and @code{unprotoize} both depend on GCC itself to
+scan the program and collect information about the functions it uses.
+So neither of these programs will work until GCC is installed.
+
+Here is a table of the options you can use with @code{protoize} and
+@code{unprotoize}.  Each option works with both programs unless
+otherwise stated.
+
+@table @code
+@item -B @var{directory}
+Look for the file @file{SYSCALLS.c.X} in @var{directory}, instead of the
+usual directory (normally @file{/usr/local/lib}).  This file contains
+prototype information about standard system functions.  This option
+applies only to @code{protoize}.
+
+@item -c @var{compilation-options}
+Use @var{compilation-options} as the options when running @command{gcc} to
+produce the @samp{.X} files.  The special option @option{-aux-info} is
+always passed in addition, to tell @command{gcc} to write a @samp{.X} file.
+
+Note that the compilation options must be given as a single argument to
+@code{protoize} or @code{unprotoize}.  If you want to specify several
+@command{gcc} options, you must quote the entire set of compilation options
+to make them a single word in the shell.
+
+There are certain @command{gcc} arguments that you cannot use, because they
+would produce the wrong kind of output.  These include @option{-g},
+@option{-O}, @option{-c}, @option{-S}, and @option{-o} If you include these in
+the @var{compilation-options}, they are ignored.
+
+@item -C
+Rename files to end in @samp{.C} (@samp{.cc} for DOS-based file
+systems) instead of @samp{.c}.  This is convenient if you are converting
+a C program to C++.  This option applies only to @code{protoize}.
+
+@item -g
+Add explicit global declarations.  This means inserting explicit
+declarations at the beginning of each source file for each function
+that is called in the file and was not declared.  These declarations
+precede the first function definition that contains a call to an
+undeclared function.  This option applies only to @code{protoize}.
+
+@item -i @var{string}
+Indent old-style parameter declarations with the string @var{string}.
+This option applies only to @code{protoize}.
+
+@code{unprotoize} converts prototyped function definitions to old-style
+function definitions, where the arguments are declared between the
+argument list and the initial @samp{@{}.  By default, @code{unprotoize}
+uses five spaces as the indentation.  If you want to indent with just
+one space instead, use @option{-i " "}.
+
+@item -k
+Keep the @samp{.X} files.  Normally, they are deleted after conversion
+is finished.
+
+@item -l
+Add explicit local declarations.  @code{protoize} with @option{-l} inserts
+a prototype declaration for each function in each block which calls the
+function without any declaration.  This option applies only to
+@code{protoize}.
+
+@item -n
+Make no real changes.  This mode just prints information about the conversions
+that would have been done without @option{-n}.
+
+@item -N
+Make no @samp{.save} files.  The original files are simply deleted.
+Use this option with caution.
+
+@item -p @var{program}
+Use the program @var{program} as the compiler.  Normally, the name
+@file{gcc} is used.
+
+@item -q
+Work quietly.  Most warnings are suppressed.
+
+@item -v
+Print the version number, just like @option{-v} for @command{gcc}.
+@end table
+
+If you need special compiler options to compile one of your program's
+source files, then you should generate that file's @samp{.X} file
+specially, by running @command{gcc} on that source file with the
+appropriate options and the option @option{-aux-info}.  Then run
+@code{protoize} on the entire set of files.  @code{protoize} will use
+the existing @samp{.X} file because it is newer than the source file.
+For example:
+
+@smallexample
+gcc -Dfoo=bar file1.c -aux-info file1.X
+protoize *.c
+@end smallexample
+
+@noindent
+You need to include the special files along with the rest in the
+@code{protoize} command, even though their @samp{.X} files already
+exist, because otherwise they won't get converted.
+
+@xref{Protoize Caveats}, for more information on how to use
+@code{protoize} successfully.
diff -Naur gcc-3.4.4/gcc/explow.c gcc-3.4.4-ssp/gcc/explow.c
--- gcc-3.4.4/gcc/explow.c	2004-11-23 15:12:28.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/explow.c	2005-05-25 14:03:22.000000000 +0300
@@ -84,7 +84,8 @@
   rtx tem;
   int all_constant = 0;
 
-  if (c == 0)
+  if (c == 0
+      && ! (flag_propolice_protection && x == virtual_stack_vars_rtx))
     return x;
 
  restart:
@@ -185,7 +186,10 @@
       break;
     }
 
-  if (c != 0)
+  /* For the use of stack protection, keep the frame and offset pattern
+     even if the offset is zero.  */
+  if (c != 0
+      || (flag_propolice_protection && x == virtual_stack_vars_rtx))
     x = gen_rtx_PLUS (mode, x, GEN_INT (c));
 
   if (GET_CODE (x) == SYMBOL_REF || GET_CODE (x) == LABEL_REF)
@@ -474,6 +478,26 @@
       if (memory_address_p (mode, oldx))
 	goto win2;
 
+      /* The stack protector keeps the addressing style of a local variable.
+	 LEGITIMIZE_ADDRESS changes the addressing to the machine-dependent
+	 style, so the protector split the frame address to a register using
+	 force_reg. */
+      if (flag_propolice_protection)
+	{
+#define FRAMEADDR_P(X) (GET_CODE (X) == PLUS				\
+			&& XEXP (X, 0) == virtual_stack_vars_rtx	\
+			&& GET_CODE (XEXP (X, 1)) == CONST_INT)
+	  rtx y;
+	  if (FRAMEADDR_P (x))
+	    goto win;
+	  for (y = x; y != 0 && GET_CODE (y) == PLUS; y = XEXP (y, 0))
+	    {
+	      if (FRAMEADDR_P (XEXP (y, 0)))
+		XEXP (y, 0) = force_reg (GET_MODE (XEXP (y, 0)), XEXP (y, 0));
+	      if (FRAMEADDR_P (XEXP (y, 1)))
+		XEXP (y, 1) = force_reg (GET_MODE (XEXP (y, 1)), XEXP (y, 1));
+	    }
+	}
       /* Perform machine-dependent transformations on X
 	 in certain cases.  This is not necessary since the code
 	 below can handle all possible cases, but machine-dependent
diff -Naur gcc-3.4.4/gcc/expr.c gcc-3.4.4-ssp/gcc/expr.c
--- gcc-3.4.4/gcc/expr.c	2005-05-12 00:19:48.000000000 +0300
+++ gcc-3.4.4-ssp/gcc/expr.c	2005-05-25 14:03:22.000000000 +0300
@@ -48,6 +48,7 @@
 #include "intl.h"
 #include "tm_p.h"
 #include "target.h"
+#include "protector.h"
 
 /* Decide whether a function's arguments should be processed
    from first to last or from last to first.
@@ -1060,7 +1061,11 @@
 
    If ENDP is 0 return to, if ENDP is 1 return memory at the end ala
    mempcpy, and if ENDP is 2 return memory the end minus one byte ala
-   stpcpy.  */
+   stpcpy.
+
+   When the stack protector is used at the reverse move, it starts the move
+   instruction from the address within the region of a variable.
+   So it eliminates the first address decrement instruction.  */
 
 rtx
 move_by_pieces (rtx to, rtx from, unsigned HOST_WIDE_INT len,
@@ -1123,6 +1128,8 @@
 
       if (USE_LOAD_PRE_DECREMENT (mode) && data.reverse && ! data.autinc_from)
 	{
+	  if (flag_propolice_protection)
+	    len = len - GET_MODE_SIZE (mode);
 	  data.from_addr = copy_addr_to_reg (plus_constant (from_addr, len));
 	  data.autinc_from = 1;
 	  data.explicit_inc_from = -1;
@@ -1137,6 +1144,8 @@
 	data.from_addr = copy_addr_to_reg (from_addr);
       if (USE_STORE_PRE_DECREMENT (mode) && data.reverse && ! data.autinc_to)
 	{
+	  if (flag_propolice_protection)
+	    len = len - GET_MODE_SIZE (mode);
 	  data.to_addr = copy_addr_to_reg (plus_constant (to_addr, len));
 	  data.autinc_to = 1;
 	  data.explicit_inc_to = -1;
@@ -1280,11 +1289,15 @@
 	from1 = adjust_address (data->from, mode, data->offset);
 
       if (HAVE_PRE_DECREMENT && data->explicit_inc_to < 0)
-	emit_insn (gen_add2_insn (data->to_addr,
-				  GEN_INT (-(HOST_WIDE_INT)size)));
+	/* The stack protector skips the first address decrement instruction
+	   at the reverse move.  */
+	if (!flag_propolice_protection || data->explicit_inc_to < -1)
+	  emit_insn (gen_add2_insn (data->to_addr,
+				    GEN_INT (-(HOST_WIDE_INT)size)));
       if (HAVE_PRE_DECREMENT && data->explicit_inc_from < 0)
-	emit_insn (gen_add2_insn (data->from_addr,
-				  GEN_INT (-(HOST_WIDE_INT)size)));
+	if (!flag_propolice_protection || data->explicit_inc_from < -1)
+	  emit_insn (gen_add2_insn (data->from_addr,
+				    GEN_INT (-(HOST_WIDE_INT)size)));
 
       if (data->to)
 	emit_insn ((*genfun) (to1, from1));
@@ -2475,7 +2488,12 @@
 
       if (USE_STORE_PRE_DECREMENT (mode) && data->reverse && ! data->autinc_to)
 	{
-	  data->to_addr = copy_addr_to_reg (plus_constant (to_addr, data->len));
+	  int len = data->len;
+	  /* The stack protector starts the store instruction from
+	     the address within the region of a variable.  */
+	  if (flag_propolice_protection)
+	    len -= GET_MODE_SIZE (mode);
+	  data->to_addr = copy_addr_to_reg (plus_constant (to_addr, len));
 	  data->autinc_to = 1;
 	  data->explicit_inc_to = -1;
 	}
@@ -2544,8 +2562,11 @@
 	to1 = adjust_address (data->to, mode, data->offset);
 
       if (HAVE_PRE_DECREMENT && data->explicit_inc_to < 0)
-	emit_insn (gen_add2_insn (data->to_addr,
-				  GEN_INT (-(HOST_WIDE_INT) size)));
+	/* The stack protector skips the first address decrement instruction
+	   at the reverse store.  */
+	if (!flag_propolice_protection || data->explicit_inc_to < -1)
+	  emit_insn (gen_add2_insn (data->to_addr,
+				    GEN_INT (-(HOST_WIDE_INT) size)));
 
       cst = (*data->constfun) (data->constfundata, data->offset, mode);
       emit_insn ((*genfun) (to1, cst));
@@ -5701,7 +5722,9 @@
 	  && GET_CODE (XEXP (value, 0)) == PLUS
 	  && GET_CODE (XEXP (XEXP (value, 0), 0)) == REG
 	  && REGNO (XEXP (XEXP (value, 0), 0)) >= FIRST_VIRTUAL_REGISTER
-	  && REGNO (XEXP (XEXP (value, 0), 0)) <= LAST_VIRTUAL_REGISTER)
+	  && REGNO (XEXP (XEXP (value, 0), 0)) <= LAST_VIRTUAL_REGISTER
+	  && (!flag_propolice_protection
+	      || XEXP (XEXP (value, 0), 0) != virtual_stack_vars_rtx))
 	{
 	  rtx temp = expand_simple_binop (GET_MODE (value), code,
 					  XEXP (XEXP (value, 0), 0), op2,
diff -Naur gcc-3.4.4/gcc/flags.h gcc-3.4.4-ssp/gcc/flags.h
--- gcc-3.4.4/gcc/flags.h	2004-02-18 02:09:04.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/flags.h	2005-05-25 14:03:22.000000000 +0300
@@ -186,6 +186,10 @@
 
 extern bool warn_strict_aliasing;
 
+/* Warn when not issuing stack smashing protection for some reason.  */
+
+extern bool warn_stack_protector;
+
 /* Nonzero if generating code to do profiling.  */
 
 extern int profile_flag;
@@ -771,4 +775,12 @@
 #define HONOR_SIGN_DEPENDENT_ROUNDING(MODE) \
   (MODE_HAS_SIGN_DEPENDENT_ROUNDING (MODE) && flag_rounding_math)
 
+/* Nonzero means use propolice as a stack protection method.  */
+
+extern int flag_propolice_protection;
+
+/* Nonzero means use a stack protection method for every function.  */
+
+extern int flag_stack_protection;
+
 #endif /* ! GCC_FLAGS_H */
diff -Naur gcc-3.4.4/gcc/function.c gcc-3.4.4-ssp/gcc/function.c
--- gcc-3.4.4/gcc/function.c	2005-05-12 00:19:49.000000000 +0300
+++ gcc-3.4.4-ssp/gcc/function.c	2005-05-25 14:13:48.000000000 +0300
@@ -63,6 +63,7 @@
 #include "integrate.h"
 #include "langhooks.h"
 #include "target.h"
+#include "protector.h"
 
 #ifndef TRAMPOLINE_ALIGNMENT
 #define TRAMPOLINE_ALIGNMENT FUNCTION_BOUNDARY
@@ -155,6 +156,10 @@
 /* Array of INSN_UIDs to hold the INSN_UIDs for each sibcall epilogue
    in this function.  */
 static GTY(()) varray_type sibcall_epilogue;
+
+/* Current boundary mark for character arrays.  */
+static int temp_boundary_mark = 0;
+
 
 /* In order to evaluate some expressions, such as function calls returning
    structures in memory, we need to temporarily allocate stack locations.
@@ -208,6 +213,8 @@
   /* The size of the slot, including extra space for alignment.  This
      info is for combine_temp_slots.  */
   HOST_WIDE_INT full_size;
+  /* Boundary mark of a character array and the others. This info is for propolice.  */
+  int boundary_mark;
 };
 
 /* This structure is used to record MEMs or pseudos used to replace VAR, any
@@ -641,7 +648,8 @@
    whose lifetime is controlled by CLEANUP_POINT_EXPRs.  KEEP is 3
    if we are to allocate something at an inner level to be treated as
    a variable in the block (e.g., a SAVE_EXPR).
-
+   KEEP is 5 if we allocate a place to return structure.
+   
    TYPE is the type that will be used for the stack slot.  */
 
 rtx
@@ -652,6 +660,9 @@
   struct temp_slot *p, *best_p = 0;
   rtx slot;
 
+  int char_array = (flag_propolice_protection
+               && keep == 1 && search_string_def (type));
+
   /* If SIZE is -1 it means that somebody tried to allocate a temporary
      of a variable size.  */
   if (size == -1)
@@ -711,6 +722,7 @@
 	      p->address = 0;
 	      p->rtl_expr = 0;
 	      p->type = best_p->type;
+	      p->boundary_mark = best_p->boundary_mark;
 	      p->next = temp_slots;
 	      temp_slots = p;
 
@@ -771,6 +783,7 @@
       p->full_size = frame_offset - frame_offset_old;
 #endif
       p->address = 0;
+      p->boundary_mark = char_array ? ++temp_boundary_mark : 0;
       p->next = temp_slots;
       temp_slots = p;
     }
diff -Naur gcc-3.4.4/gcc/gcse.c gcc-3.4.4-ssp/gcc/gcse.c
--- gcc-3.4.4/gcc/gcse.c	2004-10-30 21:02:53.000000000 +0300
+++ gcc-3.4.4-ssp/gcc/gcse.c	2005-05-25 14:03:22.000000000 +0300
@@ -4176,9 +4176,13 @@
 	continue;
 
       /* Find an assignment that sets reg_used and is available
-	 at the start of the block.  */
+	 at the start of the block.
+
+         Skip the copy propagation not to eliminate the register that is
+	 the duplicated pointer of a function argument. It is used for
+	 the function argument protection.  */
       set = find_avail_set (regno, insn);
-      if (! set)
+      if (! set || SET_VOLATILE_P (set->expr))
 	continue;
 
       pat = set->expr;
diff -Naur gcc-3.4.4/gcc/integrate.c gcc-3.4.4-ssp/gcc/integrate.c
--- gcc-3.4.4/gcc/integrate.c	2004-12-05 07:21:01.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/integrate.c	2005-05-25 14:03:22.000000000 +0300
@@ -393,6 +393,11 @@
   /* These args would always appear unused, if not for this.  */
   TREE_USED (copy) = 1;
 
+  /* The inlined variable is marked as INLINE not to change the location
+     by stack protector.  */
+  if (flag_propolice_protection && TREE_CODE (copy) == VAR_DECL)
+    DECL_COPIED (copy) = 1;
+
   /* Set the context for the new declaration.  */
   if (!DECL_CONTEXT (decl))
     /* Globals stay global.  */
@@ -1970,6 +1975,12 @@
 
 	      seq = get_insns ();
 	      end_sequence ();
+#ifdef ARGS_GROWS_DOWNWARD
+	      /* Mark this pointer as the top of the argument
+		 block. The pointer minus one is in the block.  */
+	      if (flag_propolice_protection && GET_CODE (seq) == SET)
+		RTX_INTEGRATED_P (SET_SRC (seq)) = 1;
+#endif
 	      emit_insn_after (seq, map->insns_at_start);
 	      return temp;
 	    }
diff -Naur gcc-3.4.4/gcc/libgcc2.c gcc-3.4.4-ssp/gcc/libgcc2.c
--- gcc-3.4.4/gcc/libgcc2.c	2004-12-15 14:34:24.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/libgcc2.c	2005-05-25 14:03:22.000000000 +0300
@@ -1747,3 +1747,124 @@
 #endif /* no INIT_SECTION_ASM_OP and not CTOR_LISTS_DEFINED_EXTERNALLY */
 #endif /* L_ctors */
 
+
+#ifdef L_stack_smash_handler
+#ifndef _LIBC_PROVIDES_SSP_
+#include <stdio.h>
+#include <string.h>
+#include <fcntl.h>
+#include <unistd.h>
+
+#ifdef _POSIX_SOURCE
+#include <signal.h>
+#endif
+
+#if defined(HAVE_SYSLOG)
+#include <sys/types.h>
+#include <sys/socket.h>
+#include <sys/un.h>
+
+#include <sys/syslog.h>
+#ifndef _PATH_LOG
+#define _PATH_LOG "/dev/log"
+#endif
+#endif
+
+long __guard[8] = {0, 0, 0, 0, 0, 0, 0, 0};
+static void __guard_setup (void) __attribute__ ((constructor));
+
+static void
+__guard_setup (void)
+{
+  int fd;
+  if (__guard[0] != 0)
+    return;
+  fd = open ("/dev/urandom", 0);
+  if (fd != -1) {
+    ssize_t size = read (fd, (char*)&__guard, sizeof(__guard));
+    close (fd) ;
+    if (size == sizeof(__guard))
+      return;
+  }
+  /* If a random generator can't be used, the protector switches the guard
+     to the "terminator canary".  */
+  ((char*)__guard)[0] = 0;
+  ((char*)__guard)[1] = 0;
+  ((char*)__guard)[2] = '\n';
+  ((char*)__guard)[3] = 255;
+}
+
+extern void __stack_smash_handler (char func[], ATTRIBUTE_UNUSED int damaged);
+void
+__stack_smash_handler (char func[], ATTRIBUTE_UNUSED int damaged)
+{
+#if defined (__GNU_LIBRARY__)
+  extern char * __progname;
+#endif
+  const char message[] = ": stack smashing attack in function ";
+  int bufsz = 256, len;
+  char buf[bufsz];
+#if defined(HAVE_SYSLOG)
+  int log_file;
+  struct sockaddr_un sys_log_addr;  /* AF_UNIX address of local logger.  */
+#endif
+#ifdef _POSIX_SOURCE
+  {
+    sigset_t mask;
+    sigfillset (&mask);
+    /* Block all signal handlers except SIGABRT.  */
+    sigdelset (&mask, SIGABRT);
+    sigprocmask (SIG_BLOCK, &mask, NULL);
+  }
+#endif
+
+  /* send LOG_CRIT.  */
+  strcpy (buf, "<2>"); len=3;
+#if defined (__GNU_LIBRARY__)
+  strncat (buf, __progname, bufsz - len - 1);
+  len = strlen (buf);
+#endif
+  if (bufsz > len)
+    {
+      strncat (buf, message, bufsz - len - 1);
+      len = strlen (buf);
+    }
+  if (bufsz > len)
+    {
+      strncat (buf, func, bufsz - len - 1);
+      len = strlen (buf);
+    }
+
+  /* Print error message.  */
+  write (STDERR_FILENO, buf + 3, len - 3);
+#if defined(HAVE_SYSLOG)
+  if ((log_file = socket (AF_UNIX, SOCK_DGRAM, 0)) != -1)
+    {
+
+    /* Send "found" message to the "/dev/log" path.  */
+    sys_log_addr.sun_family = AF_UNIX;
+    (void)strncpy (sys_log_addr.sun_path, _PATH_LOG,
+		   sizeof (sys_log_addr.sun_path) - 1);
+    sys_log_addr.sun_path[sizeof (sys_log_addr.sun_path) - 1] = '\0';
+    sendto(log_file, buf, len, 0, (struct sockaddr *)&sys_log_addr,
+	   sizeof (sys_log_addr));
+  }
+#endif
+
+#ifdef _POSIX_SOURCE
+  {
+    /* Make sure the default handler is associated with SIGABRT.  */
+    struct sigaction sa;
+    
+    memset (&sa, 0, sizeof(struct sigaction));
+    sigfillset (&sa.sa_mask);	/* Block all signals.  */
+    sa.sa_flags = 0;
+    sa.sa_handler = SIG_DFL;
+    sigaction (SIGABRT, &sa, NULL);
+    (void)kill (getpid(), SIGABRT);
+  }
+#endif
+  _exit (127);
+}
+#endif /* _LIBC_PROVIDES_SSP_ */
+#endif /* L_stack_smash_handler */
diff -Naur gcc-3.4.4/gcc/libgcc2.c~ gcc-3.4.4-ssp/gcc/libgcc2.c~
--- gcc-3.4.4/gcc/libgcc2.c~	1970-01-01 02:00:00.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/libgcc2.c~	2004-12-15 14:34:24.000000000 +0200
@@ -0,0 +1,1749 @@
+/* More subroutines needed by GCC output code on some machines.  */
+/* Compile this one with gcc.  */
+/* Copyright (C) 1989, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999,
+   2000, 2001, 2002, 2003  Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 2, or (at your option) any later
+version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+02111-1307, USA.  */
+
+
+/* We include auto-host.h here to get HAVE_GAS_HIDDEN.  This is
+   supposedly valid even though this is a "target" file.  */
+#include "auto-host.h"
+
+/* It is incorrect to include config.h here, because this file is being
+   compiled for the target, and hence definitions concerning only the host
+   do not apply.  */
+#include "tconfig.h"
+#include "tsystem.h"
+#include "coretypes.h"
+#include "tm.h"
+
+/* Don't use `fancy_abort' here even if config.h says to use it.  */
+#ifdef abort
+#undef abort
+#endif
+
+#ifdef HAVE_GAS_HIDDEN
+#define ATTRIBUTE_HIDDEN  __attribute__ ((__visibility__ ("hidden")))
+#else
+#define ATTRIBUTE_HIDDEN
+#endif
+
+#include "libgcc2.h"
+
+#ifdef DECLARE_LIBRARY_RENAMES
+  DECLARE_LIBRARY_RENAMES
+#endif
+
+#if defined (L_negdi2)
+DWtype
+__negdi2 (DWtype u)
+{
+  const DWunion uu = {.ll = u};
+  const DWunion w = { {.low = -uu.s.low,
+		       .high = -uu.s.high - ((UWtype) -uu.s.low > 0) } };
+
+  return w.ll;
+}
+#endif
+
+#ifdef L_addvsi3
+Wtype
+__addvSI3 (Wtype a, Wtype b)
+{
+  const Wtype w = a + b;
+
+  if (b >= 0 ? w < a : w > a)
+    abort ();
+
+  return w;
+}
+#ifdef COMPAT_SIMODE_TRAPPING_ARITHMETIC
+SItype
+__addvsi3 (SItype a, SItype b)
+{
+  const SItype w = a + b;
+
+  if (b >= 0 ? w < a : w > a)
+    abort ();
+
+  return w;
+}
+#endif /* COMPAT_SIMODE_TRAPPING_ARITHMETIC */
+#endif
+
+#ifdef L_addvdi3
+DWtype
+__addvDI3 (DWtype a, DWtype b)
+{
+  const DWtype w = a + b;
+
+  if (b >= 0 ? w < a : w > a)
+    abort ();
+
+  return w;
+}
+#endif
+
+#ifdef L_subvsi3
+Wtype
+__subvSI3 (Wtype a, Wtype b)
+{
+  const Wtype w = a - b;
+
+  if (b >= 0 ? w > a : w < a)
+    abort ();
+
+  return w;
+}
+#ifdef COMPAT_SIMODE_TRAPPING_ARITHMETIC
+SItype
+__subvsi3 (SItype a, SItype b)
+{
+  const SItype w = a - b;
+
+  if (b >= 0 ? w > a : w < a)
+    abort ();
+
+  return w;
+}
+#endif /* COMPAT_SIMODE_TRAPPING_ARITHMETIC */
+#endif
+
+#ifdef L_subvdi3
+DWtype
+__subvDI3 (DWtype a, DWtype b)
+{
+  const DWtype w = a - b;
+
+  if (b >= 0 ? w > a : w < a)
+    abort ();
+
+  return w;
+}
+#endif
+
+#ifdef L_mulvsi3
+#define WORD_SIZE (sizeof (Wtype) * BITS_PER_UNIT)
+Wtype
+__mulvSI3 (Wtype a, Wtype b)
+{
+  const DWtype w = (DWtype) a * (DWtype) b;
+
+  if ((Wtype) (w >> WORD_SIZE) != (Wtype) w >> (WORD_SIZE - 1))
+    abort ();
+
+  return w;
+}
+#ifdef COMPAT_SIMODE_TRAPPING_ARITHMETIC
+#undef WORD_SIZE
+#define WORD_SIZE (sizeof (SItype) * BITS_PER_UNIT)
+SItype
+__mulvsi3 (SItype a, SItype b)
+{
+  const DItype w = (DItype) a * (DItype) b;
+
+  if ((SItype) (w >> WORD_SIZE) != (SItype) w >> (WORD_SIZE-1))
+    abort ();
+
+  return w;
+}
+#endif /* COMPAT_SIMODE_TRAPPING_ARITHMETIC */
+#endif
+
+#ifdef L_negvsi2
+Wtype
+__negvSI2 (Wtype a)
+{
+  const Wtype w = -a;
+
+  if (a >= 0 ? w > 0 : w < 0)
+    abort ();
+
+   return w;
+}
+#ifdef COMPAT_SIMODE_TRAPPING_ARITHMETIC
+SItype
+__negvsi2 (SItype a)
+{
+  const SItype w = -a;
+
+  if (a >= 0 ? w > 0 : w < 0)
+    abort ();
+
+   return w;
+}
+#endif /* COMPAT_SIMODE_TRAPPING_ARITHMETIC */
+#endif
+
+#ifdef L_negvdi2
+DWtype
+__negvDI2 (DWtype a)
+{
+  const DWtype w = -a;
+
+  if (a >= 0 ? w > 0 : w < 0)
+    abort ();
+
+  return w;
+}
+#endif
+
+#ifdef L_absvsi2
+Wtype
+__absvSI2 (Wtype a)
+{
+  Wtype w = a;
+
+  if (a < 0)
+#ifdef L_negvsi2
+    w = __negvSI2 (a);
+#else
+    w = -a;
+
+  if (w < 0)
+    abort ();
+#endif
+
+   return w;
+}
+#ifdef COMPAT_SIMODE_TRAPPING_ARITHMETIC
+SItype
+__absvsi2 (SItype a)
+{
+  SItype w = a;
+
+  if (a < 0)
+#ifdef L_negvsi2
+    w = __negvsi2 (a);
+#else
+    w = -a;
+
+  if (w < 0)
+    abort ();
+#endif
+
+   return w;
+}
+#endif /* COMPAT_SIMODE_TRAPPING_ARITHMETIC */
+#endif
+
+#ifdef L_absvdi2
+DWtype
+__absvDI2 (DWtype a)
+{
+  DWtype w = a;
+
+  if (a < 0)
+#ifdef L_negvdi2
+    w = __negvDI2 (a);
+#else
+    w = -a;
+
+  if (w < 0)
+    abort ();
+#endif
+
+  return w;
+}
+#endif
+
+#ifdef L_mulvdi3
+#define WORD_SIZE (sizeof (Wtype) * BITS_PER_UNIT)
+DWtype
+__mulvDI3 (DWtype u, DWtype v)
+{
+  /* The unchecked multiplication needs 3 Wtype x Wtype multiplications,
+     but the checked multiplication needs only two.  */
+  const DWunion uu = {.ll = u};
+  const DWunion vv = {.ll = v};
+
+  if (__builtin_expect (uu.s.high == uu.s.low >> (WORD_SIZE - 1), 1))
+    {
+      /* u fits in a single Wtype.  */
+      if (__builtin_expect (vv.s.high == vv.s.low >> (WORD_SIZE - 1), 1))
+	{
+	  /* v fits in a single Wtype as well.  */
+	  /* A single multiplication.  No overflow risk.  */
+	  return (DWtype) uu.s.low * (DWtype) vv.s.low;
+	}
+      else
+	{
+	  /* Two multiplications.  */
+	  DWunion w0 = {.ll = (UDWtype) (UWtype) uu.s.low
+			* (UDWtype) (UWtype) vv.s.low};
+	  DWunion w1 = {.ll = (UDWtype) (UWtype) uu.s.low
+			* (UDWtype) (UWtype) vv.s.high};
+
+	  if (vv.s.high < 0)
+	    w1.s.high -= uu.s.low;
+	  if (uu.s.low < 0)
+	    w1.ll -= vv.ll;
+	  w1.ll += (UWtype) w0.s.high;
+	  if (__builtin_expect (w1.s.high == w1.s.low >> (WORD_SIZE - 1), 1))
+	    {
+	      w0.s.high = w1.s.low;
+	      return w0.ll;
+	    }
+	}
+    }
+  else
+    {
+      if (__builtin_expect (vv.s.high == vv.s.low >> (WORD_SIZE - 1), 1))
+	{
+	  /* v fits into a single Wtype.  */
+	  /* Two multiplications.  */
+	  DWunion w0 = {.ll = (UDWtype) (UWtype) uu.s.low
+			* (UDWtype) (UWtype) vv.s.low};
+	  DWunion w1 = {.ll = (UDWtype) (UWtype) uu.s.high
+			* (UDWtype) (UWtype) vv.s.low};
+
+	  if (uu.s.high < 0)
+	    w1.s.high -= vv.s.low;
+	  if (vv.s.low < 0)
+	    w1.ll -= uu.ll;
+	  w1.ll += (UWtype) w0.s.high;
+	  if (__builtin_expect (w1.s.high == w1.s.low >> (WORD_SIZE - 1), 1))
+	    {
+	      w0.s.high = w1.s.low;
+	      return w0.ll;
+	    }
+	}
+      else
+	{
+	  /* A few sign checks and a single multiplication.  */
+	  if (uu.s.high >= 0)
+	    {
+	      if (vv.s.high >= 0)
+		{
+		  if (uu.s.high == 0 && vv.s.high == 0)
+		    {
+		      const DWtype w = (UDWtype) (UWtype) uu.s.low
+			* (UDWtype) (UWtype) vv.s.low;
+		      if (__builtin_expect (w >= 0, 1))
+			return w;
+		    }
+		}
+	      else
+		{
+		  if (uu.s.high == 0 && vv.s.high == (Wtype) -1)
+		    {
+		      DWunion ww = {.ll = (UDWtype) (UWtype) uu.s.low
+				    * (UDWtype) (UWtype) vv.s.low};
+
+		      ww.s.high -= uu.s.low;
+		      if (__builtin_expect (ww.s.high < 0, 1))
+			return ww.ll;
+		    }
+		}
+	    }
+	  else
+	    {
+	      if (vv.s.high >= 0)
+		{
+		  if (uu.s.high == (Wtype) -1 && vv.s.high == 0)
+		    {
+		      DWunion ww = {.ll = (UDWtype) (UWtype) uu.s.low
+				    * (UDWtype) (UWtype) vv.s.low};
+
+		      ww.s.high -= vv.s.low;
+		      if (__builtin_expect (ww.s.high < 0, 1))
+			return ww.ll;
+		    }
+		}
+	      else
+		{
+		  if (uu.s.high == (Wtype) -1 && vv.s.high == (Wtype) - 1)
+		    {
+		      DWunion ww = {.ll = (UDWtype) (UWtype) uu.s.low
+				    * (UDWtype) (UWtype) vv.s.low};
+
+		      ww.s.high -= uu.s.low;
+		      ww.s.high -= vv.s.low;
+		      if (__builtin_expect (ww.s.high >= 0, 1))
+			return ww.ll;
+		    }
+		}
+	    }
+	}
+    }
+
+  /* Overflow.  */
+  abort ();
+}
+#endif
+
+
+/* Unless shift functions are defined with full ANSI prototypes,
+   parameter b will be promoted to int if word_type is smaller than an int.  */
+#ifdef L_lshrdi3
+DWtype
+__lshrdi3 (DWtype u, word_type b)
+{
+  if (b == 0)
+    return u;
+
+  const DWunion uu = {.ll = u};
+  const word_type bm = (sizeof (Wtype) * BITS_PER_UNIT) - b;
+  DWunion w;
+
+  if (bm <= 0)
+    {
+      w.s.high = 0;
+      w.s.low = (UWtype) uu.s.high >> -bm;
+    }
+  else
+    {
+      const UWtype carries = (UWtype) uu.s.high << bm;
+
+      w.s.high = (UWtype) uu.s.high >> b;
+      w.s.low = ((UWtype) uu.s.low >> b) | carries;
+    }
+
+  return w.ll;
+}
+#endif
+
+#ifdef L_ashldi3
+DWtype
+__ashldi3 (DWtype u, word_type b)
+{
+  if (b == 0)
+    return u;
+
+  const DWunion uu = {.ll = u};
+  const word_type bm = (sizeof (Wtype) * BITS_PER_UNIT) - b;
+  DWunion w;
+
+  if (bm <= 0)
+    {
+      w.s.low = 0;
+      w.s.high = (UWtype) uu.s.low << -bm;
+    }
+  else
+    {
+      const UWtype carries = (UWtype) uu.s.low >> bm;
+
+      w.s.low = (UWtype) uu.s.low << b;
+      w.s.high = ((UWtype) uu.s.high << b) | carries;
+    }
+
+  return w.ll;
+}
+#endif
+
+#ifdef L_ashrdi3
+DWtype
+__ashrdi3 (DWtype u, word_type b)
+{
+  if (b == 0)
+    return u;
+
+  const DWunion uu = {.ll = u};
+  const word_type bm = (sizeof (Wtype) * BITS_PER_UNIT) - b;
+  DWunion w;
+
+  if (bm <= 0)
+    {
+      /* w.s.high = 1..1 or 0..0 */
+      w.s.high = uu.s.high >> (sizeof (Wtype) * BITS_PER_UNIT - 1);
+      w.s.low = uu.s.high >> -bm;
+    }
+  else
+    {
+      const UWtype carries = (UWtype) uu.s.high << bm;
+
+      w.s.high = uu.s.high >> b;
+      w.s.low = ((UWtype) uu.s.low >> b) | carries;
+    }
+
+  return w.ll;
+}
+#endif
+
+#ifdef L_ffssi2
+#undef int
+extern int __ffsSI2 (UWtype u);
+int
+__ffsSI2 (UWtype u)
+{
+  UWtype count;
+
+  if (u == 0)
+    return 0;
+
+  count_trailing_zeros (count, u);
+  return count + 1;
+}
+#endif
+
+#ifdef L_ffsdi2
+#undef int
+extern int __ffsDI2 (DWtype u);
+int
+__ffsDI2 (DWtype u)
+{
+  const DWunion uu = {.ll = u};
+  UWtype word, count, add;
+
+  if (uu.s.low != 0)
+    word = uu.s.low, add = 0;
+  else if (uu.s.high != 0)
+    word = uu.s.high, add = BITS_PER_UNIT * sizeof (Wtype);
+  else
+    return 0;
+
+  count_trailing_zeros (count, word);
+  return count + add + 1;
+}
+#endif
+
+#ifdef L_muldi3
+DWtype
+__muldi3 (DWtype u, DWtype v)
+{
+  const DWunion uu = {.ll = u};
+  const DWunion vv = {.ll = v};
+  DWunion w = {.ll = __umulsidi3 (uu.s.low, vv.s.low)};
+
+  w.s.high += ((UWtype) uu.s.low * (UWtype) vv.s.high
+	       + (UWtype) uu.s.high * (UWtype) vv.s.low);
+
+  return w.ll;
+}
+#endif
+
+#if (defined (L_udivdi3) || defined (L_divdi3) || \
+     defined (L_umoddi3) || defined (L_moddi3))
+#if defined (sdiv_qrnnd)
+#define L_udiv_w_sdiv
+#endif
+#endif
+
+#ifdef L_udiv_w_sdiv
+#if defined (sdiv_qrnnd)
+#if (defined (L_udivdi3) || defined (L_divdi3) || \
+     defined (L_umoddi3) || defined (L_moddi3))
+static inline __attribute__ ((__always_inline__))
+#endif
+UWtype
+__udiv_w_sdiv (UWtype *rp, UWtype a1, UWtype a0, UWtype d)
+{
+  UWtype q, r;
+  UWtype c0, c1, b1;
+
+  if ((Wtype) d >= 0)
+    {
+      if (a1 < d - a1 - (a0 >> (W_TYPE_SIZE - 1)))
+	{
+	  /* dividend, divisor, and quotient are nonnegative */
+	  sdiv_qrnnd (q, r, a1, a0, d);
+	}
+      else
+	{
+	  /* Compute c1*2^32 + c0 = a1*2^32 + a0 - 2^31*d */
+	  sub_ddmmss (c1, c0, a1, a0, d >> 1, d << (W_TYPE_SIZE - 1));
+	  /* Divide (c1*2^32 + c0) by d */
+	  sdiv_qrnnd (q, r, c1, c0, d);
+	  /* Add 2^31 to quotient */
+	  q += (UWtype) 1 << (W_TYPE_SIZE - 1);
+	}
+    }
+  else
+    {
+      b1 = d >> 1;			/* d/2, between 2^30 and 2^31 - 1 */
+      c1 = a1 >> 1;			/* A/2 */
+      c0 = (a1 << (W_TYPE_SIZE - 1)) + (a0 >> 1);
+
+      if (a1 < b1)			/* A < 2^32*b1, so A/2 < 2^31*b1 */
+	{
+	  sdiv_qrnnd (q, r, c1, c0, b1); /* (A/2) / (d/2) */
+
+	  r = 2*r + (a0 & 1);		/* Remainder from A/(2*b1) */
+	  if ((d & 1) != 0)
+	    {
+	      if (r >= q)
+		r = r - q;
+	      else if (q - r <= d)
+		{
+		  r = r - q + d;
+		  q--;
+		}
+	      else
+		{
+		  r = r - q + 2*d;
+		  q -= 2;
+		}
+	    }
+	}
+      else if (c1 < b1)			/* So 2^31 <= (A/2)/b1 < 2^32 */
+	{
+	  c1 = (b1 - 1) - c1;
+	  c0 = ~c0;			/* logical NOT */
+
+	  sdiv_qrnnd (q, r, c1, c0, b1); /* (A/2) / (d/2) */
+
+	  q = ~q;			/* (A/2)/b1 */
+	  r = (b1 - 1) - r;
+
+	  r = 2*r + (a0 & 1);		/* A/(2*b1) */
+
+	  if ((d & 1) != 0)
+	    {
+	      if (r >= q)
+		r = r - q;
+	      else if (q - r <= d)
+		{
+		  r = r - q + d;
+		  q--;
+		}
+	      else
+		{
+		  r = r - q + 2*d;
+		  q -= 2;
+		}
+	    }
+	}
+      else				/* Implies c1 = b1 */
+	{				/* Hence a1 = d - 1 = 2*b1 - 1 */
+	  if (a0 >= -d)
+	    {
+	      q = -1;
+	      r = a0 + d;
+	    }
+	  else
+	    {
+	      q = -2;
+	      r = a0 + 2*d;
+	    }
+	}
+    }
+
+  *rp = r;
+  return q;
+}
+#else
+/* If sdiv_qrnnd doesn't exist, define dummy __udiv_w_sdiv.  */
+UWtype
+__udiv_w_sdiv (UWtype *rp __attribute__ ((__unused__)),
+	       UWtype a1 __attribute__ ((__unused__)),
+	       UWtype a0 __attribute__ ((__unused__)),
+	       UWtype d __attribute__ ((__unused__)))
+{
+  return 0;
+}
+#endif
+#endif
+
+#if (defined (L_udivdi3) || defined (L_divdi3) || \
+     defined (L_umoddi3) || defined (L_moddi3))
+#define L_udivmoddi4
+#endif
+
+#ifdef L_clz
+const UQItype __clz_tab[] =
+{
+  0,1,2,2,3,3,3,3,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,
+  6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,
+  7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,
+  7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,
+  8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,
+  8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,
+  8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,
+  8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,
+};
+#endif
+
+#ifdef L_clzsi2
+#undef int
+extern int __clzSI2 (UWtype x);
+int
+__clzSI2 (UWtype x)
+{
+  Wtype ret;
+
+  count_leading_zeros (ret, x);
+
+  return ret;
+}
+#endif
+
+#ifdef L_clzdi2
+#undef int
+extern int __clzDI2 (UDWtype x);
+int
+__clzDI2 (UDWtype x)
+{
+  const DWunion uu = {.ll = x};
+  UWtype word;
+  Wtype ret, add;
+
+  if (uu.s.high)
+    word = uu.s.high, add = 0;
+  else
+    word = uu.s.low, add = W_TYPE_SIZE;
+
+  count_leading_zeros (ret, word);
+  return ret + add;
+}
+#endif
+
+#ifdef L_ctzsi2
+#undef int
+extern int __ctzSI2 (UWtype x);
+int
+__ctzSI2 (UWtype x)
+{
+  Wtype ret;
+
+  count_trailing_zeros (ret, x);
+
+  return ret;
+}
+#endif
+
+#ifdef L_ctzdi2
+#undef int
+extern int __ctzDI2 (UDWtype x);
+int
+__ctzDI2 (UDWtype x)
+{
+  const DWunion uu = {.ll = x};
+  UWtype word;
+  Wtype ret, add;
+
+  if (uu.s.low)
+    word = uu.s.low, add = 0;
+  else
+    word = uu.s.high, add = W_TYPE_SIZE;
+
+  count_trailing_zeros (ret, word);
+  return ret + add;
+}
+#endif
+
+#if (defined (L_popcountsi2) || defined (L_popcountdi2)	\
+     || defined (L_popcount_tab))
+extern const UQItype __popcount_tab[] ATTRIBUTE_HIDDEN;
+#endif
+
+#ifdef L_popcount_tab
+const UQItype __popcount_tab[] =
+{
+    0,1,1,2,1,2,2,3,1,2,2,3,2,3,3,4,1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,
+    1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,
+    1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,
+    2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,
+    1,2,2,3,2,3,3,4,2,3,3,4,3,4,4,5,2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,
+    2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,
+    2,3,3,4,3,4,4,5,3,4,4,5,4,5,5,6,3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,
+    3,4,4,5,4,5,5,6,4,5,5,6,5,6,6,7,4,5,5,6,5,6,6,7,5,6,6,7,6,7,7,8,
+};
+#endif
+
+#ifdef L_popcountsi2
+#undef int
+extern int __popcountSI2 (UWtype x);
+int
+__popcountSI2 (UWtype x)
+{
+  UWtype i, ret = 0;
+
+  for (i = 0; i < W_TYPE_SIZE; i += 8)
+    ret += __popcount_tab[(x >> i) & 0xff];
+
+  return ret;
+}
+#endif
+
+#ifdef L_popcountdi2
+#undef int
+extern int __popcountDI2 (UDWtype x);
+int
+__popcountDI2 (UDWtype x)
+{
+  UWtype i, ret = 0;
+
+  for (i = 0; i < 2*W_TYPE_SIZE; i += 8)
+    ret += __popcount_tab[(x >> i) & 0xff];
+
+  return ret;
+}
+#endif
+
+#ifdef L_paritysi2
+#undef int
+extern int __paritySI2 (UWtype x);
+int
+__paritySI2 (UWtype x)
+{
+#if W_TYPE_SIZE > 64
+# error "fill out the table"
+#endif
+#if W_TYPE_SIZE > 32
+  x ^= x >> 32;
+#endif
+#if W_TYPE_SIZE > 16
+  x ^= x >> 16;
+#endif
+  x ^= x >> 8;
+  x ^= x >> 4;
+  x &= 0xf;
+  return (0x6996 >> x) & 1;
+}
+#endif
+
+#ifdef L_paritydi2
+#undef int
+extern int __parityDI2 (UDWtype x);
+int
+__parityDI2 (UDWtype x)
+{
+  const DWunion uu = {.ll = x};
+  UWtype nx = uu.s.low ^ uu.s.high;
+
+#if W_TYPE_SIZE > 64
+# error "fill out the table"
+#endif
+#if W_TYPE_SIZE > 32
+  nx ^= nx >> 32;
+#endif
+#if W_TYPE_SIZE > 16
+  nx ^= nx >> 16;
+#endif
+  nx ^= nx >> 8;
+  nx ^= nx >> 4;
+  nx &= 0xf;
+  return (0x6996 >> nx) & 1;
+}
+#endif
+
+#ifdef L_udivmoddi4
+
+#if (defined (L_udivdi3) || defined (L_divdi3) || \
+     defined (L_umoddi3) || defined (L_moddi3))
+static inline __attribute__ ((__always_inline__))
+#endif
+UDWtype
+__udivmoddi4 (UDWtype n, UDWtype d, UDWtype *rp)
+{
+  const DWunion nn = {.ll = n};
+  const DWunion dd = {.ll = d};
+  DWunion rr;
+  UWtype d0, d1, n0, n1, n2;
+  UWtype q0, q1;
+  UWtype b, bm;
+
+  d0 = dd.s.low;
+  d1 = dd.s.high;
+  n0 = nn.s.low;
+  n1 = nn.s.high;
+
+#if !UDIV_NEEDS_NORMALIZATION
+  if (d1 == 0)
+    {
+      if (d0 > n1)
+	{
+	  /* 0q = nn / 0D */
+
+	  udiv_qrnnd (q0, n0, n1, n0, d0);
+	  q1 = 0;
+
+	  /* Remainder in n0.  */
+	}
+      else
+	{
+	  /* qq = NN / 0d */
+
+	  if (d0 == 0)
+	    d0 = 1 / d0;	/* Divide intentionally by zero.  */
+
+	  udiv_qrnnd (q1, n1, 0, n1, d0);
+	  udiv_qrnnd (q0, n0, n1, n0, d0);
+
+	  /* Remainder in n0.  */
+	}
+
+      if (rp != 0)
+	{
+	  rr.s.low = n0;
+	  rr.s.high = 0;
+	  *rp = rr.ll;
+	}
+    }
+
+#else /* UDIV_NEEDS_NORMALIZATION */
+
+  if (d1 == 0)
+    {
+      if (d0 > n1)
+	{
+	  /* 0q = nn / 0D */
+
+	  count_leading_zeros (bm, d0);
+
+	  if (bm != 0)
+	    {
+	      /* Normalize, i.e. make the most significant bit of the
+		 denominator set.  */
+
+	      d0 = d0 << bm;
+	      n1 = (n1 << bm) | (n0 >> (W_TYPE_SIZE - bm));
+	      n0 = n0 << bm;
+	    }
+
+	  udiv_qrnnd (q0, n0, n1, n0, d0);
+	  q1 = 0;
+
+	  /* Remainder in n0 >> bm.  */
+	}
+      else
+	{
+	  /* qq = NN / 0d */
+
+	  if (d0 == 0)
+	    d0 = 1 / d0;	/* Divide intentionally by zero.  */
+
+	  count_leading_zeros (bm, d0);
+
+	  if (bm == 0)
+	    {
+	      /* From (n1 >= d0) /\ (the most significant bit of d0 is set),
+		 conclude (the most significant bit of n1 is set) /\ (the
+		 leading quotient digit q1 = 1).
+
+		 This special case is necessary, not an optimization.
+		 (Shifts counts of W_TYPE_SIZE are undefined.)  */
+
+	      n1 -= d0;
+	      q1 = 1;
+	    }
+	  else
+	    {
+	      /* Normalize.  */
+
+	      b = W_TYPE_SIZE - bm;
+
+	      d0 = d0 << bm;
+	      n2 = n1 >> b;
+	      n1 = (n1 << bm) | (n0 >> b);
+	      n0 = n0 << bm;
+
+	      udiv_qrnnd (q1, n1, n2, n1, d0);
+	    }
+
+	  /* n1 != d0...  */
+
+	  udiv_qrnnd (q0, n0, n1, n0, d0);
+
+	  /* Remainder in n0 >> bm.  */
+	}
+
+      if (rp != 0)
+	{
+	  rr.s.low = n0 >> bm;
+	  rr.s.high = 0;
+	  *rp = rr.ll;
+	}
+    }
+#endif /* UDIV_NEEDS_NORMALIZATION */
+
+  else
+    {
+      if (d1 > n1)
+	{
+	  /* 00 = nn / DD */
+
+	  q0 = 0;
+	  q1 = 0;
+
+	  /* Remainder in n1n0.  */
+	  if (rp != 0)
+	    {
+	      rr.s.low = n0;
+	      rr.s.high = n1;
+	      *rp = rr.ll;
+	    }
+	}
+      else
+	{
+	  /* 0q = NN / dd */
+
+	  count_leading_zeros (bm, d1);
+	  if (bm == 0)
+	    {
+	      /* From (n1 >= d1) /\ (the most significant bit of d1 is set),
+		 conclude (the most significant bit of n1 is set) /\ (the
+		 quotient digit q0 = 0 or 1).
+
+		 This special case is necessary, not an optimization.  */
+
+	      /* The condition on the next line takes advantage of that
+		 n1 >= d1 (true due to program flow).  */
+	      if (n1 > d1 || n0 >= d0)
+		{
+		  q0 = 1;
+		  sub_ddmmss (n1, n0, n1, n0, d1, d0);
+		}
+	      else
+		q0 = 0;
+
+	      q1 = 0;
+
+	      if (rp != 0)
+		{
+		  rr.s.low = n0;
+		  rr.s.high = n1;
+		  *rp = rr.ll;
+		}
+	    }
+	  else
+	    {
+	      UWtype m1, m0;
+	      /* Normalize.  */
+
+	      b = W_TYPE_SIZE - bm;
+
+	      d1 = (d1 << bm) | (d0 >> b);
+	      d0 = d0 << bm;
+	      n2 = n1 >> b;
+	      n1 = (n1 << bm) | (n0 >> b);
+	      n0 = n0 << bm;
+
+	      udiv_qrnnd (q0, n1, n2, n1, d1);
+	      umul_ppmm (m1, m0, q0, d0);
+
+	      if (m1 > n1 || (m1 == n1 && m0 > n0))
+		{
+		  q0--;
+		  sub_ddmmss (m1, m0, m1, m0, d1, d0);
+		}
+
+	      q1 = 0;
+
+	      /* Remainder in (n1n0 - m1m0) >> bm.  */
+	      if (rp != 0)
+		{
+		  sub_ddmmss (n1, n0, n1, n0, m1, m0);
+		  rr.s.low = (n1 << b) | (n0 >> bm);
+		  rr.s.high = n1 >> bm;
+		  *rp = rr.ll;
+		}
+	    }
+	}
+    }
+
+  const DWunion ww = {{.low = q0, .high = q1}};
+  return ww.ll;
+}
+#endif
+
+#ifdef L_divdi3
+DWtype
+__divdi3 (DWtype u, DWtype v)
+{
+  word_type c = 0;
+  DWunion uu = {.ll = u};
+  DWunion vv = {.ll = v};
+  DWtype w;
+
+  if (uu.s.high < 0)
+    c = ~c,
+    uu.ll = -uu.ll;
+  if (vv.s.high < 0)
+    c = ~c,
+    vv.ll = -vv.ll;
+
+  w = __udivmoddi4 (uu.ll, vv.ll, (UDWtype *) 0);
+  if (c)
+    w = -w;
+
+  return w;
+}
+#endif
+
+#ifdef L_moddi3
+DWtype
+__moddi3 (DWtype u, DWtype v)
+{
+  word_type c = 0;
+  DWunion uu = {.ll = u};
+  DWunion vv = {.ll = v};
+  DWtype w;
+
+  if (uu.s.high < 0)
+    c = ~c,
+    uu.ll = -uu.ll;
+  if (vv.s.high < 0)
+    vv.ll = -vv.ll;
+
+  (void) __udivmoddi4 (uu.ll, vv.ll, &w);
+  if (c)
+    w = -w;
+
+  return w;
+}
+#endif
+
+#ifdef L_umoddi3
+UDWtype
+__umoddi3 (UDWtype u, UDWtype v)
+{
+  UDWtype w;
+
+  (void) __udivmoddi4 (u, v, &w);
+
+  return w;
+}
+#endif
+
+#ifdef L_udivdi3
+UDWtype
+__udivdi3 (UDWtype n, UDWtype d)
+{
+  return __udivmoddi4 (n, d, (UDWtype *) 0);
+}
+#endif
+
+#ifdef L_cmpdi2
+word_type
+__cmpdi2 (DWtype a, DWtype b)
+{
+  const DWunion au = {.ll = a};
+  const DWunion bu = {.ll = b};
+
+  if (au.s.high < bu.s.high)
+    return 0;
+  else if (au.s.high > bu.s.high)
+    return 2;
+  if ((UWtype) au.s.low < (UWtype) bu.s.low)
+    return 0;
+  else if ((UWtype) au.s.low > (UWtype) bu.s.low)
+    return 2;
+  return 1;
+}
+#endif
+
+#ifdef L_ucmpdi2
+word_type
+__ucmpdi2 (DWtype a, DWtype b)
+{
+  const DWunion au = {.ll = a};
+  const DWunion bu = {.ll = b};
+
+  if ((UWtype) au.s.high < (UWtype) bu.s.high)
+    return 0;
+  else if ((UWtype) au.s.high > (UWtype) bu.s.high)
+    return 2;
+  if ((UWtype) au.s.low < (UWtype) bu.s.low)
+    return 0;
+  else if ((UWtype) au.s.low > (UWtype) bu.s.low)
+    return 2;
+  return 1;
+}
+#endif
+
+#if defined(L_fixunstfdi) && (LIBGCC2_LONG_DOUBLE_TYPE_SIZE == 128)
+#define WORD_SIZE (sizeof (Wtype) * BITS_PER_UNIT)
+#define HIGH_WORD_COEFF (((UDWtype) 1) << WORD_SIZE)
+
+DWtype
+__fixunstfDI (TFtype a)
+{
+  if (a < 0)
+    return 0;
+
+  /* Compute high word of result, as a flonum.  */
+  const TFtype b = (a / HIGH_WORD_COEFF);
+  /* Convert that to fixed (but not to DWtype!),
+     and shift it into the high word.  */
+  UDWtype v = (UWtype) b;
+  v <<= WORD_SIZE;
+  /* Remove high part from the TFtype, leaving the low part as flonum.  */
+  a -= (TFtype)v;
+  /* Convert that to fixed (but not to DWtype!) and add it in.
+     Sometimes A comes out negative.  This is significant, since
+     A has more bits than a long int does.  */
+  if (a < 0)
+    v -= (UWtype) (- a);
+  else
+    v += (UWtype) a;
+  return v;
+}
+#endif
+
+#if defined(L_fixtfdi) && (LIBGCC2_LONG_DOUBLE_TYPE_SIZE == 128)
+DWtype
+__fixtfdi (TFtype a)
+{
+  if (a < 0)
+    return - __fixunstfDI (-a);
+  return __fixunstfDI (a);
+}
+#endif
+
+#if defined(L_fixunsxfdi) && (LIBGCC2_LONG_DOUBLE_TYPE_SIZE == 96)
+#define WORD_SIZE (sizeof (Wtype) * BITS_PER_UNIT)
+#define HIGH_WORD_COEFF (((UDWtype) 1) << WORD_SIZE)
+
+DWtype
+__fixunsxfDI (XFtype a)
+{
+  if (a < 0)
+    return 0;
+
+  /* Compute high word of result, as a flonum.  */
+  const XFtype b = (a / HIGH_WORD_COEFF);
+  /* Convert that to fixed (but not to DWtype!),
+     and shift it into the high word.  */
+  UDWtype v = (UWtype) b;
+  v <<= WORD_SIZE;
+  /* Remove high part from the XFtype, leaving the low part as flonum.  */
+  a -= (XFtype)v;
+  /* Convert that to fixed (but not to DWtype!) and add it in.
+     Sometimes A comes out negative.  This is significant, since
+     A has more bits than a long int does.  */
+  if (a < 0)
+    v -= (UWtype) (- a);
+  else
+    v += (UWtype) a;
+  return v;
+}
+#endif
+
+#if defined(L_fixxfdi) && (LIBGCC2_LONG_DOUBLE_TYPE_SIZE == 96)
+DWtype
+__fixxfdi (XFtype a)
+{
+  if (a < 0)
+    return - __fixunsxfDI (-a);
+  return __fixunsxfDI (a);
+}
+#endif
+
+#ifdef L_fixunsdfdi
+#define WORD_SIZE (sizeof (Wtype) * BITS_PER_UNIT)
+#define HIGH_WORD_COEFF (((UDWtype) 1) << WORD_SIZE)
+
+DWtype
+__fixunsdfDI (DFtype a)
+{
+  /* Get high part of result.  The division here will just moves the radix
+     point and will not cause any rounding.  Then the conversion to integral
+     type chops result as desired.  */
+  const UWtype hi = a / HIGH_WORD_COEFF;
+
+  /* Get low part of result.  Convert `hi' to floating type and scale it back,
+     then subtract this from the number being converted.  This leaves the low
+     part.  Convert that to integral type.  */
+  const UWtype lo = (a - ((DFtype) hi) * HIGH_WORD_COEFF);
+
+  /* Assemble result from the two parts.  */
+  return ((UDWtype) hi << WORD_SIZE) | lo;
+}
+#endif
+
+#ifdef L_fixdfdi
+DWtype
+__fixdfdi (DFtype a)
+{
+  if (a < 0)
+    return - __fixunsdfDI (-a);
+  return __fixunsdfDI (a);
+}
+#endif
+
+#ifdef L_fixunssfdi
+#define WORD_SIZE (sizeof (Wtype) * BITS_PER_UNIT)
+#define HIGH_WORD_COEFF (((UDWtype) 1) << WORD_SIZE)
+
+DWtype
+__fixunssfDI (SFtype original_a)
+{
+  /* Convert the SFtype to a DFtype, because that is surely not going
+     to lose any bits.  Some day someone else can write a faster version
+     that avoids converting to DFtype, and verify it really works right.  */
+  const DFtype a = original_a;
+
+  /* Get high part of result.  The division here will just moves the radix
+     point and will not cause any rounding.  Then the conversion to integral
+     type chops result as desired.  */
+  const UWtype hi = a / HIGH_WORD_COEFF;
+
+  /* Get low part of result.  Convert `hi' to floating type and scale it back,
+     then subtract this from the number being converted.  This leaves the low
+     part.  Convert that to integral type.  */
+  const UWtype lo = (a - ((DFtype) hi) * HIGH_WORD_COEFF);
+
+  /* Assemble result from the two parts.  */
+  return ((UDWtype) hi << WORD_SIZE) | lo;
+}
+#endif
+
+#ifdef L_fixsfdi
+DWtype
+__fixsfdi (SFtype a)
+{
+  if (a < 0)
+    return - __fixunssfDI (-a);
+  return __fixunssfDI (a);
+}
+#endif
+
+#if defined(L_floatdixf) && (LIBGCC2_LONG_DOUBLE_TYPE_SIZE == 96)
+#define WORD_SIZE (sizeof (Wtype) * BITS_PER_UNIT)
+#define HIGH_HALFWORD_COEFF (((UDWtype) 1) << (WORD_SIZE / 2))
+#define HIGH_WORD_COEFF (((UDWtype) 1) << WORD_SIZE)
+
+XFtype
+__floatdixf (DWtype u)
+{
+  XFtype d = (Wtype) (u >> WORD_SIZE);
+  d *= HIGH_HALFWORD_COEFF;
+  d *= HIGH_HALFWORD_COEFF;
+  d += (UWtype) (u & (HIGH_WORD_COEFF - 1));
+
+  return d;
+}
+#endif
+
+#if defined(L_floatditf) && (LIBGCC2_LONG_DOUBLE_TYPE_SIZE == 128)
+#define WORD_SIZE (sizeof (Wtype) * BITS_PER_UNIT)
+#define HIGH_HALFWORD_COEFF (((UDWtype) 1) << (WORD_SIZE / 2))
+#define HIGH_WORD_COEFF (((UDWtype) 1) << WORD_SIZE)
+
+TFtype
+__floatditf (DWtype u)
+{
+  TFtype d = (Wtype) (u >> WORD_SIZE);
+  d *= HIGH_HALFWORD_COEFF;
+  d *= HIGH_HALFWORD_COEFF;
+  d += (UWtype) (u & (HIGH_WORD_COEFF - 1));
+
+  return d;
+}
+#endif
+
+#ifdef L_floatdidf
+#define WORD_SIZE (sizeof (Wtype) * BITS_PER_UNIT)
+#define HIGH_HALFWORD_COEFF (((UDWtype) 1) << (WORD_SIZE / 2))
+#define HIGH_WORD_COEFF (((UDWtype) 1) << WORD_SIZE)
+
+DFtype
+__floatdidf (DWtype u)
+{
+  DFtype d = (Wtype) (u >> WORD_SIZE);
+  d *= HIGH_HALFWORD_COEFF;
+  d *= HIGH_HALFWORD_COEFF;
+  d += (UWtype) (u & (HIGH_WORD_COEFF - 1));
+
+  return d;
+}
+#endif
+
+#ifdef L_floatdisf
+#define WORD_SIZE (sizeof (Wtype) * BITS_PER_UNIT)
+#define HIGH_HALFWORD_COEFF (((UDWtype) 1) << (WORD_SIZE / 2))
+#define HIGH_WORD_COEFF (((UDWtype) 1) << WORD_SIZE)
+
+#define DI_SIZE (sizeof (DWtype) * BITS_PER_UNIT)
+#define DF_SIZE DBL_MANT_DIG
+#define SF_SIZE FLT_MANT_DIG
+
+SFtype
+__floatdisf (DWtype u)
+{
+  /* Protect against double-rounding error.
+     Represent any low-order bits, that might be truncated in DFmode,
+     by a bit that won't be lost.  The bit can go in anywhere below the
+     rounding position of the SFmode.  A fixed mask and bit position
+     handles all usual configurations.  It doesn't handle the case
+     of 128-bit DImode, however.  */
+  if (DF_SIZE < DI_SIZE
+      && DF_SIZE > (DI_SIZE - DF_SIZE + SF_SIZE))
+    {
+#define REP_BIT ((UDWtype) 1 << (DI_SIZE - DF_SIZE))
+      if (! (- ((DWtype) 1 << DF_SIZE) < u
+	     && u < ((DWtype) 1 << DF_SIZE)))
+	{
+	  if ((UDWtype) u & (REP_BIT - 1))
+	    {
+	      u &= ~ (REP_BIT - 1);
+	      u |= REP_BIT;
+	    }
+	}
+    }
+  /* Do the calculation in DFmode
+     so that we don't lose any of the precision of the high word
+     while multiplying it.  */
+  DFtype f = (Wtype) (u >> WORD_SIZE);
+  f *= HIGH_HALFWORD_COEFF;
+  f *= HIGH_HALFWORD_COEFF;
+  f += (UWtype) (u & (HIGH_WORD_COEFF - 1));
+
+  return (SFtype) f;
+}
+#endif
+
+#if defined(L_fixunsxfsi) && LIBGCC2_LONG_DOUBLE_TYPE_SIZE == 96
+/* Reenable the normal types, in case limits.h needs them.  */
+#undef char
+#undef short
+#undef int
+#undef long
+#undef unsigned
+#undef float
+#undef double
+#undef MIN
+#undef MAX
+#include <limits.h>
+
+UWtype
+__fixunsxfSI (XFtype a)
+{
+  if (a >= - (DFtype) Wtype_MIN)
+    return (Wtype) (a + Wtype_MIN) - Wtype_MIN;
+  return (Wtype) a;
+}
+#endif
+
+#ifdef L_fixunsdfsi
+/* Reenable the normal types, in case limits.h needs them.  */
+#undef char
+#undef short
+#undef int
+#undef long
+#undef unsigned
+#undef float
+#undef double
+#undef MIN
+#undef MAX
+#include <limits.h>
+
+UWtype
+__fixunsdfSI (DFtype a)
+{
+  if (a >= - (DFtype) Wtype_MIN)
+    return (Wtype) (a + Wtype_MIN) - Wtype_MIN;
+  return (Wtype) a;
+}
+#endif
+
+#ifdef L_fixunssfsi
+/* Reenable the normal types, in case limits.h needs them.  */
+#undef char
+#undef short
+#undef int
+#undef long
+#undef unsigned
+#undef float
+#undef double
+#undef MIN
+#undef MAX
+#include <limits.h>
+
+UWtype
+__fixunssfSI (SFtype a)
+{
+  if (a >= - (SFtype) Wtype_MIN)
+    return (Wtype) (a + Wtype_MIN) - Wtype_MIN;
+  return (Wtype) a;
+}
+#endif
+
+/* From here on down, the routines use normal data types.  */
+
+#define SItype bogus_type
+#define USItype bogus_type
+#define DItype bogus_type
+#define UDItype bogus_type
+#define SFtype bogus_type
+#define DFtype bogus_type
+#undef Wtype
+#undef UWtype
+#undef HWtype
+#undef UHWtype
+#undef DWtype
+#undef UDWtype
+
+#undef char
+#undef short
+#undef int
+#undef long
+#undef unsigned
+#undef float
+#undef double
+
+#ifdef L__gcc_bcmp
+
+/* Like bcmp except the sign is meaningful.
+   Result is negative if S1 is less than S2,
+   positive if S1 is greater, 0 if S1 and S2 are equal.  */
+
+int
+__gcc_bcmp (const unsigned char *s1, const unsigned char *s2, size_t size)
+{
+  while (size > 0)
+    {
+      const unsigned char c1 = *s1++, c2 = *s2++;
+      if (c1 != c2)
+	return c1 - c2;
+      size--;
+    }
+  return 0;
+}
+
+#endif
+
+/* __eprintf used to be used by GCC's private version of <assert.h>.
+   We no longer provide that header, but this routine remains in libgcc.a
+   for binary backward compatibility.  Note that it is not included in
+   the shared version of libgcc.  */
+#ifdef L_eprintf
+#ifndef inhibit_libc
+
+#undef NULL /* Avoid errors if stdio.h and our stddef.h mismatch.  */
+#include <stdio.h>
+
+void
+__eprintf (const char *string, const char *expression,
+	   unsigned int line, const char *filename)
+{
+  fprintf (stderr, string, expression, line, filename);
+  fflush (stderr);
+  abort ();
+}
+
+#endif
+#endif
+
+
+#ifdef L_clear_cache
+/* Clear part of an instruction cache.  */
+
+void
+__clear_cache (char *beg __attribute__((__unused__)),
+	       char *end __attribute__((__unused__)))
+{
+#ifdef CLEAR_INSN_CACHE
+  CLEAR_INSN_CACHE (beg, end);
+#endif /* CLEAR_INSN_CACHE */
+}
+
+#endif /* L_clear_cache */
+
+#ifdef L_enable_execute_stack
+/* Attempt to turn on execute permission for the stack.  */
+
+#ifdef ENABLE_EXECUTE_STACK
+  ENABLE_EXECUTE_STACK
+#else
+void
+__enable_execute_stack (void *addr __attribute__((__unused__)))
+{}
+#endif /* ENABLE_EXECUTE_STACK */
+
+#endif /* L_enable_execute_stack */
+
+#ifdef L_trampoline
+
+/* Jump to a trampoline, loading the static chain address.  */
+
+#if defined(WINNT) && ! defined(__CYGWIN__) && ! defined (_UWIN)
+
+long
+getpagesize (void)
+{
+#ifdef _ALPHA_
+  return 8192;
+#else
+  return 4096;
+#endif
+}
+
+#ifdef __i386__
+extern int VirtualProtect (char *, int, int, int *) __attribute__((stdcall));
+#endif
+
+int
+mprotect (char *addr, int len, int prot)
+{
+  int np, op;
+
+  if (prot == 7)
+    np = 0x40;
+  else if (prot == 5)
+    np = 0x20;
+  else if (prot == 4)
+    np = 0x10;
+  else if (prot == 3)
+    np = 0x04;
+  else if (prot == 1)
+    np = 0x02;
+  else if (prot == 0)
+    np = 0x01;
+
+  if (VirtualProtect (addr, len, np, &op))
+    return 0;
+  else
+    return -1;
+}
+
+#endif /* WINNT && ! __CYGWIN__ && ! _UWIN */
+
+#ifdef TRANSFER_FROM_TRAMPOLINE
+TRANSFER_FROM_TRAMPOLINE
+#endif
+#endif /* L_trampoline */
+
+#ifndef __CYGWIN__
+#ifdef L__main
+
+#include "gbl-ctors.h"
+/* Some systems use __main in a way incompatible with its use in gcc, in these
+   cases use the macros NAME__MAIN to give a quoted symbol and SYMBOL__MAIN to
+   give the same symbol without quotes for an alternative entry point.  You
+   must define both, or neither.  */
+#ifndef NAME__MAIN
+#define NAME__MAIN "__main"
+#define SYMBOL__MAIN __main
+#endif
+
+#ifdef INIT_SECTION_ASM_OP
+#undef HAS_INIT_SECTION
+#define HAS_INIT_SECTION
+#endif
+
+#if !defined (HAS_INIT_SECTION) || !defined (OBJECT_FORMAT_ELF)
+
+/* Some ELF crosses use crtstuff.c to provide __CTOR_LIST__, but use this
+   code to run constructors.  In that case, we need to handle EH here, too.  */
+
+#ifdef EH_FRAME_SECTION_NAME
+#include "unwind-dw2-fde.h"
+extern unsigned char __EH_FRAME_BEGIN__[];
+#endif
+
+/* Run all the global destructors on exit from the program.  */
+
+void
+__do_global_dtors (void)
+{
+#ifdef DO_GLOBAL_DTORS_BODY
+  DO_GLOBAL_DTORS_BODY;
+#else
+  static func_ptr *p = __DTOR_LIST__ + 1;
+  while (*p)
+    {
+      p++;
+      (*(p-1)) ();
+    }
+#endif
+#if defined (EH_FRAME_SECTION_NAME) && !defined (HAS_INIT_SECTION)
+  {
+    static int completed = 0;
+    if (! completed)
+      {
+	completed = 1;
+	__deregister_frame_info (__EH_FRAME_BEGIN__);
+      }
+  }
+#endif
+}
+#endif
+
+#ifndef HAS_INIT_SECTION
+/* Run all the global constructors on entry to the program.  */
+
+void
+__do_global_ctors (void)
+{
+#ifdef EH_FRAME_SECTION_NAME
+  {
+    static struct object object;
+    __register_frame_info (__EH_FRAME_BEGIN__, &object);
+  }
+#endif
+  DO_GLOBAL_CTORS_BODY;
+  atexit (__do_global_dtors);
+}
+#endif /* no HAS_INIT_SECTION */
+
+#if !defined (HAS_INIT_SECTION) || defined (INVOKE__main)
+/* Subroutine called automatically by `main'.
+   Compiling a global function named `main'
+   produces an automatic call to this function at the beginning.
+
+   For many systems, this routine calls __do_global_ctors.
+   For systems which support a .init section we use the .init section
+   to run __do_global_ctors, so we need not do anything here.  */
+
+extern void SYMBOL__MAIN (void);
+void
+SYMBOL__MAIN (void)
+{
+  /* Support recursive calls to `main': run initializers just once.  */
+  static int initialized;
+  if (! initialized)
+    {
+      initialized = 1;
+      __do_global_ctors ();
+    }
+}
+#endif /* no HAS_INIT_SECTION or INVOKE__main */
+
+#endif /* L__main */
+#endif /* __CYGWIN__ */
+
+#ifdef L_ctors
+
+#include "gbl-ctors.h"
+
+/* Provide default definitions for the lists of constructors and
+   destructors, so that we don't get linker errors.  These symbols are
+   intentionally bss symbols, so that gld and/or collect will provide
+   the right values.  */
+
+/* We declare the lists here with two elements each,
+   so that they are valid empty lists if no other definition is loaded.
+
+   If we are using the old "set" extensions to have the gnu linker
+   collect ctors and dtors, then we __CTOR_LIST__ and __DTOR_LIST__
+   must be in the bss/common section.
+
+   Long term no port should use those extensions.  But many still do.  */
+#if !defined(INIT_SECTION_ASM_OP) && !defined(CTOR_LISTS_DEFINED_EXTERNALLY)
+#if defined (TARGET_ASM_CONSTRUCTOR) || defined (USE_COLLECT2)
+func_ptr __CTOR_LIST__[2] = {0, 0};
+func_ptr __DTOR_LIST__[2] = {0, 0};
+#else
+func_ptr __CTOR_LIST__[2];
+func_ptr __DTOR_LIST__[2];
+#endif
+#endif /* no INIT_SECTION_ASM_OP and not CTOR_LISTS_DEFINED_EXTERNALLY */
+#endif /* L_ctors */
+
diff -Naur gcc-3.4.4/gcc/libgcc-std.ver gcc-3.4.4-ssp/gcc/libgcc-std.ver
--- gcc-3.4.4/gcc/libgcc-std.ver	2004-12-15 14:34:25.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/libgcc-std.ver	2005-05-25 14:03:22.000000000 +0300
@@ -174,6 +174,12 @@
   _Unwind_SjLj_RaiseException
   _Unwind_SjLj_ForcedUnwind
   _Unwind_SjLj_Resume
+
+%if !defined(_LIBC_PROVIDES_SSP_)
+  # stack smash handler symbols
+  __guard
+  __stack_smash_handler
+%endif
 }
 
 %inherit GCC_3.3 GCC_3.0
diff -Naur gcc-3.4.4/gcc/loop.c gcc-3.4.4-ssp/gcc/loop.c
--- gcc-3.4.4/gcc/loop.c	2005-01-06 21:12:03.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/loop.c	2005-05-25 14:03:22.000000000 +0300
@@ -6525,6 +6525,14 @@
   if (GET_CODE (*mult_val) == USE)
     *mult_val = XEXP (*mult_val, 0);
 
+#ifndef FRAME_GROWS_DOWNWARD
+  if (flag_propolice_protection
+      && GET_CODE (*add_val) == PLUS
+      && (XEXP (*add_val, 0) == frame_pointer_rtx
+	  || XEXP (*add_val, 1) == frame_pointer_rtx))
+    return 0;
+#endif
+
   if (is_addr)
     *pbenefit += address_cost (orig_x, addr_mode) - reg_address_cost;
   else
diff -Naur gcc-3.4.4/gcc/Makefile.in gcc-3.4.4-ssp/gcc/Makefile.in
--- gcc-3.4.4/gcc/Makefile.in	2005-02-24 11:26:57.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/Makefile.in	2005-05-25 14:03:21.000000000 +0300
@@ -871,7 +871,7 @@
  sibcall.o simplify-rtx.o sreal.o stmt.o stor-layout.o stringpool.o 	   \
  targhooks.o timevar.o toplev.o tracer.o tree.o tree-dump.o unroll.o	   \
  varasm.o varray.o version.o vmsdbgout.o xcoffout.o alloc-pool.o	   \
- et-forest.o cfghooks.o bt-load.o pretty-print.o $(GGC) web.o
+ et-forest.o cfghooks.o bt-load.o pretty-print.o $(GGC) web.o protector.o
 
 OBJS-md = $(out_object_file)
 OBJS-archive = $(EXTRA_OBJS) $(host_hook_obj) hashtable.o tree-inline.o	   \
@@ -1556,7 +1556,7 @@
    langhooks.h insn-flags.h cfglayout.h real.h cfgloop.h \
    hosthooks.h $(LANGHOOKS_DEF_H) cgraph.h $(COVERAGE_H) alloc-pool.h
 	$(CC) $(ALL_CFLAGS) $(ALL_CPPFLAGS) $(INCLUDES) \
-	  -DTARGET_NAME=\"$(target_noncanonical)\" \
+	  -DTARGET_NAME=\"$(target_noncanonical)\" @ENABLESSP@ \
 	  -c $(srcdir)/toplev.c $(OUTPUT_OPTION)
 main.o : main.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) toplev.h
 
@@ -1860,6 +1860,10 @@
 params.o : params.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(PARAMS_H) toplev.h
 hooks.o: hooks.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(HOOKS_H)
 pretty-print.o: $(CONFIG_H) $(SYSTEM_H) pretty-print.c $(PRETTY_PRINT_H)
+protector.o : protector.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) $(TREE_H) \
+   flags.h function.h $(EXPR_H) $(OPTABS_H) $(REGS_H) toplev.h hard-reg-set.h \
+   insn-config.h insn-flags.h $(RECOG_H) output.h toplev.h except.h reload.h \
+   $(TM_P_H) conditions.h $(INSN_ATTR_H) real.h protector.h
 
 $(out_object_file): $(out_file) $(CONFIG_H) coretypes.h $(TM_H) $(TREE_H) $(GGC_H) \
    $(RTL_H) $(REGS_H) hard-reg-set.h real.h insn-config.h conditions.h \
diff -Naur gcc-3.4.4/gcc/mklibgcc.in gcc-3.4.4-ssp/gcc/mklibgcc.in
--- gcc-3.4.4/gcc/mklibgcc.in	2005-02-24 11:26:57.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/mklibgcc.in	2005-05-25 14:03:22.000000000 +0300
@@ -58,7 +58,7 @@
 	_enable_execute_stack _trampoline __main _absvsi2 _absvdi2 _addvsi3
 	_addvdi3 _subvsi3 _subvdi3 _mulvsi3 _mulvdi3 _negvsi2 _negvdi2 _ctors
 	_ffssi2 _ffsdi2 _clz _clzsi2 _clzdi2 _ctzsi2 _ctzdi2 _popcount_tab
-	_popcountsi2 _popcountdi2 _paritysi2 _paritydi2'
+	_popcountsi2 _popcountdi2 _paritysi2 _paritydi2 _stack_smash_handler'
 
 # Disable SHLIB_LINK if shared libgcc not enabled.
 if [ "@enable_shared@" = "no" ]; then
diff -Naur gcc-3.4.4/gcc/optabs.c gcc-3.4.4-ssp/gcc/optabs.c
--- gcc-3.4.4/gcc/optabs.c	2004-12-05 07:21:01.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/optabs.c	2005-05-25 14:03:22.000000000 +0300
@@ -678,6 +678,27 @@
   if (target)
     target = protect_from_queue (target, 1);
 
+  /* Keep the frame and offset pattern at the use of stack protection.  */
+  if (flag_propolice_protection
+      && binoptab->code == PLUS
+      && op0 == virtual_stack_vars_rtx
+      && GET_CODE(op1) == CONST_INT)
+    {
+      int icode = (int) binoptab->handlers[(int) mode].insn_code;
+      if (target)
+	temp = target;
+      else
+	temp = gen_reg_rtx (mode);
+
+      if (! (*insn_data[icode].operand[0].predicate) (temp, mode)
+	  || GET_CODE (temp) != REG)
+	temp = gen_reg_rtx (mode);
+
+      emit_insn (gen_rtx_SET (VOIDmode, temp,
+			      gen_rtx_PLUS (GET_MODE (op0), op0, op1)));
+      return temp;
+    }
+
   if (flag_force_mem)
     {
       /* Load duplicate non-volatile operands once.  */
diff -Naur gcc-3.4.4/gcc/opts.c gcc-3.4.4-ssp/gcc/opts.c
--- gcc-3.4.4/gcc/opts.c	2004-02-18 02:09:04.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/opts.c	2005-05-25 14:03:22.000000000 +0300
@@ -125,6 +125,9 @@
 bool warn_unused_variable;
 bool warn_unused_value;
 
+/* Warn when not issuing stack smashing protection for some reason */
+bool warn_stack_protector;
+
 /* Hack for cooperation between set_Wunused and set_Wextra.  */
 static bool maybe_warn_unused_parameter;
 
@@ -798,6 +801,10 @@
       warn_unused_variable = value;
       break;
 
+    case OPT_Wstack_protector:
+      warn_stack_protector = value;
+      break;
+
     case OPT_aux_info:
     case OPT_aux_info_:
       aux_info_file_name = arg;
@@ -1361,6 +1368,14 @@
       stack_limit_rtx = gen_rtx_SYMBOL_REF (Pmode, ggc_strdup (arg));
       break;
 
+    case OPT_fstack_protector:
+      flag_propolice_protection = value;
+      break;
+
+    case OPT_fstack_protector_all:
+      flag_stack_protection = value;
+      break;
+
     case OPT_fstrength_reduce:
       flag_strength_reduce = value;
       break;
diff -Naur gcc-3.4.4/gcc/protector.c gcc-3.4.4-ssp/gcc/protector.c
--- gcc-3.4.4/gcc/protector.c	1970-01-01 02:00:00.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/protector.c	2004-09-02 12:36:11.000000000 +0300
@@ -0,0 +1,2730 @@
+/* RTL buffer overflow protection function for GNU C compiler
+   Copyright (C) 2003 Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 2, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+02111-1307, USA.  */
+
+/* This file contains several memory arrangement functions to protect
+   the return address and the frame pointer of the stack
+   from a stack-smashing attack. It also
+   provides the function that protects pointer variables.  */
+
+#include "config.h"
+#include "system.h"
+#include "coretypes.h"
+#include "tm.h"
+#include "machmode.h"
+#include "real.h"
+#include "rtl.h"
+#include "tree.h"
+#include "regs.h"
+#include "flags.h"
+#include "insn-config.h"
+#include "insn-flags.h"
+#include "expr.h"
+#include "output.h"
+#include "recog.h"
+#include "hard-reg-set.h"
+#include "except.h"
+#include "function.h"
+#include "toplev.h"
+#include "tm_p.h"
+#include "conditions.h"
+#include "insn-attr.h"
+#include "optabs.h"
+#include "reload.h"
+#include "protector.h"
+
+
+/* Round a value to the lowest integer less than it that is a multiple of
+   the required alignment.  Avoid using division in case the value is
+   negative.  Assume the alignment is a power of two.  */
+#define FLOOR_ROUND(VALUE,ALIGN) ((VALUE) & ~((ALIGN) - 1))
+
+/* Similar, but round to the next highest integer that meets the
+   alignment.  */
+#define CEIL_ROUND(VALUE,ALIGN)	(((VALUE) + (ALIGN) - 1) & ~((ALIGN)- 1))
+
+
+/* Nonzero if function being compiled can define string buffers that may be
+   damaged by the stack-smash attack.  */
+static int current_function_defines_vulnerable_string;
+static int current_function_defines_short_string;
+static int current_function_has_variable_string;
+static int current_function_defines_vsized_array;
+static int current_function_is_inlinable;
+
+/* Nonzero if search_string_def finds the variable which contains an array.  */
+static int is_array;
+
+/* Nonzero if search_string_def finds a byte-pointer variable,
+   which may be assigned to alloca output.  */
+static int may_have_alloca_pointer;
+
+static rtx guard_area, _guard;
+static rtx function_first_insn, prologue_insert_point;
+
+/* Offset to end of sweeped area for gathering character arrays.  */
+static HOST_WIDE_INT sweep_frame_offset;
+
+/* Offset to end of allocated area for instantiating pseudo registers.  */
+static HOST_WIDE_INT push_allocated_offset = 0;
+
+/* Offset to end of assigned area for instantiating pseudo registers.  */
+static HOST_WIDE_INT push_frame_offset = 0;
+
+/* Set to 1 after cse_not_expected becomes nonzero. it is used to identify
+   which stage assign_stack_local_for_pseudo_reg is called from.  */
+static int saved_cse_not_expected = 0;
+
+static int search_string_from_argsandvars (int);
+static int search_string_from_local_vars (tree);
+static int search_pointer_def (tree);
+static int search_func_pointer (tree);
+static int check_used_flag (rtx);
+static void reset_used_flags_for_insns (rtx);
+static void reset_used_flags_for_decls (tree);
+static void reset_used_flags_of_plus (rtx);
+static void rtl_prologue (rtx);
+static void rtl_epilogue (rtx);
+static void arrange_var_order (tree);
+static void copy_args_for_protection (void);
+static void sweep_string_variable (rtx, HOST_WIDE_INT);
+static void sweep_string_in_decls (tree, HOST_WIDE_INT, HOST_WIDE_INT);
+static void sweep_string_in_args (tree, HOST_WIDE_INT, HOST_WIDE_INT);
+static void sweep_string_use_of_insns (rtx, HOST_WIDE_INT, HOST_WIDE_INT);
+static void sweep_string_in_operand (rtx, rtx *, HOST_WIDE_INT, HOST_WIDE_INT);
+static void move_arg_location (rtx, rtx, rtx, HOST_WIDE_INT);
+static void change_arg_use_of_insns (rtx, rtx, rtx *, HOST_WIDE_INT);
+static void change_arg_use_in_operand (rtx, rtx, rtx, rtx *, HOST_WIDE_INT);
+static void validate_insns_of_varrefs (rtx);
+static void validate_operand_of_varrefs (rtx, rtx *);
+
+/* Specify which size of buffers should be protected from a stack smashing
+   attack. Because small buffers are not used in situations which may
+   overflow buffer, the default size sets to the size of 64 bit register.  */
+#ifndef SUSPICIOUS_BUF_SIZE
+#define SUSPICIOUS_BUF_SIZE 8
+#endif
+
+#define AUTO_BASEPTR(X) \
+  (GET_CODE (X) == PLUS ? XEXP (X, 0) : X)
+#define AUTO_OFFSET(X) \
+  (GET_CODE (X) == PLUS ? INTVAL (XEXP (X, 1)) : 0)
+#undef PARM_PASSED_IN_MEMORY
+#define PARM_PASSED_IN_MEMORY(PARM) \
+ (GET_CODE (DECL_INCOMING_RTL (PARM)) == MEM)
+#define TREE_VISITED(NODE) ((NODE)->common.unused_0)
+
+/* Argument values for calling search_string_from_argsandvars.  */
+#define CALL_FROM_PREPARE_STACK_PROTECTION	0
+#define CALL_FROM_PUSH_FRAME			1
+
+
+/* Prepare several stack protection instruments for the current function
+   if the function has an array as a local variable, which may be vulnerable
+   from a stack smashing attack, and it is not inlinable.
+
+   The overall steps are as follows;
+   (1)search an array,
+   (2)insert guard_area on the stack,
+   (3)duplicate pointer arguments into local variables, and
+   (4)arrange the location of local variables.  */
+void
+prepare_stack_protection (int inlinable)
+{
+  tree blocks = DECL_INITIAL (current_function_decl);
+  current_function_is_inlinable = inlinable && !flag_no_inline;
+  push_frame_offset = push_allocated_offset = 0;
+  saved_cse_not_expected = 0;
+
+  /* Skip the protection if the function has no block
+    or it is an inline function.  */
+  if (current_function_is_inlinable)
+    validate_insns_of_varrefs (get_insns ());
+  if (! blocks || current_function_is_inlinable)
+    return;
+
+  current_function_defines_vulnerable_string
+    = search_string_from_argsandvars (CALL_FROM_PREPARE_STACK_PROTECTION);
+
+  if (current_function_defines_vulnerable_string
+      || flag_stack_protection)
+    {
+      function_first_insn = get_insns ();
+
+      if (current_function_contains_functions)
+	{
+	  if (warn_stack_protector)
+	    warning ("not protecting function: it contains functions");
+	  return;
+	}
+
+      /* Initialize recognition, indicating that volatile is OK.  */
+      init_recog ();
+
+      sweep_frame_offset = 0;
+	
+#ifdef STACK_GROWS_DOWNWARD
+      /* frame_offset: offset to end of allocated area of stack frame.
+	 It is defined in the function.c.  */
+
+      /* the location must be before buffers.  */
+      guard_area = assign_stack_local (BLKmode, UNITS_PER_GUARD, -1);
+      PUT_MODE (guard_area, GUARD_m);
+      MEM_VOLATILE_P (guard_area) = 1;
+
+#ifndef FRAME_GROWS_DOWNWARD
+      sweep_frame_offset = frame_offset;
+#endif
+
+      /* For making room for guard value, scan all insns and fix the offset
+	 address of the variable that is based on frame pointer.
+	 Scan all declarations of variables and fix the offset address
+	 of the variable that is based on the frame pointer.  */
+      sweep_string_variable (guard_area, UNITS_PER_GUARD);
+
+	
+      /* the location of guard area moves to the beginning of stack frame.  */
+      if (AUTO_OFFSET(XEXP (guard_area, 0)))
+	XEXP (XEXP (guard_area, 0), 1)
+	  = gen_rtx_CONST_INT (VOIDmode, sweep_frame_offset);
+
+
+      /* Insert prologue rtl instructions.  */
+      rtl_prologue (function_first_insn);
+
+      if (! current_function_has_variable_string)
+	{
+	  /* Generate argument saving instruction.  */
+	  copy_args_for_protection ();
+
+#ifndef FRAME_GROWS_DOWNWARD
+	  /* If frame grows upward, character arrays for protecting args
+	     may copy to the top of the guard variable.
+	     So sweep the guard variable again.  */
+	  sweep_frame_offset = CEIL_ROUND (frame_offset,
+					   BIGGEST_ALIGNMENT / BITS_PER_UNIT);
+	  sweep_string_variable (guard_area, UNITS_PER_GUARD);
+#endif
+	}
+      /* Variable can't be protected from the overflow of variable length
+	 buffer. But variable reordering is still effective against
+	 the overflow of fixed size character arrays.  */
+      else if (warn_stack_protector)
+	warning ("not protecting variables: it has a variable length buffer");
+#endif
+#ifndef FRAME_GROWS_DOWNWARD
+      if (STARTING_FRAME_OFFSET == 0)
+	{
+	  /* This part may be only for alpha.  */
+	  push_allocated_offset = BIGGEST_ALIGNMENT / BITS_PER_UNIT;
+	  assign_stack_local (BLKmode, push_allocated_offset, -1);
+	  sweep_frame_offset = frame_offset;
+	  sweep_string_variable (const0_rtx, -push_allocated_offset);
+	  sweep_frame_offset = AUTO_OFFSET (XEXP (guard_area, 0));
+	}
+#endif
+
+      /* Arrange the order of local variables.  */
+      arrange_var_order (blocks);
+
+#ifdef STACK_GROWS_DOWNWARD
+      /* Insert epilogue rtl instructions.  */
+      rtl_epilogue (get_last_insn ());
+#endif
+      init_recog_no_volatile ();
+    }
+  else if (current_function_defines_short_string
+	   && warn_stack_protector)
+    warning ("not protecting function: buffer is less than %d bytes long",
+	     SUSPICIOUS_BUF_SIZE);
+}
+
+/*
+  Search string from arguments and local variables.
+   caller: CALL_FROM_PREPARE_STACK_PROTECTION (0)
+	   CALL_FROM_PUSH_FRAME (1)
+*/
+static int
+search_string_from_argsandvars (int caller)
+{
+  tree blocks, parms;
+  int string_p;
+
+  /* Saves a latest search result as a cached infomation.  */
+  static tree __latest_search_decl = 0;
+  static int  __latest_search_result = FALSE;
+
+  if (__latest_search_decl == current_function_decl)
+    return __latest_search_result;
+  else
+    if (caller == CALL_FROM_PUSH_FRAME)
+      return FALSE;
+
+  __latest_search_decl = current_function_decl;
+  __latest_search_result = TRUE;
+  
+  current_function_defines_short_string = FALSE;
+  current_function_has_variable_string = FALSE;
+  current_function_defines_vsized_array = FALSE;
+  may_have_alloca_pointer = FALSE;
+
+  /* Search a string variable from local variables.  */
+  blocks = DECL_INITIAL (current_function_decl);
+  string_p = search_string_from_local_vars (blocks);
+
+  if (! current_function_defines_vsized_array
+      && may_have_alloca_pointer
+      && current_function_calls_alloca)
+    {
+      current_function_has_variable_string = TRUE;
+      return TRUE;
+    }
+
+  if (string_p)
+    return TRUE;
+
+#ifdef STACK_GROWS_DOWNWARD
+  /* Search a string variable from arguments.  */
+  parms = DECL_ARGUMENTS (current_function_decl);
+
+  for (; parms; parms = TREE_CHAIN (parms))
+    if (DECL_NAME (parms) && TREE_TYPE (parms) != error_mark_node)
+      {
+	if (PARM_PASSED_IN_MEMORY (parms))
+	  {
+	    string_p = search_string_def (TREE_TYPE(parms));
+	    if (string_p)
+	      return TRUE;
+	  }
+      }
+#endif
+
+  __latest_search_result = FALSE;
+  return FALSE;
+}
+
+
+/* Search string from local variables in the specified scope.  */
+static int
+search_string_from_local_vars (tree block)
+{
+  tree types;
+  int found = FALSE;
+
+  while (block && TREE_CODE(block)==BLOCK)
+    {
+      for (types = BLOCK_VARS(block); types; types = TREE_CHAIN(types))
+	{
+	  /* Skip the declaration that refers an external variable.  */
+	  /* name: types.decl.name.identifier.id                     */
+	  if (! DECL_EXTERNAL (types) && ! TREE_STATIC (types)
+	      && TREE_CODE (types) == VAR_DECL
+	      && ! DECL_ARTIFICIAL (types)
+	      && DECL_RTL_SET_P (types)
+	      && GET_CODE (DECL_RTL (types)) == MEM
+
+	      && search_string_def (TREE_TYPE (types)))
+	    {
+	      rtx home = DECL_RTL (types);
+
+	      if (GET_CODE (home) == MEM
+		  && (GET_CODE (XEXP (home, 0)) == MEM
+		      || (GET_CODE (XEXP (home, 0)) == REG
+			  && XEXP (home, 0) != virtual_stack_vars_rtx
+			  && REGNO (XEXP (home, 0)) != HARD_FRAME_POINTER_REGNUM
+			  && REGNO (XEXP (home, 0)) != STACK_POINTER_REGNUM
+#if ARG_POINTER_REGNUM != HARD_FRAME_POINTER_REGNUM
+			  && REGNO (XEXP (home, 0)) != ARG_POINTER_REGNUM
+#endif
+			  )))
+		/* If the value is indirect by memory or by a register
+		   that isn't the frame pointer then it means the object is
+		   variable-sized and address through
+		   that register or stack slot.
+		   The protection has no way to hide pointer variables
+		   behind the array, so all we can do is staying
+		   the order of variables and arguments.  */
+		{
+		  current_function_has_variable_string = TRUE;
+		}
+	    
+	      /* Found character array.  */
+	      found = TRUE;
+	    }
+	}
+
+      if (search_string_from_local_vars (BLOCK_SUBBLOCKS (block)))
+	{
+	  found = TRUE;
+	}
+
+      block = BLOCK_CHAIN (block);
+    }
+    
+  return found;
+}
+
+
+/* Search a character array from the specified type tree.  */
+int
+search_string_def (tree type)
+{
+  tree tem;
+    
+  if (! type)
+    return FALSE;
+
+  switch (TREE_CODE (type))
+    {
+    case ARRAY_TYPE:
+      /* Check if the array is a variable-sized array.  */
+      if (TYPE_DOMAIN (type) == 0
+	  || (TYPE_MAX_VALUE (TYPE_DOMAIN (type)) != 0
+	      && TREE_CODE (TYPE_MAX_VALUE (TYPE_DOMAIN (type))) == NOP_EXPR))
+	current_function_defines_vsized_array = TRUE;
+
+      /* Check if the array is related to char array.  */
+      if (TYPE_MAIN_VARIANT (TREE_TYPE(type)) == char_type_node
+	  || TYPE_MAIN_VARIANT (TREE_TYPE(type)) == signed_char_type_node
+	  || TYPE_MAIN_VARIANT (TREE_TYPE(type)) == unsigned_char_type_node)
+	{
+	  /* Check if the string is a variable string.  */
+	  if (TYPE_DOMAIN (type) == 0
+	      || (TYPE_MAX_VALUE (TYPE_DOMAIN (type)) != 0
+		  && TREE_CODE (TYPE_MAX_VALUE (TYPE_DOMAIN (type))) == NOP_EXPR))
+	    return TRUE;
+
+	  /* Check if the string size is greater than SUSPICIOUS_BUF_SIZE.  */
+	  if (TYPE_MAX_VALUE (TYPE_DOMAIN (type)) != 0
+	      && (TREE_INT_CST_LOW(TYPE_MAX_VALUE(TYPE_DOMAIN(type)))+1
+		  >= SUSPICIOUS_BUF_SIZE))
+	    return TRUE;
+
+	  current_function_defines_short_string = TRUE;
+	}
+      
+      /* to protect every functions, sweep any arrays to the frame top.  */
+      is_array = TRUE;
+
+      return search_string_def(TREE_TYPE(type));
+	
+    case UNION_TYPE:
+    case QUAL_UNION_TYPE:
+    case RECORD_TYPE:
+      /* Check if each field has character arrays.  */
+      for (tem = TYPE_FIELDS (type); tem; tem = TREE_CHAIN (tem))
+	{
+	  /* Omit here local type decls until we know how to support them. */
+	  if ((TREE_CODE (tem) == TYPE_DECL)
+	      || (TREE_CODE (tem) == VAR_DECL && TREE_STATIC (tem)))
+	    continue;
+
+	  if (search_string_def(TREE_TYPE(tem)))
+	    return TRUE;
+	}
+      break;
+	
+    case POINTER_TYPE:
+      /* Check if pointer variables, which may be a pointer assigned 
+	 by alloca function call, are declared.  */
+      if (TYPE_MAIN_VARIANT (TREE_TYPE(type)) == char_type_node
+	  || TYPE_MAIN_VARIANT (TREE_TYPE(type)) == signed_char_type_node
+	  || TYPE_MAIN_VARIANT (TREE_TYPE(type)) == unsigned_char_type_node)
+	may_have_alloca_pointer = TRUE;
+      break;
+
+    case REFERENCE_TYPE:
+    case OFFSET_TYPE:
+    default:
+      break;
+    }
+
+  return FALSE;
+}
+
+
+/* Examine whether the input contains frame pointer addressing.  */
+int
+contains_fp (rtx op)
+{
+  enum rtx_code code;
+  rtx x;
+  int i, j;
+  const char *fmt;
+
+  x = op;
+  if (x == 0)
+    return FALSE;
+
+  code = GET_CODE (x);
+
+  switch (code)
+    {
+    case CONST_INT:
+    case CONST_DOUBLE:
+    case CONST:
+    case SYMBOL_REF:
+    case CODE_LABEL:
+    case REG:
+    case ADDRESSOF:
+      return FALSE;
+
+    case MEM:
+      /* This case is not generated at the stack protection.
+	 see plus_constant_wide and simplify_plus_minus function.  */
+      if (XEXP (x, 0) == virtual_stack_vars_rtx)
+	abort ();
+      
+    case PLUS:
+      if (XEXP (x, 0) == virtual_stack_vars_rtx
+	  && GET_CODE (XEXP (x, 1)) == CONST_INT)
+	return TRUE;
+
+    default:
+      break;
+    }
+
+  /* Scan all subexpressions.  */
+  fmt = GET_RTX_FORMAT (code);
+  for (i = 0; i < GET_RTX_LENGTH (code); i++, fmt++)
+    if (*fmt == 'e')
+      {
+	if (contains_fp (XEXP (x, i)))
+	  return TRUE;
+      }
+    else if (*fmt == 'E')
+      for (j = 0; j < XVECLEN (x, i); j++)
+	if (contains_fp (XVECEXP (x, i, j)))
+	  return TRUE;
+
+  return FALSE;
+}
+
+
+/* Examine whether the input contains any pointer.  */
+static int
+search_pointer_def (tree type)
+{
+  tree tem;
+    
+  if (! type)
+    return FALSE;
+
+  switch (TREE_CODE (type))
+    {
+    case UNION_TYPE:
+    case QUAL_UNION_TYPE:
+    case RECORD_TYPE:
+      /* Check if each field has a pointer.  */
+      for (tem = TYPE_FIELDS (type); tem; tem = TREE_CHAIN (tem))
+	{
+	  if ((TREE_CODE (tem) == TYPE_DECL)
+	      || (TREE_CODE (tem) == VAR_DECL && TREE_STATIC (tem)))
+	    continue;
+
+	  if (search_pointer_def (TREE_TYPE(tem)))
+	    return TRUE;
+	}
+      break;
+
+    case ARRAY_TYPE:
+      return search_pointer_def (TREE_TYPE(type));
+	
+    case POINTER_TYPE:
+    case REFERENCE_TYPE:
+    case OFFSET_TYPE:
+      if (TYPE_READONLY (TREE_TYPE (type)))
+	{
+	  /* If this pointer contains function pointer,
+	     it should be protected.  */
+	  return search_func_pointer (TREE_TYPE (type));
+	}
+      return TRUE;
+	
+    default:
+      break;
+    }
+
+  return FALSE;
+}
+
+
+/* Examine whether the input contains function pointer.  */
+static int
+search_func_pointer (tree type)
+{
+  tree tem;
+    
+  if (! type)
+    return FALSE;
+
+  switch (TREE_CODE (type))
+    {
+    case UNION_TYPE:
+    case QUAL_UNION_TYPE:
+    case RECORD_TYPE:
+	if (! TREE_VISITED (type))
+	  {
+	    /* Mark the type as having been visited already.  */
+	    TREE_VISITED (type) = 1;
+
+	    /* Check if each field has a function pointer.  */
+	    for (tem = TYPE_FIELDS (type); tem; tem = TREE_CHAIN (tem))
+	      {
+		if (TREE_CODE (tem) == FIELD_DECL
+		    && search_func_pointer (TREE_TYPE(tem)))
+		  {
+		    TREE_VISITED (type) = 0;
+		    return TRUE;
+		  }
+	      }
+	    
+	    TREE_VISITED (type) = 0;
+	  }
+	break;
+
+    case ARRAY_TYPE:
+      return search_func_pointer (TREE_TYPE(type));
+	
+    case POINTER_TYPE:
+    case REFERENCE_TYPE:
+    case OFFSET_TYPE:
+      if (TREE_CODE (TREE_TYPE (type)) == FUNCTION_TYPE)
+	return TRUE;
+      return search_func_pointer (TREE_TYPE(type));
+	
+    default:
+      break;
+    }
+
+  return FALSE;
+}
+
+
+/* Check whether the specified rtx contains PLUS rtx with used flag.  */
+static int
+check_used_flag (rtx x)
+{
+  register int i, j;
+  register enum rtx_code code;
+  register const char *format_ptr;
+
+  if (x == 0)
+    return FALSE;
+
+  code = GET_CODE (x);
+
+  switch (code)
+    {
+    case REG:
+    case QUEUED:
+    case CONST_INT:
+    case CONST_DOUBLE:
+    case SYMBOL_REF:
+    case CODE_LABEL:
+    case PC:
+    case CC0:
+      return FALSE;
+
+    case PLUS:
+      if (x->used)
+	return TRUE;
+
+    default:
+      break;
+    }
+
+  format_ptr = GET_RTX_FORMAT (code);
+  for (i = 0; i < GET_RTX_LENGTH (code); i++)
+    {
+      switch (*format_ptr++)
+	{
+	case 'e':
+	  if (check_used_flag (XEXP (x, i)))
+	    return TRUE;
+	  break;
+
+	case 'E':
+	  for (j = 0; j < XVECLEN (x, i); j++)
+	    if (check_used_flag (XVECEXP (x, i, j)))
+	      return TRUE;
+	  break;
+	}
+    }
+
+  return FALSE;
+}
+
+
+/* Reset used flag of every insns after the spcecified insn.  */
+static void
+reset_used_flags_for_insns (rtx insn)
+{
+  int i, j;
+  enum rtx_code code;
+  const char *format_ptr;
+
+  for (; insn; insn = NEXT_INSN (insn))
+    if (GET_CODE (insn) == INSN || GET_CODE (insn) == JUMP_INSN
+	|| GET_CODE (insn) == CALL_INSN)
+      {
+	code = GET_CODE (insn);
+	insn->used = 0;
+	format_ptr = GET_RTX_FORMAT (code);
+
+	for (i = 0; i < GET_RTX_LENGTH (code); i++)
+	  {
+	    switch (*format_ptr++)
+	      {
+	      case 'e':
+		reset_used_flags_of_plus (XEXP (insn, i));
+		break;
+			
+	      case 'E':
+		for (j = 0; j < XVECLEN (insn, i); j++)
+		  reset_used_flags_of_plus (XVECEXP (insn, i, j));
+		break;
+	      }
+	  }
+      }
+}
+
+
+/* Reset used flag of every variables in the specified block.  */
+static void
+reset_used_flags_for_decls (tree block)
+{
+  tree types;
+  rtx home;
+
+  while (block && TREE_CODE(block)==BLOCK)
+    {
+      types = BLOCK_VARS(block);
+	
+      for (types= BLOCK_VARS(block); types; types = TREE_CHAIN(types))
+	{
+	  /* Skip the declaration that refers an external variable and
+	     also skip an global variable.  */
+	  if (! DECL_EXTERNAL (types))
+	    {
+	      if (! DECL_RTL_SET_P (types))
+		continue;
+	      home = DECL_RTL (types);
+
+	      if (GET_CODE (home) == MEM
+		  && GET_CODE (XEXP (home, 0)) == PLUS
+		  && GET_CODE (XEXP (XEXP (home, 0), 1)) == CONST_INT)
+		{
+		  XEXP (home, 0)->used = 0;
+		}
+	    }
+	}
+
+      reset_used_flags_for_decls (BLOCK_SUBBLOCKS (block));
+
+      block = BLOCK_CHAIN (block);
+    }
+}
+
+
+/* Reset the used flag of every PLUS rtx derived from the specified rtx.  */
+static void
+reset_used_flags_of_plus (rtx x)
+{
+  int i, j;
+  enum rtx_code code;
+  const char *format_ptr;
+
+  if (x == 0)
+    return;
+
+  code = GET_CODE (x);
+
+  switch (code)
+    {
+      /* These types may be freely shared so we needn't do any resetting
+	 for them.  */
+    case REG:
+    case QUEUED:
+    case CONST_INT:
+    case CONST_DOUBLE:
+    case SYMBOL_REF:
+    case CODE_LABEL:
+    case PC:
+    case CC0:
+      return;
+
+    case INSN:
+    case JUMP_INSN:
+    case CALL_INSN:
+    case NOTE:
+    case LABEL_REF:
+    case BARRIER:
+      /* The chain of insns is not being copied.  */
+      return;
+      
+    case PLUS:
+      x->used = 0;
+      break;
+
+    case CALL_PLACEHOLDER:
+      reset_used_flags_for_insns (XEXP (x, 0));
+      reset_used_flags_for_insns (XEXP (x, 1));
+      reset_used_flags_for_insns (XEXP (x, 2));
+      break;
+
+    default:
+      break;
+    }
+
+  format_ptr = GET_RTX_FORMAT (code);
+  for (i = 0; i < GET_RTX_LENGTH (code); i++)
+    {
+      switch (*format_ptr++)
+	{
+	case 'e':
+	  reset_used_flags_of_plus (XEXP (x, i));
+	  break;
+
+	case 'E':
+	  for (j = 0; j < XVECLEN (x, i); j++)
+	    reset_used_flags_of_plus (XVECEXP (x, i, j));
+	  break;
+	}
+    }
+}
+
+
+/* Generate the prologue insns of the protector into the specified insn.  */
+static void
+rtl_prologue (rtx insn)
+{
+#if defined(INIT_SECTION_ASM_OP) && !defined(INVOKE__main)
+#undef HAS_INIT_SECTION
+#define HAS_INIT_SECTION
+#endif
+
+  rtx _val;
+
+  for (; insn; insn = NEXT_INSN (insn))
+    if (GET_CODE (insn) == NOTE
+	&& NOTE_LINE_NUMBER (insn) == NOTE_INSN_FUNCTION_BEG)
+      break;
+  
+#if !defined (HAS_INIT_SECTION)
+  /* If this function is `main', skip a call to `__main'
+     to run guard instruments after global initializers, etc.  */
+  if (DECL_NAME (current_function_decl)
+      && MAIN_NAME_P (DECL_NAME (current_function_decl))
+      && DECL_CONTEXT (current_function_decl) == NULL_TREE)
+    {
+      rtx fbinsn = insn;
+      for (; insn; insn = NEXT_INSN (insn))
+	if (GET_CODE (insn) == NOTE
+	    && NOTE_LINE_NUMBER (insn) == NOTE_INSN_BLOCK_BEG)
+	  break;
+      if (insn == 0)
+	insn = fbinsn;
+    }
+#endif
+
+  /* Mark the next insn of FUNCTION_BEG insn.  */
+  prologue_insert_point = NEXT_INSN (insn);
+		
+  start_sequence ();
+
+  _guard = gen_rtx_MEM (GUARD_m, gen_rtx_SYMBOL_REF (Pmode, "__guard"));
+  emit_move_insn ( guard_area, _guard);
+
+  _val = get_insns ();
+  end_sequence ();
+
+  emit_insn_before (_val, prologue_insert_point);
+}
+
+
+/* Generate the epilogue insns of the protector into the specified insn.  */
+static void
+rtl_epilogue (rtx insn)
+{
+  rtx if_false_label;
+  rtx _val;
+  rtx funcname;
+  tree funcstr;
+  int  flag_have_return = FALSE;
+		
+  start_sequence ();
+
+#ifdef HAVE_return
+  if (HAVE_return)
+    {
+      rtx insn;
+      return_label = gen_label_rtx ();
+      
+      for (insn = prologue_insert_point; insn; insn = NEXT_INSN (insn))
+	if (GET_CODE (insn) == JUMP_INSN
+	    && GET_CODE (PATTERN (insn)) == RETURN
+	    && GET_MODE (PATTERN (insn)) == VOIDmode)
+	  {
+	    rtx pat = gen_rtx_SET (VOIDmode,
+				   pc_rtx,
+				   gen_rtx_LABEL_REF (VOIDmode,
+						      return_label));
+	    PATTERN (insn) = pat;
+	    flag_have_return = TRUE;
+	  }
+
+
+      emit_label (return_label);
+    }
+#endif
+
+  /*                                          if (guard_area != _guard) */
+  compare_from_rtx (guard_area, _guard, NE, 0, GUARD_m, NULL_RTX);
+
+  if_false_label = gen_label_rtx ();		/* { */
+  emit_jump_insn ( gen_beq(if_false_label));
+
+  /* generate string for the current function name */
+  funcstr = build_string (strlen(current_function_name ())+1,
+			  current_function_name ());
+  TREE_TYPE (funcstr) = build_array_type (char_type_node, 0);
+  funcname = output_constant_def (funcstr, 1);
+
+  emit_library_call (gen_rtx_SYMBOL_REF (Pmode, "__stack_smash_handler"),
+		     0, VOIDmode, 2,
+                     XEXP (funcname, 0), Pmode, guard_area, GUARD_m);
+
+  /* generate RTL to return from the current function */
+		
+  emit_barrier ();				/* } */
+  emit_label (if_false_label);
+
+  /* generate RTL to return from the current function */
+  if (DECL_RTL_SET_P (DECL_RESULT (current_function_decl)))
+    use_return_register ();
+
+#ifdef HAVE_return
+  if (HAVE_return && flag_have_return)
+    {
+      emit_jump_insn (gen_return ());
+      emit_barrier ();
+    }
+#endif
+  
+  _val = get_insns ();
+  end_sequence ();
+
+  emit_insn_after (_val, insn);
+}
+
+
+/* For every variable which type is character array, moves its location
+   in the stack frame to the sweep_frame_offset position.  */
+static void
+arrange_var_order (tree block)
+{
+  tree types;
+  HOST_WIDE_INT offset;
+    
+  while (block && TREE_CODE(block)==BLOCK)
+    {
+      /* arrange the location of character arrays in depth first.  */
+      arrange_var_order (BLOCK_SUBBLOCKS (block));
+      
+      for (types = BLOCK_VARS (block); types; types = TREE_CHAIN(types))
+	{
+	  /* Skip the declaration that refers an external variable.  */
+	  if (! DECL_EXTERNAL (types) && ! TREE_STATIC (types)
+	      && TREE_CODE (types) == VAR_DECL
+	      && ! DECL_ARTIFICIAL (types)
+	      && DECL_RTL_SET_P (types)
+	      && GET_CODE (DECL_RTL (types)) == MEM
+	      && GET_MODE (DECL_RTL (types)) == BLKmode
+
+	      && (is_array=0,
+		  search_string_def (TREE_TYPE (types))
+		  || (! current_function_defines_vulnerable_string && is_array)))
+	    {
+	      rtx home = DECL_RTL (types);
+
+	      if (!(GET_CODE (home) == MEM
+		    && (GET_CODE (XEXP (home, 0)) == MEM
+			|| (GET_CODE (XEXP (home, 0)) == REG
+			    && XEXP (home, 0) != virtual_stack_vars_rtx
+			    && REGNO (XEXP (home, 0)) != HARD_FRAME_POINTER_REGNUM
+			    && REGNO (XEXP (home, 0)) != STACK_POINTER_REGNUM
+#if ARG_POINTER_REGNUM != HARD_FRAME_POINTER_REGNUM
+			    && REGNO (XEXP (home, 0)) != ARG_POINTER_REGNUM
+#endif
+			    ))))
+		{
+		  /* Found a string variable.  */
+		  HOST_WIDE_INT var_size =
+		    ((TREE_INT_CST_LOW (DECL_SIZE (types)) + BITS_PER_UNIT - 1)
+		     / BITS_PER_UNIT);
+
+		  /* Confirmed it is BLKmode.  */
+		  int alignment = BIGGEST_ALIGNMENT / BITS_PER_UNIT;
+		  var_size = CEIL_ROUND (var_size, alignment);
+
+		  /* Skip the variable if it is top of the region
+		     specified by sweep_frame_offset.  */
+		  offset = AUTO_OFFSET (XEXP (DECL_RTL (types), 0));
+		  if (offset == sweep_frame_offset - var_size)
+		    sweep_frame_offset -= var_size;
+		      
+		  else if (offset < sweep_frame_offset - var_size)
+		    sweep_string_variable (DECL_RTL (types), var_size);
+		}
+	    }
+	}
+
+      block = BLOCK_CHAIN (block);
+    }
+}
+
+
+/* To protect every pointer argument and move character arrays in the argument,
+   Copy those variables to the top of the stack frame and move the location of
+   character arrays to the posion of sweep_frame_offset.  */
+static void
+copy_args_for_protection (void)
+{
+  tree parms = DECL_ARGUMENTS (current_function_decl);
+  rtx temp_rtx;
+
+  parms = DECL_ARGUMENTS (current_function_decl);
+  for (; parms; parms = TREE_CHAIN (parms))
+    if (DECL_NAME (parms) && TREE_TYPE (parms) != error_mark_node)
+      {
+	if (PARM_PASSED_IN_MEMORY (parms) && DECL_NAME (parms))
+	  {
+	    int string_p;
+	    rtx seq;
+
+	    string_p = search_string_def (TREE_TYPE(parms));
+
+	    /* Check if it is a candidate to move.  */
+	    if (string_p || search_pointer_def (TREE_TYPE (parms)))
+	      {
+		int arg_size
+		  = ((TREE_INT_CST_LOW (DECL_SIZE (parms)) + BITS_PER_UNIT - 1)
+		     / BITS_PER_UNIT);
+		tree passed_type = DECL_ARG_TYPE (parms);
+		tree nominal_type = TREE_TYPE (parms);
+		
+		start_sequence ();
+
+		if (GET_CODE (DECL_RTL (parms)) == REG)
+		  {
+		    rtx safe = 0;
+		    
+		    change_arg_use_of_insns (prologue_insert_point,
+					     DECL_RTL (parms), &safe, 0);
+		    if (safe)
+		      {
+			/* Generate codes for copying the content.  */
+			rtx movinsn = emit_move_insn (safe, DECL_RTL (parms));
+		    
+			/* Avoid register elimination in gcse.c.  */
+			PATTERN (movinsn)->volatil = 1;
+			
+			/* Save debugger info.  */
+			SET_DECL_RTL (parms, safe);
+		      }
+		  }
+		else if (GET_CODE (DECL_RTL (parms)) == MEM
+			 && GET_CODE (XEXP (DECL_RTL (parms), 0)) == ADDRESSOF)
+		  {
+		    rtx movinsn;
+		    rtx safe = gen_reg_rtx (GET_MODE (DECL_RTL (parms)));
+
+		    /* Generate codes for copying the content.  */
+		    movinsn = emit_move_insn (safe, DECL_INCOMING_RTL (parms));
+		    /* Avoid register elimination in gcse.c.  */
+		    PATTERN (movinsn)->volatil = 1;
+
+		    /* Change the addressof information to the newly
+		       allocated pseudo register.  */
+		    emit_move_insn (DECL_RTL (parms), safe);
+
+		    /* Save debugger info.  */
+		    SET_DECL_RTL (parms, safe);
+		  }
+			
+		/* See if the frontend wants to pass this by invisible
+		   reference.  */
+		else if (passed_type != nominal_type
+			 && POINTER_TYPE_P (passed_type)
+			 && TREE_TYPE (passed_type) == nominal_type)
+		  {
+		    rtx safe = 0, orig = XEXP (DECL_RTL (parms), 0);
+
+		    change_arg_use_of_insns (prologue_insert_point,
+					     orig, &safe, 0);
+		    if (safe)
+		      {
+			/* Generate codes for copying the content.  */
+			rtx movinsn = emit_move_insn (safe, orig);
+		    
+			/* Avoid register elimination in gcse.c  */
+			PATTERN (movinsn)->volatil = 1;
+			
+			/* Save debugger info.  */
+			SET_DECL_RTL (parms, safe);
+		      }
+		  }
+
+		else
+		  {
+		    /* Declare temporary local variable for parms.  */
+		    temp_rtx
+		      = assign_stack_local (DECL_MODE (parms), arg_size,
+					    DECL_MODE (parms) == BLKmode ?
+					    -1 : 0);
+		    
+		    MEM_IN_STRUCT_P (temp_rtx)
+		      = AGGREGATE_TYPE_P (TREE_TYPE (parms));
+		    set_mem_alias_set (temp_rtx, get_alias_set (parms));
+
+		    /* Generate codes for copying the content.  */
+		    store_expr (parms, temp_rtx, 0);
+
+		    /* Change the reference for each instructions.  */
+		    move_arg_location (prologue_insert_point, DECL_RTL (parms),
+				       temp_rtx, arg_size);
+
+		    /* Change the location of parms variable.  */
+		    SET_DECL_RTL (parms, temp_rtx);
+		  }
+
+		seq = get_insns ();
+		end_sequence ();
+		emit_insn_before (seq, prologue_insert_point);
+
+#ifdef FRAME_GROWS_DOWNWARD
+		/* Process the string argument.  */
+		if (string_p && DECL_MODE (parms) == BLKmode)
+		  {
+		    int alignment = BIGGEST_ALIGNMENT / BITS_PER_UNIT;
+		    arg_size = CEIL_ROUND (arg_size, alignment);
+			
+		    /* Change the reference for each instructions.  */
+		    sweep_string_variable (DECL_RTL (parms), arg_size);
+		  }
+#endif
+	      }
+	  }
+      }
+}
+
+
+/* Sweep a string variable to the positon of sweep_frame_offset in the 
+   stack frame, that is a last position of string variables.  */
+static void
+sweep_string_variable (rtx sweep_var, HOST_WIDE_INT var_size)
+{
+  HOST_WIDE_INT sweep_offset;
+
+  switch (GET_CODE (sweep_var))
+    {
+    case MEM:
+      if (GET_CODE (XEXP (sweep_var, 0)) == ADDRESSOF
+	  && GET_CODE (XEXP (XEXP (sweep_var, 0), 0)) == REG)
+	return;
+      sweep_offset = AUTO_OFFSET(XEXP (sweep_var, 0));
+      break;
+    case CONST_INT:
+      sweep_offset = INTVAL (sweep_var);
+      break;
+    default:
+      abort ();
+    }
+
+  /* Scan all declarations of variables and fix the offset address of
+     the variable based on the frame pointer.  */
+  sweep_string_in_decls (DECL_INITIAL (current_function_decl),
+			 sweep_offset, var_size);
+
+  /* Scan all argument variable and fix the offset address based on
+     the frame pointer.  */
+  sweep_string_in_args (DECL_ARGUMENTS (current_function_decl),
+			sweep_offset, var_size);
+
+  /* For making room for sweep variable, scan all insns and
+     fix the offset address of the variable that is based on frame pointer.  */
+  sweep_string_use_of_insns (function_first_insn, sweep_offset, var_size);
+
+
+  /* Clear all the USED bits in operands of all insns and declarations of
+     local variables.  */
+  reset_used_flags_for_decls (DECL_INITIAL (current_function_decl));
+  reset_used_flags_for_insns (function_first_insn);
+
+  sweep_frame_offset -= var_size;
+}
+
+
+
+/* Move an argument to the local variable addressed by frame_offset.  */
+static void
+move_arg_location (rtx insn, rtx orig, rtx new, HOST_WIDE_INT var_size)
+{
+  /* For making room for sweep variable, scan all insns and
+     fix the offset address of the variable that is based on frame pointer.  */
+  change_arg_use_of_insns (insn, orig, &new, var_size);
+
+
+  /* Clear all the USED bits in operands of all insns and declarations
+     of local variables.  */
+  reset_used_flags_for_insns (insn);
+}
+
+
+/* Sweep character arrays declared as local variable.  */
+static void
+sweep_string_in_decls (tree block, HOST_WIDE_INT sweep_offset,
+		       HOST_WIDE_INT sweep_size)
+{
+  tree types;
+  HOST_WIDE_INT offset;
+  rtx home;
+
+  while (block && TREE_CODE(block)==BLOCK)
+    {
+      for (types = BLOCK_VARS(block); types; types = TREE_CHAIN(types))
+	{
+	  /* Skip the declaration that refers an external variable and
+	     also skip an global variable.  */
+	  if (! DECL_EXTERNAL (types) && ! TREE_STATIC (types)) {
+	    
+	    if (! DECL_RTL_SET_P (types))
+	      continue;
+
+	    home = DECL_RTL (types);
+
+	    /* Process for static local variable.  */
+	    if (GET_CODE (home) == MEM
+		&& GET_CODE (XEXP (home, 0)) == SYMBOL_REF)
+	      continue;
+
+	    if (GET_CODE (home) == MEM
+		&& XEXP (home, 0) == virtual_stack_vars_rtx)
+	      {
+		offset = 0;
+		
+		/* the operand related to the sweep variable.  */
+		if (sweep_offset <= offset
+		    && offset < sweep_offset + sweep_size)
+		  {
+		    offset = sweep_frame_offset - sweep_size - sweep_offset;
+
+		    XEXP (home, 0) = plus_constant (virtual_stack_vars_rtx,
+						    offset);
+		    XEXP (home, 0)->used = 1;
+		  }
+		else if (sweep_offset <= offset
+			 && offset < sweep_frame_offset)
+		  {
+		    /* the rest of variables under sweep_frame_offset,
+		       shift the location.  */
+		    XEXP (home, 0) = plus_constant (virtual_stack_vars_rtx,
+						    -sweep_size);
+		    XEXP (home, 0)->used = 1;
+		  }
+	      }
+		
+	    if (GET_CODE (home) == MEM
+		&& GET_CODE (XEXP (home, 0)) == MEM)
+	      {
+		/* Process for dynamically allocated array.  */
+		home = XEXP (home, 0);
+	      }
+		
+	    if (GET_CODE (home) == MEM
+		&& GET_CODE (XEXP (home, 0)) == PLUS
+		&& XEXP (XEXP (home, 0), 0) == virtual_stack_vars_rtx
+		&& GET_CODE (XEXP (XEXP (home, 0), 1)) == CONST_INT)
+	      {
+		if (! XEXP (home, 0)->used)
+		  {
+		    offset = AUTO_OFFSET(XEXP (home, 0));
+
+		    /* the operand related to the sweep variable.  */
+		    if (sweep_offset <= offset
+			&& offset < sweep_offset + sweep_size)
+		      {
+
+			offset
+			  += sweep_frame_offset - sweep_size - sweep_offset;
+			XEXP (XEXP (home, 0), 1) = gen_rtx_CONST_INT (VOIDmode,
+								      offset);
+
+			/* mark */
+			XEXP (home, 0)->used = 1;
+		      }
+		    else if (sweep_offset <= offset
+			     && offset < sweep_frame_offset)
+		      {
+			/* the rest of variables under sweep_frame_offset,
+			   so shift the location.  */
+
+			XEXP (XEXP (home, 0), 1)
+			  = gen_rtx_CONST_INT (VOIDmode, offset - sweep_size);
+
+			/* mark */
+			XEXP (home, 0)->used = 1;
+		      }
+		  }
+	      }
+	  }
+	}
+
+      sweep_string_in_decls (BLOCK_SUBBLOCKS (block),
+			     sweep_offset, sweep_size);
+
+      block = BLOCK_CHAIN (block);
+    }
+}
+
+
+/* Sweep character arrays declared as argument.  */
+static void
+sweep_string_in_args (tree parms, HOST_WIDE_INT sweep_offset,
+		      HOST_WIDE_INT sweep_size)
+{
+  rtx home;
+  HOST_WIDE_INT offset;
+    
+  for (; parms; parms = TREE_CHAIN (parms))
+    if (DECL_NAME (parms) && TREE_TYPE (parms) != error_mark_node)
+      {
+	if (PARM_PASSED_IN_MEMORY (parms) && DECL_NAME (parms))
+	  {
+	    home = DECL_INCOMING_RTL (parms);
+
+	    if (XEXP (home, 0)->used)
+	      continue;
+
+	    offset = AUTO_OFFSET(XEXP (home, 0));
+
+	    /* the operand related to the sweep variable.  */
+	    if (AUTO_BASEPTR (XEXP (home, 0)) == virtual_stack_vars_rtx)
+	      {
+		if (sweep_offset <= offset
+		    && offset < sweep_offset + sweep_size)
+		  {
+		    offset += sweep_frame_offset - sweep_size - sweep_offset;
+		    XEXP (XEXP (home, 0), 1) = gen_rtx_CONST_INT (VOIDmode,
+								  offset);
+
+		    /* mark */
+		    XEXP (home, 0)->used = 1;
+		  }
+		else if (sweep_offset <= offset
+			 && offset < sweep_frame_offset)
+		  {
+		    /* the rest of variables under sweep_frame_offset,
+		       shift the location.  */
+		    XEXP (XEXP (home, 0), 1)
+		      = gen_rtx_CONST_INT (VOIDmode, offset - sweep_size);
+
+		    /* mark */
+		    XEXP (home, 0)->used = 1;
+		  }
+	      }
+	  }
+      }
+}
+
+
+/* Set to 1 when the instruction contains virtual registers.  */
+static int has_virtual_reg;
+
+/* Sweep the specified character array for every insns. The array starts from
+   the sweep_offset and its size is sweep_size.  */
+static void
+sweep_string_use_of_insns (rtx insn, HOST_WIDE_INT sweep_offset,
+			   HOST_WIDE_INT sweep_size)
+{
+  for (; insn; insn = NEXT_INSN (insn))
+    if (GET_CODE (insn) == INSN || GET_CODE (insn) == JUMP_INSN
+	|| GET_CODE (insn) == CALL_INSN)
+      {
+	has_virtual_reg = FALSE;
+	sweep_string_in_operand (insn, &PATTERN (insn),
+				 sweep_offset, sweep_size);
+	sweep_string_in_operand (insn, &REG_NOTES (insn),
+				 sweep_offset, sweep_size);
+      }
+}
+
+
+/* Sweep the specified character array, which starts from the sweep_offset and
+   its size is sweep_size.
+
+   When a pointer is given,
+   if it points the address higher than the array, it stays.
+   if it points the address inside the array, it changes to point inside
+   the sweeped array.
+   if it points the address lower than the array, it shifts higher address by
+   the sweep_size.  */
+static void
+sweep_string_in_operand (rtx insn, rtx *loc,
+			 HOST_WIDE_INT sweep_offset, HOST_WIDE_INT sweep_size)
+{
+  rtx x = *loc;
+  enum rtx_code code;
+  int i, j, k = 0;
+  HOST_WIDE_INT offset;
+  const char *fmt;
+
+  if (x == 0)
+    return;
+
+  code = GET_CODE (x);
+
+  switch (code)
+    {
+    case CONST_INT:
+    case CONST_DOUBLE:
+    case CONST:
+    case SYMBOL_REF:
+    case CODE_LABEL:
+    case PC:
+    case CC0:
+    case ASM_INPUT:
+    case ADDR_VEC:
+    case ADDR_DIFF_VEC:
+    case RETURN:
+    case ADDRESSOF:
+      return;
+	    
+    case REG:
+      if (x == virtual_incoming_args_rtx
+	  || x == virtual_stack_vars_rtx
+	  || x == virtual_stack_dynamic_rtx
+	  || x == virtual_outgoing_args_rtx
+	  || x == virtual_cfa_rtx)
+	has_virtual_reg = TRUE;
+      return;
+      
+    case SET:
+      /*
+	skip setjmp setup insn and setjmp restore insn
+	Example:
+	(set (MEM (reg:SI xx)) (virtual_stack_vars_rtx)))
+	(set (virtual_stack_vars_rtx) (REG))
+      */
+      if (GET_CODE (XEXP (x, 0)) == MEM
+	  && XEXP (x, 1) == virtual_stack_vars_rtx)
+	return;
+      if (XEXP (x, 0) == virtual_stack_vars_rtx
+	  && GET_CODE (XEXP (x, 1)) == REG)
+	return;
+      break;
+	    
+    case PLUS:
+      /* Handle typical case of frame register plus constant.  */
+      if (XEXP (x, 0) == virtual_stack_vars_rtx
+	  && GET_CODE (XEXP (x, 1)) == CONST_INT)
+	{
+	  if (x->used)
+	    goto single_use_of_virtual_reg;
+	  
+	  offset = AUTO_OFFSET(x);
+
+	  /* When arguments grow downward, the virtual incoming
+	     args pointer points to the top of the argument block,
+	     so block is identified by the pointer - 1.
+	     The flag is set at the copy_rtx_and_substitute in integrate.c  */
+	  if (RTX_INTEGRATED_P (x))
+	    k = -1;
+
+	  /* the operand related to the sweep variable.  */
+	  if (sweep_offset <= offset + k
+	      && offset + k < sweep_offset + sweep_size)
+	    {
+	      offset += sweep_frame_offset - sweep_size - sweep_offset;
+
+	      XEXP (x, 0) = virtual_stack_vars_rtx;
+	      XEXP (x, 1) = gen_rtx_CONST_INT (VOIDmode, offset);
+	      x->used = 1;
+	    }
+	  else if (sweep_offset <= offset + k
+		   && offset + k < sweep_frame_offset)
+	    {
+	      /* the rest of variables under sweep_frame_offset,
+		 shift the location.  */
+	      XEXP (x, 1) = gen_rtx_CONST_INT (VOIDmode, offset - sweep_size);
+	      x->used = 1;
+	    }
+	  
+	single_use_of_virtual_reg:
+	  if (has_virtual_reg) {
+	    /* excerpt from insn_invalid_p in recog.c  */
+	    int icode = recog_memoized (insn);
+
+	    if (icode < 0 && asm_noperands (PATTERN (insn)) < 0)
+	      {
+		rtx temp, seq;
+		
+		start_sequence ();
+		temp = force_operand (x, NULL_RTX);
+		seq = get_insns ();
+		end_sequence ();
+		
+		emit_insn_before (seq, insn);
+		if (! validate_change (insn, loc, temp, 0)
+		    && !validate_replace_rtx (x, temp, insn))
+		  fatal_insn ("sweep_string_in_operand", insn);
+	      }
+	  }
+
+	  has_virtual_reg = TRUE;
+	  return;
+	}
+
+#ifdef FRAME_GROWS_DOWNWARD
+      /* Alert the case of frame register plus constant given by reg.  */
+      else if (XEXP (x, 0) == virtual_stack_vars_rtx
+	       && GET_CODE (XEXP (x, 1)) == REG)
+	fatal_insn ("sweep_string_in_operand: unknown addressing", insn);
+#endif
+
+      /*
+	process further subtree:
+	Example:  (plus:SI (mem/s:SI (plus:SI (reg:SI 17) (const_int 8)))
+	(const_int 5))
+      */
+      break;
+
+    case CALL_PLACEHOLDER:
+      for (i = 0; i < 3; i++)
+	{
+	  rtx seq = XEXP (x, i);
+	  if (seq)
+	    {
+	      push_to_sequence (seq);
+	      sweep_string_use_of_insns (XEXP (x, i),
+					 sweep_offset, sweep_size);
+	      XEXP (x, i) = get_insns ();
+	      end_sequence ();
+	    }
+	}
+      break;
+
+    default:
+      break;
+    }
+
+  /* Scan all subexpressions.  */
+  fmt = GET_RTX_FORMAT (code);
+  for (i = 0; i < GET_RTX_LENGTH (code); i++, fmt++)
+    if (*fmt == 'e')
+      {
+	/*
+	  virtual_stack_vars_rtx without offset
+	  Example:
+	    (set (reg:SI xx) (reg:SI 78))
+	    (set (reg:SI xx) (MEM (reg:SI 78)))
+	*/
+	if (XEXP (x, i) == virtual_stack_vars_rtx)
+	  fatal_insn ("sweep_string_in_operand: unknown fp usage", insn);
+	sweep_string_in_operand (insn, &XEXP (x, i), sweep_offset, sweep_size);
+      }
+    else if (*fmt == 'E')
+      for (j = 0; j < XVECLEN (x, i); j++)
+	sweep_string_in_operand (insn, &XVECEXP (x, i, j), sweep_offset, sweep_size);
+}   
+
+
+/* Change the use of an argument to the use of the duplicated variable for
+   every insns, The variable is addressed by new rtx.  */
+static void
+change_arg_use_of_insns (rtx insn, rtx orig, rtx *new, HOST_WIDE_INT size)
+{
+  for (; insn; insn = NEXT_INSN (insn))
+    if (GET_CODE (insn) == INSN || GET_CODE (insn) == JUMP_INSN
+	|| GET_CODE (insn) == CALL_INSN)
+      {
+	rtx seq;
+	
+	start_sequence ();
+	change_arg_use_in_operand (insn, PATTERN (insn), orig, new, size);
+
+	seq = get_insns ();
+	end_sequence ();
+	emit_insn_before (seq, insn);
+
+	/* load_multiple insn from virtual_incoming_args_rtx have several
+	   load insns. If every insn change the load address of arg
+	   to frame region, those insns are moved before the PARALLEL insn
+	   and remove the PARALLEL insn.  */
+	if (GET_CODE (PATTERN (insn)) == PARALLEL
+	    && XVECLEN (PATTERN (insn), 0) == 0)
+	  delete_insn (insn);
+      }
+}
+
+
+/* Change the use of an argument to the use of the duplicated variable for
+   every rtx derived from the x.  */
+static void
+change_arg_use_in_operand (rtx insn, rtx x, rtx orig, rtx *new, HOST_WIDE_INT size)
+{
+  enum rtx_code code;
+  int i, j;
+  HOST_WIDE_INT offset;
+  const char *fmt;
+
+  if (x == 0)
+    return;
+
+  code = GET_CODE (x);
+
+  switch (code)
+    {
+    case CONST_INT:
+    case CONST_DOUBLE:
+    case CONST:
+    case SYMBOL_REF:
+    case CODE_LABEL:
+    case PC:
+    case CC0:
+    case ASM_INPUT:
+    case ADDR_VEC:
+    case ADDR_DIFF_VEC:
+    case RETURN:
+    case REG:
+    case ADDRESSOF:
+      return;
+
+    case MEM:
+      /* Handle special case of MEM (incoming_args).  */
+      if (GET_CODE (orig) == MEM
+	  && XEXP (x, 0) == virtual_incoming_args_rtx)
+	{
+	  offset = 0;
+
+	  /* the operand related to the sweep variable.  */
+	  if (AUTO_OFFSET(XEXP (orig, 0)) <= offset &&
+	      offset < AUTO_OFFSET(XEXP (orig, 0)) + size) {
+
+	    offset = AUTO_OFFSET(XEXP (*new, 0))
+	      + (offset - AUTO_OFFSET(XEXP (orig, 0)));
+
+	    XEXP (x, 0) = plus_constant (virtual_stack_vars_rtx, offset);
+	    XEXP (x, 0)->used = 1;
+
+	    return;
+	  }
+	}
+      break;
+      
+    case PLUS:
+      /* Handle special case of frame register plus constant.  */
+      if (GET_CODE (orig) == MEM
+	  && XEXP (x, 0) == virtual_incoming_args_rtx
+	  && GET_CODE (XEXP (x, 1)) == CONST_INT
+	  && ! x->used)
+	{
+	  offset = AUTO_OFFSET(x);
+
+	  /* the operand related to the sweep variable.  */
+	  if (AUTO_OFFSET(XEXP (orig, 0)) <= offset &&
+	      offset < AUTO_OFFSET(XEXP (orig, 0)) + size)
+	    {
+
+	      offset = (AUTO_OFFSET(XEXP (*new, 0))
+			+ (offset - AUTO_OFFSET(XEXP (orig, 0))));
+
+	      XEXP (x, 0) = virtual_stack_vars_rtx;
+	      XEXP (x, 1) = gen_rtx_CONST_INT (VOIDmode, offset);
+	      x->used = 1;
+
+	      return;
+	    }
+
+	  /*
+	    process further subtree:
+	    Example:  (plus:SI (mem/s:SI (plus:SI (reg:SI 17) (const_int 8)))
+	    (const_int 5))
+	  */
+	}
+      break;
+
+    case SET:
+      /* Handle special case of "set (REG or MEM) (incoming_args)".
+	 It means that the the address of the 1st argument is stored.  */
+      if (GET_CODE (orig) == MEM
+	  && XEXP (x, 1) == virtual_incoming_args_rtx)
+	{
+	  offset = 0;
+
+	  /* the operand related to the sweep variable.  */
+	  if (AUTO_OFFSET(XEXP (orig, 0)) <= offset &&
+	      offset < AUTO_OFFSET(XEXP (orig, 0)) + size)
+	    {
+	      offset = (AUTO_OFFSET(XEXP (*new, 0))
+			+ (offset - AUTO_OFFSET(XEXP (orig, 0))));
+
+	      XEXP (x, 1) = force_operand (plus_constant (virtual_stack_vars_rtx,
+							  offset), NULL_RTX);
+	      XEXP (x, 1)->used = 1;
+
+	      return;
+	    }
+	}
+      break;
+
+    case CALL_PLACEHOLDER:
+      for (i = 0; i < 3; i++)
+	{
+	  rtx seq = XEXP (x, i);
+	  if (seq)
+	    {
+	      push_to_sequence (seq);
+	      change_arg_use_of_insns (XEXP (x, i), orig, new, size);
+	      XEXP (x, i) = get_insns ();
+	      end_sequence ();
+	    }
+	}
+      break;
+
+    case PARALLEL:
+      for (j = 0; j < XVECLEN (x, 0); j++)
+	{
+	  change_arg_use_in_operand (insn, XVECEXP (x, 0, j), orig, new, size);
+	}
+      if (recog_memoized (insn) < 0)
+	{
+	  for (i = 0, j = 0; j < XVECLEN (x, 0); j++)
+	    {
+	      /* if parallel insn has a insn used virtual_incoming_args_rtx,
+		 the insn is removed from this PARALLEL insn.  */
+	      if (check_used_flag (XVECEXP (x, 0, j)))
+		{
+		  emit_insn (XVECEXP (x, 0, j));
+		  XVECEXP (x, 0, j) = NULL;
+		}
+	      else
+		XVECEXP (x, 0, i++) = XVECEXP (x, 0, j);
+	    }
+	  PUT_NUM_ELEM (XVEC (x, 0), i);
+	}
+      return;
+
+    default:
+      break;
+    }
+
+  /* Scan all subexpressions.  */
+  fmt = GET_RTX_FORMAT (code);
+  for (i = 0; i < GET_RTX_LENGTH (code); i++, fmt++)
+    if (*fmt == 'e')
+      {
+	if (XEXP (x, i) == orig)
+	  {
+	    if (*new == 0)
+	      *new = gen_reg_rtx (GET_MODE (orig));
+	    XEXP (x, i) = *new;
+	    continue;
+	  }
+	change_arg_use_in_operand (insn, XEXP (x, i), orig, new, size);
+      }
+    else if (*fmt == 'E')
+      for (j = 0; j < XVECLEN (x, i); j++)
+	{
+	  if (XVECEXP (x, i, j) == orig)
+	    {
+	      if (*new == 0)
+		*new = gen_reg_rtx (GET_MODE (orig));
+	      XVECEXP (x, i, j) = *new;
+	      continue;
+	    }
+	  change_arg_use_in_operand (insn, XVECEXP (x, i, j), orig, new, size);
+	}
+}   
+
+
+/* Validate every instructions from the specified instruction.
+   
+   The stack protector prohibits to generate machine specific frame addressing
+   for the first rtl generation. The prepare_stack_protection must convert
+   machine independent frame addressing to machine specific frame addressing,
+   so instructions for inline functions, which skip the conversion of
+   the stack protection, validate every instructions.  */
+static void
+validate_insns_of_varrefs (rtx insn)
+{
+  rtx next;
+
+  /* Initialize recognition, indicating that volatile is OK.  */
+  init_recog ();
+
+  for (; insn; insn = next)
+    {
+      next = NEXT_INSN (insn);
+      if (GET_CODE (insn) == INSN || GET_CODE (insn) == JUMP_INSN
+	  || GET_CODE (insn) == CALL_INSN)
+	{
+	  /* excerpt from insn_invalid_p in recog.c  */
+	  int icode = recog_memoized (insn);
+
+	  if (icode < 0 && asm_noperands (PATTERN (insn)) < 0)
+	    validate_operand_of_varrefs (insn, &PATTERN (insn));
+	}
+    }
+
+  init_recog_no_volatile ();
+}
+
+
+/* Validate frame addressing of the rtx and covert it to machine specific one.  */
+static void
+validate_operand_of_varrefs (rtx insn, rtx *loc)
+{
+  enum rtx_code code;
+  rtx x, temp, seq;
+  int i, j;
+  const char *fmt;
+
+  x = *loc;
+  if (x == 0)
+    return;
+
+  code = GET_CODE (x);
+
+  switch (code)
+    {
+    case USE:
+    case CONST_INT:
+    case CONST_DOUBLE:
+    case CONST:
+    case SYMBOL_REF:
+    case CODE_LABEL:
+    case PC:
+    case CC0:
+    case ASM_INPUT:
+    case ADDR_VEC:
+    case ADDR_DIFF_VEC:
+    case RETURN:
+    case REG:
+    case ADDRESSOF:
+      return;
+
+    case PLUS:
+      /* validate insn of frame register plus constant.  */
+      if (GET_CODE (x) == PLUS
+	  && XEXP (x, 0) == virtual_stack_vars_rtx
+	  && GET_CODE (XEXP (x, 1)) == CONST_INT)
+	{
+	  start_sequence ();
+
+	  { /* excerpt from expand_binop in optabs.c  */
+	    optab binoptab = add_optab;
+	    enum machine_mode mode = GET_MODE (x);
+	    int icode = (int) binoptab->handlers[(int) mode].insn_code;
+	    enum machine_mode mode1 = insn_data[icode].operand[2].mode;
+	    rtx pat;
+	    rtx xop0 = XEXP (x, 0), xop1 = XEXP (x, 1);
+	    temp = gen_reg_rtx (mode);
+
+	    /* Now, if insn's predicates don't allow offset operands,
+	       put them into pseudo regs.  */
+
+	    if (! (*insn_data[icode].operand[2].predicate) (xop1, mode1)
+		&& mode1 != VOIDmode)
+	      xop1 = copy_to_mode_reg (mode1, xop1);
+
+	    pat = GEN_FCN (icode) (temp, xop0, xop1);
+	    if (pat)
+	      emit_insn (pat);
+	    else
+	      abort (); /* there must be add_optab handler.  */
+	  }	      
+	  seq = get_insns ();
+	  end_sequence ();
+	  
+	  emit_insn_before (seq, insn);
+	  if (! validate_change (insn, loc, temp, 0))
+	    abort ();
+	  return;
+	}
+	break;
+      
+
+    case CALL_PLACEHOLDER:
+      for (i = 0; i < 3; i++)
+	{
+	  rtx seq = XEXP (x, i);
+	  if (seq)
+	    {
+	      push_to_sequence (seq);
+	      validate_insns_of_varrefs (XEXP (x, i));
+	      XEXP (x, i) = get_insns ();
+	      end_sequence ();
+	    }
+	}
+      break;
+
+    default:
+      break;
+    }
+
+  /* Scan all subexpressions.  */
+  fmt = GET_RTX_FORMAT (code);
+  for (i = 0; i < GET_RTX_LENGTH (code); i++, fmt++)
+    if (*fmt == 'e')
+      validate_operand_of_varrefs (insn, &XEXP (x, i));
+    else if (*fmt == 'E')
+      for (j = 0; j < XVECLEN (x, i); j++)
+	validate_operand_of_varrefs (insn, &XVECEXP (x, i, j));
+}
+
+
+
+/* Return size that is not allocated for stack frame. It will be allocated
+   to modify the home of pseudo registers called from global_alloc.  */
+HOST_WIDE_INT
+get_frame_free_size (void)
+{
+  if (! flag_propolice_protection)
+    return 0;
+
+  return push_allocated_offset - push_frame_offset;
+}
+
+
+/* The following codes are invoked after the instantiation of pseudo registers.
+
+   Reorder local variables to place a peudo register after buffers to avoid
+   the corruption of local variables that could be used to further corrupt
+   arbitrary memory locations.  */
+#if !defined(FRAME_GROWS_DOWNWARD) && defined(STACK_GROWS_DOWNWARD)
+static void push_frame (HOST_WIDE_INT, HOST_WIDE_INT);
+static void push_frame_in_decls (tree, HOST_WIDE_INT, HOST_WIDE_INT);
+static void push_frame_in_args (tree, HOST_WIDE_INT, HOST_WIDE_INT);
+static void push_frame_of_insns (rtx, HOST_WIDE_INT, HOST_WIDE_INT);
+static void push_frame_in_operand (rtx, rtx, HOST_WIDE_INT, HOST_WIDE_INT);
+static void push_frame_of_reg_equiv_memory_loc (HOST_WIDE_INT, HOST_WIDE_INT);
+static void push_frame_of_reg_equiv_constant (HOST_WIDE_INT, HOST_WIDE_INT);
+static void reset_used_flags_for_push_frame (void);
+static int check_out_of_frame_access (rtx, HOST_WIDE_INT);
+static int check_out_of_frame_access_in_operand (rtx, HOST_WIDE_INT);
+#endif
+
+
+/* Assign stack local at the stage of register allocater. if a pseudo reg is
+   spilled out from such an allocation, it is allocated on the stack.
+   The protector keep the location be lower stack region than the location of
+   sweeped arrays.  */
+rtx
+assign_stack_local_for_pseudo_reg (enum machine_mode mode,
+				   HOST_WIDE_INT size, int align)
+{
+#if defined(FRAME_GROWS_DOWNWARD) || !defined(STACK_GROWS_DOWNWARD)
+  return assign_stack_local (mode, size, align);
+#else
+  tree blocks = DECL_INITIAL (current_function_decl);
+  rtx new;
+  HOST_WIDE_INT saved_frame_offset, units_per_push, starting_frame;
+  int first_call_from_purge_addressof, first_call_from_global_alloc;
+
+  if (! flag_propolice_protection
+      || size == 0
+      || ! blocks
+      || current_function_is_inlinable
+      || ! search_string_from_argsandvars (CALL_FROM_PUSH_FRAME)
+      || current_function_contains_functions)
+    return assign_stack_local (mode, size, align);
+
+  first_call_from_purge_addressof = !push_frame_offset && !cse_not_expected;
+  first_call_from_global_alloc = !saved_cse_not_expected && cse_not_expected;
+  saved_cse_not_expected = cse_not_expected;
+
+  starting_frame = ((STARTING_FRAME_OFFSET)
+		    ? STARTING_FRAME_OFFSET : BIGGEST_ALIGNMENT / BITS_PER_UNIT);
+  units_per_push = MAX (BIGGEST_ALIGNMENT / BITS_PER_UNIT,
+			GET_MODE_SIZE (mode));
+    
+  if (first_call_from_purge_addressof)
+    {
+      push_frame_offset = push_allocated_offset;
+      if (check_out_of_frame_access (get_insns (), starting_frame))
+	{
+	  /* After the purge_addressof stage, there may be an instruction which
+	     have the pointer less than the starting_frame. 
+	     if there is an access below frame, push dummy region to seperate
+	     the address of instantiated variables.  */
+	  push_frame (GET_MODE_SIZE (DImode), 0);
+	  assign_stack_local (BLKmode, GET_MODE_SIZE (DImode), -1);
+	}
+    }
+
+  if (first_call_from_global_alloc)
+    {
+      push_frame_offset = push_allocated_offset = 0;
+      if (check_out_of_frame_access (get_insns (), starting_frame))
+	{
+	  if (STARTING_FRAME_OFFSET)
+	    {
+	      /* if there is an access below frame, push dummy region 
+		 to seperate the address of instantiated variables.  */
+	      push_frame (GET_MODE_SIZE (DImode), 0);
+	      assign_stack_local (BLKmode, GET_MODE_SIZE (DImode), -1);
+	    }
+	  else
+	    push_allocated_offset = starting_frame;
+	}
+    }
+
+  saved_frame_offset = frame_offset;
+  frame_offset = push_frame_offset;
+
+  new = assign_stack_local (mode, size, align);
+
+  push_frame_offset = frame_offset;
+  frame_offset = saved_frame_offset;
+  
+  if (push_frame_offset > push_allocated_offset)
+    {
+      push_frame (units_per_push,
+		  push_allocated_offset + STARTING_FRAME_OFFSET);
+
+      assign_stack_local (BLKmode, units_per_push, -1);
+      push_allocated_offset += units_per_push;
+    }
+
+  /* At the second call from global alloc, alpha push frame and assign
+     a local variable to the top of the stack.  */
+  if (first_call_from_global_alloc && STARTING_FRAME_OFFSET == 0)
+    push_frame_offset = push_allocated_offset = 0;
+
+  return new;
+#endif
+}
+
+
+#if !defined(FRAME_GROWS_DOWNWARD) && defined(STACK_GROWS_DOWNWARD)
+
+/* push frame infomation for instantiating pseudo register at the top of stack.
+   This is only for the "frame grows upward", it means FRAME_GROWS_DOWNWARD is 
+   not defined.
+
+   It is called by purge_addressof function and global_alloc (or reload)
+   function.  */
+static void
+push_frame (HOST_WIDE_INT var_size, HOST_WIDE_INT boundary)
+{
+  reset_used_flags_for_push_frame();
+
+  /* Scan all declarations of variables and fix the offset address of
+     the variable based on the frame pointer.  */
+  push_frame_in_decls (DECL_INITIAL (current_function_decl),
+		       var_size, boundary);
+
+  /* Scan all argument variable and fix the offset address based on
+     the frame pointer.  */
+  push_frame_in_args (DECL_ARGUMENTS (current_function_decl),
+		      var_size, boundary);
+
+  /* Scan all operands of all insns and fix the offset address
+     based on the frame pointer.  */
+  push_frame_of_insns (get_insns (), var_size, boundary);
+
+  /* Scan all reg_equiv_memory_loc and reg_equiv_constant.  */
+  push_frame_of_reg_equiv_memory_loc (var_size, boundary);
+  push_frame_of_reg_equiv_constant (var_size, boundary);
+
+  reset_used_flags_for_push_frame();
+}
+
+
+/* Reset used flag of every insns, reg_equiv_memory_loc,
+   and reg_equiv_constant.  */
+static void
+reset_used_flags_for_push_frame(void)
+{
+  int i;
+  extern rtx *reg_equiv_memory_loc;
+  extern rtx *reg_equiv_constant;
+
+  /* Clear all the USED bits in operands of all insns and declarations of
+     local vars.  */
+  reset_used_flags_for_decls (DECL_INITIAL (current_function_decl));
+  reset_used_flags_for_insns (get_insns ());
+
+
+  /* The following codes are processed if the push_frame is called from 
+     global_alloc (or reload) function.  */
+  if (reg_equiv_memory_loc == 0)
+    return;
+
+  for (i=LAST_VIRTUAL_REGISTER+1; i < max_regno; i++)
+    if (reg_equiv_memory_loc[i])
+      {
+	rtx x = reg_equiv_memory_loc[i];
+
+	if (GET_CODE (x) == MEM
+	    && GET_CODE (XEXP (x, 0)) == PLUS
+	    && AUTO_BASEPTR (XEXP (x, 0)) == frame_pointer_rtx)
+	  {
+	    /* reset */
+	    XEXP (x, 0)->used = 0;
+	  }
+      }
+
+  
+  if (reg_equiv_constant == 0)
+    return;
+
+  for (i=LAST_VIRTUAL_REGISTER+1; i < max_regno; i++)
+    if (reg_equiv_constant[i])
+      {
+	rtx x = reg_equiv_constant[i];
+
+	if (GET_CODE (x) == PLUS
+	    && AUTO_BASEPTR (x) == frame_pointer_rtx)
+	  {
+	    /* reset */
+	    x->used = 0;
+	  }
+      }
+}
+
+
+/* Push every variables declared as a local variable and make a room for
+   instantiated register.  */
+static void
+push_frame_in_decls (tree block, HOST_WIDE_INT push_size,
+		     HOST_WIDE_INT boundary)
+{
+  tree types;
+  HOST_WIDE_INT offset;
+  rtx home;
+
+  while (block && TREE_CODE(block)==BLOCK)
+    {
+      for (types = BLOCK_VARS(block); types; types = TREE_CHAIN(types))
+	{
+	  /* Skip the declaration that refers an external variable and
+	     also skip an global variable.  */
+	  if (! DECL_EXTERNAL (types) && ! TREE_STATIC (types))
+	    {
+	      if (! DECL_RTL_SET_P (types))
+		continue;
+
+	      home = DECL_RTL (types);
+
+	      /* Process for static local variable.  */
+	      if (GET_CODE (home) == MEM
+		  && GET_CODE (XEXP (home, 0)) == SYMBOL_REF)
+		continue;
+
+	      if (GET_CODE (home) == MEM
+		  && GET_CODE (XEXP (home, 0)) == REG)
+		{
+		  if (XEXP (home, 0) != frame_pointer_rtx
+		      || boundary != 0)
+		    continue;
+
+		  XEXP (home, 0) = plus_constant (frame_pointer_rtx,
+						  push_size);
+
+		  /* mark */
+		  XEXP (home, 0)->used = 1;
+		}
+		
+	      if (GET_CODE (home) == MEM
+		  && GET_CODE (XEXP (home, 0)) == MEM)
+		{
+		  /* Process for dynamically allocated array.  */
+		  home = XEXP (home, 0);
+		}
+		
+	      if (GET_CODE (home) == MEM
+		  && GET_CODE (XEXP (home, 0)) == PLUS
+		  && GET_CODE (XEXP (XEXP (home, 0), 1)) == CONST_INT)
+		{
+		  offset = AUTO_OFFSET(XEXP (home, 0));
+
+		  if (! XEXP (home, 0)->used
+		      && offset >= boundary)
+		    {
+		      offset += push_size;
+		      XEXP (XEXP (home, 0), 1)
+			= gen_rtx_CONST_INT (VOIDmode, offset);
+		      
+		      /* mark */
+		      XEXP (home, 0)->used = 1;
+		    }
+		}
+	    }
+	}
+
+      push_frame_in_decls (BLOCK_SUBBLOCKS (block), push_size, boundary);
+      block = BLOCK_CHAIN (block);
+    }
+}
+
+
+/* Push every variables declared as an argument and make a room for
+   instantiated register.  */
+static void
+push_frame_in_args (tree parms, HOST_WIDE_INT push_size,
+		    HOST_WIDE_INT boundary)
+{
+  rtx home;
+  HOST_WIDE_INT offset;
+    
+  for (; parms; parms = TREE_CHAIN (parms))
+    if (DECL_NAME (parms) && TREE_TYPE (parms) != error_mark_node)
+      {
+	if (PARM_PASSED_IN_MEMORY (parms))
+	  {
+	    home = DECL_INCOMING_RTL (parms);
+	    offset = AUTO_OFFSET(XEXP (home, 0));
+
+	    if (XEXP (home, 0)->used || offset < boundary)
+	      continue;
+
+	    /* the operand related to the sweep variable.  */
+	    if (AUTO_BASEPTR (XEXP (home, 0)) == frame_pointer_rtx)
+	      {
+		if (XEXP (home, 0) == frame_pointer_rtx)
+		  XEXP (home, 0) = plus_constant (frame_pointer_rtx,
+						  push_size);
+		else {
+		  offset += push_size;
+		  XEXP (XEXP (home, 0), 1) = gen_rtx_CONST_INT (VOIDmode,
+								offset);
+		}
+
+		/* mark */
+		XEXP (home, 0)->used = 1;
+	      }
+	  }
+      }
+}
+
+
+/* Set to 1 when the instruction has the reference to be pushed.  */
+static int insn_pushed;
+
+/* Tables of equivalent registers with frame pointer.  */
+static int *fp_equiv = 0;
+
+
+/* Push the frame region to make a room for allocated local variable.  */
+static void
+push_frame_of_insns (rtx insn, HOST_WIDE_INT push_size, HOST_WIDE_INT boundary)
+{
+  /* init fp_equiv */
+  fp_equiv = (int *) xcalloc (max_reg_num (), sizeof (int));
+		
+  for (; insn; insn = NEXT_INSN (insn))
+    if (GET_CODE (insn) == INSN || GET_CODE (insn) == JUMP_INSN
+	|| GET_CODE (insn) == CALL_INSN)
+      {
+	rtx last;
+	
+	insn_pushed = FALSE;
+
+	/* Push frame in INSN operation.  */
+	push_frame_in_operand (insn, PATTERN (insn), push_size, boundary);
+
+	/* Push frame in NOTE.  */
+	push_frame_in_operand (insn, REG_NOTES (insn), push_size, boundary);
+
+	/* Push frame in CALL EXPR_LIST.  */
+	if (GET_CODE (insn) == CALL_INSN)
+	  push_frame_in_operand (insn, CALL_INSN_FUNCTION_USAGE (insn),
+				 push_size, boundary);
+
+	/* Pushed frame addressing style may not be machine specific one.
+	   so the instruction should be converted to use the machine specific
+	   frame addressing.  */
+	if (insn_pushed
+	    && (last = try_split (PATTERN (insn), insn, 1)) != insn)
+	  {
+	    rtx first = NEXT_INSN (insn);
+	    rtx trial = NEXT_INSN (first);
+	    rtx pattern = PATTERN (trial);
+	    rtx set;
+
+	    /* Update REG_EQUIV info to the first splitted insn.  */
+	    if ((set = single_set (insn))
+		&& find_reg_note (insn, REG_EQUIV, SET_SRC (set))
+		&& GET_CODE (PATTERN (first)) == SET)
+	      {
+		REG_NOTES (first)
+		  = gen_rtx_EXPR_LIST (REG_EQUIV,
+				       SET_SRC (PATTERN (first)),
+				       REG_NOTES (first));
+	      }
+
+	    /* copy the first insn of splitted insns to the original insn and
+	       delete the first insn,
+	       because the original insn is pointed from records:
+	       insn_chain, reg_equiv_init, used for global_alloc.  */
+	    if (cse_not_expected)
+	      {
+		add_insn_before (insn, first);
+		
+		/* Copy the various flags, and other information.  */
+		memcpy (insn, first, sizeof (struct rtx_def) - sizeof (rtunion));
+		PATTERN (insn) = PATTERN (first);
+		INSN_CODE (insn) = INSN_CODE (first);
+		LOG_LINKS (insn) = LOG_LINKS (first);
+		REG_NOTES (insn) = REG_NOTES (first);
+
+		/* then remove the first insn of splitted insns.  */
+		remove_insn (first);
+		INSN_DELETED_P (first) = 1;
+	      }
+
+	    if (GET_CODE (pattern) == SET
+		&& GET_CODE (XEXP (pattern, 0)) == REG
+		&& GET_CODE (XEXP (pattern, 1)) == PLUS
+		&& XEXP (pattern, 0) == XEXP (XEXP (pattern, 1), 0)
+		&& GET_CODE (XEXP (XEXP (pattern, 1), 1)) == CONST_INT)
+	      {
+		rtx offset = XEXP (XEXP (pattern, 1), 1);
+		fp_equiv[REGNO (XEXP (pattern, 0))] = INTVAL (offset);
+
+		delete_insn (trial);
+	      }
+
+	    insn = last;
+	  }
+      }
+
+  /* Clean up.  */
+  free (fp_equiv);
+}
+
+
+/* Push the frame region by changing the operand that points the frame.  */
+static void
+push_frame_in_operand (rtx insn, rtx orig,
+		       HOST_WIDE_INT push_size, HOST_WIDE_INT boundary)
+{
+  rtx x = orig;
+  enum rtx_code code;
+  int i, j;
+  HOST_WIDE_INT offset;
+  const char *fmt;
+
+  if (x == 0)
+    return;
+
+  code = GET_CODE (x);
+
+  switch (code)
+    {
+    case CONST_INT:
+    case CONST_DOUBLE:
+    case CONST:
+    case SYMBOL_REF:
+    case CODE_LABEL:
+    case PC:
+    case CC0:
+    case ASM_INPUT:
+    case ADDR_VEC:
+    case ADDR_DIFF_VEC:
+    case RETURN:
+    case REG:
+    case ADDRESSOF:
+    case USE:
+      return;
+	    
+    case SET:
+      /*
+	Skip setjmp setup insn and setjmp restore insn
+	alpha case:
+	(set (MEM (reg:SI xx)) (frame_pointer_rtx)))
+	(set (frame_pointer_rtx) (REG))
+      */
+      if (GET_CODE (XEXP (x, 0)) == MEM
+	  && XEXP (x, 1) == frame_pointer_rtx)
+	return;
+      if (XEXP (x, 0) == frame_pointer_rtx
+	  && GET_CODE (XEXP (x, 1)) == REG)
+	return;
+
+      /*
+	powerpc case: restores setjmp address
+	(set (frame_pointer_rtx) (plus frame_pointer_rtx const_int -n))
+	or
+	(set (reg) (plus frame_pointer_rtx const_int -n))
+	(set (frame_pointer_rtx) (reg))
+      */
+      if (GET_CODE (XEXP (x, 0)) == REG
+	  && GET_CODE (XEXP (x, 1)) == PLUS
+	  && XEXP (XEXP (x, 1), 0) == frame_pointer_rtx
+	  && GET_CODE (XEXP (XEXP (x, 1), 1)) == CONST_INT
+	  && INTVAL (XEXP (XEXP (x, 1), 1)) < 0)
+	{
+	  x = XEXP (x, 1);
+	  offset = AUTO_OFFSET(x);
+	  if (x->used || -offset < boundary)
+	    return;
+
+	  XEXP (x, 1) = gen_rtx_CONST_INT (VOIDmode, offset - push_size);
+	  x->used = 1; insn_pushed = TRUE;
+	  return;
+	}
+
+      /* Reset fp_equiv register.  */
+      else if (GET_CODE (XEXP (x, 0)) == REG
+	  && fp_equiv[REGNO (XEXP (x, 0))])
+	fp_equiv[REGNO (XEXP (x, 0))] = 0;
+
+      /* Propagete fp_equiv register.  */
+      else if (GET_CODE (XEXP (x, 0)) == REG
+	       && GET_CODE (XEXP (x, 1)) == REG
+	       && fp_equiv[REGNO (XEXP (x, 1))])
+	if (REGNO (XEXP (x, 0)) <= LAST_VIRTUAL_REGISTER
+	    || reg_renumber[REGNO (XEXP (x, 0))] > 0)
+	  fp_equiv[REGNO (XEXP (x, 0))] = fp_equiv[REGNO (XEXP (x, 1))];
+      break;
+
+    case MEM:
+      if (XEXP (x, 0) == frame_pointer_rtx
+	  && boundary == 0)
+	{
+	  XEXP (x, 0) = plus_constant (frame_pointer_rtx, push_size);
+	  XEXP (x, 0)->used = 1; insn_pushed = TRUE;
+	  return;
+	}
+      break;
+      
+    case PLUS:
+      /* Handle special case of frame register plus constant.  */
+      if (GET_CODE (XEXP (x, 1)) == CONST_INT
+	  && XEXP (x, 0) == frame_pointer_rtx)
+	{
+	  offset = AUTO_OFFSET(x);
+
+	  if (x->used || offset < boundary)
+	    return;
+
+	  XEXP (x, 1) = gen_rtx_CONST_INT (VOIDmode, offset + push_size);
+	  x->used = 1; insn_pushed = TRUE;
+
+	  return;
+	}
+      /*
+	Handle alpha case:
+	 (plus:SI (subreg:SI (reg:DI 63 FP) 0) (const_int 64 [0x40]))
+      */
+      if (GET_CODE (XEXP (x, 1)) == CONST_INT
+	  && GET_CODE (XEXP (x, 0)) == SUBREG
+	  && SUBREG_REG (XEXP (x, 0)) == frame_pointer_rtx)
+	{
+	  offset = AUTO_OFFSET(x);
+
+	  if (x->used || offset < boundary)
+	    return;
+
+	  XEXP (x, 1) = gen_rtx_CONST_INT (VOIDmode, offset + push_size);
+	  x->used = 1; insn_pushed = TRUE;
+
+	  return;
+	}
+      /*
+	Handle powerpc case:
+	 (set (reg x) (plus fp const))
+	 (set (.....) (... (plus (reg x) (const B))))
+      */
+      else if (GET_CODE (XEXP (x, 1)) == CONST_INT
+	       && GET_CODE (XEXP (x, 0)) == REG
+	       && fp_equiv[REGNO (XEXP (x, 0))])
+	{
+	  offset = AUTO_OFFSET(x);
+
+	  if (x->used)
+	    return;
+
+	  offset += fp_equiv[REGNO (XEXP (x, 0))];
+
+	  XEXP (x, 1) = gen_rtx_CONST_INT (VOIDmode, offset);
+	  x->used = 1; insn_pushed = TRUE;
+
+	  return;
+	}
+      /*
+	Handle special case of frame register plus reg (constant).
+	 (set (reg x) (const B))
+	 (set (....) (...(plus fp (reg x))))
+      */
+      else if (XEXP (x, 0) == frame_pointer_rtx
+	       && GET_CODE (XEXP (x, 1)) == REG
+	       && PREV_INSN (insn)
+	       && PATTERN (PREV_INSN (insn))
+	       && SET_DEST (PATTERN (PREV_INSN (insn))) == XEXP (x, 1)
+	       && GET_CODE (SET_SRC (PATTERN (PREV_INSN (insn)))) == CONST_INT)
+	{
+	  offset = INTVAL (SET_SRC (PATTERN (PREV_INSN (insn))));
+
+	  if (x->used || offset < boundary)
+	    return;
+	  
+	  SET_SRC (PATTERN (PREV_INSN (insn)))
+	    = gen_rtx_CONST_INT (VOIDmode, offset + push_size);
+	  x->used = 1;
+	  XEXP (x, 1)->used = 1;
+
+	  return;
+	}
+      /*
+	Handle special case of frame register plus reg (used).
+	The register already have a pushed offset, just mark this frame
+	addressing.
+      */
+      else if (XEXP (x, 0) == frame_pointer_rtx
+	       && XEXP (x, 1)->used)
+	{
+	  x->used = 1;
+	  return;
+	}
+      /*
+	Process further subtree:
+	Example:  (plus:SI (mem/s:SI (plus:SI (FP) (const_int 8)))
+	(const_int 5))
+      */
+      break;
+
+    case CALL_PLACEHOLDER:
+      push_frame_of_insns (XEXP (x, 0), push_size, boundary);
+      push_frame_of_insns (XEXP (x, 1), push_size, boundary);
+      push_frame_of_insns (XEXP (x, 2), push_size, boundary);
+      break;
+
+    default:
+      break;
+    }
+
+  /* Scan all subexpressions.  */
+  fmt = GET_RTX_FORMAT (code);
+  for (i = 0; i < GET_RTX_LENGTH (code); i++, fmt++)
+    if (*fmt == 'e')
+      {
+	if (XEXP (x, i) == frame_pointer_rtx && boundary == 0)
+	  fatal_insn ("push_frame_in_operand", insn);
+	push_frame_in_operand (insn, XEXP (x, i), push_size, boundary);
+      }
+    else if (*fmt == 'E')
+      for (j = 0; j < XVECLEN (x, i); j++)
+	push_frame_in_operand (insn, XVECEXP (x, i, j), push_size, boundary);
+}   
+
+
+/* Change the location pointed in reg_equiv_memory_loc.  */
+static void
+push_frame_of_reg_equiv_memory_loc (HOST_WIDE_INT push_size,
+				    HOST_WIDE_INT boundary)
+{
+  int i;
+  extern rtx *reg_equiv_memory_loc;
+
+  /* This function is processed if the push_frame is called from 
+     global_alloc (or reload) function.  */
+  if (reg_equiv_memory_loc == 0)
+    return;
+
+  for (i=LAST_VIRTUAL_REGISTER+1; i < max_regno; i++)
+    if (reg_equiv_memory_loc[i])
+      {
+	rtx x = reg_equiv_memory_loc[i];
+	int offset;
+
+	if (GET_CODE (x) == MEM
+	    && GET_CODE (XEXP (x, 0)) == PLUS
+	    && XEXP (XEXP (x, 0), 0) == frame_pointer_rtx)
+	  {
+	    offset = AUTO_OFFSET(XEXP (x, 0));
+	    
+	    if (! XEXP (x, 0)->used
+		&& offset >= boundary)
+	      {
+		offset += push_size;
+		XEXP (XEXP (x, 0), 1) = gen_rtx_CONST_INT (VOIDmode, offset);
+
+		/* mark */
+		XEXP (x, 0)->used = 1;
+	      }
+	  }
+	else if (GET_CODE (x) == MEM
+		 && XEXP (x, 0) == frame_pointer_rtx
+		 && boundary == 0)
+	  {
+	    XEXP (x, 0) = plus_constant (frame_pointer_rtx, push_size);
+	    XEXP (x, 0)->used = 1; insn_pushed = TRUE;
+	  }
+      }
+}
+
+
+/* Change the location pointed in reg_equiv_constant.  */
+static void
+push_frame_of_reg_equiv_constant (HOST_WIDE_INT push_size,
+				  HOST_WIDE_INT boundary)
+{
+  int i;
+  extern rtx *reg_equiv_constant;
+
+  /* This function is processed if the push_frame is called from 
+     global_alloc (or reload) function.  */
+  if (reg_equiv_constant == 0)
+    return;
+
+  for (i = LAST_VIRTUAL_REGISTER + 1; i < max_regno; i++)
+    if (reg_equiv_constant[i])
+      {
+	rtx x = reg_equiv_constant[i];
+	int offset;
+
+	if (GET_CODE (x) == PLUS
+	    && XEXP (x, 0) == frame_pointer_rtx)
+	  {
+	    offset = AUTO_OFFSET(x);
+	    
+	    if (! x->used
+		&& offset >= boundary)
+	      {
+		offset += push_size;
+		XEXP (x, 1) = gen_rtx_CONST_INT (VOIDmode, offset);
+
+		/* mark */
+		x->used = 1;
+	      }
+	  }
+	else if (x == frame_pointer_rtx
+		 && boundary == 0)
+	  {
+	    reg_equiv_constant[i]
+	      = plus_constant (frame_pointer_rtx, push_size);
+	    reg_equiv_constant[i]->used = 1; insn_pushed = TRUE;
+	  }
+      }
+}
+
+
+/* Check every instructions if insn's memory reference is out of frame.  */
+static int
+check_out_of_frame_access (rtx insn, HOST_WIDE_INT boundary)
+{
+  for (; insn; insn = NEXT_INSN (insn))
+    if (GET_CODE (insn) == INSN || GET_CODE (insn) == JUMP_INSN
+	|| GET_CODE (insn) == CALL_INSN)
+      {
+	if (check_out_of_frame_access_in_operand (PATTERN (insn), boundary))
+	  return TRUE;
+      }
+  return FALSE;
+}
+
+
+/* Check every operands if the reference is out of frame.  */
+static int
+check_out_of_frame_access_in_operand (rtx orig, HOST_WIDE_INT boundary)
+{
+  rtx x = orig;
+  enum rtx_code code;
+  int i, j;
+  const char *fmt;
+
+  if (x == 0)
+    return FALSE;
+
+  code = GET_CODE (x);
+
+  switch (code)
+    {
+    case CONST_INT:
+    case CONST_DOUBLE:
+    case CONST:
+    case SYMBOL_REF:
+    case CODE_LABEL:
+    case PC:
+    case CC0:
+    case ASM_INPUT:
+    case ADDR_VEC:
+    case ADDR_DIFF_VEC:
+    case RETURN:
+    case REG:
+    case ADDRESSOF:
+      return FALSE;
+	    
+    case MEM:
+      if (XEXP (x, 0) == frame_pointer_rtx)
+	if (0 < boundary)
+	  return TRUE;
+      break;
+      
+    case PLUS:
+      /* Handle special case of frame register plus constant.  */
+      if (GET_CODE (XEXP (x, 1)) == CONST_INT
+	  && XEXP (x, 0) == frame_pointer_rtx)
+	{
+	  if (0 <= AUTO_OFFSET(x)
+	      && AUTO_OFFSET(x) < boundary)
+	    return TRUE;
+	  return FALSE;
+	}
+      /*
+	Process further subtree:
+	Example:  (plus:SI (mem/s:SI (plus:SI (reg:SI 17) (const_int 8)))
+	(const_int 5))
+      */
+      break;
+
+    case CALL_PLACEHOLDER:
+      if (check_out_of_frame_access (XEXP (x, 0), boundary))
+	return TRUE;
+      if (check_out_of_frame_access (XEXP (x, 1), boundary))
+	return TRUE;
+      if (check_out_of_frame_access (XEXP (x, 2), boundary))
+	return TRUE;
+      break;
+
+    default:
+      break;
+    }
+
+  /* Scan all subexpressions.  */
+  fmt = GET_RTX_FORMAT (code);
+  for (i = 0; i < GET_RTX_LENGTH (code); i++, fmt++)
+    if (*fmt == 'e')
+      {
+	if (check_out_of_frame_access_in_operand (XEXP (x, i), boundary))
+	  return TRUE;
+      }
+    else if (*fmt == 'E')
+      for (j = 0; j < XVECLEN (x, i); j++)
+	if (check_out_of_frame_access_in_operand (XVECEXP (x, i, j), boundary))
+	  return TRUE;
+
+  return FALSE;
+}
+#endif
diff -Naur gcc-3.4.4/gcc/protector.h gcc-3.4.4-ssp/gcc/protector.h
--- gcc-3.4.4/gcc/protector.h	1970-01-01 02:00:00.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/protector.h	2004-01-20 04:01:39.000000000 +0200
@@ -0,0 +1,55 @@
+/* RTL buffer overflow protection function for GNU C compiler
+   Copyright (C) 2003 Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 2, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+02111-1307, USA.  */
+
+
+/* Declare GUARD variable.  */
+#define GUARD_m		Pmode
+#define UNITS_PER_GUARD						\
+  MAX(BIGGEST_ALIGNMENT / BITS_PER_UNIT, GET_MODE_SIZE (GUARD_m))
+
+#ifndef L_stack_smash_handler
+
+/* Insert a guard variable before a character buffer and change the order
+ of pointer variables, character buffers and pointer arguments.  */
+
+extern void prepare_stack_protection  (int);
+
+#ifdef TREE_CODE
+/* Search a character array from the specified type tree.  */
+
+extern int search_string_def (tree);
+#endif
+
+/* Examine whether the input contains frame pointer addressing.  */
+
+extern int contains_fp (rtx);
+
+/* Return size that is not allocated for stack frame. It will be allocated
+   to modify the home of pseudo registers called from global_alloc.  */
+
+extern HOST_WIDE_INT get_frame_free_size (void);
+
+/* Allocate a local variable in the stack area before character buffers
+   to avoid the corruption of it.  */
+
+extern rtx assign_stack_local_for_pseudo_reg (enum machine_mode,
+					      HOST_WIDE_INT, int);
+
+#endif
diff -Naur gcc-3.4.4/gcc/reload1.c gcc-3.4.4-ssp/gcc/reload1.c
--- gcc-3.4.4/gcc/reload1.c	2005-03-17 23:11:35.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/reload1.c	2005-05-25 14:03:22.000000000 +0300
@@ -43,6 +43,7 @@
 #include "toplev.h"
 #include "except.h"
 #include "tree.h"
+#include "protector.h"
 
 /* This file contains the reload pass of the compiler, which is
    run after register allocation has been done.  It checks that
@@ -891,7 +892,7 @@
       if (cfun->stack_alignment_needed)
         assign_stack_local (BLKmode, 0, cfun->stack_alignment_needed);
 
-      starting_frame_size = get_frame_size ();
+      starting_frame_size = get_frame_size () - get_frame_free_size ();
 
       set_initial_elim_offsets ();
       set_initial_label_offsets ();
@@ -955,7 +956,7 @@
 	setup_save_areas ();
 
       /* If we allocated another stack slot, redo elimination bookkeeping.  */
-      if (starting_frame_size != get_frame_size ())
+      if (starting_frame_size != get_frame_size () - get_frame_free_size ())
 	continue;
 
       if (caller_save_needed)
@@ -974,7 +975,7 @@
 
       /* If we allocated any new memory locations, make another pass
 	 since it might have changed elimination offsets.  */
-      if (starting_frame_size != get_frame_size ())
+      if (starting_frame_size != get_frame_size () - get_frame_free_size ())
 	something_changed = 1;
 
       {
@@ -1066,11 +1067,11 @@
   if (insns_need_reload != 0 || something_needs_elimination
       || something_needs_operands_changed)
     {
-      HOST_WIDE_INT old_frame_size = get_frame_size ();
+      HOST_WIDE_INT old_frame_size = get_frame_size () - get_frame_free_size ();
 
       reload_as_needed (global);
 
-      if (old_frame_size != get_frame_size ())
+      if (old_frame_size != get_frame_size () - get_frame_free_size ())
 	abort ();
 
       if (num_eliminable)
@@ -1957,8 +1958,10 @@
 	 inherent space, and no less total space, then the previous slot.  */
       if (from_reg == -1)
 	{
-	  /* No known place to spill from => no slot to reuse.  */
-	  x = assign_stack_local (GET_MODE (regno_reg_rtx[i]), total_size,
+	  /* No known place to spill from => no slot to reuse.
+	     For the stack protection, an allocated slot should be placed in
+	     the safe region from the stack smaching attack.  */
+	  x = assign_stack_local_for_pseudo_reg (GET_MODE (regno_reg_rtx[i]), total_size,
 				  inherent_size == total_size ? 0 : -1);
 	  if (BYTES_BIG_ENDIAN)
 	    /* Cancel the  big-endian correction done in assign_stack_local.
diff -Naur gcc-3.4.4/gcc/rtl.h gcc-3.4.4-ssp/gcc/rtl.h
--- gcc-3.4.4/gcc/rtl.h	2004-12-05 07:21:01.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/rtl.h	2005-05-25 14:03:22.000000000 +0300
@@ -473,6 +473,18 @@
 			     __FUNCTION__);				\
    _rtx; })
 
+#define RTL_FLAG_CHECK9(NAME, RTX, C1, C2, C3, C4, C5, C6, C7, C8, C9)	\
+  __extension__								\
+({ rtx const _rtx = (RTX);						\
+   if (GET_CODE(_rtx) != C1 && GET_CODE(_rtx) != C2			\
+       && GET_CODE(_rtx) != C3 && GET_CODE(_rtx) != C4			\
+       && GET_CODE(_rtx) != C5 && GET_CODE(_rtx) != C6			\
+       && GET_CODE(_rtx) != C7 && GET_CODE(_rtx) != C8			\
+       && GET_CODE(_rtx) != C9)						\
+     rtl_check_failed_flag  (NAME, _rtx, __FILE__, __LINE__,		\
+			     __FUNCTION__);				\
+   _rtx; })
+
 extern void rtl_check_failed_flag (const char *, rtx, const char *,
 				   int, const char *)
     ATTRIBUTE_NORETURN
@@ -488,6 +500,7 @@
 #define RTL_FLAG_CHECK6(NAME, RTX, C1, C2, C3, C4, C5, C6)		(RTX)
 #define RTL_FLAG_CHECK7(NAME, RTX, C1, C2, C3, C4, C5, C6, C7)		(RTX)
 #define RTL_FLAG_CHECK8(NAME, RTX, C1, C2, C3, C4, C5, C6, C7, C8)	(RTX)
+#define RTL_FLAG_CHECK9(NAME, RTX, C1, C2, C3, C4, C5, C6, C7, C8, C9)	(RTX)
 #endif
 
 #define CLEAR_RTX_FLAGS(RTX)	\
@@ -583,9 +596,9 @@
 #define LOG_LINKS(INSN)	XEXP(INSN, 7)
 
 #define RTX_INTEGRATED_P(RTX)						\
-  (RTL_FLAG_CHECK8("RTX_INTEGRATED_P", (RTX), INSN, CALL_INSN,		\
+  (RTL_FLAG_CHECK9("RTX_INTEGRATED_P", (RTX), INSN, CALL_INSN,		\
 		   JUMP_INSN, INSN_LIST, BARRIER, CODE_LABEL, CONST,	\
-		   NOTE)->integrated)
+		   PLUS, NOTE)->integrated)
 #define RTX_UNCHANGING_P(RTX)						\
   (RTL_FLAG_CHECK3("RTX_UNCHANGING_P", (RTX), REG, MEM, CONCAT)->unchanging)
 #define RTX_FRAME_RELATED_P(RTX)					\
@@ -1125,6 +1138,10 @@
   (RTL_FLAG_CHECK3("MEM_VOLATILE_P", (RTX), MEM, ASM_OPERANDS,		\
 		   ASM_INPUT)->volatil)
 
+/* 1 if RTX is an SET rtx that is not eliminated for the stack protection.  */
+#define SET_VOLATILE_P(RTX)					\
+  (RTL_FLAG_CHECK1("SET_VOLATILE_P", (RTX), SET)->volatil)
+
 /* 1 if RTX is a mem that refers to an aggregate, either to the
    aggregate itself of to a field of the aggregate.  If zero, RTX may
    or may not be such a reference.  */
diff -Naur gcc-3.4.4/gcc/simplify-rtx.c gcc-3.4.4-ssp/gcc/simplify-rtx.c
--- gcc-3.4.4/gcc/simplify-rtx.c	2005-03-23 16:41:59.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/simplify-rtx.c	2005-05-25 14:03:22.000000000 +0300
@@ -2329,6 +2329,7 @@
   int n_ops = 2, input_ops = 2, input_consts = 0, n_consts;
   int first, changed;
   int i, j;
+  HOST_WIDE_INT fp_offset = 0;
 
   memset (ops, 0, sizeof ops);
 
@@ -2354,6 +2355,10 @@
 	  switch (this_code)
 	    {
 	    case PLUS:
+	    if (flag_propolice_protection
+		&& XEXP (this_op, 0) == virtual_stack_vars_rtx
+		&& GET_CODE (XEXP (this_op, 1)) == CONST_INT)
+	      fp_offset = INTVAL (XEXP (this_op, 1));
 	    case MINUS:
 	      if (n_ops == 7)
 		return NULL_RTX;
@@ -2515,11 +2520,24 @@
       && GET_CODE (ops[n_ops - 1].op) == CONST_INT
       && CONSTANT_P (ops[n_ops - 2].op))
     {
-      rtx value = ops[n_ops - 1].op;
-      if (ops[n_ops - 1].neg ^ ops[n_ops - 2].neg)
-	value = neg_const_int (mode, value);
-      ops[n_ops - 2].op = plus_constant (ops[n_ops - 2].op, INTVAL (value));
-      n_ops--;
+      if (!flag_propolice_protection)
+	{
+	  rtx value = ops[n_ops - 1].op;
+	  if (ops[n_ops - 1].neg ^ ops[n_ops - 2].neg)
+	    value = neg_const_int (mode, value);
+	  ops[n_ops - 2].op = plus_constant (ops[n_ops - 2].op, INTVAL (value));
+	  n_ops--;
+	}
+      /* The stack protector keeps the addressing style of a local variable,
+	 so it doesn't use neg_const_int function not to change
+	 the offset value.  */
+      else {
+	HOST_WIDE_INT value = INTVAL (ops[n_ops - 1].op);
+	if (ops[n_ops - 1].neg ^ ops[n_ops - 2].neg)
+	  value = -value;
+	ops[n_ops - 2].op = plus_constant (ops[n_ops - 2].op, value);
+	n_ops--;
+      }
     }
 
   /* Count the number of CONSTs that we generated.  */
@@ -2537,6 +2555,59 @@
 	  || (n_ops + n_consts == input_ops && n_consts <= input_consts)))
     return NULL_RTX;
 
+  if (flag_propolice_protection)
+    {
+      /* keep the addressing style of local variables
+	 as (plus (virtual_stack_vars_rtx) (CONST_int x)).
+	 For the case array[r-1],
+	 converts from (+ (+VFP c1) (+r -1)) to (SET R (+VFP c1)) (+ R (+r -1)).
+
+	 This loop finds ops[i] which is the register for the frame
+	 addressing, Then, makes the frame addressing using the register and
+	 the constant of ops[n_ops - 1].  */
+      for (i = 0; i < n_ops; i++)
+#ifdef FRAME_GROWS_DOWNWARD
+	if (ops[i].op == virtual_stack_vars_rtx)
+#else
+	if (ops[i].op == virtual_stack_vars_rtx
+	    || ops[i].op == frame_pointer_rtx)
+#endif
+	  {
+	    if (GET_CODE (ops[n_ops - 1].op) == CONST_INT)
+	      {
+		HOST_WIDE_INT value = INTVAL (ops[n_ops - 1].op);
+		if (value >= fp_offset)
+		  {
+		    ops[i].op = plus_constant (ops[i].op, value);
+		    n_ops--;
+		  }
+		else
+		  {
+		    if (!force
+			&& (n_ops + 1 + n_consts > input_ops
+			    || (n_ops + 1 + n_consts == input_ops
+				&& n_consts <= input_consts)))
+		      return NULL_RTX;
+		    ops[n_ops - 1].op = GEN_INT (value-fp_offset);
+		    ops[i].op = plus_constant (ops[i].op, fp_offset);
+		  }
+	      }
+	    /* keep the following address pattern;
+	       (1) buf[BUFSIZE] is the first assigned variable.
+	       (+ (+ fp -BUFSIZE) BUFSIZE)
+	       (2) ((+ (+ fp 1) r) -1).  */
+	    else if (fp_offset != 0)
+	      return NULL_RTX;
+	    /* keep the (+ fp 0) pattern for the following case;
+	       (1) buf[i]: i: REG, buf: (+ fp 0) in !FRAME_GROWS_DOWNWARD
+	       (2) argument: the address is (+ fp 0).  */
+	    else if (fp_offset == 0)
+	      return NULL_RTX;
+
+	    break;
+	  }
+    }
+
   /* Put a non-negated operand first, if possible.  */
 
   for (i = 0; i < n_ops && ops[i].neg; i++)
diff -Naur gcc-3.4.4/gcc/testsuite/gcc.dg/ssp-warn.c gcc-3.4.4-ssp/gcc/testsuite/gcc.dg/ssp-warn.c
--- gcc-3.4.4/gcc/testsuite/gcc.dg/ssp-warn.c	1970-01-01 02:00:00.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/testsuite/gcc.dg/ssp-warn.c	2003-11-21 10:41:19.000000000 +0200
@@ -0,0 +1,32 @@
+/* { dg-do compile } */
+/* { dg-options "-fstack-protector" } */
+void
+test1()
+{
+  void intest1(int *a)
+    {
+      *a ++;
+    }
+  
+  char buf[80];
+
+  buf[0] = 0;
+} /* { dg-bogus "not protecting function: it contains functions" } */
+
+void
+test2(int n)
+{
+  char buf[80];
+  char vbuf[n];
+
+  buf[0] = 0;
+  vbuf[0] = 0;
+} /* { dg-bogus "not protecting variables: it has a variable length buffer" } */
+
+void
+test3()
+{
+  char buf[5];
+
+  buf[0] = 0;
+} /* { dg-bogus "not protecting function: buffer is less than 8 bytes long" } */
diff -Naur gcc-3.4.4/gcc/testsuite/gcc.misc-tests/ssp-execute1.c gcc-3.4.4-ssp/gcc/testsuite/gcc.misc-tests/ssp-execute1.c
--- gcc-3.4.4/gcc/testsuite/gcc.misc-tests/ssp-execute1.c	1970-01-01 02:00:00.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/testsuite/gcc.misc-tests/ssp-execute1.c	2004-02-16 07:15:39.000000000 +0200
@@ -0,0 +1,54 @@
+/* Test location changes of character array.  */
+
+void
+test(int i)
+{
+  int  ibuf1[10];
+  char buf[50];
+  int  ibuf2[10];
+  char buf2[50000];
+  int  ibuf3[10];
+  char *p;
+
+  /* c1: the frame offset of buf[0]
+     c2: the frame offset of buf2[0]
+  */
+  p= &buf[0]; *p=1;		/* expected rtl: (+ fp -c1) */
+  if (*p != buf[0])
+    abort();
+  p= &buf[5]; *p=2;		/* expected rtl: (+ fp -c1+5) */
+  if (*p != buf[5])
+    abort();
+  p= &buf[-1]; *p=3;		/* expected rtl: (+ (+ fp -c1) -1) */
+  if (*p != buf[-1])
+    abort();
+  p= &buf[49]; *p=4;		/* expected rtl: (+ fp -c1+49) */
+  if (*p != buf[49])
+    abort();
+  p = &buf[i+5]; *p=5;		/* expected rtl: (+ (+ fp -c1) (+ i 5)) */
+  if (*p != buf[i+5])
+    abort ();
+  p = buf - 1; *p=6;		/* expected rtl: (+ (+ fp -c1) -1) */
+  if (*p != buf[-1])
+    abort ();
+  p = 1 + buf; *p=7;		/* expected rtl: (+ (+ fp -c1) 1) */
+  if (*p != buf[1])
+    abort ();
+  p = &buf[1] - 1; *p=8;	/* expected rtl: (+ (+ fp -c1+1) -1) */
+  if (*p != buf[0])
+    abort ();
+
+  /* test big offset which is greater than the max value of signed 16 bit integer.  */
+  p = &buf2[45555]; *p=9;	/* expected rtl: (+ fp -c2+45555) */
+  if (*p != buf2[45555])
+    abort ();
+}
+
+int main()
+{
+  test(10);
+  exit(0);
+}
+
+
+  
diff -Naur gcc-3.4.4/gcc/testsuite/gcc.misc-tests/ssp-execute2.c gcc-3.4.4-ssp/gcc/testsuite/gcc.misc-tests/ssp-execute2.c
--- gcc-3.4.4/gcc/testsuite/gcc.misc-tests/ssp-execute2.c	1970-01-01 02:00:00.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/testsuite/gcc.misc-tests/ssp-execute2.c	2003-11-22 10:44:33.000000000 +0200
@@ -0,0 +1,49 @@
+void
+test(int i, char *j, int k)
+{
+  int  a[10];
+  char b;
+  int  c;
+  long *d;
+  char buf[50];
+  long e[10];
+  int  n;
+
+  a[0] = 4;
+  b = 5;
+  c = 6;
+  d = (long*)7;
+  e[0] = 8;
+
+  /* overflow buffer */
+  for (n = 0; n < 120; n++)
+    buf[n] = 0;
+  
+  if (j == 0 || *j != 2)
+    abort ();
+  if (a[0] == 0)
+    abort ();
+  if (b == 0)
+    abort ();
+  if (c == 0)
+    abort ();
+  if (d == 0)
+    abort ();
+  if (e[0] == 0)
+    abort ();
+
+  exit (0);
+}
+
+int main()
+{
+  int i, k;
+  int j[40];
+  i = 1;
+  j[39] = 2;
+  k = 3;
+  test(i, &j[39], k);
+}
+
+
+  
diff -Naur gcc-3.4.4/gcc/testsuite/gcc.misc-tests/ssp-execute.exp gcc-3.4.4-ssp/gcc/testsuite/gcc.misc-tests/ssp-execute.exp
--- gcc-3.4.4/gcc/testsuite/gcc.misc-tests/ssp-execute.exp	1970-01-01 02:00:00.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/testsuite/gcc.misc-tests/ssp-execute.exp	2004-06-02 14:23:36.000000000 +0300
@@ -0,0 +1,35 @@
+#   Copyright (C) 2003, 2004 Free Software Foundation, Inc.
+
+# This program is free software; you can redistribute it and/or modify
+# it under the terms of the GNU General Public License as published by
+# the Free Software Foundation; either version 2 of the License, or
+# (at your option) any later version.
+# 
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+# GNU General Public License for more details.
+# 
+# You should have received a copy of the GNU General Public License
+# along with this program; if not, write to the Free Software
+# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  
+
+if $tracelevel then {
+    strace $tracelevel
+}
+
+# Load support procs.
+load_lib c-torture.exp
+
+#
+# main test loop
+#
+
+foreach src [lsort [glob -nocomplain $srcdir/$subdir/ssp-execute*.c]] {
+    # If we're only testing specific files and this isn't one of them, skip it.
+    if ![runtest_file_p $runtests $src] then {
+	continue
+    }
+
+    c-torture-execute $src -fstack-protector
+}
diff -Naur gcc-3.4.4/gcc/toplev.c gcc-3.4.4-ssp/gcc/toplev.c
--- gcc-3.4.4/gcc/toplev.c	2005-03-09 02:50:25.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/toplev.c	2005-05-25 14:03:22.000000000 +0300
@@ -79,6 +79,7 @@
 #include "coverage.h"
 #include "value-prof.h"
 #include "alloc-pool.h"
+#include "protector.h"
 
 #if defined (DWARF2_UNWIND_INFO) || defined (DWARF2_DEBUGGING_INFO)
 #include "dwarf2out.h"
@@ -97,6 +98,10 @@
 				   declarations for e.g. AIX 4.x.  */
 #endif
 
+#ifdef STACK_PROTECTOR
+#include "protector.h"
+#endif
+
 #ifndef HAVE_conditional_execution
 #define HAVE_conditional_execution 0
 #endif
@@ -979,6 +984,15 @@
    minimum function alignment.  Zero means no alignment is forced.  */
 int force_align_functions_log;
 
+#if defined(STACK_PROTECTOR) && defined(STACK_GROWS_DOWNWARD)
+/* Nonzero means use propolice as a stack protection method */
+int flag_propolice_protection = 1;
+int flag_stack_protection = 0;
+#else
+int flag_propolice_protection = 0;
+int flag_stack_protection = 0;
+#endif
+
 typedef struct
 {
   const char *const string;
@@ -1154,7 +1168,9 @@
   {"mem-report", &mem_report, 1 },
   { "trapv", &flag_trapv, 1 },
   { "wrapv", &flag_wrapv, 1 },
-  { "new-ra", &flag_new_regalloc, 1 }
+  { "new-ra", &flag_new_regalloc, 1 },
+  {"stack-protector", &flag_propolice_protection, 1 },
+  {"stack-protector-all", &flag_stack_protection, 1 }
 };
 
 /* Here is a table, controlled by the tm.h file, listing each -m switch
@@ -2689,6 +2705,9 @@
 
   insns = get_insns ();
 
+  if (flag_propolice_protection)
+    prepare_stack_protection (inlinable);
+
   /* Dump the rtl code if we are dumping rtl.  */
 
   if (open_dump_file (DFI_rtl, decl))
@@ -4485,6 +4504,12 @@
     /* The presence of IEEE signaling NaNs, implies all math can trap.  */
     if (flag_signaling_nans)
       flag_trapping_math = 1;
+
+  /* This combination makes optimized frame addressings and causes
+    a internal compilation error at prepare_stack_protection.
+    so don't allow it.  */
+  if (flag_stack_protection && !flag_propolice_protection)
+    flag_propolice_protection = TRUE;
 }
 
 /* Initialize the compiler back end.  */
diff -Naur gcc-3.4.4/gcc/tree.h gcc-3.4.4-ssp/gcc/tree.h
--- gcc-3.4.4/gcc/tree.h	2005-01-16 18:01:19.000000000 +0200
+++ gcc-3.4.4-ssp/gcc/tree.h	2005-05-25 14:03:22.000000000 +0300
@@ -1489,6 +1489,10 @@
    where it is called.  */
 #define DECL_INLINE(NODE) (FUNCTION_DECL_CHECK (NODE)->decl.inline_flag)
 
+/* In a VAR_DECL, nonzero if the declaration is copied for inlining.
+   The stack protector should keep its location in the stack.  */
+#define DECL_COPIED(NODE) (VAR_DECL_CHECK (NODE)->decl.inline_flag)
+
 /* Nonzero in a FUNCTION_DECL means that this function was declared inline,
    such as via the `inline' keyword in C/C++.  This flag controls the linkage
    semantics of 'inline'; whether or not the function is inlined is
